<?xml version="1.0" ?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/" version="2.0">
  <channel>
    <title>dpo reference Papers</title>
    <link>http://zct_Paper_with_abstract.com/rss</link>
    <description>Latest dpo reference papers.</description>
    <atom:link href="http://zct_Paper_with_abstract.com/rss" rel="self" type="application/rss+xml"/>
    <language>en-us</language>
    <lastBuildDate>Thu, 18 Jul 2024 04:51:52 GMT</lastBuildDate>
    <pubDate>Thu, 18 Jul 2024 04:51:52 GMT</pubDate>
    <item>
      <title>Challenges and Applications of Large Language Models</title>
      <link>http://arxiv.org/abs/2307.10169v1</link>
      <description>Large Language Models (LLMs) went from non-existent to ubiquitous in the machine learning discourse within a few years. Due to the fast pace of the field, it is difficult to identify the remaining challenges and already fruitful application areas. In this paper, we aim to establish a systematic set of open problems and application successes so that ML researchers can comprehend the field's current state more quickly and become productive.\n\n\nChallenges and applications of large language models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2307.10169v1</guid>
      <dc:creator>Jean Kaddour, Joshua Harris, Maximilian Mozes, Herbie Bradley, Roberta Raileanu, Robert McHardy</dc:creator>
      <pubDate>Wed, 19 Jul 2023 17:55:13 GMT</pubDate>
    </item>
    <item>
      <title>A Comprehensive Overview of Large Language Models</title>
      <link>http://arxiv.org/abs/2307.06435v9</link>
      <description>Large Language Models (LLMs) have recently demonstrated remarkable capabilities in natural language processing tasks and beyond. This success of LLMs has led to a large influx of research contributions in this direction. These works encompass diverse topics such as architectural innovations, better training strategies, context length improvements, fine-tuning, multi-modal LLMs, robotics, datasets, benchmarking, efficiency, and more. With the rapid development of techniques and regular breakthroughs in LLM research, it has become considerably challenging to perceive the bigger picture of the advances in this direction. Considering the rapidly emerging plethora of literature on LLMs, it is imperative that the research community is able to benefit from a concise yet comprehensive overview of the recent developments in this field. This article provides an overview of the existing literature on a broad range of LLM-related concepts. Our self-contained comprehensive overview of LLMs discusses relevant background concepts along with covering the advanced topics at the frontier of research in LLMs. This review article is intended to not only provide a systematic survey but also a quick comprehensive reference for the researchers and practitioners to draw insights from extensive informative summaries of the existing works to advance the LLM research.\n\n\nA comprehensive overview of large language models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2307.06435v9</guid>
      <dc:creator>Humza Naveed, Asad Ullah Khan, Shi Qiu, Muhammad Saqib, Saeed Anwar, Muhammad Usman, Naveed Akhtar, Nick Barnes, Ajmal Mian</dc:creator>
      <pubDate>Tue, 09 Apr 2024 21:38:33 GMT</pubDate>
    </item>
    <item>
      <title>A Survey on Transformer Compression</title>
      <link>http://arxiv.org/abs/2402.05964v2</link>
      <description>Transformer plays a vital role in the realms of natural language processing (NLP) and computer vision (CV), specially for constructing large language models (LLM) and large vision models (LVM). Model compression methods reduce the memory and computational cost of Transformer, which is a necessary step to implement large language/vision models on practical devices. Given the unique architecture of Transformer, featuring alternative attention and feedforward neural network (FFN) modules, specific compression techniques are usually required. The efficiency of these compression methods is also paramount, as retraining large models on the entire training dataset is usually impractical. This survey provides a comprehensive review of recent compression methods, with a specific focus on their application to Transformer-based models. The compression methods are primarily categorized into pruning, quantization, knowledge distillation, and efficient architecture design (Mamba, RetNet, RWKV, etc.). In each category, we discuss compression methods for both language and vision tasks, highlighting common underlying principles. Finally, we delve into the relation between various compression methods, and discuss further directions in this domain.\n\n\nA survey of large language models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.05964v2</guid>
      <dc:creator>Yehui Tang, Yunhe Wang, Jianyuan Guo, Zhijun Tu, Kai Han, Hailin Hu, Dacheng Tao</dc:creator>
      <pubDate>Sun, 07 Apr 2024 13:03:58 GMT</pubDate>
    </item>
    <item>
      <title>Qwen Technical Report</title>
      <link>http://arxiv.org/abs/2309.16609v1</link>
      <description>Large language models (LLMs) have revolutionized the field of artificial intelligence, enabling natural language processing tasks that were previously thought to be exclusive to humans. In this work, we introduce Qwen, the first installment of our large language model series. Qwen is a comprehensive language model series that encompasses distinct models with varying parameter counts. It includes Qwen, the base pretrained language models, and Qwen-Chat, the chat models finetuned with human alignment techniques. The base language models consistently demonstrate superior performance across a multitude of downstream tasks, and the chat models, particularly those trained using Reinforcement Learning from Human Feedback (RLHF), are highly competitive. The chat models possess advanced tool-use and planning capabilities for creating agent applications, showcasing impressive performance even when compared to bigger models on complex tasks like utilizing a code interpreter. Furthermore, we have developed coding-specialized models, Code-Qwen and Code-Qwen-Chat, as well as mathematics-focused models, Math-Qwen-Chat, which are built upon base language models. These models demonstrate significantly improved performance in comparison with open-source models, and slightly fall behind the proprietary models.\n\n\nQwen technical report</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2309.16609v1</guid>
      <dc:creator>Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, Tianhang Zhu</dc:creator>
      <pubDate>Thu, 28 Sep 2023 17:07:49 GMT</pubDate>
    </item>
    <item>
      <title>Mixtral of Experts</title>
      <link>http://arxiv.org/abs/2401.04088v1</link>
      <description>We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine-tuned to follow instructions, Mixtral 8x7B - Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B - chat model on human benchmarks. Both the base and instruct models are released under the Apache 2.0 license.\n\n\nMixtral of experts</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2401.04088v1</guid>
      <dc:creator>Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Lélio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, William El Sayed</dc:creator>
      <pubDate>Mon, 08 Jan 2024 18:47:34 GMT</pubDate>
    </item>
    <item>
      <title>Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback</title>
      <link>http://arxiv.org/abs/2307.15217v2</link>
      <description>Reinforcement learning from human feedback (RLHF) is a technique for training AI systems to align with human goals. RLHF has emerged as the central method used to finetune state-of-the-art large language models (LLMs). Despite this popularity, there has been relatively little public work systematizing its flaws. In this paper, we (1) survey open problems and fundamental limitations of RLHF and related methods; (2) overview techniques to understand, improve, and complement RLHF in practice; and (3) propose auditing and disclosure standards to improve societal oversight of RLHF systems. Our work emphasizes the limitations of RLHF and highlights the importance of a multi-faceted approach to the development of safer AI systems.\n\n\nOpen problems and fundamental limitations of reinforcement learning from human feedback</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2307.15217v2</guid>
      <dc:creator>Stephen Casper, Xander Davies, Claudia Shi, Thomas Krendl Gilbert, Jérémy Scheurer, Javier Rando, Rachel Freedman, Tomasz Korbak, David Lindner, Pedro Freire, Tony Wang, Samuel Marks, Charbel-Raphaël Segerie, Micah Carroll, Andi Peng, Phillip Christoffersen, Mehul Damani, Stewart Slocum, Usman Anwar, Anand Siththaranjan, Max Nadeau, Eric J. Michaud, Jacob Pfau, Dmitrii Krasheninnikov, Xin Chen, Lauro Langosco, Peter Hase, Erdem Bıyık, Anca Dragan, David Krueger, Dorsa Sadigh, Dylan Hadfield-Menell</dc:creator>
      <pubDate>Mon, 11 Sep 2023 17:25:24 GMT</pubDate>
    </item>
    <item>
      <title>Zephyr: Direct Distillation of LM Alignment</title>
      <link>http://arxiv.org/abs/2310.16944v1</link>
      <description>We aim to produce a smaller language model that is aligned to user intent. Previous research has shown that applying distilled supervised fine-tuning (dSFT) on larger models significantly improves task accuracy; however, these models are unaligned, i.e. they do not respond well to natural prompts. To distill this property, we experiment with the use of preference data from AI Feedback (AIF). Starting from a dataset of outputs ranked by a teacher model, we apply distilled direct preference optimization (dDPO) to learn a chat model with significantly improved intent alignment. The approach requires only a few hours of training without any additional sampling during fine-tuning. The final result, Zephyr-7B, sets the state-of-the-art on chat benchmarks for 7B parameter models, and requires no human annotation. In particular, results on MT-Bench show that Zephyr-7B surpasses Llama2-Chat-70B, the best open-access RLHF-based model. Code, models, data, and tutorials for the system are available at https://github.com/huggingface/alignment-handbook.\n\n\nZephyr: Direct distillation of lm alignment</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2310.16944v1</guid>
      <dc:creator>Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Clémentine Fourrier, Nathan Habib, Nathan Sarrazin, Omar Sanseviero, Alexander M. Rush, Thomas Wolf</dc:creator>
      <pubDate>Wed, 25 Oct 2023 19:25:16 GMT</pubDate>
    </item>
    <item>
      <title>Counterfactually Probing Language Identity in Multilingual Models</title>
      <link>http://arxiv.org/abs/2310.18862v1</link>
      <description>Techniques in causal analysis of language models illuminate how linguistic information is organized in LLMs. We use one such technique, AlterRep, a method of counterfactual probing, to explore the internal structure of multilingual models (mBERT and XLM-R). We train a linear classifier on a binary language identity task, to classify tokens between Language X and Language Y. Applying a counterfactual probing procedure, we use the classifier weights to project the embeddings into the null space and push the resulting embeddings either in the direction of Language X or Language Y. Then we evaluate on a masked language modeling task. We find that, given a template in Language X, pushing towards Language Y systematically increases the probability of Language Y words, above and beyond a third-party control language. But it does not specifically push the model towards translation-equivalent words in Language Y. Pushing towards Language X (the same direction as the template) has a minimal effect, but somewhat degrades these models. Overall, we take these results as further evidence of the rich structure of massive multilingual language models, which include both a language-specific and language-general component. And we show that counterfactual probing can be fruitfully applied to multilingual models.\n\n\nSelf-rewarding language models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2310.18862v1</guid>
      <dc:creator>Anirudh Srinivasan, Venkata S Govindarajan, Kyle Mahowald</dc:creator>
      <pubDate>Sun, 29 Oct 2023 01:21:36 GMT</pubDate>
    </item>
    <item>
      <title>Reformatted Alignment</title>
      <link>http://arxiv.org/abs/2402.12219v2</link>
      <description>The quality of finetuning data is crucial for aligning large language models (LLMs) with human values. Current methods to improve data quality are either labor-intensive or prone to factual errors caused by LLM hallucinations. This paper explores elevating the quality of existing instruction data to better align with human values, introducing a simple and effective approach named ReAlign, which reformats the responses of instruction data into a format that better aligns with pre-established criteria and the collated evidence. This approach minimizes human annotation, hallucination, and the difficulty in scaling, remaining orthogonal to existing alignment techniques. Experimentally, ReAlign significantly boosts the general alignment ability, math reasoning, factuality, and readability of the LLMs.   Encouragingly, without introducing any additional data or advanced training techniques, and merely by reformatting the response, LLaMA-2-13B's mathematical reasoning ability on GSM8K can be improved from 46.77% to 56.63% in accuracy. Additionally, a mere 5% of ReAlign data yields a 67% boost in general alignment ability measured by the Alpaca dataset. This work highlights the need for further research into the science and mechanistic interpretability of LLMs. We have made the associated code and data publicly accessible to support future studies at https://github.com/GAIR-NLP/ReAlign.\n\n\nAligning large language models with human: A survey</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.12219v2</guid>
      <dc:creator>Run-Ze Fan, Xuefeng Li, Haoyang Zou, Junlong Li, Shwai He, Ethan Chern, Jiewen Hu, Pengfei Liu</dc:creator>
      <pubDate>Wed, 17 Apr 2024 15:03:19 GMT</pubDate>
    </item>
    <item>
      <title>Preference Ranking Optimization for Human Alignment</title>
      <link>http://arxiv.org/abs/2306.17492v2</link>
      <description>Large language models (LLMs) often contain misleading content, emphasizing the need to align them with human values to ensure secure AI systems. Reinforcement learning from human feedback (RLHF) has been employed to achieve this alignment. However, it encompasses two main drawbacks: (1) RLHF exhibits complexity, instability, and sensitivity to hyperparameters in contrast to SFT. (2) Despite massive trial-and-error, multiple sampling is reduced to pair-wise contrast, thus lacking contrasts from a macro perspective. In this paper, we propose Preference Ranking Optimization (PRO) as an efficient SFT algorithm to directly fine-tune LLMs for human alignment. PRO extends the pair-wise contrast to accommodate preference rankings of any length. By iteratively contrasting candidates, PRO instructs the LLM to prioritize the best response while progressively ranking the rest responses. In this manner, PRO effectively transforms human alignment into aligning the probability ranking of n responses generated by LLM with the preference ranking of humans towards these responses. Experiments have shown that PRO outperforms baseline algorithms, achieving comparable results to ChatGPT and human responses through automatic-based, reward-based, GPT-4, and human evaluations.\n\n\nPreference ranking optimization for human alignment</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2306.17492v2</guid>
      <dc:creator>Feifan Song, Bowen Yu, Minghao Li, Haiyang Yu, Fei Huang, Yongbin Li, Houfeng Wang</dc:creator>
      <pubDate>Tue, 27 Feb 2024 18:42:42 GMT</pubDate>
    </item>
    <item>
      <title>Reinforced Self-Training (ReST) for Language Modeling</title>
      <link>http://arxiv.org/abs/2308.08998v2</link>
      <description>Reinforcement learning from human feedback (RLHF) can improve the quality of large language model's (LLM) outputs by aligning them with human preferences. We propose a simple algorithm for aligning LLMs with human preferences inspired by growing batch reinforcement learning (RL), which we call Reinforced Self-Training (ReST). Given an initial LLM policy, ReST produces a dataset by generating samples from the policy, which are then used to improve the LLM policy using offline RL algorithms. ReST is more efficient than typical online RLHF methods because the training dataset is produced offline, which allows data reuse. While ReST is a general approach applicable to all generative learning settings, we focus on its application to machine translation. Our results show that ReST can substantially improve translation quality, as measured by automated metrics and human evaluation on machine translation benchmarks in a compute and sample-efficient manner.\n\n\nReinforced self-training (rest) for language modeling</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2308.08998v2</guid>
      <dc:creator>Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, Wolfgang Macherey, Arnaud Doucet, Orhan Firat, Nando de Freitas</dc:creator>
      <pubDate>Mon, 21 Aug 2023 10:23:42 GMT</pubDate>
    </item>
    <item>
      <title>RLHF-V: Towards Trustworthy MLLMs via Behavior Alignment from Fine-grained Correctional Human Feedback</title>
      <link>http://arxiv.org/abs/2312.00849v2</link>
      <description>Multimodal Large Language Models (MLLMs) have recently demonstrated impressive capabilities in multimodal understanding, reasoning, and interaction. However, existing MLLMs prevalently suffer from serious hallucination problems, generating text that is not factually grounded in associated images. The problem makes existing MLLMs untrustworthy and thus impractical in real-world (especially high-stakes) applications. To address the challenge, we present RLHF-V, which enhances MLLM trustworthiness via behavior alignment from fine-grained correctional human feedback. Specifically, RLHF-V collects human preference in the form of segment-level corrections on hallucinations, and performs dense direct preference optimization over the human feedback. Comprehensive experiments on five benchmarks in both automatic and human evaluation show that, RLHF-V can enable substantially more trustworthy MLLM behaviors with promising data and computation efficiency. Remarkably, using 1.4k annotated data samples, RLHF-V significantly reduces the hallucination rate of the base MLLM by 34.8%, outperforming the concurrent LLaVA-RLHF trained on 10k annotated data. The final model achieves state-of-the-art performance in trustworthiness among open-source MLLMs, and shows better robustness than GPT-4V in preventing hallucinations aroused from over-generalization. We open-source our code, model, and data at https://github.com/RLHF-V/RLHF-V.\n\n\nRlhf-v: Towards trustworthy mllms via behavior alignment from fine-grained correctional human feedback</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2312.00849v2</guid>
      <dc:creator>Tianyu Yu, Yuan Yao, Haoye Zhang, Taiwen He, Yifeng Han, Ganqu Cui, Jinyi Hu, Zhiyuan Liu, Hai-Tao Zheng, Maosong Sun, Tat-Seng Chua</dc:creator>
      <pubDate>Fri, 08 Mar 2024 06:42:37 GMT</pubDate>
    </item>
    <item>
      <title>Safe RLHF: Safe Reinforcement Learning from Human Feedback</title>
      <link>http://arxiv.org/abs/2310.12773v1</link>
      <description>With the development of large language models (LLMs), striking a balance between the performance and safety of AI systems has never been more critical. However, the inherent tension between the objectives of helpfulness and harmlessness presents a significant challenge during LLM training. To address this issue, we propose Safe Reinforcement Learning from Human Feedback (Safe RLHF), a novel algorithm for human value alignment. Safe RLHF explicitly decouples human preferences regarding helpfulness and harmlessness, effectively avoiding the crowdworkers' confusion about the tension and allowing us to train separate reward and cost models. We formalize the safety concern of LLMs as an optimization task of maximizing the reward function while satisfying specified cost constraints. Leveraging the Lagrangian method to solve this constrained problem, Safe RLHF dynamically adjusts the balance between the two objectives during fine-tuning. Through a three-round fine-tuning using Safe RLHF, we demonstrate a superior ability to mitigate harmful responses while enhancing model performance compared to existing value-aligned algorithms. Experimentally, we fine-tuned the Alpaca-7B using Safe RLHF and aligned it with collected human preferences, significantly improving its helpfulness and harmlessness according to human evaluations.\n\n\nSafe rlhf: Safe reinforcement learning from human feedback</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2310.12773v1</guid>
      <dc:creator>Josef Dai, Xuehai Pan, Ruiyang Sun, Jiaming Ji, Xinbo Xu, Mickel Liu, Yizhou Wang, Yaodong Yang</dc:creator>
      <pubDate>Thu, 19 Oct 2023 14:22:03 GMT</pubDate>
    </item>
    <item>
      <title>OctoPack: Instruction Tuning Code Large Language Models</title>
      <link>http://arxiv.org/abs/2308.07124v2</link>
      <description>Finetuning large language models (LLMs) on instructions leads to vast performance improvements on natural language tasks. We apply instruction tuning using code, leveraging the natural structure of Git commits, which pair code changes with human instructions. We compile CommitPack: 4 terabytes of Git commits across 350 programming languages. We benchmark CommitPack against other natural and synthetic code instructions (xP3x, Self-Instruct, OASST) on the 16B parameter StarCoder model, and achieve state-of-the-art performance among models not trained on OpenAI outputs, on the HumanEval Python benchmark (46.2% pass@1). We further introduce HumanEvalPack, expanding the HumanEval benchmark to a total of 3 coding tasks (Code Repair, Code Explanation, Code Synthesis) across 6 languages (Python, JavaScript, Java, Go, C++, Rust). Our models, OctoCoder and OctoGeeX, achieve the best performance across HumanEvalPack among all permissive models, demonstrating CommitPack's benefits in generalizing to a wider set of languages and natural coding tasks. Code, models and data are freely available at https://github.com/bigcode-project/octopack.\n\n\nOctopack: Instruction tuning code large language models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2308.07124v2</guid>
      <dc:creator>Niklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai Zheng, Binyuan Hui, Terry Yue Zhuo, Swayam Singh, Xiangru Tang, Leandro von Werra, Shayne Longpre</dc:creator>
      <pubDate>Sun, 18 Feb 2024 09:46:06 GMT</pubDate>
    </item>
    <item>
      <title>Instruction Distillation Makes Large Language Models Efficient Zero-shot Rankers</title>
      <link>http://arxiv.org/abs/2311.01555v1</link>
      <description>Recent studies have demonstrated the great potential of Large Language Models (LLMs) serving as zero-shot relevance rankers. The typical approach involves making comparisons between pairs or lists of documents. Although effective, these listwise and pairwise methods are not efficient and also heavily rely on intricate prompt engineering. To tackle this problem, we introduce a novel instruction distillation method. The key idea is to distill the pairwise ranking ability of open-sourced LLMs to a simpler but more efficient pointwise ranking. Specifically, given the same LLM, we first rank documents using the effective pairwise approach with complex instructions, and then distill the teacher predictions to the pointwise approach with simpler instructions. Evaluation results on the BEIR, TREC, and ReDial datasets demonstrate that instruction distillation can improve efficiency by 10 to 100x and also enhance the ranking performance of LLMs. Furthermore, our approach surpasses the performance of existing supervised methods like monoT5 and is on par with the state-of-the-art zero-shot methods. The code to reproduce our results is available at www.github.com/sunnweiwei/RankGPT.\n\n\nLarge language models are effective text rankers with pairwise ranking prompting</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2311.01555v1</guid>
      <dc:creator>Weiwei Sun, Zheng Chen, Xinyu Ma, Lingyong Yan, Shuaiqiang Wang, Pengjie Ren, Zhumin Chen, Dawei Yin, Zhaochun Ren</dc:creator>
      <pubDate>Thu, 02 Nov 2023 19:16:21 GMT</pubDate>
    </item>
    <item>
      <title>Diffusion Language Models Can Perform Many Tasks with Scaling and Instruction-Finetuning</title>
      <link>http://arxiv.org/abs/2308.12219v2</link>
      <description>The recent surge of generative AI has been fueled by the generative power of diffusion probabilistic models and the scalable capabilities of large language models. Despite their potential, it remains elusive whether diffusion language models can solve general language tasks comparable to their autoregressive counterparts. This paper demonstrates that scaling diffusion models w.r.t. data, sizes, and tasks can effectively make them strong language learners. We build competent diffusion language models at scale by first acquiring knowledge from massive data via masked language modeling pretraining thanks to their intrinsic connections. We then reprogram pretrained masked language models into diffusion language models via diffusive adaptation, wherein task-specific finetuning and instruction finetuning are explored to unlock their versatility in solving general language tasks. Experiments show that scaling diffusion language models consistently improves performance across downstream language tasks. We further discover that instruction finetuning can elicit zero-shot and few-shot in-context learning abilities that help tackle many unseen tasks by following natural language instructions, and show promise in advanced and challenging abilities such as reasoning.\n\n\nSelf-play fine-tuning converts weak language models to strong language models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2308.12219v2</guid>
      <dc:creator>Jiasheng Ye, Zaixiang Zheng, Yu Bao, Lihua Qian, Quanquan Gu</dc:creator>
      <pubDate>Fri, 25 Aug 2023 16:32:31 GMT</pubDate>
    </item>
    <item>
      <title>Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language Models' Alignment</title>
      <link>http://arxiv.org/abs/2308.05374v2</link>
      <description>Ensuring alignment, which refers to making models behave in accordance with human intentions [1,2], has become a critical task before deploying large language models (LLMs) in real-world applications. For instance, OpenAI devoted six months to iteratively aligning GPT-4 before its release [3]. However, a major challenge faced by practitioners is the lack of clear guidance on evaluating whether LLM outputs align with social norms, values, and regulations. This obstacle hinders systematic iteration and deployment of LLMs. To address this issue, this paper presents a comprehensive survey of key dimensions that are crucial to consider when assessing LLM trustworthiness. The survey covers seven major categories of LLM trustworthiness: reliability, safety, fairness, resistance to misuse, explainability and reasoning, adherence to social norms, and robustness. Each major category is further divided into several sub-categories, resulting in a total of 29 sub-categories. Additionally, a subset of 8 sub-categories is selected for further investigation, where corresponding measurement studies are designed and conducted on several widely-used LLMs. The measurement results indicate that, in general, more aligned models tend to perform better in terms of overall trustworthiness. However, the effectiveness of alignment varies across the different trustworthiness categories considered. This highlights the importance of conducting more fine-grained analyses, testing, and making continuous improvements on LLM alignment. By shedding light on these key dimensions of LLM trustworthiness, this paper aims to provide valuable insights and guidance to practitioners in the field. Understanding and addressing these concerns will be crucial in achieving reliable and ethically sound deployment of LLMs in various applications.\n\n\nTrustworthy LLMs: A survey and guideline for evaluating large language models' alignment</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2308.05374v2</guid>
      <dc:creator>Yang Liu, Yuanshun Yao, Jean-Francois Ton, Xiaoying Zhang, Ruocheng Guo, Hao Cheng, Yegor Klochkov, Muhammad Faaiz Taufiq, Hang Li</dc:creator>
      <pubDate>Thu, 21 Mar 2024 00:21:14 GMT</pubDate>
    </item>
    <item>
      <title>Yi: Open Foundation Models by 01.AI</title>
      <link>http://arxiv.org/abs/2403.04652v1</link>
      <description>We introduce the Yi model family, a series of language and multimodal models that demonstrate strong multi-dimensional capabilities. The Yi model family is based on 6B and 34B pretrained language models, then we extend them to chat models, 200K long context models, depth-upscaled models, and vision-language models. Our base models achieve strong performance on a wide range of benchmarks like MMLU, and our finetuned chat models deliver strong human preference rate on major evaluation platforms like AlpacaEval and Chatbot Arena. Building upon our scalable super-computing infrastructure and the classical transformer architecture, we attribute the performance of Yi models primarily to its data quality resulting from our data-engineering efforts. For pretraining, we construct 3.1 trillion tokens of English and Chinese corpora using a cascaded data deduplication and quality filtering pipeline. For finetuning, we polish a small scale (less than 10K) instruction dataset over multiple iterations such that every single instance has been verified directly by our machine learning engineers. For vision-language, we combine the chat language model with a vision transformer encoder and train the model to align visual representations to the semantic space of the language model. We further extend the context length to 200K through lightweight continual pretraining and demonstrate strong needle-in-a-haystack retrieval performance. We show that extending the depth of the pretrained checkpoint through continual pretraining further improves performance. We believe that given our current results, continuing to scale up model parameters using thoroughly optimized data will lead to even stronger frontier models.\n\n\nYi: Open foundation models by 01. ai</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2403.04652v1</guid>
      <dc:creator>01. AI, :, Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang, Shiming Yang, Tao Yu, Wen Xie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie, Yuchi Xu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, Zonghong Dai</dc:creator>
      <pubDate>Thu, 07 Mar 2024 16:52:49 GMT</pubDate>
    </item>
    <item>
      <title>Detecting and Preventing Hallucinations in Large Vision Language Models</title>
      <link>http://arxiv.org/abs/2308.06394v3</link>
      <description>Instruction tuned Large Vision Language Models (LVLMs) have significantly advanced in generalizing across a diverse set of multi-modal tasks, especially for Visual Question Answering (VQA). However, generating detailed responses that are visually grounded is still a challenging task for these models. We find that even the current state-of-the-art LVLMs (InstructBLIP) still contain a staggering 30 percent of the hallucinatory text in the form of non-existent objects, unfaithful descriptions, and inaccurate relationships. To address this, we introduce M-HalDetect, a (M)ultimodal (Hal)lucination (Detect)ion Dataset that can be used to train and benchmark models for hallucination detection and prevention. M-HalDetect consists of 16k fine-grained annotations on VQA examples, making it the first comprehensive multi-modal hallucination detection dataset for detailed image descriptions. Unlike previous work that only consider object hallucination, we additionally annotate both entity descriptions and relationships that are unfaithful. To demonstrate the potential of this dataset for hallucination prevention, we optimize InstructBLIP through our novel Fine-grained Direct Preference Optimization (FDPO). We also train fine-grained multi-modal reward models from InstructBLIP and evaluate their effectiveness with best-of-n rejection sampling. We perform human evaluation on both FDPO and rejection sampling, and find that they reduce hallucination rates in InstructBLIP by 41% and 55% respectively. We also find that our reward model generalizes to other multi-modal models, reducing hallucinations in LLaVA and mPLUG-OWL by 15% and 57% respectively, and has strong correlation with human evaluated accuracy scores.\n\n\nDetecting and preventing hallucinations in large vision language models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2308.06394v3</guid>
      <dc:creator>Anisha Gunjal, Jihan Yin, Erhan Bas</dc:creator>
      <pubDate>Sun, 11 Feb 2024 08:38:07 GMT</pubDate>
    </item>
    <item>
      <title>Large Language Models</title>
      <link>http://arxiv.org/abs/2307.05782v2</link>
      <description>Artificial intelligence is making spectacular progress, and one of the best examples is the development of large language models (LLMs) such as OpenAI's GPT series. In these lectures, written for readers with a background in mathematics or physics, we give a brief history and survey of the state of the art, and describe the underlying transformer architecture in detail. We then explore some current ideas on how LLMs work and how models trained to predict the next word in a text are able to perform other tasks displaying intelligence.\n\n\nLarge language models: A survey</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2307.05782v2</guid>
      <dc:creator>Michael R. Douglas</dc:creator>
      <pubDate>Fri, 06 Oct 2023 12:13:46 GMT</pubDate>
    </item>
    <item>
      <title>Statistical Rejection Sampling Improves Preference Optimization</title>
      <link>http://arxiv.org/abs/2309.06657v2</link>
      <description>Improving the alignment of language models with human preferences remains an active research challenge. Previous approaches have primarily utilized Reinforcement Learning from Human Feedback (RLHF) via online RL methods such as Proximal Policy Optimization (PPO). Recently, offline methods such as Sequence Likelihood Calibration (SLiC) and Direct Preference Optimization (DPO) have emerged as attractive alternatives, offering improvements in stability and scalability while maintaining competitive performance. SLiC refines its loss function using sequence pairs sampled from a supervised fine-tuned (SFT) policy, while DPO directly optimizes language models based on preference data, foregoing the need for a separate reward model. However, the maximum likelihood estimator (MLE) of the target optimal policy requires labeled preference pairs sampled from that policy. DPO's lack of a reward model constrains its ability to sample preference pairs from the optimal policy, and SLiC is restricted to sampling preference pairs only from the SFT policy. To address these limitations, we introduce a novel approach called Statistical Rejection Sampling Optimization (RSO) that aims to source preference data from the target optimal policy using rejection sampling, enabling a more accurate estimation of the optimal policy. We also propose a unified framework that enhances the loss functions used in both SLiC and DPO from a preference modeling standpoint. Through extensive experiments across three diverse tasks, we demonstrate that RSO consistently outperforms both SLiC and DPO on evaluations from both Large Language Model (LLM) and human raters.\n\n\nStatistical rejection sampling improves preference optimization</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2309.06657v2</guid>
      <dc:creator>Tianqi Liu, Yao Zhao, Rishabh Joshi, Misha Khalman, Mohammad Saleh, Peter J. Liu, Jialu Liu</dc:creator>
      <pubDate>Tue, 23 Jan 2024 23:16:11 GMT</pubDate>
    </item>
    <item>
      <title>OpenChat: Advancing Open-source Language Models with Mixed-Quality Data</title>
      <link>http://arxiv.org/abs/2309.11235v2</link>
      <description>Nowadays, open-source large language models like LLaMA have emerged. Recent developments have incorporated supervised fine-tuning (SFT) and reinforcement learning fine-tuning (RLFT) to align these models with human goals. However, SFT methods treat all training data with mixed quality equally, while RLFT methods require high-quality pairwise or ranking-based preference data. In this study, we present a novel framework, named OpenChat, to advance open-source language models with mixed-quality data. Specifically, we consider the general SFT training data, consisting of a small amount of expert data mixed with a large proportion of sub-optimal data, without any preference labels. We propose the C(onditioned)-RLFT, which regards different data sources as coarse-grained reward labels and learns a class-conditioned policy to leverage complementary data quality information. Interestingly, the optimal policy in C-RLFT can be easily solved through single-stage, RL-free supervised learning, which is lightweight and avoids costly human preference labeling. Through extensive experiments on three standard benchmarks, our openchat-13b fine-tuned with C-RLFT achieves the highest average performance among all 13b open-source language models. Moreover, we use AGIEval to validate the model generalization performance, in which only openchat-13b surpasses the base model. Finally, we conduct a series of analyses to shed light on the effectiveness and robustness of OpenChat. Our code, data, and models are publicly available at https://github.com/imoneoi/openchat and https://huggingface.co/openchat.\n\n\nOpenchat: Advancing open-source language models with mixed-quality data</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2309.11235v2</guid>
      <dc:creator>Guan Wang, Sijie Cheng, Xianyuan Zhan, Xiangang Li, Sen Song, Yang Liu</dc:creator>
      <pubDate>Sat, 16 Mar 2024 04:32:25 GMT</pubDate>
    </item>
    <item>
      <title>Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation</title>
      <link>http://arxiv.org/abs/2305.00955v2</link>
      <description>Many recent advances in natural language generation have been fueled by training large language models on internet-scale data. However, this paradigm can lead to models that generate toxic, inaccurate, and unhelpful content, and automatic evaluation metrics often fail to identify these behaviors. As models become more capable, human feedback is an invaluable signal for evaluating and improving models. This survey aims to provide an overview of the recent research that has leveraged human feedback to improve natural language generation. First, we introduce an encompassing formalization of feedback, and identify and organize existing research into a taxonomy following this formalization. Next, we discuss how feedback can be described by its format and objective, and cover the two approaches proposed to use feedback (either for training or decoding): directly using the feedback or training feedback models. We also discuss existing datasets for human-feedback data collection, and concerns surrounding feedback collection. Finally, we provide an overview of the nascent field of AI feedback, which exploits large language models to make judgments based on a set of principles and minimize the need for human intervention.\n\n\nBridging the gap: A survey on integrating (human) feedback for natural language generation</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2305.00955v2</guid>
      <dc:creator>Patrick Fernandes, Aman Madaan, Emmy Liu, António Farinhas, Pedro Henrique Martins, Amanda Bertsch, José G. C. de Souza, Shuyan Zhou, Tongshuang Wu, Graham Neubig, André F. T. Martins</dc:creator>
      <pubDate>Thu, 01 Jun 2023 01:24:53 GMT</pubDate>
    </item>
    <item>
      <title>AI Alignment: A Comprehensive Survey</title>
      <link>http://arxiv.org/abs/2310.19852v5</link>
      <description>AI alignment aims to make AI systems behave in line with human intentions and values. As AI systems grow more capable, so do risks from misalignment. To provide a comprehensive and up-to-date overview of the alignment field, in this survey, we delve into the core concepts, methodology, and practice of alignment. First, we identify four principles as the key objectives of AI alignment: Robustness, Interpretability, Controllability, and Ethicality (RICE). Guided by these four principles, we outline the landscape of current alignment research and decompose them into two key components: forward alignment and backward alignment. The former aims to make AI systems aligned via alignment training, while the latter aims to gain evidence about the systems' alignment and govern them appropriately to avoid exacerbating misalignment risks. On forward alignment, we discuss techniques for learning from feedback and learning under distribution shift. On backward alignment, we discuss assurance techniques and governance practices.   We also release and continually update the website (www.alignmentsurvey.com) which features tutorials, collections of papers, blog posts, and other resources.\n\n\nAi alignment: A comprehensive survey</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2310.19852v5</guid>
      <dc:creator>Jiaming Ji, Tianyi Qiu, Boyuan Chen, Borong Zhang, Hantao Lou, Kaile Wang, Yawen Duan, Zhonghao He, Jiayi Zhou, Zhaowei Zhang, Fanzhi Zeng, Kwan Yee Ng, Juntao Dai, Xuehai Pan, Aidan O'Gara, Yingshan Lei, Hua Xu, Brian Tse, Jie Fu, Stephen McAleer, Yaodong Yang, Yizhou Wang, Song-Chun Zhu, Yike Guo, Wen Gao</dc:creator>
      <pubDate>Wed, 01 May 2024 07:30:50 GMT</pubDate>
    </item>
    <item>
      <title>Evaluating Large Language Models at Evaluating Instruction Following</title>
      <link>http://arxiv.org/abs/2310.07641v2</link>
      <description>As research in large language models (LLMs) continues to accelerate, LLM-based evaluation has emerged as a scalable and cost-effective alternative to human evaluations for comparing the ever increasing list of models. This paper investigates the efficacy of these ``LLM evaluators'', particularly in using them to assess instruction following, a metric that gauges how closely generated text adheres to the given instruction. We introduce a challenging meta-evaluation benchmark, LLMBar, designed to test the ability of an LLM evaluator in discerning instruction-following outputs. The authors manually curated 419 pairs of outputs, one adhering to instructions while the other diverging, yet may possess deceptive qualities that mislead an LLM evaluator, e.g., a more engaging tone. Contrary to existing meta-evaluation, we discover that different evaluators (i.e., combinations of LLMs and prompts) exhibit distinct performance on LLMBar and even the highest-scoring ones have substantial room for improvement. We also present a novel suite of prompting strategies that further close the gap between LLM and human evaluators. With LLMBar, we hope to offer more insight into LLM evaluators and foster future research in developing better instruction-following models.\n\n\nEvaluating large language models at evaluating instruction following</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2310.07641v2</guid>
      <dc:creator>Zhiyuan Zeng, Jiatong Yu, Tianyu Gao, Yu Meng, Tanya Goyal, Danqi Chen</dc:creator>
      <pubDate>Tue, 16 Apr 2024 04:50:08 GMT</pubDate>
    </item>
    <item>
      <title>RAIN: Your Language Models Can Align Themselves without Finetuning</title>
      <link>http://arxiv.org/abs/2309.07124v2</link>
      <description>Large language models (LLMs) often demonstrate inconsistencies with human preferences. Previous research typically gathered human preference data and then aligned the pre-trained models using reinforcement learning or instruction tuning, a.k.a. the finetuning step. In contrast, aligning frozen LLMs without requiring alignment data is more appealing. This work explores the potential of the latter setting. We discover that by integrating self-evaluation and rewind mechanisms, unaligned LLMs can directly produce responses consistent with human preferences via self-boosting. We introduce a novel inference method, Rewindable Auto-regressive INference (RAIN), that allows pre-trained LLMs to evaluate their own generation and use the evaluation results to guide rewind and generation for AI safety. Notably, RAIN operates without the need of extra data for model alignment and abstains from any training, gradient computation, or parameter updates. Experimental results evaluated by GPT-4 and humans demonstrate the effectiveness of RAIN: on the HH dataset, RAIN improves the harmlessness rate of LLaMA 30B from 82% of vanilla inference to 97%, while maintaining the helpfulness rate. On the TruthfulQA dataset, RAIN improves the truthfulness of the already-well-aligned LLaMA-2-chat 13B model by 5%.\n\n\nRain: Your language models can align themselves without finetuning</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2309.07124v2</guid>
      <dc:creator>Yuhui Li, Fangyun Wei, Jinjing Zhao, Chao Zhang, Hongyang Zhang</dc:creator>
      <pubDate>Mon, 09 Oct 2023 03:34:01 GMT</pubDate>
    </item>
    <item>
      <title>Camels in a Changing Climate: Enhancing LM Adaptation with Tulu 2</title>
      <link>http://arxiv.org/abs/2311.10702v2</link>
      <description>Since the release of T\&quot;ULU [Wang et al., 2023b], open resources for instruction tuning have developed quickly, from better base models to new finetuning techniques. We test and incorporate a number of these advances into T\&quot;ULU, resulting in T\&quot;ULU 2, a suite of improved T\&quot;ULU models for advancing the understanding and best practices of adapting pretrained language models to downstream tasks and user preferences. Concretely, we release: (1) T\&quot;ULU-V2-mix, an improved collection of high-quality instruction datasets; (2) T\&quot;ULU 2, LLAMA-2 models finetuned on the V2 mixture; (3) T\&quot;ULU 2+DPO, T\&quot;ULU 2 models trained with direct preference optimization (DPO), including the largest DPO-trained model to date (T\&quot;ULU 2+DPO 70B); (4) CODE T\&quot;ULU 2, CODE LLAMA models finetuned on our V2 mix that outperform CODE LLAMA and its instruction-tuned variant, CODE LLAMA-Instruct. Our evaluation from multiple perspectives shows that the T\&quot;ULU 2 suite achieves state-of-the-art performance among open models and matches or exceeds the performance of GPT-3.5-turbo-0301 on several benchmarks. We release all the checkpoints, data, training and evaluation code to facilitate future open efforts on adapting large language models.\n\n\nCamels in a changing climate: Enhancing lm adaptation with tulu 2</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2311.10702v2</guid>
      <dc:creator>Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah A. Smith, Iz Beltagy, Hannaneh Hajishirzi</dc:creator>
      <pubDate>Mon, 20 Nov 2023 02:01:33 GMT</pubDate>
    </item>
    <item>
      <title>Nash Learning from Human Feedback</title>
      <link>http://arxiv.org/abs/2312.00886v4</link>
      <description>Reinforcement learning from human feedback (RLHF) has emerged as the main paradigm for aligning large language models (LLMs) with human preferences. Typically, RLHF involves the initial step of learning a reward model from human feedback, often expressed as preferences between pairs of text generations produced by a pre-trained LLM. Subsequently, the LLM's policy is fine-tuned by optimizing it to maximize the reward model through a reinforcement learning algorithm. However, an inherent limitation of current reward models is their inability to fully represent the richness of human preferences and their dependency on the sampling distribution.   In this study, we introduce an alternative pipeline for the fine-tuning of LLMs using pairwise human feedback. Our approach entails the initial learning of a preference model, which is conditioned on two inputs given a prompt, followed by the pursuit of a policy that consistently generates responses preferred over those generated by any competing policy, thus defining the Nash equilibrium of this preference model. We term this approach Nash learning from human feedback (NLHF).   In the context of a tabular policy representation, we present a novel algorithmic solution, Nash-MD, founded on the principles of mirror descent. This algorithm produces a sequence of policies, with the last iteration converging to the regularized Nash equilibrium. Additionally, we explore parametric representations of policies and introduce gradient descent algorithms for deep-learning architectures. To demonstrate the effectiveness of our approach, we present experimental results involving the fine-tuning of a LLM for a text summarization task. We believe NLHF offers a compelling avenue for preference learning and policy optimization with the potential of advancing the field of aligning LLMs with human preferences.\n\n\nNash learning from human feedback</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2312.00886v4</guid>
      <dc:creator>Rémi Munos, Michal Valko, Daniele Calandriello, Mohammad Gheshlaghi Azar, Mark Rowland, Zhaohan Daniel Guo, Yunhao Tang, Matthieu Geist, Thomas Mesnard, Andrea Michi, Marco Selvi, Sertan Girgin, Nikola Momchev, Olivier Bachem, Daniel J. Mankowitz, Doina Precup, Bilal Piot</dc:creator>
      <pubDate>Tue, 11 Jun 2024 16:25:52 GMT</pubDate>
    </item>
    <item>
      <title>Investigating the Catastrophic Forgetting in Multimodal Large Language Models</title>
      <link>http://arxiv.org/abs/2309.10313v4</link>
      <description>Following the success of GPT4, there has been a surge in interest in multimodal large language model (MLLM) research. This line of research focuses on developing general-purpose LLMs through fine-tuning pre-trained LLMs and vision models. However, catastrophic forgetting, a notorious phenomenon where the fine-tuned model fails to retain similar performance compared to the pre-trained model, still remains an inherent problem in multimodal LLMs (MLLM). In this paper, we introduce EMT: Evaluating MulTimodality for evaluating the catastrophic forgetting in MLLMs, by treating each MLLM as an image classifier. We first apply EMT to evaluate several open-source fine-tuned MLLMs and we discover that almost all evaluated MLLMs fail to retain the same performance levels as their vision encoders on standard image classification tasks. Moreover, we continue fine-tuning LLaVA, an MLLM and utilize EMT to assess performance throughout the fine-tuning. Interestingly, our results suggest that early-stage fine-tuning on an image dataset improves performance across other image datasets, by enhancing the alignment of text and visual features. However, as fine-tuning proceeds, the MLLMs begin to hallucinate, resulting in a significant loss of generalizability, even when the image encoder remains frozen. Our results suggest that MLLMs have yet to demonstrate performance on par with their vision models on standard image classification tasks and the current MLLM fine-tuning procedure still has room for improvement.\n\n\nInvestigating the catastrophic forgetting in multimodal large language models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2309.10313v4</guid>
      <dc:creator>Yuexiang Zhai, Shengbang Tong, Xiao Li, Mu Cai, Qing Qu, Yong Jae Lee, Yi Ma</dc:creator>
      <pubDate>Tue, 05 Dec 2023 08:59:33 GMT</pubDate>
    </item>
    <item>
      <title>Drive as You Speak: Enabling Human-Like Interaction with Large Language Models in Autonomous Vehicles</title>
      <link>http://arxiv.org/abs/2309.10228v1</link>
      <description>The future of autonomous vehicles lies in the convergence of human-centric design and advanced AI capabilities. Autonomous vehicles of the future will not only transport passengers but also interact and adapt to their desires, making the journey comfortable, efficient, and pleasant. In this paper, we present a novel framework that leverages Large Language Models (LLMs) to enhance autonomous vehicles' decision-making processes. By integrating LLMs' natural language capabilities and contextual understanding, specialized tools usage, synergizing reasoning, and acting with various modules on autonomous vehicles, this framework aims to seamlessly integrate the advanced language and reasoning capabilities of LLMs into autonomous vehicles. The proposed framework holds the potential to revolutionize the way autonomous vehicles operate, offering personalized assistance, continuous learning, and transparent decision-making, ultimately contributing to safer and more efficient autonomous driving technologies.\n\n\nDrive as you speak: Enabling human-like interaction with large language models in autonomous vehicles</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2309.10228v1</guid>
      <dc:creator>Can Cui, Yunsheng Ma, Xu Cao, Wenqian Ye, Ziran Wang</dc:creator>
      <pubDate>Tue, 19 Sep 2023 00:47:13 GMT</pubDate>
    </item>
    <item>
      <title>A Survey of Large Language Models for Healthcare: from Data, Technology, and Applications to Accountability and Ethics</title>
      <link>http://arxiv.org/abs/2310.05694v2</link>
      <description>The utilization of large language models (LLMs) in the Healthcare domain has generated both excitement and concern due to their ability to effectively respond to freetext queries with certain professional knowledge. This survey outlines the capabilities of the currently developed LLMs for Healthcare and explicates their development process, with the aim of providing an overview of the development roadmap from traditional Pretrained Language Models (PLMs) to LLMs. Specifically, we first explore the potential of LLMs to enhance the efficiency and effectiveness of various Healthcare applications highlighting both the strengths and limitations. Secondly, we conduct a comparison between the previous PLMs and the latest LLMs, as well as comparing various LLMs with each other. Then we summarize related Healthcare training data, training methods, optimization strategies, and usage. Finally, the unique concerns associated with deploying LLMs in Healthcare settings are investigated, particularly regarding fairness, accountability, transparency and ethics. Our survey provide a comprehensive investigation from perspectives of both computer science and Healthcare specialty. Besides the discussion about Healthcare concerns, we supports the computer science community by compiling a collection of open source resources, such as accessible datasets, the latest methodologies, code implementations, and evaluation benchmarks in the Github. Summarily, we contend that a significant paradigm shift is underway, transitioning from PLMs to LLMs. This shift encompasses a move from discriminative AI approaches to generative AI approaches, as well as a shift from model-centered methodologies to data-centered methodologies. Also, we determine that the biggest obstacle of using LLMs in Healthcare are fairness, accountability, transparency and ethics.\n\n\nA survey of large language models for healthcare: from data, technology, and applications to accountability and ethics</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2310.05694v2</guid>
      <dc:creator>Kai He, Rui Mao, Qika Lin, Yucheng Ruan, Xiang Lan, Mengling Feng, Erik Cambria</dc:creator>
      <pubDate>Tue, 11 Jun 2024 13:13:59 GMT</pubDate>
    </item>
    <item>
      <title>Large Language Model Alignment: A Survey</title>
      <link>http://arxiv.org/abs/2309.15025v1</link>
      <description>Recent years have witnessed remarkable progress made in large language models (LLMs). Such advancements, while garnering significant attention, have concurrently elicited various concerns. The potential of these models is undeniably vast; however, they may yield texts that are imprecise, misleading, or even detrimental. Consequently, it becomes paramount to employ alignment techniques to ensure these models to exhibit behaviors consistent with human values.   This survey endeavors to furnish an extensive exploration of alignment methodologies designed for LLMs, in conjunction with the extant capability research in this domain. Adopting the lens of AI alignment, we categorize the prevailing methods and emergent proposals for the alignment of LLMs into outer and inner alignment. We also probe into salient issues including the models' interpretability, and potential vulnerabilities to adversarial attacks. To assess LLM alignment, we present a wide variety of benchmarks and evaluation methodologies. After discussing the state of alignment research for LLMs, we finally cast a vision toward the future, contemplating the promising avenues of research that lie ahead.   Our aspiration for this survey extends beyond merely spurring research interests in this realm. We also envision bridging the gap between the AI alignment research community and the researchers engrossed in the capability exploration of LLMs for both capable and safe LLMs.\n\n\nLarge language model alignment: A survey</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2309.15025v1</guid>
      <dc:creator>Tianhao Shen, Renren Jin, Yufei Huang, Chuang Liu, Weilong Dong, Zishan Guo, Xinwei Wu, Yan Liu, Deyi Xiong</dc:creator>
      <pubDate>Tue, 26 Sep 2023 15:49:23 GMT</pubDate>
    </item>
    <item>
      <title>Inverse Preference Learning: Preference-based RL without a Reward Function</title>
      <link>http://arxiv.org/abs/2305.15363v2</link>
      <description>Reward functions are difficult to design and often hard to align with human intent. Preference-based Reinforcement Learning (RL) algorithms address these problems by learning reward functions from human feedback. However, the majority of preference-based RL methods na\&quot;ively combine supervised reward models with off-the-shelf RL algorithms. Contemporary approaches have sought to improve performance and query complexity by using larger and more complex reward architectures such as transformers. Instead of using highly complex architectures, we develop a new and parameter-efficient algorithm, Inverse Preference Learning (IPL), specifically designed for learning from offline preference data. Our key insight is that for a fixed policy, the $Q$-function encodes all information about the reward function, effectively making them interchangeable. Using this insight, we completely eliminate the need for a learned reward function. Our resulting algorithm is simpler and more parameter-efficient. Across a suite of continuous control and robotics benchmarks, IPL attains competitive performance compared to more complex approaches that leverage transformer-based and non-Markovian reward functions while having fewer algorithmic hyperparameters and learned network parameters. Our code is publicly released.\n\n\nInverse preference learning: Preference-based rl without a reward function</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2305.15363v2</guid>
      <dc:creator>Joey Hejna, Dorsa Sadigh</dc:creator>
      <pubDate>Fri, 24 Nov 2023 22:12:33 GMT</pubDate>
    </item>
    <item>
      <title>Reward Model Ensembles Help Mitigate Overoptimization</title>
      <link>http://arxiv.org/abs/2310.02743v2</link>
      <description>Reinforcement learning from human feedback (RLHF) is a standard approach for fine-tuning large language models to follow instructions. As part of this process, learned reward models are used to approximately model human preferences. However, as imperfect representations of the &quot;true&quot; reward, these learned reward models are susceptible to overoptimization. Gao et al. (2023) studied this phenomenon in a synthetic human feedback setup with a significantly larger &quot;gold&quot; reward model acting as the true reward (instead of humans) and showed that overoptimization remains a persistent problem regardless of the size of the proxy reward model and training data used. Using a similar setup, we conduct a systematic study to evaluate the efficacy of using ensemble-based conservative optimization objectives, specifically worst-case optimization (WCO) and uncertainty-weighted optimization (UWO), for mitigating reward model overoptimization when using two optimization methods: (a) best-of-n sampling (BoN) (b) proximal policy optimization (PPO). We additionally extend the setup of Gao et al. (2023) to include 25% label noise to better mirror real-world conditions. Both with and without label noise, we find that conservative optimization practically eliminates overoptimization and improves performance by up to 70% for BoN sampling. For PPO, ensemble-based conservative optimization always reduces overoptimization and outperforms single reward model optimization. Moreover, combining it with a small KL penalty successfully prevents overoptimization at no performance cost. Overall, our results demonstrate that ensemble-based conservative optimization can effectively counter overoptimization.\n\n\nReward model ensembles help mitigate overoptimization</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2310.02743v2</guid>
      <dc:creator>Thomas Coste, Usman Anwar, Robert Kirk, David Krueger</dc:creator>
      <pubDate>Sun, 10 Mar 2024 16:14:58 GMT</pubDate>
    </item>
    <item>
      <title>HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal</title>
      <link>http://arxiv.org/abs/2402.04249v2</link>
      <description>Automated red teaming holds substantial promise for uncovering and mitigating the risks associated with the malicious use of large language models (LLMs), yet the field lacks a standardized evaluation framework to rigorously assess new methods. To address this issue, we introduce HarmBench, a standardized evaluation framework for automated red teaming. We identify several desirable properties previously unaccounted for in red teaming evaluations and systematically design HarmBench to meet these criteria. Using HarmBench, we conduct a large-scale comparison of 18 red teaming methods and 33 target LLMs and defenses, yielding novel insights. We also introduce a highly efficient adversarial training method that greatly enhances LLM robustness across a wide range of attacks, demonstrating how HarmBench enables codevelopment of attacks and defenses. We open source HarmBench at https://github.com/centerforaisafety/HarmBench.\n\n\nHarmbench: A standardized evaluation framework for automated red teaming and robust refusal</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.04249v2</guid>
      <dc:creator>Mantas Mazeika, Long Phan, Xuwang Yin, Andy Zou, Zifan Wang, Norman Mu, Elham Sakhaee, Nathaniel Li, Steven Basart, Bo Li, David Forsyth, Dan Hendrycks</dc:creator>
      <pubDate>Tue, 27 Feb 2024 04:43:08 GMT</pubDate>
    </item>
    <item>
      <title>Retroformer: Retrospective Large Language Agents with Policy Gradient Optimization</title>
      <link>http://arxiv.org/abs/2308.02151v3</link>
      <description>Recent months have seen the emergence of a powerful new trend in which large language models (LLMs) are augmented to become autonomous language agents capable of performing objective oriented multi-step tasks on their own, rather than merely responding to queries from human users. Most existing language agents, however, are not optimized using environment-specific rewards. Although some agents enable iterative refinement through verbal feedback, they do not reason and plan in ways that are compatible with gradient-based learning from rewards. This paper introduces a principled framework for reinforcing large language agents by learning a retrospective model, which automatically tunes the language agent prompts from environment feedback through policy gradient. Specifically, our proposed agent architecture learns from rewards across multiple environments and tasks, for fine-tuning a pre-trained language model which refines the language agent prompt by summarizing the root cause of prior failed attempts and proposing action plans. Experimental results on various tasks demonstrate that the language agents improve over time and that our approach considerably outperforms baselines that do not properly leverage gradients from the environment. This demonstrates that using policy gradient optimization to improve language agents, for which we believe our work is one of the first, seems promising and can be applied to optimize other models in the agent architecture to enhance agent performances over time.\n\n\nRetroformer: Retrospective large language agents with policy gradient optimization</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2308.02151v3</guid>
      <dc:creator>Weiran Yao, Shelby Heinecke, Juan Carlos Niebles, Zhiwei Liu, Yihao Feng, Le Xue, Rithesh Murthy, Zeyuan Chen, Jianguo Zhang, Devansh Arpit, Ran Xu, Phil Mui, Huan Wang, Caiming Xiong, Silvio Savarese</dc:creator>
      <pubDate>Sun, 05 May 2024 05:04:49 GMT</pubDate>
    </item>
    <item>
      <title>RLCD: Reinforcement Learning from Contrastive Distillation for Language Model Alignment</title>
      <link>http://arxiv.org/abs/2307.12950v3</link>
      <description>We propose Reinforcement Learning from Contrastive Distillation (RLCD), a method for aligning language models to follow principles expressed in natural language (e.g., to be more harmless) without using human feedback. RLCD creates preference pairs from two contrasting model outputs, one using a positive prompt designed to encourage following the given principles, and one using a negative prompt designed to encourage violating them. Using two different prompts causes model outputs to be more differentiated on average, resulting in cleaner preference labels in the absence of human annotations. We then use the preference pairs to train a preference model, which is in turn used to improve a base unaligned language model via reinforcement learning. Empirically, RLCD outperforms RLAIF (Bai et al., 2022b) and context distillation (Huang et al., 2022) baselines across three diverse alignment tasks--harmlessness, helpfulness, and story outline generation--and when using both 7B and 30B model scales for simulating preference data.\n\n\nRlcd: Reinforcement learning from contrast distillation for language model alignment</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2307.12950v3</guid>
      <dc:creator>Kevin Yang, Dan Klein, Asli Celikyilmaz, Nanyun Peng, Yuandong Tian</dc:creator>
      <pubDate>Sat, 16 Mar 2024 04:22:09 GMT</pubDate>
    </item>
    <item>
      <title>LightHouse: A Survey of AGI Hallucination</title>
      <link>http://arxiv.org/abs/2401.06792v2</link>
      <description>With the development of artificial intelligence, large-scale models have become increasingly intelligent. However, numerous studies indicate that hallucinations within these large models are a bottleneck hindering the development of AI research. In the pursuit of achieving strong artificial intelligence, a significant volume of research effort is being invested in the AGI (Artificial General Intelligence) hallucination research. Previous explorations have been conducted in researching hallucinations within LLMs (Large Language Models). As for multimodal AGI, research on hallucinations is still in an early stage. To further the progress of research in the domain of hallucinatory phenomena, we present a bird's eye view of hallucinations in AGI, summarizing the current work on AGI hallucinations and proposing some directions for future research.\n\n\nA survey on hallucination in large vision-language models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2401.06792v2</guid>
      <dc:creator>Feng Wang</dc:creator>
      <pubDate>Wed, 17 Jan 2024 04:40:13 GMT</pubDate>
    </item>
    <item>
      <title>Rethinking Machine Unlearning for Large Language Models</title>
      <link>http://arxiv.org/abs/2402.08787v5</link>
      <description>We explore machine unlearning (MU) in the domain of large language models (LLMs), referred to as LLM unlearning. This initiative aims to eliminate undesirable data influence (e.g., sensitive or illegal information) and the associated model capabilities, while maintaining the integrity of essential knowledge generation and not affecting causally unrelated information. We envision LLM unlearning becoming a pivotal element in the life-cycle management of LLMs, potentially standing as an essential foundation for developing generative AI that is not only safe, secure, and trustworthy, but also resource-efficient without the need of full retraining. We navigate the unlearning landscape in LLMs from conceptual formulation, methodologies, metrics, and applications. In particular, we highlight the often-overlooked aspects of existing LLM unlearning research, e.g., unlearning scope, data-model interaction, and multifaceted efficacy assessment. We also draw connections between LLM unlearning and related areas such as model editing, influence functions, model explanation, adversarial training, and reinforcement learning. Furthermore, we outline an effective assessment framework for LLM unlearning and explore its applications in copyright and privacy safeguards and sociotechnical harm reduction.\n\n\nRethinking machine unlearning for large language models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.08787v5</guid>
      <dc:creator>Sijia Liu, Yuanshun Yao, Jinghan Jia, Stephen Casper, Nathalie Baracaldo, Peter Hase, Yuguang Yao, Chris Yuhao Liu, Xiaojun Xu, Hang Li, Kush R. Varshney, Mohit Bansal, Sanmi Koyejo, Yang Liu</dc:creator>
      <pubDate>Mon, 15 Jul 2024 00:18:21 GMT</pubDate>
    </item>
    <item>
      <title>Contrastive Preference Learning: Learning from Human Feedback without RL</title>
      <link>http://arxiv.org/abs/2310.13639v3</link>
      <description>Reinforcement Learning from Human Feedback (RLHF) has emerged as a popular paradigm for aligning models with human intent. Typically RLHF algorithms operate in two phases: first, use human preferences to learn a reward function and second, align the model by optimizing the learned reward via reinforcement learning (RL). This paradigm assumes that human preferences are distributed according to reward, but recent work suggests that they instead follow the regret under the user's optimal policy. Thus, learning a reward function from feedback is not only based on a flawed assumption of human preference, but also leads to unwieldy optimization challenges that stem from policy gradients or bootstrapping in the RL phase. Because of these optimization challenges, contemporary RLHF methods restrict themselves to contextual bandit settings (e.g., as in large language models) or limit observation dimensionality (e.g., state-based robotics). We overcome these limitations by introducing a new family of algorithms for optimizing behavior from human feedback using the regret-based model of human preferences. Using the principle of maximum entropy, we derive Contrastive Preference Learning (CPL), an algorithm for learning optimal policies from preferences without learning reward functions, circumventing the need for RL. CPL is fully off-policy, uses only a simple contrastive objective, and can be applied to arbitrary MDPs. This enables CPL to elegantly scale to high-dimensional and sequential RLHF problems while being simpler than prior methods.\n\n\nContrastive prefence learning: Learning from human feedback without rl</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2310.13639v3</guid>
      <dc:creator>Joey Hejna, Rafael Rafailov, Harshit Sikchi, Chelsea Finn, Scott Niekum, W. Bradley Knox, Dorsa Sadigh</dc:creator>
      <pubDate>Tue, 30 Apr 2024 14:36:26 GMT</pubDate>
    </item>
    <item>
      <title>LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models</title>
      <link>http://arxiv.org/abs/2403.13372v4</link>
      <description>Efficient fine-tuning is vital for adapting large language models (LLMs) to downstream tasks. However, it requires non-trivial efforts to implement these methods on different models. We present LlamaFactory, a unified framework that integrates a suite of cutting-edge efficient training methods. It provides a solution for flexibly customizing the fine-tuning of 100+ LLMs without the need for coding through the built-in web UI LlamaBoard. We empirically validate the efficiency and effectiveness of our framework on language modeling and text generation tasks. It has been released at https://github.com/hiyouga/LLaMA-Factory and received over 25,000 stars and 3,000 forks.\n\n\nLlamafactory: Unified efficient fine-tuning of 100+ language models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2403.13372v4</guid>
      <dc:creator>Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, Yongqiang Ma</dc:creator>
      <pubDate>Thu, 27 Jun 2024 22:44:48 GMT</pubDate>
    </item>
    <item>
      <title>Iterative Preference Learning from Human Feedback: Bridging Theory and Practice for RLHF under KL-Constraint</title>
      <link>http://arxiv.org/abs/2312.11456v4</link>
      <description>This paper studies the alignment process of generative models with Reinforcement Learning from Human Feedback (RLHF). We first identify the primary challenges of existing popular methods like offline PPO and offline DPO as lacking in strategical exploration of the environment. Then, to understand the mathematical principle of RLHF, we consider a standard mathematical formulation, the reverse-KL regularized contextual bandit for RLHF. Despite its widespread practical application, a rigorous theoretical analysis of this formulation remains open. We investigate its behavior in three distinct settings -- offline, online, and hybrid -- and propose efficient algorithms with finite-sample theoretical guarantees.   Moving towards practical applications, our framework, with a robust approximation of the information-theoretical policy improvement oracle, naturally gives rise to several novel RLHF algorithms. This includes an iterative version of the Direct Preference Optimization (DPO) algorithm for online settings, and a multi-step rejection sampling strategy for offline scenarios. Our empirical evaluations on real-world alignment experiment of large language model demonstrate that these proposed methods significantly surpass existing strong baselines, such as DPO and Rejection Sampling Optimization (RSO), showcasing the connections between solid theoretical foundations and their potent practical implementations.\n\n\nIterative preference learning from human feedback: Bridging theory and practice for rlhf under kl-constraint</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2312.11456v4</guid>
      <dc:creator>Wei Xiong, Hanze Dong, Chenlu Ye, Ziqi Wang, Han Zhong, Heng Ji, Nan Jiang, Tong Zhang</dc:creator>
      <pubDate>Wed, 01 May 2024 14:50:56 GMT</pubDate>
    </item>
    <item>
      <title>What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning</title>
      <link>http://arxiv.org/abs/2312.15685v2</link>
      <description>Instruction tuning is a standard technique employed to align large language models to end tasks and user preferences after the initial pretraining phase. Recent research indicates the critical role of data engineering in instruction tuning -- when appropriately selected, only limited data is necessary to achieve superior performance. However, we still lack a principled understanding of what makes good instruction tuning data for alignment, and how we should select data automatically and effectively. In this work, we delve deeply into automatic data selection strategies for alignment. We start with controlled studies to measure data across three dimensions: complexity, quality, and diversity, along which we examine existing methods and introduce novel techniques for enhanced data measurement. Subsequently, we propose a simple strategy to select data samples based on the measurement. We present deita (short for Data-Efficient Instruction Tuning for Alignment), a series of models fine-tuned from LLaMA and Mistral models using data samples automatically selected with our proposed approach. Empirically, deita performs better or on par with the state-of-the-art open-source alignment models with only 6K SFT training data samples -- over 10x less than the data used in the baselines. When further trained with direct preference optimization (DPO), deita-Mistral-7B + DPO trained with 6K SFT and 10K DPO samples achieve 7.55 MT-Bench and 90.06% AlpacaEval scores. We anticipate this work to provide tools on automatic data selection, facilitating data-efficient alignment. We release our models as well as the selected datasets for future researches to effectively align models more efficiently.\n\n\nWhat makes good data for alignment? a comprehensive study of automatic data selection in instruction tuning</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2312.15685v2</guid>
      <dc:creator>Wei Liu, Weihao Zeng, Keqing He, Yong Jiang, Junxian He</dc:creator>
      <pubDate>Tue, 16 Apr 2024 02:46:58 GMT</pubDate>
    </item>
    <item>
      <title>Continual Learning Under Language Shift</title>
      <link>http://arxiv.org/abs/2311.01200v4</link>
      <description>The recent increase in data and model scale for language model pre-training has led to huge training costs. In scenarios where new data become available over time, updating a model instead of fully retraining it would therefore provide significant gains. We study the pros and cons of updating a language model when new data comes from new languages -- the case of continual learning under language shift. Starting from a monolingual English language model, we incrementally add data from Danish, Icelandic, and Norwegian to investigate how forward and backward transfer effects depend on pre-training order and characteristics of languages, for three different model sizes. Our results show that, while forward transfer is largely positive and independent of language order, backward transfer can be positive or negative depending on the order and characteristics of new languages. We explore a number of potentially explanatory factors and find that a combination of language contamination and syntactic similarity best fits our results.\n\n\nContinual learning for large language models: A survey</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2311.01200v4</guid>
      <dc:creator>Evangelia Gogoulou, Timothée Lesort, Magnus Boman, Joakim Nivre</dc:creator>
      <pubDate>Thu, 27 Jun 2024 08:35:53 GMT</pubDate>
    </item>
    <item>
      <title>Foundational Challenges in Assuring Alignment and Safety of Large Language Models</title>
      <link>http://arxiv.org/abs/2404.09932v1</link>
      <description>This work identifies 18 foundational challenges in assuring the alignment and safety of large language models (LLMs). These challenges are organized into three different categories: scientific understanding of LLMs, development and deployment methods, and sociotechnical challenges. Based on the identified challenges, we pose $200+$ concrete research questions.\n\n\nFoundational challenges in assuring alignment and safety of large language models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2404.09932v1</guid>
      <dc:creator>Usman Anwar, Abulhair Saparov, Javier Rando, Daniel Paleka, Miles Turpin, Peter Hase, Ekdeep Singh Lubana, Erik Jenner, Stephen Casper, Oliver Sourbut, Benjamin L. Edelman, Zhaowei Zhang, Mario Günther, Anton Korinek, Jose Hernandez-Orallo, Lewis Hammond, Eric Bigelow, Alexander Pan, Lauro Langosco, Tomasz Korbak, Heidi Zhang, Ruiqi Zhong, Seán Ó hÉigeartaigh, Gabriel Recchia, Giulio Corsi, Alan Chan, Markus Anderljung, Lilian Edwards, Yoshua Bengio, Danqi Chen, Samuel Albanie, Tegan Maharaj, Jakob Foerster, Florian Tramer, He He, Atoosa Kasirzadeh, Yejin Choi, David Krueger</dc:creator>
      <pubDate>Mon, 15 Apr 2024 16:58:28 GMT</pubDate>
    </item>
    <item>
      <title>Using Human Feedback to Fine-tune Diffusion Models without Any Reward Model</title>
      <link>http://arxiv.org/abs/2311.13231v3</link>
      <description>Using reinforcement learning with human feedback (RLHF) has shown significant promise in fine-tuning diffusion models. Previous methods start by training a reward model that aligns with human preferences, then leverage RL techniques to fine-tune the underlying models. However, crafting an efficient reward model demands extensive datasets, optimal architecture, and manual hyperparameter tuning, making the process both time and cost-intensive. The direct preference optimization (DPO) method, effective in fine-tuning large language models, eliminates the necessity for a reward model. However, the extensive GPU memory requirement of the diffusion model's denoising process hinders the direct application of the DPO method. To address this issue, we introduce the Direct Preference for Denoising Diffusion Policy Optimization (D3PO) method to directly fine-tune diffusion models. The theoretical analysis demonstrates that although D3PO omits training a reward model, it effectively functions as the optimal reward model trained using human feedback data to guide the learning process. This approach requires no training of a reward model, proving to be more direct, cost-effective, and minimizing computational overhead. In experiments, our method uses the relative scale of objectives as a proxy for human preference, delivering comparable results to methods using ground-truth rewards. Moreover, D3PO demonstrates the ability to reduce image distortion rates and generate safer images, overcoming challenges lacking robust reward models. Our code is publicly available at https://github.com/yk7333/D3PO.\n\n\nUsing human feedback to fine-tune diffusion models without any reward model</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2311.13231v3</guid>
      <dc:creator>Kai Yang, Jian Tao, Jiafei Lyu, Chunjiang Ge, Jiaxin Chen, Qimai Li, Weihan Shen, Xiaolong Zhu, Xiu Li</dc:creator>
      <pubDate>Sat, 23 Mar 2024 05:23:00 GMT</pubDate>
    </item>
    <item>
      <title>A Survey on Interpretable Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2112.13112v2</link>
      <description>Although deep reinforcement learning has become a promising machine learning approach for sequential decision-making problems, it is still not mature enough for high-stake domains such as autonomous driving or medical applications. In such contexts, a learned policy needs for instance to be interpretable, so that it can be inspected before any deployment (e.g., for safety and verifiability reasons). This survey provides an overview of various approaches to achieve higher interpretability in reinforcement learning (RL). To that aim, we distinguish interpretability (as a property of a model) and explainability (as a post-hoc operation, with the intervention of a proxy) and discuss them in the context of RL with an emphasis on the former notion. In particular, we argue that interpretable RL may embrace different facets: interpretable inputs, interpretable (transition/reward) models, and interpretable decision-making. Based on this scheme, we summarize and analyze recent work related to interpretable RL with an emphasis on papers published in the past 10 years. We also discuss briefly some related research areas and point to some potential promising research directions.\n\n\nA survey on interpretable reinforcement learning</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2112.13112v2</guid>
      <dc:creator>Claire Glanois, Paul Weng, Matthieu Zimmer, Dong Li, Tianpei Yang, Jianye Hao, Wulong Liu</dc:creator>
      <pubDate>Thu, 24 Feb 2022 10:32:08 GMT</pubDate>
    </item>
    <item>
      <title>SOLAR 10.7B: Scaling Large Language Models with Simple yet Effective Depth Up-Scaling</title>
      <link>http://arxiv.org/abs/2312.15166v3</link>
      <description>We introduce SOLAR 10.7B, a large language model (LLM) with 10.7 billion parameters, demonstrating superior performance in various natural language processing (NLP) tasks. Inspired by recent efforts to efficiently up-scale LLMs, we present a method for scaling LLMs called depth up-scaling (DUS), which encompasses depthwise scaling and continued pretraining. In contrast to other LLM up-scaling methods that use mixture-of-experts, DUS does not require complex changes to train and inference efficiently. We show experimentally that DUS is simple yet effective in scaling up high-performance LLMs from small ones. Building on the DUS model, we additionally present SOLAR 10.7B-Instruct, a variant fine-tuned for instruction-following capabilities, surpassing Mixtral-8x7B-Instruct. SOLAR 10.7B is publicly available under the Apache 2.0 license, promoting broad access and application in the LLM field.\n\n\nSolar 10.7 b: Scaling large language models with simple yet effective depth up-scaling</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2312.15166v3</guid>
      <dc:creator>Dahyun Kim, Chanjun Park, Sanghoon Kim, Wonsung Lee, Wonho Song, Yunsu Kim, Hyeonwoo Kim, Yungi Kim, Hyeonju Lee, Jihoo Kim, Changbae Ahn, Seonghoon Yang, Sukyung Lee, Hyunbyung Park, Gyoungjin Gim, Mikyoung Cha, Hwalsuk Lee, Sunghun Kim</dc:creator>
      <pubDate>Thu, 04 Apr 2024 01:53:38 GMT</pubDate>
    </item>
    <item>
      <title>A Minimaximalist Approach to Reinforcement Learning from Human Feedback</title>
      <link>http://arxiv.org/abs/2401.04056v2</link>
      <description>We present Self-Play Preference Optimization (SPO), an algorithm for reinforcement learning from human feedback. Our approach is minimalist in that it does not require training a reward model nor unstable adversarial training and is therefore rather simple to implement. Our approach is maximalist in that it provably handles non-Markovian, intransitive, and stochastic preferences while being robust to the compounding errors that plague offline approaches to sequential prediction. To achieve the preceding qualities, we build upon the concept of a Minimax Winner (MW), a notion of preference aggregation from the social choice theory literature that frames learning from preferences as a zero-sum game between two policies. By leveraging the symmetry of this game, we prove that rather than using the traditional technique of dueling two policies to compute the MW, we can simply have a single agent play against itself while maintaining strong convergence guarantees. Practically, this corresponds to sampling multiple trajectories from a policy, asking a preference or teacher model to compare them, and then using the proportion of wins as the reward for a particular trajectory. We demonstrate that on a suite of continuous control tasks, we are able to learn significantly more efficiently than reward-model based approaches while maintaining robustness to the intransitive and stochastic preferences that frequently occur in practice when aggregating human judgments.\n\n\nA minimaximalist approach to reinforcement learning from human feedback</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2401.04056v2</guid>
      <dc:creator>Gokul Swamy, Christoph Dann, Rahul Kidambi, Zhiwei Steven Wu, Alekh Agarwal</dc:creator>
      <pubDate>Thu, 13 Jun 2024 14:45:36 GMT</pubDate>
    </item>
    <item>
      <title>Contrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation</title>
      <link>http://arxiv.org/abs/2401.08417v4</link>
      <description>Moderate-sized large language models (LLMs) -- those with 7B or 13B parameters -- exhibit promising machine translation (MT) performance. However, even the top-performing 13B LLM-based translation models, like ALMA, does not match the performance of state-of-the-art conventional encoder-decoder translation models or larger-scale LLMs such as GPT-4. In this study, we bridge this performance gap. We first assess the shortcomings of supervised fine-tuning for LLMs in the MT task, emphasizing the quality issues present in the reference data, despite being human-generated. Then, in contrast to SFT which mimics reference translations, we introduce Contrastive Preference Optimization (CPO), a novel approach that trains models to avoid generating adequate but not perfect translations. Applying CPO to ALMA models with only 22K parallel sentences and 12M parameters yields significant improvements. The resulting model, called ALMA-R, can match or exceed the performance of the WMT competition winners and GPT-4 on WMT'21, WMT'22 and WMT'23 test datasets.\n\n\nContrastive preference optimization: Pushing the boundaries of llm performance in machine translation</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2401.08417v4</guid>
      <dc:creator>Haoran Xu, Amr Sharaf, Yunmo Chen, Weiting Tan, Lingfeng Shen, Benjamin Van Durme, Kenton Murray, Young Jin Kim</dc:creator>
      <pubDate>Mon, 03 Jun 2024 01:28:06 GMT</pubDate>
    </item>
    <item>
      <title>Direct Nash Optimization: Teaching Language Models to Self-Improve with General Preferences</title>
      <link>http://arxiv.org/abs/2404.03715v1</link>
      <description>This paper studies post-training large language models (LLMs) using preference feedback from a powerful oracle to help a model iteratively improve over itself. The typical approach for post-training LLMs involves Reinforcement Learning from Human Feedback (RLHF), which traditionally separates reward learning and subsequent policy optimization. However, such a reward maximization approach is limited by the nature of &quot;point-wise&quot; rewards (such as Bradley-Terry model), which fails to express complex intransitive or cyclic preference relations. While advances on RLHF show reward learning and policy optimization can be merged into a single contrastive objective for stability, they yet still remain tethered to the reward maximization framework. Recently, a new wave of research sidesteps the reward maximization presumptions in favor of directly optimizing over &quot;pair-wise&quot; or general preferences. In this paper, we introduce Direct Nash Optimization (DNO), a provable and scalable algorithm that marries the simplicity and stability of contrastive learning with theoretical generality from optimizing general preferences. Because DNO is a batched on-policy algorithm using a regression-based objective, its implementation is straightforward and efficient. Moreover, DNO enjoys monotonic improvement across iterations that help it improve even over a strong teacher (such as GPT-4). In our experiments, a resulting 7B parameter Orca-2.5 model aligned by DNO achieves the state-of-the-art win-rate against GPT-4-Turbo of 33% on AlpacaEval 2.0 (even after controlling for response length), an absolute gain of 26% (7% to 33%) over the initializing model. It outperforms models with far more parameters, including Mistral Large, Self-Rewarding LM (70B parameters), and older versions of GPT-4.\n\n\nDirect nash optimization: Teaching language models to self-improve with general preferences</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2404.03715v1</guid>
      <dc:creator>Corby Rosset, Ching-An Cheng, Arindam Mitra, Michael Santacroce, Ahmed Awadallah, Tengyang Xie</dc:creator>
      <pubDate>Thu, 04 Apr 2024 17:56:41 GMT</pubDate>
    </item>
    <item>
      <title>A Survey of Reinforcement Learning from Human Feedback</title>
      <link>http://arxiv.org/abs/2312.14925v2</link>
      <description>Reinforcement learning from human feedback (RLHF) is a variant of reinforcement learning (RL) that learns from human feedback instead of relying on an engineered reward function. Building on prior work on the related setting of preference-based reinforcement learning (PbRL), it stands at the intersection of artificial intelligence and human-computer interaction. This positioning offers a promising avenue to enhance the performance and adaptability of intelligent systems while also improving the alignment of their objectives with human values. The training of large language models (LLMs) has impressively demonstrated this potential in recent years, where RLHF played a decisive role in directing the model's capabilities toward human objectives. This article provides a comprehensive overview of the fundamentals of RLHF, exploring the intricate dynamics between RL agents and human input. While recent focus has been on RLHF for LLMs, our survey adopts a broader perspective, examining the diverse applications and wide-ranging impact of the technique. We delve into the core principles that underpin RLHF, shedding light on the symbiotic relationship between algorithms and human feedback, and discuss the main research trends in the field. By synthesizing the current landscape of RLHF research, this article aims to provide researchers as well as practitioners with a comprehensive understanding of this rapidly growing field of research.\n\n\nA survey of reinforcement learning from human feedback</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2312.14925v2</guid>
      <dc:creator>Timo Kaufmann, Paul Weng, Viktor Bengs, Eyke Hüllermeier</dc:creator>
      <pubDate>Tue, 30 Apr 2024 17:59:01 GMT</pubDate>
    </item>
    <item>
      <title>A Long Way to Go: Investigating Length Correlations in RLHF</title>
      <link>http://arxiv.org/abs/2310.03716v2</link>
      <description>Great success has been reported using Reinforcement Learning from Human Feedback (RLHF) to align large language models, with open preference datasets enabling wider experimentation, particularly for &quot;helpfulness&quot; in tasks like dialogue and web question answering. Alongside these improvements, however, RLHF also often drives models to produce longer outputs. This paper demonstrates, on three diverse settings, that optimizing for response length is, much more than previously thought, a significant factor behind RLHF. Studying the strategies RL optimization uses to maximize reward, we find improvements in reward to largely be driven by increasing response length, instead of other features. Indeed, we find that even a purely length-based reward reproduces most downstream RLHF improvements over supervised fine-tuned models. Testing a comprehensive set of length-countering interventions, we identify the dominant source of these biases to be reward models, which, by studying training dynamics, we find are non-robust and easily influenced by length biases in preference data.\n\n\nA long way to go: Investigating length correlations in rlhf</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2310.03716v2</guid>
      <dc:creator>Prasann Singhal, Tanya Goyal, Jiacheng Xu, Greg Durrett</dc:creator>
      <pubDate>Wed, 10 Jul 2024 23:15:49 GMT</pubDate>
    </item>
    <item>
      <title>Some things are more CRINGE than others: Iterative Preference Optimization with the Pairwise Cringe Loss</title>
      <link>http://arxiv.org/abs/2312.16682v2</link>
      <description>Practitioners commonly align large language models using pairwise preferences, i.e., given labels of the type response A is preferred to response B for a given input. Perhaps less commonly, methods have also been developed for binary feedback, i.e. training models given labels of type response A is good or bad. We show how an existing performant binary feedback method, the Cringe Loss (Adolphs et al., 2022), can be generalized to the pairwise preference setting using a simple soft margin extension. Pairwise Cringe Loss is straightforward to implement and efficient to train, and we find it outperforms state-of-the-art preference optimization algorithms such as PPO and DPO on the AlpacaFarm benchmark. We show that iterations of training of our model are important for improved results, and that we can generalize DPO to Iterative DPO in the same way.\n\n\nSome things are more cringe than others: Preference optimization with the pairwise cringe loss</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2312.16682v2</guid>
      <dc:creator>Jing Xu, Andrew Lee, Sainbayar Sukhbaatar, Jason Weston</dc:creator>
      <pubDate>Mon, 22 Apr 2024 22:51:32 GMT</pubDate>
    </item>
    <item>
      <title>ChatGPT's One-year Anniversary: Are Open-Source Large Language Models Catching up?</title>
      <link>http://arxiv.org/abs/2311.16989v4</link>
      <description>Upon its release in late 2022, ChatGPT has brought a seismic shift in the entire landscape of AI, both in research and commerce. Through instruction-tuning a large language model (LLM) with supervised fine-tuning and reinforcement learning from human feedback, it showed that a model could answer human questions and follow instructions on a broad panel of tasks. Following this success, interests in LLMs have intensified, with new LLMs flourishing at frequent interval across academia and industry, including many start-ups focused on LLMs. While closed-source LLMs (e.g., OpenAI's GPT, Anthropic's Claude) generally outperform their open-source counterparts, the progress on the latter has been rapid with claims of achieving parity or even better on certain tasks. This has crucial implications not only on research but also on business. In this work, on the first anniversary of ChatGPT, we provide an exhaustive overview of this success, surveying all tasks where an open-source LLM has claimed to be on par or better than ChatGPT.\n\n\nChatgpt's one-year anniversary: are open-source large language models catching up?</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2311.16989v4</guid>
      <dc:creator>Hailin Chen, Fangkai Jiao, Xingxuan Li, Chengwei Qin, Mathieu Ravaut, Ruochen Zhao, Caiming Xiong, Shafiq Joty</dc:creator>
      <pubDate>Mon, 15 Jan 2024 09:55:05 GMT</pubDate>
    </item>
    <item>
      <title>MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies</title>
      <link>http://arxiv.org/abs/2404.06395v3</link>
      <description>The burgeoning interest in developing Large Language Models (LLMs) with up to trillion parameters has been met with concerns regarding resource efficiency and practical expense, particularly given the immense cost of experimentation. This scenario underscores the importance of exploring the potential of Small Language Models (SLMs) as a resource-efficient alternative. In this context, we introduce MiniCPM, specifically the 1.2B and 2.4B non-embedding parameter variants, not only excel in their respective categories but also demonstrate capabilities on par with 7B-13B LLMs. While focusing on SLMs, our approach exhibits scalability in both model and data dimensions for future LLM research. Regarding model scaling, we employ extensive model wind tunnel experiments for stable and optimal scaling. For data scaling, we introduce a Warmup-Stable-Decay (WSD) learning rate scheduler (LRS), conducive to continuous training and domain adaptation. We present an in-depth analysis of the intriguing training dynamics that occurred in the WSD LRS. With WSD LRS, we are now able to efficiently study data-model scaling law without extensive retraining experiments on both axes of model and data, from which we derive the much higher compute optimal data-model ratio than Chinchilla Optimal. Additionally, we introduce MiniCPM family, including MiniCPM-DPO, MiniCPM-MoE and MiniCPM-128K, whose excellent performance further cementing MiniCPM's foundation in diverse SLM applications. MiniCPM models are available publicly at https://github.com/OpenBMB/MiniCPM .\n\n\nMinicpm: Unveiling the potential of small language models with scalable training strategies</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2404.06395v3</guid>
      <dc:creator>Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, Xinrong Zhang, Zheng Leng Thai, Kaihuo Zhang, Chongyi Wang, Yuan Yao, Chenyang Zhao, Jie Zhou, Jie Cai, Zhongwu Zhai, Ning Ding, Chao Jia, Guoyang Zeng, Dahai Li, Zhiyuan Liu, Maosong Sun</dc:creator>
      <pubDate>Mon, 03 Jun 2024 08:54:38 GMT</pubDate>
    </item>
    <item>
      <title>Direct Language Model Alignment from Online AI Feedback</title>
      <link>http://arxiv.org/abs/2402.04792v2</link>
      <description>Direct alignment from preferences (DAP) methods, such as DPO, have recently emerged as efficient alternatives to reinforcement learning from human feedback (RLHF), that do not require a separate reward model. However, the preference datasets used in DAP methods are usually collected ahead of training and never updated, thus the feedback is purely offline. Moreover, responses in these datasets are often sampled from a language model distinct from the one being aligned, and since the model evolves over training, the alignment phase is inevitably off-policy. In this study, we posit that online feedback is key and improves DAP methods. Our method, online AI feedback (OAIF), uses an LLM as annotator: on each training iteration, we sample two responses from the current model and prompt the LLM annotator to choose which one is preferred, thus providing online feedback. Despite its simplicity, we demonstrate via human evaluation in several tasks that OAIF outperforms both offline DAP and RLHF methods. We further show that the feedback leveraged in OAIF is easily controllable, via instruction prompts to the LLM annotator.\n\n\nDirect language model alignment from online ai feedback</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.04792v2</guid>
      <dc:creator>Shangmin Guo, Biao Zhang, Tianlin Liu, Tianqi Liu, Misha Khalman, Felipe Llinares, Alexandre Rame, Thomas Mesnard, Yao Zhao, Bilal Piot, Johan Ferret, Mathieu Blondel</dc:creator>
      <pubDate>Thu, 29 Feb 2024 20:59:17 GMT</pubDate>
    </item>
    <item>
      <title>SimPO: Simple Preference Optimization with a Reference-Free Reward</title>
      <link>http://arxiv.org/abs/2405.14734v2</link>
      <description>Direct Preference Optimization (DPO) is a widely used offline preference optimization algorithm that reparameterizes reward functions in reinforcement learning from human feedback (RLHF) to enhance simplicity and training stability. In this work, we propose SimPO, a simpler yet more effective approach. The effectiveness of SimPO is attributed to a key design: using the average log probability of a sequence as the implicit reward. This reward formulation better aligns with model generation and eliminates the need for a reference model, making it more compute and memory efficient. Additionally, we introduce a target reward margin to the Bradley-Terry objective to encourage a larger margin between the winning and losing responses, further enhancing the algorithm's performance. We compare SimPO to DPO and its latest variants across various state-of-the-art training setups, including both base and instruction-tuned models like Mistral and Llama3. We evaluated on extensive instruction-following benchmarks, including AlpacaEval 2, MT-Bench, and the recent challenging Arena-Hard benchmark. Our results demonstrate that SimPO consistently and significantly outperforms existing approaches without substantially increasing response length. Specifically, SimPO outperforms DPO by up to 6.4 points on AlpacaEval 2 and by up to 7.5 points on Arena-Hard. Our top-performing model, built on Llama3-8B-Instruct, achieves a remarkable 53.7 length-controlled win rate on AlpacaEval 2 -- surpassing Claude 3 Opus on the leaderboard, and a 36.5 win rate on Arena-Hard -- making it the strongest 8B open-source model.\n\n\nSimpo: Simple preference optimization with a reference-free reward</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.14734v2</guid>
      <dc:creator>Yu Meng, Mengzhou Xia, Danqi Chen</dc:creator>
      <pubDate>Mon, 08 Jul 2024 17:55:24 GMT</pubDate>
    </item>
    <item>
      <title>How to Protect Copyright Data in Optimization of Large Language Models?</title>
      <link>http://arxiv.org/abs/2308.12247v1</link>
      <description>Large language models (LLMs) and generative AI have played a transformative role in computer research and applications. Controversy has arisen as to whether these models output copyrighted data, which can occur if the data the models are trained on is copyrighted. LLMs are built on the transformer neural network architecture, which in turn relies on a mathematical computation called Attention that uses the softmax function.   In this paper, we show that large language model training and optimization can be seen as a softmax regression problem. We then establish a method of efficiently performing softmax regression, in a way that prevents the regression function from generating copyright data. This establishes a theoretical method of training large language models in a way that avoids generating copyright data.\n\n\nHow to Protect Copyright Data in Optimization of Large Language Models?</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2308.12247v1</guid>
      <dc:creator>Timothy Chu, Zhao Song, Chiwun Yang</dc:creator>
      <pubDate>Wed, 23 Aug 2023 16:48:04 GMT</pubDate>
    </item>
    <item>
      <title>The Wisdom of Hindsight Makes Language Models Better Instruction Followers</title>
      <link>http://arxiv.org/abs/2302.05206v1</link>
      <description>Reinforcement learning has seen wide success in finetuning large language models to better align with instructions via human feedback. The so-called algorithm, Reinforcement Learning with Human Feedback (RLHF) demonstrates impressive performance on the GPT series models. However, the underlying Reinforcement Learning (RL) algorithm is complex and requires an additional training pipeline for reward and value networks. In this paper, we consider an alternative approach: converting feedback to instruction by relabeling the original one and training the model for better alignment in a supervised manner. Such an algorithm doesn't require any additional parameters except for the original language model and maximally reuses the pretraining pipeline. To achieve this, we formulate instruction alignment problem for language models as a goal-reaching problem in decision making. We propose Hindsight Instruction Relabeling (HIR), a novel algorithm for aligning language models with instructions. The resulting two-stage algorithm shed light to a family of reward-free approaches that utilize the hindsightly relabeled instructions based on feedback. We evaluate the performance of HIR extensively on 12 challenging BigBench reasoning tasks and show that HIR outperforms the baseline algorithms and is comparable to or even surpasses supervised finetuning.\n\n\nMaking large language models better reasoners with alignment</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2302.05206v1</guid>
      <dc:creator>Tianjun Zhang, Fangchen Liu, Justin Wong, Pieter Abbeel, Joseph E. Gonzalez</dc:creator>
      <pubDate>Fri, 10 Feb 2023 12:16:38 GMT</pubDate>
    </item>
    <item>
      <title>Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank Modifications</title>
      <link>http://arxiv.org/abs/2402.05162v3</link>
      <description>Large language models (LLMs) show inherent brittleness in their safety mechanisms, as evidenced by their susceptibility to jailbreaking and even non-malicious fine-tuning. This study explores this brittleness of safety alignment by leveraging pruning and low-rank modifications. We develop methods to identify critical regions that are vital for safety guardrails, and that are disentangled from utility-relevant regions at both the neuron and rank levels. Surprisingly, the isolated regions we find are sparse, comprising about $3\%$ at the parameter level and $2.5\%$ at the rank level. Removing these regions compromises safety without significantly impacting utility, corroborating the inherent brittleness of the model's safety mechanisms. Moreover, we show that LLMs remain vulnerable to low-cost fine-tuning attacks even when modifications to the safety-critical regions are restricted. These findings underscore the urgent need for more robust safety strategies in LLMs.\n\n\nAssessing the brittleness of safety alignment via pruning and low-rank modifications</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.05162v3</guid>
      <dc:creator>Boyi Wei, Kaixuan Huang, Yangsibo Huang, Tinghao Xie, Xiangyu Qi, Mengzhou Xia, Prateek Mittal, Mengdi Wang, Peter Henderson</dc:creator>
      <pubDate>Mon, 01 Jul 2024 07:11:17 GMT</pubDate>
    </item>
    <item>
      <title>Beyond Text: Frozen Large Language Models in Visual Signal Comprehension</title>
      <link>http://arxiv.org/abs/2403.07874v1</link>
      <description>In this work, we investigate the potential of a large language model (LLM) to directly comprehend visual signals without the necessity of fine-tuning on multi-modal datasets. The foundational concept of our method views an image as a linguistic entity, and translates it to a set of discrete words derived from the LLM's vocabulary. To achieve this, we present the Vision-to-Language Tokenizer, abbreviated as V2T Tokenizer, which transforms an image into a ``foreign language'' with the combined aid of an encoder-decoder, the LLM vocabulary, and a CLIP model. With this innovative image encoding, the LLM gains the ability not only for visual comprehension but also for image denoising and restoration in an auto-regressive fashion-crucially, without any fine-tuning. We undertake rigorous experiments to validate our method, encompassing understanding tasks like image recognition, image captioning, and visual question answering, as well as image denoising tasks like inpainting, outpainting, deblurring, and shift restoration. Code and models are available at https://github.com/zh460045050/V2L-Tokenizer.\n\n\nBeyond text: Frozen large language models in visual signal comprehension</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2403.07874v1</guid>
      <dc:creator>Lei Zhu, Fangyun Wei, Yanye Lu</dc:creator>
      <pubDate>Tue, 12 Mar 2024 17:59:51 GMT</pubDate>
    </item>
    <item>
      <title>COGNET-MD, an evaluation framework and dataset for Large Language Model benchmarks in the medical domain</title>
      <link>http://arxiv.org/abs/2405.10893v1</link>
      <description>Large Language Models (LLMs) constitute a breakthrough state-of-the-art Artificial Intelligence (AI) technology which is rapidly evolving and promises to aid in medical diagnosis either by assisting doctors or by simulating a doctor's workflow in more advanced and complex implementations. In this technical paper, we outline Cognitive Network Evaluation Toolkit for Medical Domains (COGNET-MD), which constitutes a novel benchmark for LLM evaluation in the medical domain. Specifically, we propose a scoring-framework with increased difficulty to assess the ability of LLMs in interpreting medical text. The proposed framework is accompanied with a database of Multiple Choice Quizzes (MCQs). To ensure alignment with current medical trends and enhance safety, usefulness, and applicability, these MCQs have been constructed in collaboration with several associated medical experts in various medical domains and are characterized by varying degrees of difficulty. The current (first) version of the database includes the medical domains of Psychiatry, Dentistry, Pulmonology, Dermatology and Endocrinology, but it will be continuously extended and expanded to include additional medical domains.\n\n\nThe breakthrough of large language models release for medical applications: 1-year timeline and perspectives</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.10893v1</guid>
      <dc:creator>Dimitrios P. Panagoulias, Persephone Papatheodosiou, Anastasios P. Palamidas, Mattheos Sanoudos, Evridiki Tsoureli-Nikita, Maria Virvou, George A. Tsihrintzis</dc:creator>
      <pubDate>Fri, 17 May 2024 16:31:56 GMT</pubDate>
    </item>
    <item>
      <title>Receive, Reason, and React: Drive as You Say with Large Language Models in Autonomous Vehicles</title>
      <link>http://arxiv.org/abs/2310.08034v1</link>
      <description>The fusion of human-centric design and artificial intelligence (AI) capabilities has opened up new possibilities for next-generation autonomous vehicles that go beyond transportation. These vehicles can dynamically interact with passengers and adapt to their preferences. This paper proposes a novel framework that leverages Large Language Models (LLMs) to enhance the decision-making process in autonomous vehicles. By utilizing LLMs' linguistic and contextual understanding abilities with specialized tools, we aim to integrate the language and reasoning capabilities of LLMs into autonomous vehicles. Our research includes experiments in HighwayEnv, a collection of environments for autonomous driving and tactical decision-making tasks, to explore LLMs' interpretation, interaction, and reasoning in various scenarios. We also examine real-time personalization, demonstrating how LLMs can influence driving behaviors based on verbal commands. Our empirical results highlight the substantial advantages of utilizing chain-of-thought prompting, leading to improved driving decisions, and showing the potential for LLMs to enhance personalized driving experiences through ongoing verbal feedback. The proposed framework aims to transform autonomous vehicle operations, offering personalized support, transparent decision-making, and continuous learning to enhance safety and effectiveness. We achieve user-centric, transparent, and adaptive autonomous driving ecosystems supported by the integration of LLMs into autonomous vehicles.\n\n\nReceive, reason, and react: Drive as you say, with large language models in autonomous vehicles</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2310.08034v1</guid>
      <dc:creator>Can Cui, Yunsheng Ma, Xu Cao, Wenqian Ye, Ziran Wang</dc:creator>
      <pubDate>Thu, 12 Oct 2023 04:56:01 GMT</pubDate>
    </item>
    <item>
      <title>Large Language Models for Data Annotation: A Survey</title>
      <link>http://arxiv.org/abs/2402.13446v2</link>
      <description>Data annotation generally refers to the labeling or generating of raw data with relevant information, which could be used for improving the efficacy of machine learning models. The process, however, is labor-intensive and costly. The emergence of advanced Large Language Models (LLMs), exemplified by GPT-4, presents an unprecedented opportunity to automate the complicated process of data annotation. While existing surveys have extensively covered LLM architecture, training, and general applications, we uniquely focus on their specific utility for data annotation. This survey contributes to three core aspects: LLM-Based Annotation Generation, LLM-Generated Annotations Assessment, and LLM-Generated Annotations Utilization. Furthermore, this survey includes an in-depth taxonomy of data types that LLMs can annotate, a comprehensive review of learning strategies for models utilizing LLM-generated annotations, and a detailed discussion of the primary challenges and limitations associated with using LLMs for data annotation. Serving as a key guide, this survey aims to assist researchers and practitioners in exploring the potential of the latest LLMs for data annotation, thereby fostering future advancements in this critical field.\n\n\nLarge language models for data annotation: A survey</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.13446v2</guid>
      <dc:creator>Zhen Tan, Dawei Li, Song Wang, Alimohammad Beigi, Bohan Jiang, Amrita Bhattacharjee, Mansooreh Karami, Jundong Li, Lu Cheng, Huan Liu</dc:creator>
      <pubDate>Sun, 23 Jun 2024 21:51:45 GMT</pubDate>
    </item>
    <item>
      <title>Peering Through Preferences: Unraveling Feedback Acquisition for Aligning Large Language Models</title>
      <link>http://arxiv.org/abs/2308.15812v3</link>
      <description>Aligning large language models (LLMs) with human values and intents critically involves the use of human or AI feedback. While dense feedback annotations are expensive to acquire and integrate, sparse feedback presents a structural design choice between ratings (e.g., score Response A on a scale of 1-7) and rankings (e.g., is Response A better than Response B?). In this work, we analyze the effect of this design choice for the alignment and evaluation of LLMs. We uncover an inconsistency problem wherein the preferences inferred from ratings and rankings significantly disagree 60% for both human and AI annotators. Our subsequent analysis identifies various facets of annotator biases that explain this phenomena, such as human annotators would rate denser responses higher while preferring accuracy during pairwise judgments. To our surprise, we also observe that the choice of feedback protocol also has a significant effect on the evaluation of aligned LLMs. In particular, we find that LLMs that leverage rankings data for alignment (say model X) are preferred over those that leverage ratings data (say model Y), with a rank-based evaluation protocol (is X/Y's response better than reference response?) but not with a rating-based evaluation protocol (score Rank X/Y's response on a scale of 1-7). Our findings thus shed light on critical gaps in methods for evaluating the real-world utility of language models and their strong dependence on the feedback protocol used for alignment. Our code and data are available at https://github.com/Hritikbansal/sparse_feedback.\n\n\nPeering through preferences: Unraveling feedback acquisition for aligning large language models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2308.15812v3</guid>
      <dc:creator>Hritik Bansal, John Dang, Aditya Grover</dc:creator>
      <pubDate>Mon, 05 Feb 2024 19:59:46 GMT</pubDate>
    </item>
    <item>
      <title>Fast High-Resolution Image Synthesis with Latent Adversarial Diffusion Distillation</title>
      <link>http://arxiv.org/abs/2403.12015v1</link>
      <description>Diffusion models are the main driver of progress in image and video synthesis, but suffer from slow inference speed. Distillation methods, like the recently introduced adversarial diffusion distillation (ADD) aim to shift the model from many-shot to single-step inference, albeit at the cost of expensive and difficult optimization due to its reliance on a fixed pretrained DINOv2 discriminator. We introduce Latent Adversarial Diffusion Distillation (LADD), a novel distillation approach overcoming the limitations of ADD. In contrast to pixel-based ADD, LADD utilizes generative features from pretrained latent diffusion models. This approach simplifies training and enhances performance, enabling high-resolution multi-aspect ratio image synthesis. We apply LADD to Stable Diffusion 3 (8B) to obtain SD3-Turbo, a fast model that matches the performance of state-of-the-art text-to-image generators using only four unguided sampling steps. Moreover, we systematically investigate its scaling behavior and demonstrate LADD's effectiveness in various applications such as image editing and inpainting.\n\n\nFast high-resolution image synthesis with latent adversarial diffusion distillation</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2403.12015v1</guid>
      <dc:creator>Axel Sauer, Frederic Boesel, Tim Dockhorn, Andreas Blattmann, Patrick Esser, Robin Rombach</dc:creator>
      <pubDate>Mon, 18 Mar 2024 17:51:43 GMT</pubDate>
    </item>
    <item>
      <title>Defending Large Language Models Against Jailbreaking Attacks Through Goal Prioritization</title>
      <link>http://arxiv.org/abs/2311.09096v2</link>
      <description>While significant attention has been dedicated to exploiting weaknesses in LLMs through jailbreaking attacks, there remains a paucity of effort in defending against these attacks. We point out a pivotal factor contributing to the success of jailbreaks: the intrinsic conflict between the goals of being helpful and ensuring safety. Accordingly, we propose to integrate goal prioritization at both training and inference stages to counteract. Implementing goal prioritization during inference substantially diminishes the Attack Success Rate (ASR) of jailbreaking from 66.4% to 3.6% for ChatGPT. And integrating goal prioritization into model training reduces the ASR from 71.0% to 6.6% for Llama2-13B. Remarkably, even in scenarios where no jailbreaking samples are included during training, our approach slashes the ASR by half. Additionally, our findings reveal that while stronger LLMs face greater safety risks, they also possess a greater capacity to be steered towards defending against such attacks, both because of their stronger ability in instruction following. Our work thus contributes to the comprehension of jailbreaking attacks and defenses, and sheds light on the relationship between LLMs' capability and safety. Our code is available at \url{https://github.com/thu-coai/JailbreakDefense_GoalPriority}.\n\n\nDefending large language models against jailbreaking attacks through goal prioritization</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2311.09096v2</guid>
      <dc:creator>Zhexin Zhang, Junxiao Yang, Pei Ke, Fei Mi, Hongning Wang, Minlie Huang</dc:creator>
      <pubDate>Wed, 12 Jun 2024 08:28:15 GMT</pubDate>
    </item>
    <item>
      <title>ARM: Efficient Guided Decoding with Autoregressive Reward Models</title>
      <link>http://arxiv.org/abs/2407.04615v1</link>
      <description>Language models trained on large amounts of data require careful tuning to be safely deployed in real world. We revisit the guided decoding paradigm, where the goal is to augment the logits of the base language model using the scores from a task-specific reward model. We propose a simple but efficient parameterization of the autoregressive reward model enabling fast and effective guided decoding. On detoxification and sentiment control tasks, we show that our efficient parameterization performs on par with RAD, a strong but less efficient guided decoding approach.\n\n\nAlphazero-like tree-search can guide large language model decoding and training</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2407.04615v1</guid>
      <dc:creator>Sergey Troshin, Vlad Niculae, Antske Fokkens</dc:creator>
      <pubDate>Fri, 05 Jul 2024 16:11:03 GMT</pubDate>
    </item>
    <item>
      <title>From Instructions to Intrinsic Human Values -- A Survey of Alignment Goals for Big Models</title>
      <link>http://arxiv.org/abs/2308.12014v2</link>
      <description>Big models, exemplified by Large Language Models (LLMs), are models typically pre-trained on massive data and comprised of enormous parameters, which not only obtain significantly improved performance across diverse tasks but also present emergent capabilities absent in smaller models. However, the growing intertwining of big models with everyday human lives poses potential risks and might cause serious social harm. Therefore, many efforts have been made to align LLMs with humans to make them better follow user instructions and satisfy human preferences. Nevertheless, `what to align with' has not been fully discussed, and inappropriate alignment goals might even backfire. In this paper, we conduct a comprehensive survey of different alignment goals in existing work and trace their evolution paths to help identify the most essential goal. Particularly, we investigate related works from two perspectives: the definition of alignment goals and alignment evaluation. Our analysis encompasses three distinct levels of alignment goals and reveals a goal transformation from fundamental abilities to value orientation, indicating the potential of intrinsic human values as the alignment goal for enhanced LLMs. Based on such results, we further discuss the challenges of achieving such intrinsic value alignment and provide a collection of available resources for future research on the alignment of big models.\n\n\nFrom Instructions to Intrinsic Human Values--A Survey of Alignment Goals for Big Models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2308.12014v2</guid>
      <dc:creator>Jing Yao, Xiaoyuan Yi, Xiting Wang, Jindong Wang, Xing Xie</dc:creator>
      <pubDate>Mon, 04 Sep 2023 03:32:05 GMT</pubDate>
    </item>
    <item>
      <title>SGHateCheck: Functional Tests for Detecting Hate Speech in Low-Resource Languages of Singapore</title>
      <link>http://arxiv.org/abs/2405.01842v1</link>
      <description>To address the limitations of current hate speech detection models, we introduce \textsf{SGHateCheck}, a novel framework designed for the linguistic and cultural context of Singapore and Southeast Asia. It extends the functional testing approach of HateCheck and MHC, employing large language models for translation and paraphrasing into Singapore's main languages, and refining these with native annotators. \textsf{SGHateCheck} reveals critical flaws in state-of-the-art models, highlighting their inadequacy in sensitive content moderation. This work aims to foster the development of more effective hate speech detection tools for diverse linguistic environments, particularly for Singapore and Southeast Asia contexts.\n\n\nSeaLLMs--Large Language Models for Southeast Asia</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.01842v1</guid>
      <dc:creator>Ri Chi Ng, Nirmalendu Prakash, Ming Shan Hee, Kenny Tsu Wei Choo, Roy Ka-Wei Lee</dc:creator>
      <pubDate>Fri, 03 May 2024 04:18:10 GMT</pubDate>
    </item>
    <item>
      <title>Gibbs Sampling with People</title>
      <link>http://arxiv.org/abs/2008.02595v2</link>
      <description>A core problem in cognitive science and machine learning is to understand how humans derive semantic representations from perceptual objects, such as color from an apple, pleasantness from a musical chord, or seriousness from a face. Markov Chain Monte Carlo with People (MCMCP) is a prominent method for studying such representations, in which participants are presented with binary choice trials constructed such that the decisions follow a Markov Chain Monte Carlo acceptance rule. However, while MCMCP has strong asymptotic properties, its binary choice paradigm generates relatively little information per trial, and its local proposal function makes it slow to explore the parameter space and find the modes of the distribution. Here we therefore generalize MCMCP to a continuous-sampling paradigm, where in each iteration the participant uses a slider to continuously manipulate a single stimulus dimension to optimize a given criterion such as 'pleasantness'. We formulate both methods from a utility-theory perspective, and show that the new method can be interpreted as 'Gibbs Sampling with People' (GSP). Further, we introduce an aggregation parameter to the transition step, and show that this parameter can be manipulated to flexibly shift between Gibbs sampling and deterministic optimization. In an initial study, we show GSP clearly outperforming MCMCP; we then show that GSP provides novel and interpretable results in three other domains, namely musical chords, vocal emotions, and faces. We validate these results through large-scale perceptual rating experiments. The final experiments use GSP to navigate the latent space of a state-of-the-art image synthesis network (StyleGAN), a promising approach for applying GSP to high-dimensional perceptual spaces. We conclude by discussing future cognitive applications and ethical implications.\n\n\nGibbs sampling from human feedback: A provable kl-constrained framework for rlhf</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2008.02595v2</guid>
      <dc:creator>Peter M. C. Harrison, Raja Marjieh, Federico Adolfi, Pol van Rijn, Manuel Anglada-Tort, Ofer Tchernichovski, Pauline Larrouy-Maestri, Nori Jacoby</dc:creator>
      <pubDate>Mon, 02 Nov 2020 16:55:40 GMT</pubDate>
    </item>
    <item>
      <title>Aya Model: An Instruction Finetuned Open-Access Multilingual Language Model</title>
      <link>http://arxiv.org/abs/2402.07827v1</link>
      <description>Recent breakthroughs in large language models (LLMs) have centered around a handful of data-rich languages. What does it take to broaden access to breakthroughs beyond first-class citizen languages? Our work introduces Aya, a massively multilingual generative language model that follows instructions in 101 languages of which over 50% are considered as lower-resourced. Aya outperforms mT0 and BLOOMZ on the majority of tasks while covering double the number of languages. We introduce extensive new evaluation suites that broaden the state-of-art for multilingual eval across 99 languages -- including discriminative and generative tasks, human evaluation, and simulated win rates that cover both held-out tasks and in-distribution performance. Furthermore, we conduct detailed investigations on the optimal finetuning mixture composition, data pruning, as well as the toxicity, bias, and safety of our models. We open-source our instruction datasets and our model at https://hf.co/CohereForAI/aya-101\n\n\nAya model: An instruction finetuned open-access multilingual language model</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.07827v1</guid>
      <dc:creator>Ahmet Üstün, Viraat Aryabumi, Zheng-Xin Yong, Wei-Yin Ko, Daniel D'souza, Gbemileke Onilude, Neel Bhandari, Shivalika Singh, Hui-Lee Ooi, Amr Kayid, Freddie Vargus, Phil Blunsom, Shayne Longpre, Niklas Muennighoff, Marzieh Fadaee, Julia Kreutzer, Sara Hooker</dc:creator>
      <pubDate>Mon, 12 Feb 2024 17:34:13 GMT</pubDate>
    </item>
    <item>
      <title>Motif: Intrinsic Motivation from Artificial Intelligence Feedback</title>
      <link>http://arxiv.org/abs/2310.00166v1</link>
      <description>Exploring rich environments and evaluating one's actions without prior knowledge is immensely challenging. In this paper, we propose Motif, a general method to interface such prior knowledge from a Large Language Model (LLM) with an agent. Motif is based on the idea of grounding LLMs for decision-making without requiring them to interact with the environment: it elicits preferences from an LLM over pairs of captions to construct an intrinsic reward, which is then used to train agents with reinforcement learning. We evaluate Motif's performance and behavior on the challenging, open-ended and procedurally-generated NetHack game. Surprisingly, by only learning to maximize its intrinsic reward, Motif achieves a higher game score than an algorithm directly trained to maximize the score itself. When combining Motif's intrinsic reward with the environment reward, our method significantly outperforms existing approaches and makes progress on tasks where no advancements have ever been made without demonstrations. Finally, we show that Motif mostly generates intuitive human-aligned behaviors which can be steered easily through prompt modifications, while scaling well with the LLM size and the amount of information given in the prompt.\n\n\nMotif: Intrinsic motivation from artificial intelligence feedback</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2310.00166v1</guid>
      <dc:creator>Martin Klissarov, Pierluca D'Oro, Shagun Sodhani, Roberta Raileanu, Pierre-Luc Bacon, Pascal Vincent, Amy Zhang, Mikael Henaff</dc:creator>
      <pubDate>Fri, 29 Sep 2023 22:10:01 GMT</pubDate>
    </item>
    <item>
      <title>RAFT: Adapting Language Model to Domain Specific RAG</title>
      <link>http://arxiv.org/abs/2403.10131v2</link>
      <description>Pretraining Large Language Models (LLMs) on large corpora of textual data is now a standard paradigm. When using these LLMs for many downstream applications, it is common to additionally bake in new knowledge (e.g., time-critical news, or private domain knowledge) into the pretrained model either through RAG-based-prompting, or fine-tuning. However, the optimal methodology for the model to gain such new knowledge remains an open question. In this paper, we present Retrieval Augmented FineTuning (RAFT), a training recipe that improves the model's ability to answer questions in a &quot;open-book&quot; in-domain settings. In RAFT, given a question, and a set of retrieved documents, we train the model to ignore those documents that don't help in answering the question, which we call, distractor documents. RAFT accomplishes this by citing verbatim the right sequence from the relevant document that would help answer the question. This coupled with RAFT's chain-of-thought-style response helps improve the model's ability to reason. In domain-specific RAG, RAFT consistently improves the model's performance across PubMed, HotpotQA, and Gorilla datasets, presenting a post-training recipe to improve pre-trained LLMs to in-domain RAG. RAFT's code and demo are open-sourced at github.com/ShishirPatil/gorilla.\n\n\nRaft: Adapting language model to domain specific rag</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2403.10131v2</guid>
      <dc:creator>Tianjun Zhang, Shishir G. Patil, Naman Jain, Sheng Shen, Matei Zaharia, Ion Stoica, Joseph E. Gonzalez</dc:creator>
      <pubDate>Wed, 05 Jun 2024 17:27:51 GMT</pubDate>
    </item>
    <item>
      <title>Group Preference Optimization: Few-Shot Alignment of Large Language Models</title>
      <link>http://arxiv.org/abs/2310.11523v1</link>
      <description>Many applications of large language models (LLMs), ranging from chatbots to creative writing, require nuanced subjective judgments that can differ significantly across different groups. Existing alignment algorithms can be expensive to align for each group, requiring prohibitive amounts of group-specific preference data and computation for real-world use cases. We introduce Group Preference Optimization (GPO), an alignment framework that steers language models to preferences of individual groups in a few-shot manner. In GPO, we augment the base LLM with an independent transformer module trained to predict the preferences of a group for the LLM generations. For few-shot learning, we parameterize this module as an in-context autoregressive transformer and train it via meta-learning on several groups. We empirically validate the efficacy of GPO through rigorous evaluations using LLMs with varied sizes on three human opinion adaptation tasks. These tasks involve adapting to the preferences of US demographic groups, global countries, and individual users. Our results demonstrate that GPO not only aligns models more accurately but also requires fewer group-specific preferences, and less training and inference computing resources, outperforming existing strategies such as in-context steering and fine-tuning methods.\n\n\nGroup preference optimization: Few-shot alignment of large language models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2310.11523v1</guid>
      <dc:creator>Siyan Zhao, John Dang, Aditya Grover</dc:creator>
      <pubDate>Tue, 17 Oct 2023 18:41:57 GMT</pubDate>
    </item>
    <item>
      <title>Vanishing Gradients in Reinforcement Finetuning of Language Models</title>
      <link>http://arxiv.org/abs/2310.20703v3</link>
      <description>Pretrained language models are commonly aligned with human preferences and downstream tasks via reinforcement finetuning (RFT), which refers to maximizing a (possibly learned) reward function using policy gradient algorithms. This work identifies a fundamental optimization obstacle in RFT: we prove that the expected gradient for an input vanishes when its reward standard deviation under the model is small, even if the expected reward is far from optimal. Through experiments on an RFT benchmark and controlled environments, as well as a theoretical analysis, we then demonstrate that vanishing gradients due to small reward standard deviation are prevalent and detrimental, leading to extremely slow reward maximization. Lastly, we explore ways to overcome vanishing gradients in RFT. We find the common practice of an initial supervised finetuning (SFT) phase to be the most promising candidate, which sheds light on its importance in an RFT pipeline. Moreover, we show that a relatively small number of SFT optimization steps on as few as 1% of the input samples can suffice, indicating that the initial SFT phase need not be expensive in terms of compute and data labeling efforts. Overall, our results emphasize that being mindful for inputs whose expected gradient vanishes, as measured by the reward standard deviation, is crucial for successful execution of RFT.\n\n\nFine-tuning language models with advantage-induced policy alignment</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2310.20703v3</guid>
      <dc:creator>Noam Razin, Hattie Zhou, Omid Saremi, Vimal Thilak, Arwen Bradley, Preetum Nakkiran, Joshua Susskind, Etai Littwin</dc:creator>
      <pubDate>Thu, 14 Mar 2024 08:05:18 GMT</pubDate>
    </item>
    <item>
      <title>Robust Prompt Optimization for Defending Language Models Against Jailbreaking Attacks</title>
      <link>http://arxiv.org/abs/2401.17263v4</link>
      <description>Despite advances in AI alignment, large language models (LLMs) remain vulnerable to adversarial attacks or jailbreaking, in which adversaries can modify prompts to induce unwanted behavior. While some defenses have been proposed, they have not been adapted to newly proposed attacks and more challenging threat models. To address this, we propose an optimization-based objective for defending LLMs against jailbreaking attacks and an algorithm, Robust Prompt Optimization (RPO) to create robust system-level defenses. Our approach directly incorporates the adversary into the defensive objective and optimizes a lightweight and transferable suffix, enabling RPO to adapt to worst-case adaptive attacks. Our theoretical and experimental results show improved robustness to both jailbreaks seen during optimization and unknown jailbreaks, reducing the attack success rate (ASR) on GPT-4 to 6% and Llama-2 to 0% on JailbreakBench, setting the state-of-the-art. Code can be found at https://github.com/lapisrocks/rpo\n\n\nRobust prompt optimization for defending language models against jailbreaking attacks</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2401.17263v4</guid>
      <dc:creator>Andy Zhou, Bo Li, Haohan Wang</dc:creator>
      <pubDate>Mon, 08 Jul 2024 20:33:36 GMT</pubDate>
    </item>
    <item>
      <title>Beyond Reverse KL: Generalizing Direct Preference Optimization with Diverse Divergence Constraints</title>
      <link>http://arxiv.org/abs/2309.16240v1</link>
      <description>The increasing capabilities of large language models (LLMs) raise opportunities for artificial general intelligence but concurrently amplify safety concerns, such as potential misuse of AI systems, necessitating effective AI alignment. Reinforcement Learning from Human Feedback (RLHF) has emerged as a promising pathway towards AI alignment but brings forth challenges due to its complexity and dependence on a separate reward model. Direct Preference Optimization (DPO) has been proposed as an alternative, and it remains equivalent to RLHF under the reverse KL regularization constraint. This paper presents $f$-DPO, a generalized approach to DPO by incorporating diverse divergence constraints. We show that under certain $f$-divergences, including Jensen-Shannon divergence, forward KL divergences and $\alpha$-divergences, the complex relationship between the reward and optimal policy can also be simplified by addressing the Karush-Kuhn-Tucker conditions. This eliminates the need for estimating the normalizing constant in the Bradley-Terry model and enables a tractable mapping between the reward function and the optimal policy. Our approach optimizes LLMs to align with human preferences in a more efficient and supervised manner under a broad set of divergence constraints. Empirically, adopting these divergences ensures a balance between alignment performance and generation diversity. Importantly, $f$-DPO outperforms PPO-based methods in divergence efficiency, and divergence constraints directly influence expected calibration error (ECE).\n\n\nBeyond reverse kl: Generalizing direct preference optimization with diverse divergence constraints</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2309.16240v1</guid>
      <dc:creator>Chaoqi Wang, Yibo Jiang, Chenghao Yang, Han Liu, Yuxin Chen</dc:creator>
      <pubDate>Thu, 28 Sep 2023 08:29:44 GMT</pubDate>
    </item>
    <item>
      <title>Learning and Forgetting Unsafe Examples in Large Language Models</title>
      <link>http://arxiv.org/abs/2312.12736v2</link>
      <description>As the number of large language models (LLMs) released to the public grows, there is a pressing need to understand the safety implications associated with these models learning from third-party custom finetuning data. We explore the behavior of LLMs finetuned on noisy custom data containing unsafe content, represented by datasets that contain biases, toxicity, and harmfulness, finding that while aligned LLMs can readily learn this unsafe content, they also tend to forget it more significantly than other examples when subsequently finetuned on safer content. Drawing inspiration from the discrepancies in forgetting, we introduce the &quot;ForgetFilter&quot; algorithm, which filters unsafe data based on how strong the model's forgetting signal is for that data. We demonstrate that the ForgetFilter algorithm ensures safety in customized finetuning without compromising downstream task performance, unlike sequential safety finetuning. ForgetFilter outperforms alternative strategies like replay and moral self-correction in curbing LLMs' ability to assimilate unsafe content during custom finetuning, e.g. 75% lower than not applying any safety measures and 62% lower than using self-correction in toxicity score.\n\n\nLearning and forgetting unsafe examples in large language models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2312.12736v2</guid>
      <dc:creator>Jiachen Zhao, Zhun Deng, David Madras, James Zou, Mengye Ren</dc:creator>
      <pubDate>Wed, 03 Jul 2024 06:13:31 GMT</pubDate>
    </item>
    <item>
      <title>Generalized Preference Optimization: A Unified Approach to Offline Alignment</title>
      <link>http://arxiv.org/abs/2402.05749v2</link>
      <description>Offline preference optimization allows fine-tuning large models directly from offline data, and has proved effective in recent alignment practices. We propose generalized preference optimization (GPO), a family of offline losses parameterized by a general class of convex functions. GPO enables a unified view over preference optimization, encompassing existing algorithms such as DPO, IPO and SLiC as special cases, while naturally introducing new variants. The GPO framework also sheds light on how offline algorithms enforce regularization, through the design of the convex function that defines the loss. Our analysis and experiments reveal the connections and subtle differences between the offline regularization and the KL divergence regularization intended by the canonical RLHF formulation. In a controlled setting akin to Gao et al 2023, we also show that different GPO variants achieve similar trade-offs between regularization and performance, though the optimal values of hyper-parameter might differ as predicted by theory. In all, our results present new algorithmic toolkits and empirical insights to alignment practitioners.\n\n\nGeneralized preference optimization: A unified approach to offline alignment</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.05749v2</guid>
      <dc:creator>Yunhao Tang, Zhaohan Daniel Guo, Zeyu Zheng, Daniele Calandriello, Rémi Munos, Mark Rowland, Pierre Harvey Richemond, Michal Valko, Bernardo Ávila Pires, Bilal Piot</dc:creator>
      <pubDate>Tue, 28 May 2024 23:25:15 GMT</pubDate>
    </item>
    <item>
      <title>Probing Multimodal LLMs as World Models for Driving</title>
      <link>http://arxiv.org/abs/2405.05956v1</link>
      <description>We provide a sober look at the application of Multimodal Large Language Models (MLLMs) within the domain of autonomous driving and challenge/verify some common assumptions, focusing on their ability to reason and interpret dynamic driving scenarios through sequences of images/frames in a closed-loop control environment. Despite the significant advancements in MLLMs like GPT-4V, their performance in complex, dynamic driving environments remains largely untested and presents a wide area of exploration. We conduct a comprehensive experimental study to evaluate the capability of various MLLMs as world models for driving from the perspective of a fixed in-car camera. Our findings reveal that, while these models proficiently interpret individual images, they struggle significantly with synthesizing coherent narratives or logical sequences across frames depicting dynamic behavior. The experiments demonstrate considerable inaccuracies in predicting (i) basic vehicle dynamics (forward/backward, acceleration/deceleration, turning right or left), (ii) interactions with other road actors (e.g., identifying speeding cars or heavy traffic), (iii) trajectory planning, and (iv) open-set dynamic scene reasoning, suggesting biases in the models' training data. To enable this experimental study we introduce a specialized simulator, DriveSim, designed to generate diverse driving scenarios, providing a platform for evaluating MLLMs in the realms of driving. Additionally, we contribute the full open-source code and a new dataset, &quot;Eval-LLM-Drive&quot;, for evaluating MLLMs in driving. Our results highlight a critical gap in the current capabilities of state-of-the-art MLLMs, underscoring the need for enhanced foundation models to improve their applicability in real-world dynamic environments.\n\n\nLLM multimodal traffic accident forecasting</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.05956v1</guid>
      <dc:creator>Shiva Sreeram, Tsun-Hsuan Wang, Alaa Maalouf, Guy Rosman, Sertac Karaman, Daniela Rus</dc:creator>
      <pubDate>Thu, 09 May 2024 17:52:42 GMT</pubDate>
    </item>
    <item>
      <title>JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models</title>
      <link>http://arxiv.org/abs/2404.01318v4</link>
      <description>Jailbreak attacks cause large language models (LLMs) to generate harmful, unethical, or otherwise objectionable content. Evaluating these attacks presents a number of challenges, which the current collection of benchmarks and evaluation techniques do not adequately address. First, there is no clear standard of practice regarding jailbreaking evaluation. Second, existing works compute costs and success rates in incomparable ways. And third, numerous works are not reproducible, as they withhold adversarial prompts, involve closed-source code, or rely on evolving proprietary APIs. To address these challenges, we introduce JailbreakBench, an open-sourced benchmark with the following components: (1) an evolving repository of state-of-the-art adversarial prompts, which we refer to as jailbreak artifacts; (2) a jailbreaking dataset comprising 100 behaviors -- both original and sourced from prior work (Zou et al., 2023; Mazeika et al., 2023, 2024) -- which align with OpenAI's usage policies; (3) a standardized evaluation framework at https://github.com/JailbreakBench/jailbreakbench that includes a clearly defined threat model, system prompts, chat templates, and scoring functions; and (4) a leaderboard at https://jailbreakbench.github.io/ that tracks the performance of attacks and defenses for various LLMs. We have carefully considered the potential ethical implications of releasing this benchmark, and believe that it will be a net positive for the community.\n\n\nJailbreakbench: An open robustness benchmark for jailbreaking large language models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2404.01318v4</guid>
      <dc:creator>Patrick Chao, Edoardo Debenedetti, Alexander Robey, Maksym Andriushchenko, Francesco Croce, Vikash Sehwag, Edgar Dobriban, Nicolas Flammarion, George J. Pappas, Florian Tramer, Hamed Hassani, Eric Wong</dc:creator>
      <pubDate>Tue, 16 Jul 2024 16:15:10 GMT</pubDate>
    </item>
    <item>
      <title>Pairwise Proximal Policy Optimization: Harnessing Relative Feedback for LLM Alignment</title>
      <link>http://arxiv.org/abs/2310.00212v3</link>
      <description>Large Language Models (LLMs) can acquire extensive world knowledge through pre-training on large corpora. However, due to exposure to low-quality data, LLMs may exhibit harmful behavior without aligning with human values. The dominant approach for steering LLMs towards beneficial behavior involves Reinforcement Learning with Human Feedback (RLHF), with Proximal Policy Optimization (PPO) serving as the default RL optimizer. Despite its effectiveness, PPO has limitations when optimizing rewards trained from comparison-based loss. Primarily, PPO is not invariant to equivalent reward functions containing identical preference information due to the need to calibrate the reward scale. Additionally, PPO's necessity for token-wise updates introduces complexity in both function approximation and algorithm design compared to trajectory-wise optimization. This paper proposes a new framework, reinforcement learning with relative feedback, and a novel trajectory-wise policy gradient algorithm, Pairwise Proximal Policy Optimization (P3O) that operates directly on comparative rewards. We show theoretically that P3O is invariant to equivalent rewards and avoids the complexity of PPO. Empirical evaluations demonstrate that P3O outperforms PPO in the KL-Reward trade-off and can align with human preferences as well as or better than prior methods. In summary, this work introduces a simpler yet effective approach for aligning LLMs to human preferences through relative feedback.\n\n\nPairwise proximal policy optimization: Harnessing relative feedback for llm alignment</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2310.00212v3</guid>
      <dc:creator>Tianhao Wu, Banghua Zhu, Ruoyu Zhang, Zhaojin Wen, Kannan Ramchandran, Jiantao Jiao</dc:creator>
      <pubDate>Tue, 10 Oct 2023 02:32:08 GMT</pubDate>
    </item>
    <item>
      <title>Silkie: Preference Distillation for Large Visual Language Models</title>
      <link>http://arxiv.org/abs/2312.10665v1</link>
      <description>This paper explores preference distillation for large vision language models (LVLMs), improving their ability to generate helpful and faithful responses anchoring the visual context. We first build a vision-language feedback (VLFeedback) dataset utilizing AI annotation. Specifically, responses are generated by models sampled from 12 LVLMs, conditioned on multi-modal instructions sourced from various datasets. We adopt GPT-4V to assess the generated outputs regarding helpfulness, visual faithfulness, and ethical considerations. Furthermore, the preference supervision is distilled into Qwen-VL-Chat through the direct preference optimization (DPO) method. The resulting model Silkie, achieves 6.9% and 9.5% relative improvement on the MME benchmark regarding the perception and cognition capabilities, respectively. Silkie also demonstrates reduced hallucination by setting a new state-of-the-art score of 3.02 on the MMHal-Bench benchmark. Further analysis shows that DPO with our VLFeedback dataset mainly boosts the fine-grained perception and complex cognition abilities of LVLMs, leading to more comprehensive improvements compared to human-annotated preference datasets.\n\n\nSilkie: Preference distillation for large visual language models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2312.10665v1</guid>
      <dc:creator>Lei Li, Zhihui Xie, Mukai Li, Shunian Chen, Peiyi Wang, Liang Chen, Yazheng Yang, Benyou Wang, Lingpeng Kong</dc:creator>
      <pubDate>Sun, 17 Dec 2023 09:44:27 GMT</pubDate>
    </item>
    <item>
      <title>Take the essence and discard the dross: A Rethinking on Data Selection for Fine-Tuning Large Language Models</title>
      <link>http://arxiv.org/abs/2406.14115v1</link>
      <description>Data selection for fine-tuning Large Language Models (LLMs) aims to select a high-quality subset from a given candidate dataset to train a Pending Fine-tune Model (PFM) into a Selective-Enhanced Model (SEM). It can improve the model performance and accelerate the training process. Although a few surveys have investigated related works of data selection, there is a lack of comprehensive comparison between existing methods due to their various experimental settings. To address this issue, we first propose a three-stage scheme for data selection and comprehensively review existing works according to this scheme. Then, we design a unified comparing method with ratio-based efficiency indicators and ranking-based feasibility indicators to overcome the difficulty of comparing various models with diverse experimental settings. After an in-depth comparative analysis, we find that the more targeted method with data-specific and model-specific quality labels has higher efficiency, but the introduction of additional noise information should be avoided when designing selection algorithms. Finally, we summarize the trends in data selection and highlight the short-term and long-term challenges to guide future research.\n\n\nA survey on data selection for language models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.14115v1</guid>
      <dc:creator>Ziche Liu, Rui Ke, Feng Jiang, Haizhou Li</dc:creator>
      <pubDate>Thu, 20 Jun 2024 08:58:58 GMT</pubDate>
    </item>
    <item>
      <title>Critic-Guided Decoding for Controlled Text Generation</title>
      <link>http://arxiv.org/abs/2212.10938v1</link>
      <description>Steering language generation towards objectives or away from undesired content has been a long-standing goal in utilizing language models (LM). Recent work has demonstrated reinforcement learning and weighted decoding as effective approaches to achieve a higher level of language control and quality with pros and cons. In this work, we propose a novel critic decoding method for controlled language generation (CriticControl) that combines the strengths of reinforcement learning and weighted decoding. Specifically, we adopt the actor-critic framework to train an LM-steering critic from non-differentiable reward models. And similar to weighted decoding, our method freezes the language model and manipulates the output token distribution using called critic, improving training efficiency and stability. Evaluation of our method on three controlled generation tasks, namely topic control, sentiment control, and detoxification, shows that our approach generates more coherent and well-controlled texts than previous methods. In addition, CriticControl demonstrates superior generalization ability in zero-shot settings. Human evaluation studies also corroborate our findings.\n\n\nControlled decoding from language models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2212.10938v1</guid>
      <dc:creator>Minbeom Kim, Hwanhee Lee, Kang Min Yoo, Joonsuk Park, Hwaran Lee, Kyomin Jung</dc:creator>
      <pubDate>Wed, 21 Dec 2022 11:25:41 GMT</pubDate>
    </item>
    <item>
      <title>Gaining Wisdom from Setbacks: Aligning Large Language Models via Mistake Analysis</title>
      <link>http://arxiv.org/abs/2310.10477v6</link>
      <description>The rapid development of large language models (LLMs) has not only provided numerous opportunities but also presented significant challenges. This becomes particularly evident when LLMs inadvertently generate harmful or toxic content, either unintentionally or because of intentional inducement. Existing alignment methods usually direct LLMs toward the favorable outcomes by utilizing human-annotated, flawless instruction-response pairs. Conversely, this study proposes a novel alignment technique based on mistake analysis, which deliberately exposes LLMs to erroneous content to learn the reasons for mistakes and how to avoid them. In this case, mistakes are repurposed into valuable data for alignment, effectively helping to avoid the production of erroneous responses. Without external models or human annotations, our method leverages a model's intrinsic ability to discern undesirable mistakes and improves the safety of its generated responses. Experimental results reveal that our method outperforms existing alignment approaches in enhancing model safety while maintaining the overall utility.\n\n\nGaining wisdom from setbacks: Aligning large language models via mistake analysis</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2310.10477v6</guid>
      <dc:creator>Kai Chen, Chunwei Wang, Kuo Yang, Jianhua Han, Lanqing Hong, Fei Mi, Hang Xu, Zhengying Liu, Wenyong Huang, Zhenguo Li, Dit-Yan Yeung, Lifeng Shang, Xin Jiang, Qun Liu</dc:creator>
      <pubDate>Sat, 17 Feb 2024 01:50:10 GMT</pubDate>
    </item>
    <item>
      <title>Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive</title>
      <link>http://arxiv.org/abs/2402.13228v2</link>
      <description>Direct Preference Optimisation (DPO) is effective at significantly improving the performance of large language models (LLMs) on downstream tasks such as reasoning, summarisation, and alignment. Using pairs of preferred and dispreferred data, DPO models the relative probability of picking one response over another. In this work, first we show theoretically that the standard DPO loss can lead to a reduction of the model's likelihood of the preferred examples, as long as the relative probability between the preferred and dispreferred classes increases. We then show empirically that this phenomenon occurs when fine-tuning LLMs on common datasets, especially datasets in which the edit distance between pairs of completions is low. Using these insights, we design DPO-Positive (DPOP), a new loss function and training procedure which avoids this failure mode. Surprisingly, we find that DPOP outperforms DPO and other fine-tuning procedures across a wide variety of datasets and downstream tasks, including datasets with high edit distances between completions. Furthermore, we find that the DPOP-tuned model outperforms the DPO-tuned model (all else equal) on benchmarks independent of the fine-tuning data, such as MT-Bench. Finally, using DPOP, we create and open-source Smaug-34B and Smaug-72B, with the latter becoming the first open-source LLM to surpass an average accuracy of 80% on the HuggingFace Open LLM Leaderboard.\n\n\nSmaug: Fixing failure modes of preference optimisation with dpo-positive</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.13228v2</guid>
      <dc:creator>Arka Pal, Deep Karkhanis, Samuel Dooley, Manley Roberts, Siddartha Naidu, Colin White</dc:creator>
      <pubDate>Wed, 03 Jul 2024 13:46:33 GMT</pubDate>
    </item>
    <item>
      <title>Risk Taxonomy, Mitigation, and Assessment Benchmarks of Large Language Model Systems</title>
      <link>http://arxiv.org/abs/2401.05778v1</link>
      <description>Large language models (LLMs) have strong capabilities in solving diverse natural language processing tasks. However, the safety and security issues of LLM systems have become the major obstacle to their widespread application. Many studies have extensively investigated risks in LLM systems and developed the corresponding mitigation strategies. Leading-edge enterprises such as OpenAI, Google, Meta, and Anthropic have also made lots of efforts on responsible LLMs. Therefore, there is a growing need to organize the existing studies and establish comprehensive taxonomies for the community. In this paper, we delve into four essential modules of an LLM system, including an input module for receiving prompts, a language model trained on extensive corpora, a toolchain module for development and deployment, and an output module for exporting LLM-generated content. Based on this, we propose a comprehensive taxonomy, which systematically analyzes potential risks associated with each module of an LLM system and discusses the corresponding mitigation strategies. Furthermore, we review prevalent benchmarks, aiming to facilitate the risk assessment of LLM systems. We hope that this paper can help LLM participants embrace a systematic perspective to build their responsible LLM systems.\n\n\nRisk taxonomy, mitigation, and assessment benchmarks of large language model systems</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2401.05778v1</guid>
      <dc:creator>Tianyu Cui, Yanling Wang, Chuanpu Fu, Yong Xiao, Sijia Li, Xinhao Deng, Yunpeng Liu, Qinglin Zhang, Ziyi Qiu, Peiyang Li, Zhixing Tan, Junwu Xiong, Xinyu Kong, Zujie Wen, Ke Xu, Qi Li</dc:creator>
      <pubDate>Thu, 11 Jan 2024 09:29:56 GMT</pubDate>
    </item>
    <item>
      <title>Aligner: Efficient Alignment by Learning to Correct</title>
      <link>http://arxiv.org/abs/2402.02416v4</link>
      <description>With the rapid development of large language models (LLMs) and ever-evolving practical requirements, finding an efficient and effective alignment method has never been more critical. However, the tension between the complexity of current alignment methods and the need for rapid iteration in deployment scenarios necessitates the development of a model-agnostic alignment approach that can operate under these constraints. In this paper, we introduce Aligner, a novel and simple alignment paradigm that learns the correctional residuals between preferred and dispreferred answers using a small model. Designed as a model-agnostic, plug-and-play module, Aligner can be directly applied to various open-source and API-based models with only one-off training, making it suitable for rapid iteration. Notably, Aligner can be applied to any powerful, large-scale upstream models. Moreover, it can even iteratively bootstrap the upstream models using corrected responses as synthetic human preference data, breaking through the model's performance ceiling. Our experiments demonstrate performance improvements by deploying the same Aligner model across 11 different LLMs, evaluated on the 3H dimensions (helpfulness, harmlessness, and honesty). Specifically, Aligner-7B has achieved an average improvement of 68.9% in helpfulness and 23.8% in harmlessness across the tested LLMs while also effectively reducing hallucination. In the Alpaca-Eval leaderboard, stacking Aligner-2B on GPT-4 Turbo improved its LC Win Rate from 55.0% to 58.3%, surpassing GPT-4 Omni's 57.5% Win Rate (community report).\n\n\nAligner: Achieving efficient alignment through weak-to-strong correction</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.02416v4</guid>
      <dc:creator>Jiaming Ji, Boyuan Chen, Hantao Lou, Donghai Hong, Borong Zhang, Xuehai Pan, Juntao Dai, Tianyi Qiu, Yaodong Yang</dc:creator>
      <pubDate>Mon, 24 Jun 2024 18:55:16 GMT</pubDate>
    </item>
    <item>
      <title>Weak Supervision helps Emergence of Word-Object Alignment and improves Vision-Language Tasks</title>
      <link>http://arxiv.org/abs/1912.03063v1</link>
      <description>The large adoption of the self-attention (i.e. transformer model) and BERT-like training principles has recently resulted in a number of high performing models on a large panoply of vision-and-language problems (such as Visual Question Answering (VQA), image retrieval, etc.). In this paper we claim that these State-Of-The-Art (SOTA) approaches perform reasonably well in structuring information inside a single modality but, despite their impressive performances , they tend to struggle to identify fine-grained inter-modality relationships. Indeed, such relations are frequently assumed to be implicitly learned during training from application-specific losses, mostly cross-entropy for classification. While most recent works provide inductive bias for inter-modality relationships via cross attention modules, in this work, we demonstrate (1) that the latter assumption does not hold, i.e. modality alignment does not necessarily emerge automatically, and (2) that adding weak supervision for alignment between visual objects and words improves the quality of the learned models on tasks requiring reasoning. In particular , we integrate an object-word alignment loss into SOTA vision-language reasoning models and evaluate it on two tasks VQA and Language-driven Comparison of Images. We show that the proposed fine-grained inter-modality supervision significantly improves performance on both tasks. In particular, this new learning signal allows obtaining SOTA-level performances on GQA dataset (VQA task) with pre-trained models without finetuning on the task, and a new SOTA on NLVR2 dataset (Language-driven Comparison of Images). Finally, we also illustrate the impact of the contribution on the models reasoning by visualizing attention distributions.\n\n\nAligning modalities in vision large language models via preference fine-tuning</description>
      <guid isPermaLink="false">http://arxiv.org/abs/1912.03063v1</guid>
      <dc:creator>Corentin Kervadec, Grigory Antipov, Moez Baccouche, Christian Wolf</dc:creator>
      <pubDate>Fri, 06 Dec 2019 11:04:08 GMT</pubDate>
    </item>
    <item>
      <title>Reinforcement Learning for Generative AI: State of the Art, Opportunities and Open Research Challenges</title>
      <link>http://arxiv.org/abs/2308.00031v4</link>
      <description>Generative Artificial Intelligence (AI) is one of the most exciting developments in Computer Science of the last decade. At the same time, Reinforcement Learning (RL) has emerged as a very successful paradigm for a variety of machine learning tasks. In this survey, we discuss the state of the art, opportunities and open research questions in applying RL to generative AI. In particular, we will discuss three types of applications, namely, RL as an alternative way for generation without specified objectives; as a way for generating outputs while concurrently maximizing an objective function; and, finally, as a way of embedding desired characteristics, which cannot be easily captured by means of an objective function, into the generative process. We conclude the survey with an in-depth discussion of the opportunities and challenges in this fascinating emerging area.\n\n\nReinforcement learning for generative ai: State of the art, opportunities and open research challenges</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2308.00031v4</guid>
      <dc:creator>Giorgio Franceschelli, Mirco Musolesi</dc:creator>
      <pubDate>Thu, 08 Feb 2024 12:48:23 GMT</pubDate>
    </item>
    <item>
      <title>AlignBench: Benchmarking Chinese Alignment of Large Language Models</title>
      <link>http://arxiv.org/abs/2311.18743v3</link>
      <description>Alignment has become a critical step for instruction-tuned Large Language Models (LLMs) to become helpful assistants. However, effective evaluation of alignment for emerging Chinese LLMs is still significantly lacking, calling for real-scenario grounded, open-ended, challenging and automatic evaluations tailored for alignment. To fill in this gap, we introduce AlignBench, a comprehensive multi-dimensional benchmark for evaluating LLMs' alignment in Chinese. Equipped with a human-in-the-loop data curation pipeline, our benchmark employs a rule-calibrated multi-dimensional LLM-as-Judge with Chain-of-Thought to generate explanations and final ratings as evaluations, ensuring high reliability and interpretability. Furthermore, we report AlignBench evaluated by CritiqueLLM, a dedicated Chinese evaluator LLM that recovers 95% of GPT-4's evaluation ability. We will provide public APIs for evaluating AlignBench with CritiqueLLM to facilitate the evaluation of LLMs' Chinese alignment. All evaluation codes, data, and LLM generations are available at \url{https://github.com/THUDM/AlignBench}.\n\n\nAlignbench: Benchmarking chinese alignment of large language models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2311.18743v3</guid>
      <dc:creator>Xiao Liu, Xuanyu Lei, Shengyuan Wang, Yue Huang, Zhuoer Feng, Bosi Wen, Jiale Cheng, Pei Ke, Yifan Xu, Weng Lam Tam, Xiaohan Zhang, Lichao Sun, Hongning Wang, Jing Zhang, Minlie Huang, Yuxiao Dong, Jie Tang</dc:creator>
      <pubDate>Tue, 05 Dec 2023 16:04:15 GMT</pubDate>
    </item>
    <item>
      <title>Blackbox Adaptation for Medical Image Segmentation</title>
      <link>http://arxiv.org/abs/2405.10913v1</link>
      <description>In recent years, various large foundation models have been proposed for image segmentation. There models are often trained on large amounts of data corresponding to general computer vision tasks. Hence, these models do not perform well on medical data. There have been some attempts in the literature to perform parameter-efficient finetuning of such foundation models for medical image segmentation. However, these approaches assume that all the parameters of the model are available for adaptation. But, in many cases, these models are released as APIs or blackboxes, with no or limited access to the model parameters and data. In addition, finetuning methods also require a significant amount of compute, which may not be available for the downstream task. At the same time, medical data can't be shared with third-party agents for finetuning due to privacy reasons. To tackle these challenges, we pioneer a blackbox adaptation technique for prompted medical image segmentation, called BAPS. BAPS has two components - (i) An Image-Prompt decoder (IP decoder) module that generates visual prompts given an image and a prompt, and (ii) A Zero Order Optimization (ZOO) Method, called SPSA-GC that is used to update the IP decoder without the need for backpropagating through the foundation model. Thus, our method does not require any knowledge about the foundation model's weights or gradients. We test BAPS on four different modalities and show that our method can improve the original model's performance by around 4%.\n\n\nBlack-box prompt optimization: Aligning large language models without model training</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.10913v1</guid>
      <dc:creator>Jay N. Paranjape, Shameema Sikder, S. Swaroop Vedula, Vishal M. Patel</dc:creator>
      <pubDate>Fri, 17 May 2024 17:02:04 GMT</pubDate>
    </item>
    <item>
      <title>CrossCheckGPT: Universal Hallucination Ranking for Multimodal Foundation Models</title>
      <link>http://arxiv.org/abs/2405.13684v1</link>
      <description>Multimodal foundation models are prone to hallucination, generating outputs that either contradict the input or are not grounded by factual information. Given the diversity in architectures, training data and instruction tuning techniques, there can be large variations in systems' susceptibility to hallucinations. To assess system hallucination robustness, hallucination ranking approaches have been developed for specific tasks such as image captioning, question answering, summarization, or biography generation. However, these approaches typically compare model outputs to gold-standard references or labels, limiting hallucination benchmarking for new domains. This work proposes &quot;CrossCheckGPT&quot;, a reference-free universal hallucination ranking for multimodal foundation models. The core idea of CrossCheckGPT is that the same hallucinated content is unlikely to be generated by different independent systems, hence cross-system consistency can provide meaningful and accurate hallucination assessment scores. CrossCheckGPT can be applied to any model or task, provided that the information consistency between outputs can be measured through an appropriate distance metric. Focusing on multimodal large language models that generate text, we explore two information consistency measures: CrossCheck-explicit and CrossCheck-implicit. We showcase the applicability of our method for hallucination ranking across various modalities, namely the text, image, and audio-visual domains. Further, we propose the first audio-visual hallucination benchmark, &quot;AVHalluBench&quot;, and illustrate the effectiveness of CrossCheckGPT, achieving correlations of 98% and 89% with human judgements on MHaluBench and AVHalluBench, respectively.\n\n\nMulti-modal hallucination control by visual information grounding</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.13684v1</guid>
      <dc:creator>Guangzhi Sun, Potsawee Manakul, Adian Liusie, Kunat Pipatanakul, Chao Zhang, Phil Woodland, Mark Gales</dc:creator>
      <pubDate>Wed, 22 May 2024 14:25:41 GMT</pubDate>
    </item>
    <item>
      <title>WildChat: 1M ChatGPT Interaction Logs in the Wild</title>
      <link>http://arxiv.org/abs/2405.01470v1</link>
      <description>Chatbots such as GPT-4 and ChatGPT are now serving millions of users. Despite their widespread use, there remains a lack of public datasets showcasing how these tools are used by a population of users in practice. To bridge this gap, we offered free access to ChatGPT for online users in exchange for their affirmative, consensual opt-in to anonymously collect their chat transcripts and request headers. From this, we compiled WildChat, a corpus of 1 million user-ChatGPT conversations, which consists of over 2.5 million interaction turns. We compare WildChat with other popular user-chatbot interaction datasets, and find that our dataset offers the most diverse user prompts, contains the largest number of languages, and presents the richest variety of potentially toxic use-cases for researchers to study. In addition to timestamped chat transcripts, we enrich the dataset with demographic data, including state, country, and hashed IP addresses, alongside request headers. This augmentation allows for more detailed analysis of user behaviors across different geographical regions and temporal dimensions. Finally, because it captures a broad range of use cases, we demonstrate the dataset's potential utility in fine-tuning instruction-following models. WildChat is released at https://wildchat.allen.ai under AI2 ImpACT Licenses.\n\n\nWildchat: 1m chatGPT interaction logs in the wild</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.01470v1</guid>
      <dc:creator>Wenting Zhao, Xiang Ren, Jack Hessel, Claire Cardie, Yejin Choi, Yuntian Deng</dc:creator>
      <pubDate>Thu, 02 May 2024 17:00:02 GMT</pubDate>
    </item>
    <item>
      <title>LoRA Fine-tuning Efficiently Undoes Safety Training in Llama 2-Chat 70B</title>
      <link>http://arxiv.org/abs/2310.20624v2</link>
      <description>AI developers often apply safety alignment procedures to prevent the misuse of their AI systems. For example, before Meta released Llama 2-Chat - a collection of instruction fine-tuned large language models - they invested heavily in safety training, incorporating extensive red-teaming and reinforcement learning from human feedback. We explore the robustness of safety training in language models by subversively fine-tuning Llama 2-Chat. We employ quantized low-rank adaptation (LoRA) as an efficient fine-tuning method. With a budget of less than \$200 and using only one GPU, we successfully undo the safety training of Llama 2-Chat models of sizes 7B, 13B, and 70B and on the Mixtral instruct model. Specifically, our fine-tuning technique significantly reduces the rate at which the model refuses to follow harmful instructions. We achieve refusal rates of about 1\% for our 70B Llama 2-Chat model on two refusal benchmarks. Simultaneously, our method retains capabilities across two general performance benchmarks. We show that subversive fine-tuning is practical and effective, and hence argue that evaluating risks from fine-tuning should be a core part of risk assessments for releasing model weights. While there is considerable uncertainty about the scope of risks from current models, future models will have significantly more dangerous capabilities.\n\n\nLora fine-tuning efficiently undoes safety training in llama 2-chat 70b</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2310.20624v2</guid>
      <dc:creator>Simon Lermen, Charlie Rogers-Smith, Jeffrey Ladish</dc:creator>
      <pubDate>Wed, 22 May 2024 08:39:46 GMT</pubDate>
    </item>
    <item>
      <title>Embodied Multi-Modal Agent trained by an LLM from a Parallel TextWorld</title>
      <link>http://arxiv.org/abs/2311.16714v2</link>
      <description>While large language models (LLMs) excel in a simulated world of texts, they struggle to interact with the more realistic world without perceptions of other modalities such as visual or audio signals. Although vision-language models (VLMs) integrate LLM modules (1) aligned with static image features, and (2) may possess prior knowledge of world dynamics (as demonstrated in the text world), they have not been trained in an embodied visual world and thus cannot align with its dynamics. On the other hand, training an embodied agent in a noisy visual world without expert guidance is often challenging and inefficient. In this paper, we train a VLM agent living in a visual world using an LLM agent excelling in a parallel text world. Specifically, we distill LLM's reflection outcomes (improved actions by analyzing mistakes) in a text world's tasks to finetune the VLM on the same tasks of the visual world, resulting in an Embodied Multi-Modal Agent (EMMA) quickly adapting to the visual world dynamics. Such cross-modality imitation learning between the two parallel worlds is achieved by a novel DAgger-DPO algorithm, enabling EMMA to generalize to a broad scope of new tasks without any further guidance from the LLM expert. Extensive evaluations on the ALFWorld benchmark's diverse tasks highlight EMMA's superior performance to SOTA VLM-based agents, e.g., 20%-70% improvement in the success rate.\n\n\nEmbodied multi-modal agent trained by an llm from a parallel textworld</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2311.16714v2</guid>
      <dc:creator>Yijun Yang, Tianyi Zhou, Kanxue Li, Dapeng Tao, Lusong Li, Li Shen, Xiaodong He, Jing Jiang, Yuhui Shi</dc:creator>
      <pubDate>Fri, 29 Mar 2024 04:07:25 GMT</pubDate>
    </item>
    <item>
      <title>Advancing LLM Reasoning Generalists with Preference Trees</title>
      <link>http://arxiv.org/abs/2404.02078v1</link>
      <description>We introduce Eurus, a suite of large language models (LLMs) optimized for reasoning. Finetuned from Mistral-7B and CodeLlama-70B, Eurus models achieve state-of-the-art results among open-source models on a diverse set of benchmarks covering mathematics, code generation, and logical reasoning problems. Notably, Eurus-70B beats GPT-3.5 Turbo in reasoning through a comprehensive benchmarking across 12 tests covering five tasks, and achieves a 33.3% pass@1 accuracy on LeetCode and 32.6% on TheoremQA, two challenging benchmarks, substantially outperforming existing open-source models by margins more than 13.3%. The strong performance of Eurus can be primarily attributed to UltraInteract, our newly-curated large-scale, high-quality alignment dataset specifically designed for complex reasoning tasks. UltraInteract can be used in both supervised fine-tuning and preference learning. For each instruction, it includes a preference tree consisting of (1) reasoning chains with diverse planning strategies in a unified format, (2) multi-turn interaction trajectories with the environment and the critique, and (3) pairwise data to facilitate preference learning. UltraInteract allows us to conduct an in-depth exploration of preference learning for reasoning tasks. Our investigation reveals that some well-established preference learning algorithms may be less suitable for reasoning tasks compared to their effectiveness in general conversations. Inspired by this, we derive a novel reward modeling objective which, together with UltraInteract, leads to a strong reward model.\n\n\nAdvancing llm reasoning generalists with preference trees</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2404.02078v1</guid>
      <dc:creator>Lifan Yuan, Ganqu Cui, Hanbin Wang, Ning Ding, Xingyao Wang, Jia Deng, Boji Shan, Huimin Chen, Ruobing Xie, Yankai Lin, Zhenghao Liu, Bowen Zhou, Hao Peng, Zhiyuan Liu, Maosong Sun</dc:creator>
      <pubDate>Tue, 02 Apr 2024 16:25:30 GMT</pubDate>
    </item>
    <item>
      <title>Negative Preference Optimization: From Catastrophic Collapse to Effective Unlearning</title>
      <link>http://arxiv.org/abs/2404.05868v1</link>
      <description>Large Language Models (LLMs) often memorize sensitive, private, or copyrighted data during pre-training. LLM unlearning aims to eliminate the influence of undesirable data from the pre-trained model while preserving the model's utilities on other tasks. Several practical methods have recently been proposed for LLM unlearning, mostly based on gradient ascent (GA) on the loss of undesirable data. However, on certain unlearning tasks, these methods either fail to effectively unlearn the target data or suffer from catastrophic collapse -- a drastic degradation of the model's utilities.   In this paper, we propose Negative Preference Optimization (NPO), a simple alignment-inspired method that could efficiently and effectively unlearn a target dataset. We theoretically show that the progression toward catastrophic collapse by minimizing the NPO loss is exponentially slower than GA. Through experiments on synthetic data and the benchmark TOFU dataset, we demonstrate that NPO-based methods achieve a better balance between unlearning the undesirable data and maintaining the model's utilities. We also observe that NPO-based methods generate more sensible outputs than GA-based methods, whose outputs are often gibberish. Remarkably, on TOFU, NPO-based methods are the first to achieve reasonable unlearning results in forgetting 50% (or more) of the training data, whereas existing methods already struggle with forgetting 10% of training data.\n\n\nNegative preference optimization: From catastrophic collapse to effective unlearning</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2404.05868v1</guid>
      <dc:creator>Ruiqi Zhang, Licong Lin, Yu Bai, Song Mei</dc:creator>
      <pubDate>Mon, 08 Apr 2024 21:05:42 GMT</pubDate>
    </item>
    <item>
      <title>DPO Meets PPO: Reinforced Token Optimization for RLHF</title>
      <link>http://arxiv.org/abs/2404.18922v1</link>
      <description>In the classical Reinforcement Learning from Human Feedback (RLHF) framework, Proximal Policy Optimization (PPO) is employed to learn from sparse, sentence-level rewards -- a challenging scenario in traditional deep reinforcement learning. Despite the great successes of PPO in the alignment of state-of-the-art closed-source large language models (LLMs), its open-source implementation is still largely sub-optimal, as widely reported by numerous research studies. To address these issues, we introduce a framework that models RLHF problems as a Markov decision process (MDP), enabling the capture of fine-grained token-wise information. Furthermore, we provide theoretical insights that demonstrate the superiority of our MDP framework over the previous sentence-level bandit formulation. Under this framework, we introduce an algorithm, dubbed as Reinforced Token Optimization (\texttt{RTO}), which learns the token-wise reward function from preference data and performs policy optimization based on this learned token-wise reward signal. Theoretically, \texttt{RTO} is proven to have the capability of finding the near-optimal policy sample-efficiently. For its practical implementation, \texttt{RTO} innovatively integrates Direct Preference Optimization (DPO) and PPO. DPO, originally derived from sparse sentence rewards, surprisingly provides us with a token-wise characterization of response quality, which is seamlessly incorporated into our subsequent PPO training stage. Extensive real-world alignment experiments verify the effectiveness of the proposed approach.\n\n\nDpo meets ppo: Reinforced token optimization for rlhf</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2404.18922v1</guid>
      <dc:creator>Han Zhong, Guhao Feng, Wei Xiong, Li Zhao, Di He, Jiang Bian, Liwei Wang</dc:creator>
      <pubDate>Mon, 29 Apr 2024 17:58:30 GMT</pubDate>
    </item>
    <item>
      <title>Arcee's MergeKit: A Toolkit for Merging Large Language Models</title>
      <link>http://arxiv.org/abs/2403.13257v2</link>
      <description>The rapid expansion of the open-source language model landscape presents an opportunity to merge the competencies of these model checkpoints by combining their parameters. Advances in transfer learning, the process of fine-tuning pretrained models for specific tasks, has resulted in the development of vast amounts of task-specific models, typically specialized in individual tasks and unable to utilize each other's strengths. Model merging facilitates the creation of multitask models without the need for additional training, offering a promising avenue for enhancing model performance and versatility. By preserving the intrinsic capabilities of the original models, model merging addresses complex challenges in AI - including the difficulties of catastrophic forgetting and multitask learning. To support this expanding area of research, we introduce MergeKit, a comprehensive, open-source library designed to facilitate the application of model merging strategies. MergeKit offers an extensible framework to efficiently merge models on any hardware, providing utility to researchers and practitioners. To date, thousands of models have been merged by the open-source community, leading to the creation of some of the worlds most powerful open-source model checkpoints, as assessed by the Open LLM Leaderboard. The library is accessible at https://github.com/arcee-ai/MergeKit.\n\n\nArcee's MergeKit: A Toolkit for Merging Large Language Models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2403.13257v2</guid>
      <dc:creator>Charles Goddard, Shamane Siriwardhana, Malikeh Ehghaghi, Luke Meyers, Vlad Karpukhin, Brian Benedict, Mark McQuade, Jacob Solawetz</dc:creator>
      <pubDate>Thu, 21 Mar 2024 03:13:30 GMT</pubDate>
    </item>
    <item>
      <title>Dataset Reset Policy Optimization for RLHF</title>
      <link>http://arxiv.org/abs/2404.08495v3</link>
      <description>Reinforcement Learning (RL) from Human Preference-based feedback is a popular paradigm for fine-tuning generative models, which has produced impressive models such as GPT-4 and Claude3 Opus. This framework often consists of two steps: learning a reward model from an offline preference dataset followed by running online RL to optimize the learned reward model. In this work, leveraging the idea of reset, we propose a new RLHF algorithm with provable guarantees. Motivated by the fact that offline preference dataset provides informative states (i.e., data that is preferred by the labelers), our new algorithm, Dataset Reset Policy Optimization (DR-PO), integrates the existing offline preference dataset into the online policy training procedure via dataset reset: it directly resets the policy optimizer to the states in the offline dataset, instead of always starting from the initial state distribution. In theory, we show that DR-PO learns to perform at least as good as any policy that is covered by the offline dataset under general function approximation with finite sample complexity. In experiments, we demonstrate that on both the TL;DR summarization and the Anthropic Helpful Harmful (HH) dataset, the generation from DR-PO is better than that from Proximal Policy Optimization (PPO) and Direction Preference Optimization (DPO), under the metric of GPT4 win-rate. Code for this work can be found at https://github.com/Cornell-RL/drpo.\n\n\nDataset reset policy optimization for rlhf</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2404.08495v3</guid>
      <dc:creator>Jonathan D. Chang, Wenhao Zhan, Owen Oertell, Kianté Brantley, Dipendra Misra, Jason D. Lee, Wen Sun</dc:creator>
      <pubDate>Tue, 16 Apr 2024 17:36:39 GMT</pubDate>
    </item>
    <item>
      <title>Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey</title>
      <link>http://arxiv.org/abs/2402.09283v3</link>
      <description>Large Language Models (LLMs) are now commonplace in conversation applications. However, their risks of misuse for generating harmful responses have raised serious societal concerns and spurred recent research on LLM conversation safety. Therefore, in this survey, we provide a comprehensive overview of recent studies, covering three critical aspects of LLM conversation safety: attacks, defenses, and evaluations. Our goal is to provide a structured summary that enhances understanding of LLM conversation safety and encourages further investigation into this important subject. For easy reference, we have categorized all the studies mentioned in this survey according to our taxonomy, available at: https://github.com/niconi19/LLM-conversation-safety.\n\n\nAttacks, defenses and evaluations for llm conversation safety: A survey</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.09283v3</guid>
      <dc:creator>Zhichen Dong, Zhanhui Zhou, Chao Yang, Jing Shao, Yu Qiao</dc:creator>
      <pubDate>Wed, 27 Mar 2024 13:55:14 GMT</pubDate>
    </item>
    <item>
      <title>AutoML in the Age of Large Language Models: Current Challenges, Future Opportunities and Risks</title>
      <link>http://arxiv.org/abs/2306.08107v3</link>
      <description>The fields of both Natural Language Processing (NLP) and Automated Machine Learning (AutoML) have achieved remarkable results over the past years. In NLP, especially Large Language Models (LLMs) have experienced a rapid series of breakthroughs very recently. We envision that the two fields can radically push the boundaries of each other through tight integration. To showcase this vision, we explore the potential of a symbiotic relationship between AutoML and LLMs, shedding light on how they can benefit each other. In particular, we investigate both the opportunities to enhance AutoML approaches with LLMs from different perspectives and the challenges of leveraging AutoML to further improve LLMs. To this end, we survey existing work, and we critically assess risks. We strongly believe that the integration of the two fields has the potential to disrupt both fields, NLP and AutoML. By highlighting conceivable synergies, but also risks, we aim to foster further exploration at the intersection of AutoML and LLMs.\n\n\nAutoml in the age of large language models: Current challenges, future opportunities and risks</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2306.08107v3</guid>
      <dc:creator>Alexander Tornede, Difan Deng, Theresa Eimer, Joseph Giovanelli, Aditya Mohan, Tim Ruhkopf, Sarah Segel, Daphne Theodorakopoulos, Tanja Tornede, Henning Wachsmuth, Marius Lindauer</dc:creator>
      <pubDate>Wed, 21 Feb 2024 11:18:20 GMT</pubDate>
    </item>
    <item>
      <title>Stay on topic with Classifier-Free Guidance</title>
      <link>http://arxiv.org/abs/2306.17806v1</link>
      <description>Classifier-Free Guidance (CFG) has recently emerged in text-to-image generation as a lightweight technique to encourage prompt-adherence in generations. In this work, we demonstrate that CFG can be used broadly as an inference-time technique in pure language modeling. We show that CFG (1) improves the performance of Pythia, GPT-2 and LLaMA-family models across an array of tasks: Q\&amp;A, reasoning, code generation, and machine translation, achieving SOTA on LAMBADA with LLaMA-7B over PaLM-540B; (2) brings improvements equivalent to a model with twice the parameter-count; (3) can stack alongside other inference-time methods like Chain-of-Thought and Self-Consistency, yielding further improvements in difficult tasks; (4) can be used to increase the faithfulness and coherence of assistants in challenging form-driven and content-driven prompts: in a human evaluation we show a 75\% preference for GPT4All using CFG over baseline.\n\n\nStay on topic with classifier-free guidance</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2306.17806v1</guid>
      <dc:creator>Guillaume Sanchez, Honglu Fan, Alexander Spangher, Elad Levi, Pawan Sasanka Ammanamanchi, Stella Biderman</dc:creator>
      <pubDate>Fri, 30 Jun 2023 17:07:02 GMT</pubDate>
    </item>
    <item>
      <title>Data Augmentation using Large Language Models: Data Perspectives, Learning Paradigms and Challenges</title>
      <link>http://arxiv.org/abs/2403.02990v4</link>
      <description>In the rapidly evolving field of large language models (LLMs), data augmentation (DA) has emerged as a pivotal technique for enhancing model performance by diversifying training examples without the need for additional data collection. This survey explores the transformative impact of LLMs on DA, particularly addressing the unique challenges and opportunities they present in the context of natural language processing (NLP) and beyond. From both data and learning perspectives, we examine various strategies that utilize LLMs for data augmentation, including a novel exploration of learning paradigms where LLM-generated data is used for diverse forms of further training. Additionally, this paper highlights the primary open challenges faced in this domain, ranging from controllable data augmentation to multi-modal data augmentation. This survey highlights a paradigm shift introduced by LLMs in DA, and aims to serve as a comprehensive guide for researchers and practitioners.\n\n\nData augmentation using llms: Data perspectives, learning paradigms and challenges</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2403.02990v4</guid>
      <dc:creator>Bosheng Ding, Chengwei Qin, Ruochen Zhao, Tianze Luo, Xinze Li, Guizhen Chen, Wenhan Xia, Junjie Hu, Anh Tuan Luu, Shafiq Joty</dc:creator>
      <pubDate>Tue, 02 Jul 2024 07:59:40 GMT</pubDate>
    </item>
    <item>
      <title>It Takes Four to Tango: Multiagent Selfplay for Automatic Curriculum Generation</title>
      <link>http://arxiv.org/abs/2202.10608v1</link>
      <description>We are interested in training general-purpose reinforcement learning agents that can solve a wide variety of goals. Training such agents efficiently requires automatic generation of a goal curriculum. This is challenging as it requires (a) exploring goals of increasing difficulty, while ensuring that the agent (b) is exposed to a diverse set of goals in a sample efficient manner and (c) does not catastrophically forget previously solved goals. We propose Curriculum Self Play (CuSP), an automated goal generation framework that seeks to satisfy these desiderata by virtue of a multi-player game with four agents. We extend the asymmetric curricula learning in PAIRED (Dennis et al., 2020) to a symmetrized game that carefully balances cooperation and competition between two off-policy student learners and two regret-maximizing teachers. CuSP additionally introduces entropic goal coverage and accounts for the non-stationary nature of the students, allowing us to automatically induce a curriculum that balances progressive exploration with anti-catastrophic exploitation. We demonstrate that our method succeeds at generating an effective curricula of goals for a range of control tasks, outperforming other methods at zero-shot test-time generalization to novel out-of-distribution goals.\n\n\nSelf-play preference optimization for language model alignment</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2202.10608v1</guid>
      <dc:creator>Yuqing Du, Pieter Abbeel, Aditya Grover</dc:creator>
      <pubDate>Tue, 22 Feb 2022 01:23:23 GMT</pubDate>
    </item>
    <item>
      <title>Helping or Herding? Reward Model Ensembles Mitigate but do not Eliminate Reward Hacking</title>
      <link>http://arxiv.org/abs/2312.09244v2</link>
      <description>Reward models play a key role in aligning language model applications towards human preferences. However, this setup creates an incentive for the language model to exploit errors in the reward model to achieve high estimated reward, a phenomenon often termed \emph{reward hacking}. A natural mitigation is to train an ensemble of reward models, aggregating over model outputs to obtain a more robust reward estimate. We explore the application of reward ensembles to alignment at both training time (through reinforcement learning) and inference time (through reranking). First, we show that reward models are \emph{underspecified}: reward models that perform similarly in-distribution can yield very different rewards when used in alignment, due to distribution shift. Second, underspecification results in overoptimization, where alignment to one reward model does not improve reward as measured by another reward model trained on the same data. Third, overoptimization is mitigated by the use of reward ensembles, and ensembles that vary by their \emph{pretraining} seeds lead to better generalization than ensembles that differ only by their \emph{fine-tuning} seeds, with both outperforming individual reward models. However, even pretrain reward ensembles do not eliminate reward hacking: we show several qualitative reward hacking phenomena that are not mitigated by ensembling because all reward models in the ensemble exhibit similar error patterns.\n\n\nHelping or herding? reward model ensembles mitigate but do not eliminate reward hacking</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2312.09244v2</guid>
      <dc:creator>Jacob Eisenstein, Chirag Nagpal, Alekh Agarwal, Ahmad Beirami, Alex D'Amour, DJ Dvijotham, Adam Fisch, Katherine Heller, Stephen Pfohl, Deepak Ramachandran, Peter Shaw, Jonathan Berant</dc:creator>
      <pubDate>Thu, 21 Dec 2023 03:40:07 GMT</pubDate>
    </item>
    <item>
      <title>Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study</title>
      <link>http://arxiv.org/abs/2404.10719v2</link>
      <description>Reinforcement Learning from Human Feedback (RLHF) is currently the most widely used method to align large language models (LLMs) with human preferences. Existing RLHF methods can be roughly categorized as either reward-based or reward-free. Novel applications such as ChatGPT and Claude leverage reward-based methods that first learn a reward model and apply actor-critic algorithms, such as Proximal Policy Optimization (PPO). However, in academic benchmarks, state-of-the-art results are often achieved via reward-free methods, such as Direct Preference Optimization (DPO). Is DPO truly superior to PPO? Why does PPO perform poorly on these benchmarks? In this paper, we first conduct both theoretical and empirical studies on the algorithmic properties of DPO and show that DPO may have fundamental limitations. Moreover, we also comprehensively examine PPO and reveal the key factors for the best performances of PPO in fine-tuning LLMs. Finally, we benchmark DPO and PPO across a collection of RLHF testbeds, ranging from dialogue to code generation. Experiment results demonstrate that PPO is able to surpass other alignment methods in all cases and achieve state-of-the-art results in challenging code competitions.\n\n\nIs dpo superior to ppo for llm alignment? a comprehensive study</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2404.10719v2</guid>
      <dc:creator>Shusheng Xu, Wei Fu, Jiaxuan Gao, Wenjie Ye, Weilin Liu, Zhiyu Mei, Guangju Wang, Chao Yu, Yi Wu</dc:creator>
      <pubDate>Sun, 21 Apr 2024 11:58:54 GMT</pubDate>
    </item>
    <item>
      <title>Vision-and-Language Pretraining</title>
      <link>http://arxiv.org/abs/2207.01772v2</link>
      <description>With the burgeoning amount of data of image-text pairs and diversity of Vision-and-Language (V\&amp;L) tasks, scholars have introduced an abundance of deep learning models in this research domain. Furthermore, in recent years, transfer learning has also shown tremendous success in Computer Vision for tasks such as Image Classification, Object Detection, etc., and in Natural Language Processing for Question Answering, Machine Translation, etc. Inheriting the spirit of Transfer Learning, research works in V\&amp;L have devised multiple pretraining techniques on large-scale datasets in order to enhance the performance of downstream tasks. The aim of this article is to provide a comprehensive revision of contemporary V\&amp;L pretraining models. In particular, we categorize and delineate pretraining approaches, along with the summary of state-of-the-art vision-and-language pretrained models. Moreover, a list of training datasets and downstream tasks is supplied to further polish the perspective into V\&amp;L pretraining. Lastly, we decided to take a further step to discuss numerous directions for future research.\n\n\nLanguage models scale reliably with over-training and on downstream tasks</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2207.01772v2</guid>
      <dc:creator>Thong Nguyen, Cong-Duy Nguyen, Xiaobao Wu, See-Kiong Ng, Anh Tuan Luu</dc:creator>
      <pubDate>Sat, 24 Jun 2023 00:16:51 GMT</pubDate>
    </item>
    <item>
      <title>PRDP: Proximal Reward Difference Prediction for Large-Scale Reward Finetuning of Diffusion Models</title>
      <link>http://arxiv.org/abs/2402.08714v2</link>
      <description>Reward finetuning has emerged as a promising approach to aligning foundation models with downstream objectives. Remarkable success has been achieved in the language domain by using reinforcement learning (RL) to maximize rewards that reflect human preference. However, in the vision domain, existing RL-based reward finetuning methods are limited by their instability in large-scale training, rendering them incapable of generalizing to complex, unseen prompts. In this paper, we propose Proximal Reward Difference Prediction (PRDP), enabling stable black-box reward finetuning for diffusion models for the first time on large-scale prompt datasets with over 100K prompts. Our key innovation is the Reward Difference Prediction (RDP) objective that has the same optimal solution as the RL objective while enjoying better training stability. Specifically, the RDP objective is a supervised regression objective that tasks the diffusion model with predicting the reward difference of generated image pairs from their denoising trajectories. We theoretically prove that the diffusion model that obtains perfect reward difference prediction is exactly the maximizer of the RL objective. We further develop an online algorithm with proximal updates to stably optimize the RDP objective. In experiments, we demonstrate that PRDP can match the reward maximization ability of well-established RL-based methods in small-scale training. Furthermore, through large-scale training on text prompts from the Human Preference Dataset v2 and the Pick-a-Pic v1 dataset, PRDP achieves superior generation quality on a diverse set of complex, unseen prompts whereas RL-based methods completely fail.\n\n\nPrdp: Proximal reward difference prediction for large-scale reward finetuning of diffusion models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.08714v2</guid>
      <dc:creator>Fei Deng, Qifei Wang, Wei Wei, Matthias Grundmann, Tingbo Hou</dc:creator>
      <pubDate>Wed, 27 Mar 2024 21:37:39 GMT</pubDate>
    </item>
    <item>
      <title>Learning to Generate Better Than Your LLM</title>
      <link>http://arxiv.org/abs/2306.11816v2</link>
      <description>Reinforcement learning (RL) has emerged as a powerful paradigm for fine-tuning Large Language Models (LLMs) for text generation. In particular, recent LLMs such as ChatGPT and GPT-4 can engage in fluent conversations with users after finetuning with RL. Capitalizing on key properties of text generation, we seek to investigate RL algorithms beyond general purpose algorithms like Proximal Policy Optimization (PPO). In particular, we extend RL algorithms to allow them to interact with a dynamic black-box guide LLM and propose RL with guided feedback (RLGF), a suite of RL algorithms for LLM fine-tuning. We provide two ways for the guide LLM to interact with the LLM to be optimized for maximizing rewards. The guide LLM can generate text which serves as additional starting states for the RL optimization procedure. The guide LLM can also be used to complete the partial sentences generated by the LLM that is being optimized, treating the guide LLM as an expert to imitate and surpass eventually. We experiment on the IMDB positive sentiment, CommonGen, and TL;DR summarization tasks. We show that our RL algorithms achieve higher performance than supervised learning (SL) and the RL baseline PPO, demonstrating the benefit of interaction with the guide LLM. On both CommonGen and TL;DR, we not only outperform our SL baselines but also improve upon PPO across a variety of metrics beyond the one we optimized for. Our code can be found at https://github.com/Cornell-RL/tril.\n\n\nLearning to generate better than your llm</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2306.11816v2</guid>
      <dc:creator>Jonathan D. Chang, Kiante Brantley, Rajkumar Ramamurthy, Dipendra Misra, Wen Sun</dc:creator>
      <pubDate>Mon, 13 Nov 2023 18:51:42 GMT</pubDate>
    </item>
    <item>
      <title>Beyond Imitation: Leveraging Fine-grained Quality Signals for Alignment</title>
      <link>http://arxiv.org/abs/2311.04072v2</link>
      <description>Alignment with human preference is a desired property of large language models (LLMs). Currently, the main alignment approach is based on reinforcement learning from human feedback (RLHF). Despite the effectiveness of RLHF, it is intricate to implement and train, thus recent studies explore how to develop alternative alignment approaches based on supervised fine-tuning (SFT). A major limitation of SFT is that it essentially does imitation learning, which cannot fully understand what are the expected behaviors. To address this issue, we propose an improved alignment approach named FIGA. Different from prior methods, we incorporate fine-grained (i.e., token or phrase level) quality signals that are derived by contrasting good and bad responses. Our approach has made two major contributions. Firstly, we curate a refined alignment dataset that pairs initial responses and the corresponding revised ones. Secondly, we devise a new loss function can leverage fine-grained quality signals to instruct the learning of LLMs for alignment. Extensive experiments have demonstrated the effectiveness of our approaches by comparing a number of competitive baselines.\n\n\nBeyond imitation: Leveraging fine-grained quality signals for alignment</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2311.04072v2</guid>
      <dc:creator>Geyang Guo, Ranchi Zhao, Tianyi Tang, Wayne Xin Zhao, Ji-Rong Wen</dc:creator>
      <pubDate>Mon, 15 Apr 2024 15:25:53 GMT</pubDate>
    </item>
    <item>
      <title>Knowledgeable Preference Alignment for LLMs in Domain-specific Question Answering</title>
      <link>http://arxiv.org/abs/2311.06503v3</link>
      <description>Deploying large language models (LLMs) to real scenarios for domain-specific question answering (QA) is a key thrust for LLM applications, which poses numerous challenges, especially in ensuring that responses are both accommodating to user requirements and appropriately leveraging domain-specific knowledge bases. They are the two major difficulties for LLM application as vanilla fine-tuning falls short of addressing. Combining these requirements, we conceive of them as the requirement for the model's preference to be harmoniously aligned with humans'. Thus, we introduce Knowledgeable Preference AlignmenT (KnowPAT), which constructs two kinds of preference sets to tackle the two issues. Besides, we design a new alignment objective to align the LLM preference with different human preferences uniformly, aiming to optimize LLM performance in real-world, domain-specific QA settings. Adequate experiments and comprehensive comparisons with 15 baseline methods illustrate that our KnowPAT is a superior pipeline for real-scenario domain-specific QA with LLMs.\n\n\nKnowledgeable preference alignment for llms in domain-specific question answering</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2311.06503v3</guid>
      <dc:creator>Yichi Zhang, Zhuo Chen, Yin Fang, Yanxi Lu, Fangming Li, Wen Zhang, Huajun Chen</dc:creator>
      <pubDate>Mon, 10 Jun 2024 09:06:10 GMT</pubDate>
    </item>
    <item>
      <title>Cambrian Lattices</title>
      <link>http://arxiv.org/abs/math/0402086v2</link>
      <description>For an arbitrary finite Coxeter group W we define the family of Cambrian lattices for W as quotients of the weak order on W with respect to certain lattice congruences. We associate to each Cambrian lattice a complete fan, which we conjecture is the normal fan of a polytope combinatorially isomorphic to the generalized associahedron for W. In types A and B we obtain, by means of a fiber-polytope construction, combinatorial realizations of the Cambrian lattices in terms of triangulations and in terms of permutations. Using this combinatorial information, we prove in types A and B that the Cambrian fans are combinatorially isomorphic to the normal fans of the generalized associahedra and that one of the Cambrian fans is linearly isomorphic to Fomin and Zelevinsky's construction of the normal fan as a &quot;cluster fan.&quot; Our construction does not require a crystallographic Coxeter group and therefore suggests a definition, at least on the level of cellular spheres, of a generalized associahedron for any finite Coxeter group. The Tamari lattice is one of the Cambrian lattices of type A, and two &quot;Tamari&quot; lattices in type B are identified and characterized in terms of signed pattern avoidance. We also show that open intervals in Cambrian lattices are either contractible or homotopy equivalent to spheres.\n\n\nCambrian-1: A fully open, vision-centric exploration of multimodal llms</description>
      <guid isPermaLink="false">http://arxiv.org/abs/math/0402086v2</guid>
      <dc:creator>Nathan Reading</dc:creator>
      <pubDate>Mon, 18 Jul 2005 20:56:35 GMT</pubDate>
    </item>
    <item>
      <title>Iterative Nash Policy Optimization: Aligning LLMs with General Preferences via No-Regret Learning</title>
      <link>http://arxiv.org/abs/2407.00617v2</link>
      <description>Reinforcement Learning with Human Feedback (RLHF) has achieved great success in aligning large language models (LLMs) with human preferences. Prevalent RLHF approaches are reward-based, following the Bradley-Terry (BT) model assumption, which may not fully capture the complexity of human preferences. In this paper, we explore RLHF under a general preference framework and approach it from a game-theoretic perspective. Specifically, we formulate the problem as a two-player game and propose a novel algorithm, iterative Nash policy optimization (INPO). The key idea is to let the policy play against itself via no-regret learning, thereby approximating the Nash policy. Unlike previous methods, INPO bypasses the need for estimating the expected win rate for individual responses, which typically incurs high computational or annotation costs. Instead, we introduce a new loss objective that is directly minimized over a preference dataset. We provide theoretical analysis for our approach and demonstrate its effectiveness through experiments on various representative benchmarks. With an LLaMA-3-8B-based SFT model, INPO achieves a 41.5% length-controlled win rate on AlpacaEval 2.0 and a 38.3% win rate on Arena-Hard, showing substantial improvement over the state-of-the-art iterative algorithm [Dong et al., 2024] under the BT model assumption. Additionally, our ablation study highlights the benefits of incorporating KL regularization for response length control.\n\n\nA theoretical analysis of nash learning from human feedback under general kl-regularized preference</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2407.00617v2</guid>
      <dc:creator>Yuheng Zhang, Dian Yu, Baolin Peng, Linfeng Song, Ye Tian, Mingyue Huo, Nan Jiang, Haitao Mi, Dong Yu</dc:creator>
      <pubDate>Sun, 07 Jul 2024 09:51:26 GMT</pubDate>
    </item>
    <item>
      <title>Adversarial Multi-dueling Bandits</title>
      <link>http://arxiv.org/abs/2406.12475v2</link>
      <description>We introduce the problem of regret minimization in adversarial multi-dueling bandits. While adversarial preferences have been studied in dueling bandits, they have not been explored in multi-dueling bandits. In this setting, the learner is required to select $m \geq 2$ arms at each round and observes as feedback the identity of the most preferred arm which is based on an arbitrary preference matrix chosen obliviously. We introduce a novel algorithm, MiDEX (Multi Dueling EXP3), to learn from such preference feedback that is assumed to be generated from a pairwise-subset choice model. We prove that the expected cumulative $T$-round regret of MiDEX compared to a Borda-winner from a set of $K$ arms is upper bounded by $O((K \log K)^{1/3} T^{2/3})$. Moreover, we prove a lower bound of $\Omega(K^{1/3} T^{2/3})$ for the expected regret in this setting which demonstrates that our proposed algorithm is near-optimal.\n\n\nAdversarial preference optimization</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.12475v2</guid>
      <dc:creator>Pratik Gajane</dc:creator>
      <pubDate>Wed, 26 Jun 2024 10:57:40 GMT</pubDate>
    </item>
    <item>
      <title>Multi-Reference Preference Optimization for Large Language Models</title>
      <link>http://arxiv.org/abs/2405.16388v1</link>
      <description>How can Large Language Models (LLMs) be aligned with human intentions and values? A typical solution is to gather human preference on model outputs and finetune the LLMs accordingly while ensuring that updates do not deviate too far from a reference model. Recent approaches, such as direct preference optimization (DPO), have eliminated the need for unstable and sluggish reinforcement learning optimization by introducing close-formed supervised losses. However, a significant limitation of the current approach is its design for a single reference model only, neglecting to leverage the collective power of numerous pretrained LLMs. To overcome this limitation, we introduce a novel closed-form formulation for direct preference optimization using multiple reference models. The resulting algorithm, Multi-Reference Preference Optimization (MRPO), leverages broader prior knowledge from diverse reference models, substantially enhancing preference learning capabilities compared to the single-reference DPO. Our experiments demonstrate that LLMs finetuned with MRPO generalize better in various preference data, regardless of data scarcity or abundance. Furthermore, MRPO effectively finetunes LLMs to exhibit superior performance in several downstream natural language processing tasks such as GSM8K and TruthfulQA.\n\n\nPreference fine-tuning of llms should leverage suboptimal, on-policy data</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.16388v1</guid>
      <dc:creator>Hung Le, Quan Tran, Dung Nguyen, Kien Do, Saloni Mittal, Kelechi Ogueji, Svetha Venkatesh</dc:creator>
      <pubDate>Sun, 26 May 2024 00:29:04 GMT</pubDate>
    </item>
    <item>
      <title>SA-MDKIF: A Scalable and Adaptable Medical Domain Knowledge Injection Framework for Large Language Models</title>
      <link>http://arxiv.org/abs/2402.00474v1</link>
      <description>Recent advances in large language models (LLMs) have demonstrated exceptional performance in various natural language processing (NLP) tasks. However, their effective application in the medical domain is hampered by a lack of medical domain knowledge. In this study, we present SA-MDKIF, a scalable and adaptable framework that aims to inject medical knowledge into general-purpose LLMs through instruction tuning, thereby enabling adaptability for various downstream tasks. SA-MDKIF consists of two stages: skill training and skill adaptation. In the first stage, we define 12 basic medical skills and use AdaLoRA to train these skills based on uniformly formatted instructional datasets that we have constructed. In the next stage, we train the skill router using task-specific downstream data and use this router to integrate the acquired skills with LLMs during inference. Experimental results on 9 different medical tasks show that SA-MDKIF improves performance by 10-20% compared to the original LLMs. Notably, this improvement is particularly pronounced for unseen medical tasks, showing an improvement of up to 30%.\n\n\nQilin-med: Multi-stage knowledge injection advanced medical large language model</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.00474v1</guid>
      <dc:creator>Tianhan Xu, Zhe Hu, Ling Chen, Bin Li</dc:creator>
      <pubDate>Thu, 01 Feb 2024 10:26:27 GMT</pubDate>
    </item>
    <item>
      <title>RLHF Workflow: From Reward Modeling to Online RLHF</title>
      <link>http://arxiv.org/abs/2405.07863v2</link>
      <description>We present the workflow of Online Iterative Reinforcement Learning from Human Feedback (RLHF) in this technical report, which is widely reported to outperform its offline counterpart by a large margin in the recent large language model (LLM) literature. However, existing open-source RLHF projects are still largely confined to the offline learning setting. In this technical report, we aim to fill in this gap and provide a detailed recipe that is easy to reproduce for online iterative RLHF. In particular, since online human feedback is usually infeasible for open-source communities with limited resources, we start by constructing preference models using a diverse set of open-source datasets and use the constructed proxy preference model to approximate human feedback. Then, we discuss the theoretical insights and algorithmic principles behind online iterative RLHF, followed by a detailed practical implementation. Our trained LLM, LLaMA-3-8B-SFR-Iterative-DPO-R, achieves impressive performance on LLM chatbot benchmarks, including AlpacaEval-2, Arena-Hard, and MT-Bench, as well as other academic benchmarks such as HumanEval and TruthfulQA. We have shown that supervised fine-tuning (SFT) and iterative RLHF can obtain state-of-the-art performance with fully open-source datasets. Further, we have made our models, curated datasets, and comprehensive step-by-step code guidebooks publicly available. Please refer to https://github.com/RLHFlow/RLHF-Reward-Modeling and https://github.com/RLHFlow/Online-RLHF for more detailed information.\n\n\nRlhf workflow: From reward modeling to online rlhf</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.07863v2</guid>
      <dc:creator>Hanze Dong, Wei Xiong, Bo Pang, Haoxiang Wang, Han Zhao, Yingbo Zhou, Nan Jiang, Doyen Sahoo, Caiming Xiong, Tong Zhang</dc:creator>
      <pubDate>Wed, 12 Jun 2024 04:40:53 GMT</pubDate>
    </item>
    <item>
      <title>Iterative Reasoning Preference Optimization</title>
      <link>http://arxiv.org/abs/2404.19733v3</link>
      <description>Iterative preference optimization methods have recently been shown to perform well for general instruction tuning tasks, but typically make little improvement on reasoning tasks (Yuan et al., 2024, Chen et al., 2024). In this work we develop an iterative approach that optimizes the preference between competing generated Chain-of-Thought (CoT) candidates by optimizing for winning vs. losing reasoning steps that lead to the correct answer. We train using a modified DPO loss (Rafailov et al., 2023) with an additional negative log-likelihood term, which we find to be crucial. We show reasoning improves across repeated iterations of this scheme. While only relying on examples in the training set, our approach results in increasing accuracy on GSM8K, MATH, and ARC-Challenge for Llama-2-70B-Chat, outperforming other Llama-2-based models not relying on additionally sourced datasets. For example, we see a large improvement from 55.6% to 81.6% on GSM8K and an accuracy of 88.7% with majority voting out of 32 samples.\n\n\nIterative reasoning preference optimization</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2404.19733v3</guid>
      <dc:creator>Richard Yuanzhe Pang, Weizhe Yuan, Kyunghyun Cho, He He, Sainbayar Sukhbaatar, Jason Weston</dc:creator>
      <pubDate>Wed, 26 Jun 2024 01:28:35 GMT</pubDate>
    </item>
    <item>
      <title>Emptying the Ocean with a Spoon: Should We Edit Models?</title>
      <link>http://arxiv.org/abs/2310.11958v1</link>
      <description>We call into question the recently popularized method of direct model editing as a means of correcting factual errors in LLM generations. We contrast model editing with three similar but distinct approaches that pursue better defined objectives: (1) retrieval-based architectures, which decouple factual memory from inference and linguistic capabilities embodied in LLMs; (2) concept erasure methods, which aim at preventing systemic bias in generated text; and (3) attribution methods, which aim at grounding generations into identified textual sources. We argue that direct model editing cannot be trusted as a systematic remedy for the disadvantages inherent to LLMs, and while it has proven potential in improving model explainability, it opens risks by reinforcing the notion that models can be trusted for factuality. We call for cautious promotion and application of model editing as part of the LLM deployment process, and for responsibly limiting the use cases of LLMs to those not relying on editing as a critical component.\n\n\nEmptying the Ocean with a Spoon: Should We Edit Models?</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2310.11958v1</guid>
      <dc:creator>Yuval Pinter, Michael Elhadad</dc:creator>
      <pubDate>Wed, 18 Oct 2023 13:38:03 GMT</pubDate>
    </item>
    <item>
      <title>LLM-Grounder: Open-Vocabulary 3D Visual Grounding with Large Language Model as an Agent</title>
      <link>http://arxiv.org/abs/2309.12311v1</link>
      <description>3D visual grounding is a critical skill for household robots, enabling them to navigate, manipulate objects, and answer questions based on their environment. While existing approaches often rely on extensive labeled data or exhibit limitations in handling complex language queries, we propose LLM-Grounder, a novel zero-shot, open-vocabulary, Large Language Model (LLM)-based 3D visual grounding pipeline. LLM-Grounder utilizes an LLM to decompose complex natural language queries into semantic constituents and employs a visual grounding tool, such as OpenScene or LERF, to identify objects in a 3D scene. The LLM then evaluates the spatial and commonsense relations among the proposed objects to make a final grounding decision. Our method does not require any labeled training data and can generalize to novel 3D scenes and arbitrary text queries. We evaluate LLM-Grounder on the ScanRefer benchmark and demonstrate state-of-the-art zero-shot grounding accuracy. Our findings indicate that LLMs significantly improve the grounding capability, especially for complex language queries, making LLM-Grounder an effective approach for 3D vision-language tasks in robotics. Videos and interactive demos can be found on the project website https://chat-with-nerf.github.io/ .\n\n\nGrounding or guesswork? large language models are presumptive grounders</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2309.12311v1</guid>
      <dc:creator>Jianing Yang, Xuweiyi Chen, Shengyi Qian, Nikhil Madaan, Madhavan Iyengar, David F. Fouhey, Joyce Chai</dc:creator>
      <pubDate>Thu, 21 Sep 2023 17:59:45 GMT</pubDate>
    </item>
    <item>
      <title>Mind Switches in Futurama and Stargate</title>
      <link>http://arxiv.org/abs/1209.4991v3</link>
      <description>Let P be a permutation expressed as a product of nontrivial disjoint cycles. When writing P as a product of distinct transpositions none equal to a factor of P, what is the smallest number of transpositions that can be used? We answer this question and give applications to mind-switching problems that have arisen in connection with the popular sci-fi television series Futurama and Stargate SG-1.\n\n\nStar-gate: Teaching language models to ask clarifying questions</description>
      <guid isPermaLink="false">http://arxiv.org/abs/1209.4991v3</guid>
      <dc:creator>Ron Evans, Lihua Huang</dc:creator>
      <pubDate>Thu, 27 Nov 2014 10:52:40 GMT</pubDate>
    </item>
    <item>
      <title>Training Socially Aligned Language Models on Simulated Social Interactions</title>
      <link>http://arxiv.org/abs/2305.16960v3</link>
      <description>Social alignment in AI systems aims to ensure that these models behave according to established societal values. However, unlike humans, who derive consensus on value judgments through social interaction, current language models (LMs) are trained to rigidly replicate their training corpus in isolation, leading to subpar generalization in unfamiliar scenarios and vulnerability to adversarial attacks. This work presents a novel training paradigm that permits LMs to learn from simulated social interactions. In comparison to existing methodologies, our approach is considerably more scalable and efficient, demonstrating superior performance in alignment benchmarks and human evaluations. This paradigm shift in the training of LMs brings us a step closer to developing AI systems that can robustly and accurately reflect societal norms and values.\n\n\nTraining socially aligned language models on simulated social interactions</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2305.16960v3</guid>
      <dc:creator>Ruibo Liu, Ruixin Yang, Chenyan Jia, Ge Zhang, Denny Zhou, Andrew M. Dai, Diyi Yang, Soroush Vosoughi</dc:creator>
      <pubDate>Sat, 28 Oct 2023 09:02:39 GMT</pubDate>
    </item>
    <item>
      <title>Superiority of Softmax: Unveiling the Performance Edge Over Linear Attention</title>
      <link>http://arxiv.org/abs/2310.11685v1</link>
      <description>Large transformer models have achieved state-of-the-art results in numerous natural language processing tasks. Among the pivotal components of the transformer architecture, the attention mechanism plays a crucial role in capturing token interactions within sequences through the utilization of softmax function.   Conversely, linear attention presents a more computationally efficient alternative by approximating the softmax operation with linear complexity. However, it exhibits substantial performance degradation when compared to the traditional softmax attention mechanism.   In this paper, we bridge the gap in our theoretical understanding of the reasons behind the practical performance gap between softmax and linear attention. By conducting a comprehensive comparative analysis of these two attention mechanisms, we shed light on the underlying reasons for why softmax attention outperforms linear attention in most scenarios.\n\n\nSuperiority of softmax: Unveiling the performance edge over linear attention</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2310.11685v1</guid>
      <dc:creator>Yichuan Deng, Zhao Song, Tianyi Zhou</dc:creator>
      <pubDate>Wed, 18 Oct 2023 03:17:57 GMT</pubDate>
    </item>
    <item>
      <title>Defending Large Language Models against Jailbreak Attacks via Semantic Smoothing</title>
      <link>http://arxiv.org/abs/2402.16192v2</link>
      <description>Aligned large language models (LLMs) are vulnerable to jailbreaking attacks, which bypass the safeguards of targeted LLMs and fool them into generating objectionable content. While initial defenses show promise against token-based threat models, there do not exist defenses that provide robustness against semantic attacks and avoid unfavorable trade-offs between robustness and nominal performance. To meet this need, we propose SEMANTICSMOOTH, a smoothing-based defense that aggregates the predictions of multiple semantically transformed copies of a given input prompt. Experimental results demonstrate that SEMANTICSMOOTH achieves state-of-the-art robustness against GCG, PAIR, and AutoDAN attacks while maintaining strong nominal performance on instruction following benchmarks such as InstructionFollowing and AlpacaEval. The codes will be publicly available at https://github.com/UCSB-NLP-Chang/SemanticSmooth.\n\n\nDefending large language models against jailbreak attacks via semantic smoothing</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.16192v2</guid>
      <dc:creator>Jiabao Ji, Bairu Hou, Alexander Robey, George J. Pappas, Hamed Hassani, Yang Zhang, Eric Wong, Shiyu Chang</dc:creator>
      <pubDate>Wed, 28 Feb 2024 23:11:33 GMT</pubDate>
    </item>
    <item>
      <title>Urban Generative Intelligence (UGI): A Foundational Platform for Agents in Embodied City Environment</title>
      <link>http://arxiv.org/abs/2312.11813v1</link>
      <description>Urban environments, characterized by their complex, multi-layered networks encompassing physical, social, economic, and environmental dimensions, face significant challenges in the face of rapid urbanization. These challenges, ranging from traffic congestion and pollution to social inequality, call for advanced technological interventions. Recent developments in big data, artificial intelligence, urban computing, and digital twins have laid the groundwork for sophisticated city modeling and simulation. However, a gap persists between these technological capabilities and their practical implementation in addressing urban challenges in an systemic-intelligent way. This paper proposes Urban Generative Intelligence (UGI), a novel foundational platform integrating Large Language Models (LLMs) into urban systems to foster a new paradigm of urban intelligence. UGI leverages CityGPT, a foundation model trained on city-specific multi-source data, to create embodied agents for various urban tasks. These agents, operating within a textual urban environment emulated by city simulator and urban knowledge graph, interact through a natural language interface, offering an open platform for diverse intelligent and embodied agent development. This platform not only addresses specific urban issues but also simulates complex urban systems, providing a multidisciplinary approach to understand and manage urban complexity. This work signifies a transformative step in city science and urban intelligence, harnessing the power of LLMs to unravel and address the intricate dynamics of urban systems. The code repository with demonstrations will soon be released here https://github.com/tsinghua-fib-lab/UGI.\n\n\nUrban generative intelligence (ugi): A foundational platform for agents in embodied city environment</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2312.11813v1</guid>
      <dc:creator>Fengli Xu, Jun Zhang, Chen Gao, Jie Feng, Yong Li</dc:creator>
      <pubDate>Tue, 19 Dec 2023 03:12:13 GMT</pubDate>
    </item>
    <item>
      <title>Teaching Algorithmic Reasoning via In-context Learning</title>
      <link>http://arxiv.org/abs/2211.09066v1</link>
      <description>Large language models (LLMs) have shown increasing in-context learning capabilities through scaling up model and data size. Despite this progress, LLMs are still unable to solve algorithmic reasoning problems. While providing a rationale with the final answer has led to further improvements in multi-step reasoning problems, Anil et al. 2022 showed that even simple algorithmic reasoning tasks such as parity are far from solved. In this work, we identify and study four key stages for successfully teaching algorithmic reasoning to LLMs: (1) formulating algorithms as skills, (2) teaching multiple skills simultaneously (skill accumulation), (3) teaching how to combine skills (skill composition) and (4) teaching how to use skills as tools. We show that it is possible to teach algorithmic reasoning to LLMs via in-context learning, which we refer to as algorithmic prompting. We evaluate our approach on a variety of arithmetic and quantitative reasoning tasks, and demonstrate significant boosts in performance over existing prompting techniques. In particular, for long parity, addition, multiplication and subtraction, we achieve an error reduction of approximately 10x, 9x, 5x and 2x respectively compared to the best available baselines.\n\n\nTeaching large language models to reason with reinforcement learning</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2211.09066v1</guid>
      <dc:creator>Hattie Zhou, Azade Nova, Hugo Larochelle, Aaron Courville, Behnam Neyshabur, Hanie Sedghi</dc:creator>
      <pubDate>Tue, 15 Nov 2022 06:12:28 GMT</pubDate>
    </item>
    <item>
      <title>Arithmetic Control of LLMs for Diverse User Preferences: Directional Preference Alignment with Multi-Objective Rewards</title>
      <link>http://arxiv.org/abs/2402.18571v3</link>
      <description>Fine-grained control over large language models (LLMs) remains a significant challenge, hindering their adaptability to diverse user needs. While Reinforcement Learning from Human Feedback (RLHF) shows promise in aligning LLMs, its reliance on scalar rewards often limits its ability to capture diverse user preferences in real-world applications. To address this limitation, we introduce the Directional Preference Alignment (DPA) framework. Unlike the scalar-reward RLHF, DPA incorporates multi-objective reward modeling to represent diverse preference profiles. Additionally, DPA models user preferences as directions (i.e., unit vectors) in the reward space to achieve user-dependent preference control. Our method involves training a multi-objective reward model and then fine-tuning the LLM with a preference-conditioned variant of Rejection Sampling Finetuning (RSF), an RLHF method adopted by Llama 2. This method enjoys a better performance trade-off across various reward objectives. In comparison with the scalar-reward RLHF, DPA offers users intuitive control over LLM generation: they can arithmetically specify their desired trade-offs (e.g., more helpfulness with less verbosity). We also validate the effectiveness of DPA with real-world alignment experiments on Mistral-7B. Our method provides straightforward arithmetic control over the trade-off between helpfulness and verbosity while maintaining competitive performance with strong baselines such as Direct Preference Optimization (DPO).\n\n\nArithmetic control of llms for diverse user preferences: Directional preference alignment with multi-objective rewards</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.18571v3</guid>
      <dc:creator>Haoxiang Wang, Yong Lin, Wei Xiong, Rui Yang, Shizhe Diao, Shuang Qiu, Han Zhao, Tong Zhang</dc:creator>
      <pubDate>Wed, 06 Mar 2024 08:07:02 GMT</pubDate>
    </item>
    <item>
      <title>Red Teaming Visual Language Models</title>
      <link>http://arxiv.org/abs/2401.12915v1</link>
      <description>VLMs (Vision-Language Models) extend the capabilities of LLMs (Large Language Models) to accept multimodal inputs. Since it has been verified that LLMs can be induced to generate harmful or inaccurate content through specific test cases (termed as Red Teaming), how VLMs perform in similar scenarios, especially with their combination of textual and visual inputs, remains a question. To explore this problem, we present a novel red teaming dataset RTVLM, which encompasses 10 subtasks (e.g., image misleading, multi-modal jail-breaking, face fairness, etc) under 4 primary aspects (faithfulness, privacy, safety, fairness). Our RTVLM is the first red-teaming dataset to benchmark current VLMs in terms of these 4 different aspects. Detailed analysis shows that 10 prominent open-sourced VLMs struggle with the red teaming in different degrees and have up to 31% performance gap with GPT-4V. Additionally, we simply apply red teaming alignment to LLaVA-v1.5 with Supervised Fine-tuning (SFT) using RTVLM, and this bolsters the models' performance with 10% in RTVLM test set, 13% in MM-Hal, and without noticeable decline in MM-Bench, overpassing other LLaVA-based models with regular alignment data. This reveals that current open-sourced VLMs still lack red teaming alignment. Our code and datasets will be open-source.\n\n\nRed teaming visual language models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2401.12915v1</guid>
      <dc:creator>Mukai Li, Lei Li, Yuwei Yin, Masood Ahmed, Zhenguang Liu, Qi Liu</dc:creator>
      <pubDate>Tue, 23 Jan 2024 17:07:18 GMT</pubDate>
    </item>
    <item>
      <title>Typhoon: Thai Large Language Models</title>
      <link>http://arxiv.org/abs/2312.13951v1</link>
      <description>Typhoon is a series of Thai large language models (LLMs) developed specifically for the Thai language. This technical report presents challenges and insights in developing Thai LLMs, including data preparation, pretraining, instruction-tuning, and evaluation. As one of the challenges of low-resource languages is the amount of pretraining data, we apply continual training to transfer existing world knowledge from a strong LLM. To evaluate the Thai knowledge encapsulated in each model from the pretraining stage, we develop ThaiExam, a benchmark based on examinations for high-school students and investment professionals in Thailand. In addition, we fine-tune Typhoon to follow Thai instructions, and we evaluate instruction-tuned models on Thai instruction datasets as well as translation, summarization, and question-answering tasks. Experimental results on a suite of Thai benchmarks show that Typhoon outperforms all open-source Thai language models, and its performance is on par with GPT-3.5 in Thai while having only 7 billion parameters and being 2.62 times more efficient in tokenizing Thai text.\n\n\nTyphoon: Thai large language models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2312.13951v1</guid>
      <dc:creator>Kunat Pipatanakul, Phatrasek Jirabovonvisut, Potsawee Manakul, Sittipong Sripaisarnmongkol, Ruangsak Patomwong, Pathomporn Chokchainant, Kasima Tharnpipitchai</dc:creator>
      <pubDate>Thu, 21 Dec 2023 15:38:41 GMT</pubDate>
    </item>
    <item>
      <title>FLM-101B: An Open LLM and How to Train It with $100K Budget</title>
      <link>http://arxiv.org/abs/2309.03852v2</link>
      <description>Large language models (LLMs) have achieved remarkable success in NLP and multimodal tasks, among others. Despite these successes, two main challenges remain in developing LLMs: (i) high computational cost, and (ii) fair and objective evaluations. In this paper, we report a solution to significantly reduce LLM training cost through a growth strategy. We demonstrate that a 101B-parameter LLM with 0.31T tokens can be trained with a budget of 100K US dollars. Inspired by IQ tests, we also consolidate an additional range of evaluations on top of existing evaluations that focus on knowledge-oriented abilities. These IQ evaluations include symbolic mapping, rule understanding, pattern mining, and anti-interference. Such evaluations minimize the potential impact of memorization. Experimental results show that our model, named FLM-101B, trained with a budget of 100K US dollars, achieves performance comparable to powerful and well-known models, e.g., GPT-3 and GLM-130B, especially on the additional range of IQ evaluations. The checkpoint of FLM-101B is released at https://huggingface.co/CofeAI/FLM-101B.\n\n\nFlm-101b: An open llm and how to train it with $100 k budget</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2309.03852v2</guid>
      <dc:creator>Xiang Li, Yiqun Yao, Xin Jiang, Xuezhi Fang, Xuying Meng, Siqi Fan, Peng Han, Jing Li, Li Du, Bowen Qin, Zheng Zhang, Aixin Sun, Yequan Wang</dc:creator>
      <pubDate>Sun, 17 Sep 2023 07:38:10 GMT</pubDate>
    </item>
    <item>
      <title>SoK: Memorization in General-Purpose Large Language Models</title>
      <link>http://arxiv.org/abs/2310.18362v1</link>
      <description>Large Language Models (LLMs) are advancing at a remarkable pace, with myriad applications under development. Unlike most earlier machine learning models, they are no longer built for one specific application but are designed to excel in a wide range of tasks. A major part of this success is due to their huge training datasets and the unprecedented number of model parameters, which allow them to memorize large amounts of information contained in the training data. This memorization goes beyond mere language, and encompasses information only present in a few documents. This is often desirable since it is necessary for performing tasks such as question answering, and therefore an important part of learning, but also brings a whole array of issues, from privacy and security to copyright and beyond. LLMs can memorize short secrets in the training data, but can also memorize concepts like facts or writing styles that can be expressed in text in many different ways. We propose a taxonomy for memorization in LLMs that covers verbatim text, facts, ideas and algorithms, writing styles, distributional properties, and alignment goals. We describe the implications of each type of memorization - both positive and negative - for model performance, privacy, security and confidentiality, copyright, and auditing, and ways to detect and prevent memorization. We further highlight the challenges that arise from the predominant way of defining memorization with respect to model behavior instead of model weights, due to LLM-specific phenomena such as reasoning capabilities or differences between decoding algorithms. Throughout the paper, we describe potential risks and opportunities arising from memorization in LLMs that we hope will motivate new research directions.\n\n\nSok: Memorization in general-purpose large language models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2310.18362v1</guid>
      <dc:creator>Valentin Hartmann, Anshuman Suri, Vincent Bindschaedler, David Evans, Shruti Tople, Robert West</dc:creator>
      <pubDate>Tue, 24 Oct 2023 14:25:53 GMT</pubDate>
    </item>
    <item>
      <title>AnyTool: Self-Reflective, Hierarchical Agents for Large-Scale API Calls</title>
      <link>http://arxiv.org/abs/2402.04253v1</link>
      <description>We introduce AnyTool, a large language model agent designed to revolutionize the utilization of a vast array of tools in addressing user queries. We utilize over 16,000 APIs from Rapid API, operating under the assumption that a subset of these APIs could potentially resolve the queries. AnyTool primarily incorporates three elements: an API retriever with a hierarchical structure, a solver aimed at resolving user queries using a selected set of API candidates, and a self-reflection mechanism, which re-activates AnyTool if the initial solution proves impracticable. AnyTool is powered by the function calling feature of GPT-4, eliminating the need for training external modules. We also revisit the evaluation protocol introduced by previous works and identify a limitation in this protocol that leads to an artificially high pass rate. By revising the evaluation protocol to better reflect practical application scenarios, we introduce an additional benchmark, termed AnyToolBench. Experiments across various datasets demonstrate the superiority of our AnyTool over strong baselines such as ToolLLM and a GPT-4 variant tailored for tool utilization. For instance, AnyTool outperforms ToolLLM by +35.4% in terms of average pass rate on ToolBench. Code will be available at https://github.com/dyabel/AnyTool.\n\n\nAnytool: Self-reflective, hierarchical agents for large-scale api calls</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.04253v1</guid>
      <dc:creator>Yu Du, Fangyun Wei, Hongyang Zhang</dc:creator>
      <pubDate>Tue, 06 Feb 2024 18:59:57 GMT</pubDate>
    </item>
    <item>
      <title>Exploring the Reasoning Abilities of Multimodal Large Language Models (MLLMs): A Comprehensive Survey on Emerging Trends in Multimodal Reasoning</title>
      <link>http://arxiv.org/abs/2401.06805v2</link>
      <description>Strong Artificial Intelligence (Strong AI) or Artificial General Intelligence (AGI) with abstract reasoning ability is the goal of next-generation AI. Recent advancements in Large Language Models (LLMs), along with the emerging field of Multimodal Large Language Models (MLLMs), have demonstrated impressive capabilities across a wide range of multimodal tasks and applications. Particularly, various MLLMs, each with distinct model architectures, training data, and training stages, have been evaluated across a broad range of MLLM benchmarks. These studies have, to varying degrees, revealed different aspects of the current capabilities of MLLMs. However, the reasoning abilities of MLLMs have not been systematically investigated. In this survey, we comprehensively review the existing evaluation protocols of multimodal reasoning, categorize and illustrate the frontiers of MLLMs, introduce recent trends in applications of MLLMs on reasoning-intensive tasks, and finally discuss current practices and future directions. We believe our survey establishes a solid base and sheds light on this important topic, multimodal reasoning.\n\n\nExploring the reasoning abilities of multimodal large language models (mllms): A comprehensive survey on emerging trends in multimodal reasoning</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2401.06805v2</guid>
      <dc:creator>Yiqi Wang, Wentao Chen, Xiaotian Han, Xudong Lin, Haiteng Zhao, Yongfei Liu, Bohan Zhai, Jianbo Yuan, Quanzeng You, Hongxia Yang</dc:creator>
      <pubDate>Thu, 18 Jan 2024 07:31:47 GMT</pubDate>
    </item>
    <item>
      <title>Accessible Instruction-Following Agent</title>
      <link>http://arxiv.org/abs/2305.06358v1</link>
      <description>Humans can collaborate and complete tasks based on visual signals and instruction from the environment. Training such a robot is difficult especially due to the understanding of the instruction and the complicated environment. Previous instruction-following agents are biased to English-centric corpus, making it unrealizable to be applied to users that use multiple languages or even low-resource languages. Nevertheless, the instruction-following agents are pre-trained in a mode that assumes the user can observe the environment, which limits its accessibility. In this work, we're trying to generalize the success of instruction-following agents to non-English languages with little corpus resources, and improve its intractability and accessibility. We introduce UVLN (Universal Vision-Language Navigation), a novel machine-translation instructional augmented framework for cross-lingual vision-language navigation, with a novel composition of state-of-the-art large language model (GPT3) with the image caption model (BLIP). We first collect a multilanguage vision-language navigation dataset via machine translation. Then we extend the standard VLN training objectives to a multilingual setting via a cross-lingual language encoder. The alignment between different languages is captured through a shared vision and action context via a cross-modal transformer, which encodes the inputs of language instruction, visual observation, and action decision sequences. To improve the intractability, we connect our agent with the large language model that informs the situation and current state to the user and also explains the action decisions. Experiments over Room Across Room Dataset prove the effectiveness of our approach. And the qualitative results show the promising intractability and accessibility of our instruction-following agent.\n\n\nInstruction position matters in sequence generation with large language models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2305.06358v1</guid>
      <dc:creator>Kairui Zhou</dc:creator>
      <pubDate>Mon, 08 May 2023 23:57:26 GMT</pubDate>
    </item>
    <item>
      <title>Personalized Language Modeling from Personalized Human Feedback</title>
      <link>http://arxiv.org/abs/2402.05133v2</link>
      <description>Reinforcement Learning from Human Feedback (RLHF) is commonly used to fine-tune large language models to better align with human preferences. However, the underlying premise of algorithms developed under this framework can be problematic when user preferences encoded in human feedback are diverse. In this work, we aim to address this problem by developing methods for building personalized language models. We first formally introduce the task of learning from personalized human feedback and explain why vanilla RLHF can be ineffective in this context. We then propose a general Personalized-RLHF (P-RLHF) framework, including a user model that maps user information to user representations and can flexibly encode our assumptions on user preferences. We develop new learning objectives to perform personalized Direct Preference Optimization that jointly learns a user model and a personalized language model. We demonstrate the efficacy of our proposed method through (1) a synthetic task where we fine-tune a GPT-J 6B model to align with users with conflicting preferences on generation length; and (2) an instruction following task where we fine-tune a Tulu-7B model to generate responses for users with diverse preferences on the style of responses. In both cases, our learned models can generate personalized responses that are better aligned with the preferences of individual users.\n\n\nPersonalized language modeling from personalized human feedback</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.05133v2</guid>
      <dc:creator>Xinyu Li, Zachary C. Lipton, Liu Leqi</dc:creator>
      <pubDate>Sun, 07 Jul 2024 19:31:21 GMT</pubDate>
    </item>
    <item>
      <title>RLAIF-V: Aligning MLLMs through Open-Source AI Feedback for Super GPT-4V Trustworthiness</title>
      <link>http://arxiv.org/abs/2405.17220v1</link>
      <description>Learning from feedback reduces the hallucination of multimodal large language models (MLLMs) by aligning them with human preferences. While traditional methods rely on labor-intensive and time-consuming manual labeling, recent approaches employing models as automatic labelers have shown promising results without human intervention. However, these methods heavily rely on costly proprietary models like GPT-4V, resulting in scalability issues. Moreover, this paradigm essentially distills the proprietary models to provide a temporary solution to quickly bridge the performance gap. As this gap continues to shrink, the community is soon facing the essential challenge of aligning MLLMs using labeler models of comparable capability. In this work, we introduce RLAIF-V, a novel framework that aligns MLLMs in a fully open-source paradigm for super GPT-4V trustworthiness. RLAIF-V maximally exploits the open-source feedback from two perspectives, including high-quality feedback data and online feedback learning algorithm. Extensive experiments on seven benchmarks in both automatic and human evaluation show that RLAIF-V substantially enhances the trustworthiness of models without sacrificing performance on other tasks. Using a 34B model as labeler, RLAIF-V 7B model reduces object hallucination by 82.9\% and overall hallucination by 42.1\%, outperforming the labeler model. Remarkably, RLAIF-V also reveals the self-alignment potential of open-source MLLMs, where a 12B model can learn from the feedback of itself to achieve less than 29.5\% overall hallucination rate, surpassing GPT-4V (45.9\%) by a large margin. The results shed light on a promising route to enhance the efficacy of leading-edge MLLMs.\n\n\nRlaif-v: Aligning mllms through open-source ai feedback for super gpt-4v trustworthiness</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.17220v1</guid>
      <dc:creator>Tianyu Yu, Haoye Zhang, Yuan Yao, Yunkai Dang, Da Chen, Xiaoman Lu, Ganqu Cui, Taiwen He, Zhiyuan Liu, Tat-Seng Chua, Maosong Sun</dc:creator>
      <pubDate>Mon, 27 May 2024 14:37:01 GMT</pubDate>
    </item>
    <item>
      <title>Dense Reward for Free in Reinforcement Learning from Human Feedback</title>
      <link>http://arxiv.org/abs/2402.00782v1</link>
      <description>Reinforcement Learning from Human Feedback (RLHF) has been credited as the key advance that has allowed Large Language Models (LLMs) to effectively follow instructions and produce useful assistance. Classically, this involves generating completions from the LLM in response to a query before using a separate reward model to assign a score to the full completion. As an auto-regressive process, the LLM has to take many &quot;actions&quot; (selecting individual tokens) and only receives a single, sparse reward at the end of an episode, a setup that is known to be difficult to optimise in traditional reinforcement learning. In this work we leverage the fact that the reward model contains more information than just its scalar output, in particular, it calculates an attention map over tokens as part of the transformer architecture. We use these attention weights to redistribute the reward along the whole completion, effectively densifying the signal and highlighting the most important tokens, all without incurring extra computational cost or requiring any additional modelling. We demonstrate that, theoretically, this approach is equivalent to potential-based reward shaping, ensuring that the optimal policy remains unchanged. Empirically, we show that it stabilises training, accelerates the rate of learning, and, in practical cases, may lead to better local optima.\n\n\nDense reward for free in reinforcement learning from human feedback</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.00782v1</guid>
      <dc:creator>Alex J. Chan, Hao Sun, Samuel Holt, Mihaela van der Schaar</dc:creator>
      <pubDate>Thu, 01 Feb 2024 17:10:35 GMT</pubDate>
    </item>
    <item>
      <title>On the Safety of Open-Sourced Large Language Models: Does Alignment Really Prevent Them From Being Misused?</title>
      <link>http://arxiv.org/abs/2310.01581v1</link>
      <description>Large Language Models (LLMs) have achieved unprecedented performance in Natural Language Generation (NLG) tasks. However, many existing studies have shown that they could be misused to generate undesired content. In response, before releasing LLMs for public access, model developers usually align those language models through Supervised Fine-Tuning (SFT) or Reinforcement Learning with Human Feedback (RLHF). Consequently, those aligned large language models refuse to generate undesired content when facing potentially harmful/unethical requests. A natural question is &quot;could alignment really prevent those open-sourced large language models from being misused to generate undesired content?''. In this work, we provide a negative answer to this question. In particular, we show those open-sourced, aligned large language models could be easily misguided to generate undesired content without heavy computations or careful prompt designs. Our key idea is to directly manipulate the generation process of open-sourced LLMs to misguide it to generate undesired content including harmful or biased information and even private data. We evaluate our method on 4 open-sourced LLMs accessible publicly and our finding highlights the need for more advanced mitigation strategies for open-sourced LLMs.\n\n\nOn the Safety of Open-Sourced Large Language Models: Does Alignment Really Prevent Them From Being Misused?</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2310.01581v1</guid>
      <dc:creator>Hangfan Zhang, Zhimeng Guo, Huaisheng Zhu, Bochuan Cao, Lu Lin, Jinyuan Jia, Jinghui Chen, Dinghao Wu</dc:creator>
      <pubDate>Mon, 02 Oct 2023 19:22:01 GMT</pubDate>
    </item>
    <item>
      <title>Value FULCRA: Mapping Large Language Models to the Multidimensional Spectrum of Basic Human Values</title>
      <link>http://arxiv.org/abs/2311.10766v1</link>
      <description>The rapid advancement of Large Language Models (LLMs) has attracted much attention to value alignment for their responsible development. However, how to define values in this context remains a largely unexplored question. Existing work mainly follows the Helpful, Honest, Harmless principle and specifies values as risk criteria formulated in the AI community, e.g., fairness and privacy protection, suffering from poor clarity, adaptability and transparency. Inspired by basic values in humanity and social science across cultures, this work proposes a novel basic value alignment paradigm and introduces a value space spanned by basic value dimensions. All LLMs' behaviors can be mapped into the space by identifying the underlying values, possessing the potential to address the three challenges. To foster future research, we apply the representative Schwartz's Theory of Basic Values as an initialized example and construct FULCRA, a dataset consisting of 5k (LLM output, value vector) pairs. Our extensive analysis of FULCRA reveals the underlying relation between basic values and LLMs' behaviors, demonstrating that our approach not only covers existing mainstream risks but also anticipates possibly unidentified ones. Additionally, we present an initial implementation of the basic value evaluation and alignment, paving the way for future research in this line.\n\n\nValue fulcra: Mapping large language models to the multidimensional spectrum of basic human values</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2311.10766v1</guid>
      <dc:creator>Jing Yao, Xiaoyuan Yi, Xiting Wang, Yifan Gong, Xing Xie</dc:creator>
      <pubDate>Wed, 15 Nov 2023 10:29:28 GMT</pubDate>
    </item>
    <item>
      <title>Aligning Large Language Models with Human Preferences through Representation Engineering</title>
      <link>http://arxiv.org/abs/2312.15997v3</link>
      <description>Aligning large language models (LLMs) with human preferences is crucial for enhancing their utility in terms of helpfulness, truthfulness, safety, harmlessness, and interestingness. Existing methods for achieving this alignment often involves employing reinforcement learning from human feedback (RLHF) to fine-tune LLMs based on human labels assessing the relative quality of model responses. Nevertheless, RLHF is susceptible to instability during fine-tuning and presents challenges in implementation.Drawing inspiration from the emerging field of representation engineering (RepE), this study aims to identify relevant representations for high-level human preferences embedded in patterns of activity within an LLM, and achieve precise control of model behavior by transforming its representations. This novel approach, denoted as Representation Alignment from Human Feedback (RAHF), proves to be effective, computationally efficient, and easy to implement.Extensive experiments demonstrate the efficacy of RAHF in not only capturing but also manipulating representations to align with a broad spectrum of human preferences or values, rather than being confined to a singular concept or function (e.g. honesty or bias). RAHF's versatility in accommodating diverse human preferences shows its potential for advancing LLM performance.\n\n\nAligning large language models with human preferences through representation engineering</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2312.15997v3</guid>
      <dc:creator>Wenhao Liu, Xiaohua Wang, Muling Wu, Tianlong Li, Changze Lv, Zixuan Ling, Jianhao Zhu, Cenyuan Zhang, Xiaoqing Zheng, Xuanjing Huang</dc:creator>
      <pubDate>Wed, 03 Jul 2024 05:21:02 GMT</pubDate>
    </item>
    <item>
      <title>When to Show a Suggestion? Integrating Human Feedback in AI-Assisted Programming</title>
      <link>http://arxiv.org/abs/2306.04930v3</link>
      <description>AI powered code-recommendation systems, such as Copilot and CodeWhisperer, provide code suggestions inside a programmer's environment (e.g., an IDE) with the aim of improving productivity. We pursue mechanisms for leveraging signals about programmers' acceptance and rejection of code suggestions to guide recommendations. We harness data drawn from interactions with GitHub Copilot, a system used by millions of programmers, to develop interventions that can save time for programmers. We introduce a utility-theoretic framework to drive decisions about suggestions to display versus withhold. The approach, conditional suggestion display from human feedback (CDHF), relies on a cascade of models that provide the likelihood that recommended code will be accepted. These likelihoods are used to selectively hide suggestions, reducing both latency and programmer verification time. Using data from 535 programmers, we perform a retrospective evaluation of CDHF and show that we can avoid displaying a significant fraction of suggestions that would have been rejected. We further demonstrate the importance of incorporating the programmer's latent unobserved state in decisions about when to display suggestions through an ablation study. Finally, we showcase how using suggestion acceptance as a reward signal for guiding the display of suggestions can lead to suggestions of reduced quality, indicating an unexpected pitfall.\n\n\nWhen to show a suggestion? Integrating human feedback in AI-assisted programming</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2306.04930v3</guid>
      <dc:creator>Hussein Mozannar, Gagan Bansal, Adam Fourney, Eric Horvitz</dc:creator>
      <pubDate>Mon, 22 Apr 2024 04:12:44 GMT</pubDate>
    </item>
    <item>
      <title>Token-level Direct Preference Optimization</title>
      <link>http://arxiv.org/abs/2404.11999v4</link>
      <description>Fine-tuning pre-trained Large Language Models (LLMs) is essential to align them with human values and intentions. This process often utilizes methods like pairwise comparisons and KL divergence against a reference LLM, focusing on the evaluation of full answers generated by the models. However, the generation of these responses occurs in a token level, following a sequential, auto-regressive fashion. In this paper, we introduce Token-level Direct Preference Optimization (TDPO), a novel approach to align LLMs with human preferences by optimizing policy at the token level. Unlike previous methods, which face challenges in divergence efficiency, TDPO incorporates forward KL divergence constraints for each token, improving alignment and diversity. Utilizing the Bradley-Terry model for a token-based reward system, TDPO enhances the regulation of KL divergence, while preserving simplicity without the need for explicit reward modeling. Experimental results across various text tasks demonstrate TDPO's superior performance in balancing alignment with generation diversity. Notably, fine-tuning with TDPO strikes a better balance than DPO in the controlled sentiment generation and single-turn dialogue datasets, and significantly improves the quality of generated responses compared to both DPO and PPO-based RLHF methods. Our code is open-sourced at https://github.com/Vance0124/Token-level-Direct-Preference-Optimization.\n\n\nDirect preference-based policy optimization without reward modeling</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2404.11999v4</guid>
      <dc:creator>Yongcheng Zeng, Guoqing Liu, Weiyu Ma, Ning Yang, Haifeng Zhang, Jun Wang</dc:creator>
      <pubDate>Thu, 27 Jun 2024 15:27:41 GMT</pubDate>
    </item>
    <item>
      <title>Long Is More for Alignment: A Simple but Tough-to-Beat Baseline for Instruction Fine-Tuning</title>
      <link>http://arxiv.org/abs/2402.04833v2</link>
      <description>There is a consensus that instruction fine-tuning of LLMs requires high-quality data, but what are they? LIMA (NeurIPS 2023) and AlpaGasus (ICLR 2024) are state-of-the-art methods for selecting such high-quality examples, either via manual curation or using GPT-3.5-Turbo as a quality scorer. We show that the extremely simple baseline of selecting the 1,000 instructions with longest responses -- that intuitively contain more learnable information and are harder to overfit -- from standard datasets can consistently outperform these sophisticated methods according to GPT-4 and PaLM-2 as judges, while remaining competitive on the Open LLM benchmarks that test factual knowledge. We demonstrate this for several LLMs (Llama-2-7B, Llama-2-13B, Mistral-7B-v0.1) and datasets (Alpaca-52k, Evol-Instruct-70k). In addition, a lightweight refinement of such long instructions can further improve the abilities of the fine-tuned LLMs, and allows us to obtain competitive results on MT-Bench and the 2nd highest-ranked Llama-2-7B-based model on AlpacaEval 2.0, while training on only 1,000 examples and no extra preference data. We also conduct a thorough analysis of our models to ensure that their enhanced performance is not simply due to GPT-4's preference for longer responses. Overall, our findings suggest that fine-tuning on the longest responses should be the default baseline for any work on instruction fine-tuning. We provide our code at https://github.com/tml-epfl/long-is-more-for-alignment.\n\n\nLong is more for alignment: A simple but tough-to-beat baseline for instruction fine-tuning</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.04833v2</guid>
      <dc:creator>Hao Zhao, Maksym Andriushchenko, Francesco Croce, Nicolas Flammarion</dc:creator>
      <pubDate>Tue, 04 Jun 2024 17:20:01 GMT</pubDate>
    </item>
    <item>
      <title>Calibrated Self-Rewarding Vision Language Models</title>
      <link>http://arxiv.org/abs/2405.14622v3</link>
      <description>Large Vision-Language Models (LVLMs) have made substantial progress by integrating pre-trained large language models (LLMs) and vision models through instruction tuning. Despite these advancements, LVLMs often exhibit the hallucination phenomenon, where generated text responses appear linguistically plausible but contradict the input image, indicating a misalignment between image and text pairs. This misalignment arises because the model tends to prioritize textual information over visual input, even when both the language model and visual representations are of high quality. Existing methods leverage additional models or human annotations to curate preference data and enhance modality alignment through preference optimization. These approaches may not effectively reflect the target LVLM's preferences, making the curated preferences easily distinguishable. Our work addresses these challenges by proposing the Calibrated Self-Rewarding (CSR) approach, which enables the model to self-improve by iteratively generating candidate responses, evaluating the reward for each response, and curating preference data for fine-tuning. In the reward modeling, we employ a step-wise strategy and incorporate visual constraints into the self-rewarding process to place greater emphasis on visual input. Empirical results demonstrate that CSR enhances performance and reduces hallucinations across ten benchmarks and tasks, achieving substantial improvements over existing methods by 7.62%. Our empirical results are further supported by rigorous theoretical analysis, under mild assumptions, verifying the effectiveness of introducing visual constraints into the self-rewarding paradigm. Additionally, CSR shows compatibility with different vision-language models and the ability to incrementally improve performance through iterative fine-tuning. Our data and code are available at https://github.com/YiyangZhou/CSR.\n\n\nCalibrated self-rewarding vision language models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.14622v3</guid>
      <dc:creator>Yiyang Zhou, Zhiyuan Fan, Dongjie Cheng, Sihan Yang, Zhaorun Chen, Chenhang Cui, Xiyao Wang, Yun Li, Linjun Zhang, Huaxiu Yao</dc:creator>
      <pubDate>Fri, 31 May 2024 16:37:53 GMT</pubDate>
    </item>
    <item>
      <title>A hybrid entity-centric approach to Persian pronoun resolution</title>
      <link>http://arxiv.org/abs/2211.06257v1</link>
      <description>Pronoun resolution is a challenging subset of an essential field in natural language processing called coreference resolution. Coreference resolution is about finding all entities in the text that refers to the same real-world entity. This paper presents a hybrid model combining multiple rulebased sieves with a machine-learning sieve for pronouns. For this purpose, seven high-precision rule-based sieves are designed for the Persian language. Then, a random forest classifier links pronouns to the previous partial clusters. The presented method demonstrates exemplary performance using pipeline design and combining the advantages of machine learning and rulebased methods. This method has solved some challenges in end-to-end models. In this paper, the authors develop a Persian coreference corpus called Mehr in the form of 400 documents. This corpus fixes some weaknesses of the previous corpora in the Persian language. Finally, the efficiency of the presented system compared to the earlier model in Persian is reported by evaluating the proposed method on the Mehr and Uppsala test sets.\n\n\nA survey on automatic generation of figurative language: From rule-based systems to large language models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2211.06257v1</guid>
      <dc:creator>Hassan Haji Mohammadi, Alireza Talebpour, Ahmad Mahmoudi Aznaveh, Samaneh Yazdani</dc:creator>
      <pubDate>Fri, 11 Nov 2022 14:59:58 GMT</pubDate>
    </item>
    <item>
      <title>Iterative Data Smoothing: Mitigating Reward Overfitting and Overoptimization in RLHF</title>
      <link>http://arxiv.org/abs/2401.16335v1</link>
      <description>Reinforcement Learning from Human Feedback (RLHF) is a pivotal technique that aligns language models closely with human-centric values. The initial phase of RLHF involves learning human values using a reward model from ranking data. It is observed that the performance of the reward model degrades after one epoch of training, and optimizing too much against the learned reward model eventually hinders the true objective. This paper delves into these issues, leveraging the theoretical insights to design improved reward learning algorithm termed 'Iterative Data Smoothing' (IDS). The core idea is that during each training epoch, we not only update the model with the data, but also update the date using the model, replacing hard labels with soft labels. Our empirical findings highlight the superior performance of this approach over the traditional methods.\n\n\nIterative data smoothing: Mitigating reward overfitting and overoptimization in rlhf</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2401.16335v1</guid>
      <dc:creator>Banghua Zhu, Michael I. Jordan, Jiantao Jiao</dc:creator>
      <pubDate>Mon, 29 Jan 2024 17:43:42 GMT</pubDate>
    </item>
    <item>
      <title>West-of-N: Synthetic Preference Generation for Improved Reward Modeling</title>
      <link>http://arxiv.org/abs/2401.12086v1</link>
      <description>The success of reinforcement learning from human feedback (RLHF) in language model alignment is strongly dependent on the quality of the underlying reward model. In this paper, we present a novel approach to improve reward model quality by generating synthetic preference data, thereby augmenting the training dataset with on-policy, high-quality preference pairs. Motivated by the promising results of Best-of-N sampling strategies in language model training, we extend their application to reward model training. This results in a self-training strategy to generate preference pairs by selecting the best and worst candidates in a pool of responses to a given query. Empirically, we find that this approach improves the performance of any reward model, with an effect comparable to the addition of a similar quantity of human preference data. This work opens up new avenues of research for improving RLHF for language model alignment, by offering synthetic preference generation as a solution to reward modeling challenges.\n\n\nWest-of-n: Synthetic preference generation for improved reward modeling</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2401.12086v1</guid>
      <dc:creator>Alizée Pace, Jonathan Mallinson, Eric Malmi, Sebastian Krause, Aliaksei Severyn</dc:creator>
      <pubDate>Mon, 22 Jan 2024 16:24:43 GMT</pubDate>
    </item>
    <item>
      <title>Systematic Biases in LLM Simulations of Debates</title>
      <link>http://arxiv.org/abs/2402.04049v1</link>
      <description>Recent advancements in natural language processing, especially the emergence of Large Language Models (LLMs), have opened exciting possibilities for constructing computational simulations designed to replicate human behavior accurately. However, LLMs are complex statistical learners without straightforward deductive rules, making them prone to unexpected behaviors. In this study, we highlight the limitations of LLMs in simulating human interactions, particularly focusing on LLMs' ability to simulate political debates. Our findings indicate a tendency for LLM agents to conform to the model's inherent social biases despite being directed to debate from certain political perspectives. This tendency results in behavioral patterns that seem to deviate from well-established social dynamics among humans. We reinforce these observations using an automatic self-fine-tuning method, which enables us to manipulate the biases within the LLM and demonstrate that agents subsequently align with the altered biases. These results underscore the need for further research to develop methods that help agents overcome these biases, a critical step toward creating more realistic simulations.\n\n\nSystematic biases in LLM simulations of debates</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.04049v1</guid>
      <dc:creator>Amir Taubenfeld, Yaniv Dover, Roi Reichart, Ariel Goldstein</dc:creator>
      <pubDate>Tue, 06 Feb 2024 14:51:55 GMT</pubDate>
    </item>
    <item>
      <title>Creativity</title>
      <link>http://arxiv.org/abs/1912.00091v1</link>
      <description>Creativity is perhaps what most differentiates humans from other species. It involves the capacity to shift between divergent and convergent modes of thought in response to task demands. Divergent thought has been characterized as the kind of thinking needed to generate multiple solutions, while convergent thought has been characterized as the kind of thinking needed for tasks in with one solution. Divergent thought has been conceived of as reflecting on the task from unconventional perspectives, while convergent thought has been conceived of as reflecting on it from conventional perspectives. Personality traits correlated with creativity include openness to experience, tolerance of ambiguity, and self-confidence. Evidence that creativity is linked with affective disorders is mixed. Neuroscientific research using electroencephalography (EEG) or functional magnetic resonance imaging (fMRI) suggests that creativity is associated with a loosening of cognitive control and decreased arousal. The distributed, content-addressable structure of associative memory is conducive to bringing task-relevant items to mind without the need for explicit search. Human creativity dates back to the earliest stone tools over three million years ago, with the Paleolithic marking the onset of art, science, and religion. Areas of controversy concern the relative contributions of expertise, chance, and intuition, the importance of process versus product, whether creativity is domain-specific versus domain-general, the extent to which creativity is correlated with affective disorders, and whether divergent thought entails the generation of multiple ideas or the honing of a single initially ambiguous mental representation that may manifest as different external outputs. Areas for further research include computational modeling, the biological basis of creativity, and studies that track ideation processes over time.\n\n\nCreativity and machine learning: A survey</description>
      <guid isPermaLink="false">http://arxiv.org/abs/1912.00091v1</guid>
      <dc:creator>Liane Gabora</dc:creator>
      <pubDate>Fri, 29 Nov 2019 23:17:03 GMT</pubDate>
    </item>
    <item>
      <title>TIM: Teaching Large Language Models to Translate with Comparison</title>
      <link>http://arxiv.org/abs/2307.04408v3</link>
      <description>Open-sourced large language models (LLMs) have demonstrated remarkable efficacy in various tasks with instruction tuning. However, these models can sometimes struggle with tasks that require more specialized knowledge such as translation. One possible reason for such deficiency is that instruction tuning aims to generate fluent and coherent text that continues from a given instruction without being constrained by any task-specific requirements. Moreover, it can be more challenging for tuning smaller LLMs with lower-quality training data. To address this issue, we propose a novel framework using examples in comparison to teach LLMs to learn translation. Our approach involves presenting the model with examples of correct and incorrect translations and using a preference loss to guide the model's learning. We evaluate our method on WMT2022 test sets and show that it outperforms existing methods. Our findings offer a new perspective on fine-tuning LLMs for translation tasks and provide a promising solution for generating high-quality translations. Please refer to Github for more details: https://github.com/lemon0830/TIM.\n\n\nTim: Teaching large language models to translate with comparison</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2307.04408v3</guid>
      <dc:creator>Jiali Zeng, Fandong Meng, Yongjing Yin, Jie Zhou</dc:creator>
      <pubDate>Mon, 22 Jan 2024 07:40:02 GMT</pubDate>
    </item>
    <item>
      <title>Enabling Language Models to Implicitly Learn Self-Improvement</title>
      <link>http://arxiv.org/abs/2310.00898v3</link>
      <description>Large Language Models (LLMs) have demonstrated remarkable capabilities in open-ended text generation tasks. However, the inherent open-ended nature of these tasks implies that there is always room for improvement in the quality of model responses. To address this challenge, various approaches have been proposed to enhance the performance of LLMs. There has been a growing focus on enabling LLMs to self-improve their response quality, thereby reducing the reliance on extensive human annotation efforts for collecting diverse and high-quality training data. Recently, prompting-based methods have been widely explored among self-improvement methods owing to their effectiveness, efficiency, and convenience. However, those methods usually require explicitly and thoroughly written rubrics as inputs to LLMs. It is expensive and challenging to manually derive and provide all necessary rubrics with a real-world complex goal for improvement (e.g., being more helpful and less harmful). To this end, we propose an ImPlicit Self-ImprovemenT (PIT) framework that implicitly learns the improvement goal from human preference data. PIT only requires preference data that are used to train reward models without extra human efforts. Specifically, we reformulate the training objective of reinforcement learning from human feedback (RLHF) -- instead of maximizing response quality for a given input, we maximize the quality gap of the response conditioned on a reference response. In this way, PIT is implicitly trained with the improvement goal of better aligning with human preferences. Experiments on two real-world datasets and one synthetic dataset show that our method significantly outperforms prompting-based methods.\n\n\nEnable language models to implicitly learn self-improvement from data</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2310.00898v3</guid>
      <dc:creator>Ziqi Wang, Le Hou, Tianjian Lu, Yuexin Wu, Yunxuan Li, Hongkun Yu, Heng Ji</dc:creator>
      <pubDate>Thu, 14 Mar 2024 19:27:23 GMT</pubDate>
    </item>
    <item>
      <title>Can AI Assistants Know What They Don't Know?</title>
      <link>http://arxiv.org/abs/2401.13275v2</link>
      <description>Recently, AI assistants based on large language models (LLMs) show surprising performance in many tasks, such as dialogue, solving math problems, writing code, and using tools. Although LLMs possess intensive world knowledge, they still make factual errors when facing some knowledge intensive tasks, like open-domain question answering. These untruthful responses from the AI assistant may cause significant risks in practical applications. We believe that an AI assistant's refusal to answer questions it does not know is a crucial method for reducing hallucinations and making the assistant truthful. Therefore, in this paper, we ask the question &quot;Can AI assistants know what they don't know and express them through natural language?&quot; To answer this question, we construct a model-specific &quot;I don't know&quot; (Idk) dataset for an assistant, which contains its known and unknown questions, based on existing open-domain question answering datasets. Then we align the assistant with its corresponding Idk dataset and observe whether it can refuse to answer its unknown questions after alignment. Experimental results show that after alignment with Idk datasets, the assistant can refuse to answer most its unknown questions. For questions they attempt to answer, the accuracy is significantly higher than before the alignment.\n\n\nCan AI Assistants Know What They Don't Know?</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2401.13275v2</guid>
      <dc:creator>Qinyuan Cheng, Tianxiang Sun, Xiangyang Liu, Wenwei Zhang, Zhangyue Yin, Shimin Li, Linyang Li, Zhengfu He, Kai Chen, Xipeng Qiu</dc:creator>
      <pubDate>Sun, 28 Jan 2024 09:07:13 GMT</pubDate>
    </item>
    <item>
      <title>TurkishBERTweet: Fast and Reliable Large Language Model for Social Media Analysis</title>
      <link>http://arxiv.org/abs/2311.18063v1</link>
      <description>Turkish is one of the most popular languages in the world. Wide us of this language on social media platforms such as Twitter, Instagram, or Tiktok and strategic position of the country in the world politics makes it appealing for the social network researchers and industry. To address this need, we introduce TurkishBERTweet, the first large scale pre-trained language model for Turkish social media built using almost 900 million tweets. The model shares the same architecture as base BERT model with smaller input length, making TurkishBERTweet lighter than BERTurk and can have significantly lower inference time. We trained our model using the same approach for RoBERTa model and evaluated on two text classification tasks: Sentiment Classification and Hate Speech Detection. We demonstrate that TurkishBERTweet outperforms the other available alternatives on generalizability and its lower inference time gives significant advantage to process large-scale datasets. We also compared our models with the commercial OpenAI solutions in terms of cost and performance to demonstrate TurkishBERTweet is scalable and cost-effective solution. As part of our research, we released TurkishBERTweet and fine-tuned LoRA adapters for the mentioned tasks under the MIT License to facilitate future research and applications on Turkish social media. Our TurkishBERTweet model is available at: https://github.com/ViralLab/TurkishBERTweet\n\n\nTurkishbertweet: Fast and reliable large language model for social media analysis</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2311.18063v1</guid>
      <dc:creator>Ali Najafi, Onur Varol</dc:creator>
      <pubDate>Wed, 29 Nov 2023 20:22:44 GMT</pubDate>
    </item>
    <item>
      <title>Sequential Monte Carlo Steering of Large Language Models using Probabilistic Programs</title>
      <link>http://arxiv.org/abs/2306.03081v2</link>
      <description>Even after fine-tuning and reinforcement learning, large language models (LLMs) can be difficult, if not impossible, to control reliably with prompts alone. We propose a new inference-time approach to enforcing syntactic and semantic constraints on the outputs of LLMs, called sequential Monte Carlo (SMC) steering. The key idea is to specify language generation tasks as posterior inference problems in a class of discrete probabilistic sequence models, and replace standard decoding with sequential Monte Carlo inference. For a computational cost similar to that of beam search, SMC can steer LLMs to solve diverse tasks, including infilling, generation under syntactic constraints, and prompt intersection. To facilitate experimentation with SMC steering, we present a probabilistic programming library, LLaMPPL (https://github.com/probcomp/hfppl), for concisely specifying new generation tasks as language model probabilistic programs, and automating steering of LLaMA-family Transformers.\n\n\nSequential monte carlo steering of large language models using probabilistic programs</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2306.03081v2</guid>
      <dc:creator>Alexander K. Lew, Tan Zhi-Xuan, Gabriel Grand, Vikash K. Mansinghka</dc:creator>
      <pubDate>Sun, 26 Nov 2023 21:40:00 GMT</pubDate>
    </item>
    <item>
      <title>Continual Learning of Large Language Models: A Comprehensive Survey</title>
      <link>http://arxiv.org/abs/2404.16789v2</link>
      <description>The recent success of large language models (LLMs) trained on static, pre-collected, general datasets has sparked numerous research directions and applications. One such direction addresses the non-trivial challenge of integrating pre-trained LLMs into dynamic data distributions, task structures, and user preferences. Pre-trained LLMs, when tailored for specific needs, often experience significant performance degradation in previous knowledge domains -- a phenomenon known as &quot;catastrophic forgetting&quot;. While extensively studied in the continual learning (CL) community, it presents new manifestations in the realm of LLMs. In this survey, we provide a comprehensive overview of the current research progress on LLMs within the context of CL. This survey is structured into four main sections: we first describe an overview of continually learning LLMs, consisting of two directions of continuity: vertical continuity (or vertical continual learning), i.e., continual adaptation from general to specific capabilities, and horizontal continuity (or horizontal continual learning), i.e., continual adaptation across time and domains (Section 3). We then summarize three stages of learning LLMs in the context of modern CL: Continual Pre-Training (CPT), Domain-Adaptive Pre-training (DAP), and Continual Fine-Tuning (CFT) (Section 4). Then we provide an overview of evaluation protocols for continual learning with LLMs, along with the current available data sources (Section 5). Finally, we discuss intriguing questions pertaining to continual learning for LLMs (Section 6). The full list of papers examined in this survey is available at https://github.com/Wang-ML-Lab/llm-continual-learning-survey.\n\n\nContinual learning of large language models: A comprehensive survey</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2404.16789v2</guid>
      <dc:creator>Haizhou Shi, Zihao Xu, Hengyi Wang, Weiyi Qin, Wenyuan Wang, Yibin Wang, Zifeng Wang, Sayna Ebrahimi, Hao Wang</dc:creator>
      <pubDate>Sun, 30 Jun 2024 02:19:00 GMT</pubDate>
    </item>
    <item>
      <title>Bootstrapping LLM-based Task-Oriented Dialogue Agents via Self-Talk</title>
      <link>http://arxiv.org/abs/2401.05033v1</link>
      <description>Large language models (LLMs) are powerful dialogue agents, but specializing them towards fulfilling a specific function can be challenging. Instructing tuning, i.e. tuning models on instruction and sample responses generated by humans (Ouyang et al., 2022), has proven as an effective method to do so, yet requires a number of data samples that a) might not be available or b) costly to generate. Furthermore, this cost increases when the goal is to make the LLM follow a specific workflow within a dialogue instead of single instructions. Inspired by the self-play technique in reinforcement learning and the use of LLMs to simulate human agents, we propose a more effective method for data collection through LLMs engaging in a conversation in various roles. This approach generates a training data via &quot;self-talk&quot; of LLMs that can be refined and utilized for supervised fine-tuning. We introduce an automated way to measure the (partial) success of a dialogue. This metric is used to filter the generated conversational data that is fed back in LLM for training. Based on our automated and human evaluations of conversation quality, we demonstrate that such self-talk data improves results. In addition, we examine the various characteristics that showcase the quality of generated dialogues and how they can be connected to their potential utility as training data.\n\n\nBootstrapping llm-based task-oriented dialogue agents via self-talk</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2401.05033v1</guid>
      <dc:creator>Dennis Ulmer, Elman Mansimov, Kaixiang Lin, Justin Sun, Xibin Gao, Yi Zhang</dc:creator>
      <pubDate>Wed, 10 Jan 2024 09:49:10 GMT</pubDate>
    </item>
    <item>
      <title>MusicRL: Aligning Music Generation to Human Preferences</title>
      <link>http://arxiv.org/abs/2402.04229v1</link>
      <description>We propose MusicRL, the first music generation system finetuned from human feedback. Appreciation of text-to-music models is particularly subjective since the concept of musicality as well as the specific intention behind a caption are user-dependent (e.g. a caption such as &quot;upbeat work-out music&quot; can map to a retro guitar solo or a techno pop beat). Not only this makes supervised training of such models challenging, but it also calls for integrating continuous human feedback in their post-deployment finetuning. MusicRL is a pretrained autoregressive MusicLM (Agostinelli et al., 2023) model of discrete audio tokens finetuned with reinforcement learning to maximise sequence-level rewards. We design reward functions related specifically to text-adherence and audio quality with the help from selected raters, and use those to finetune MusicLM into MusicRL-R. We deploy MusicLM to users and collect a substantial dataset comprising 300,000 pairwise preferences. Using Reinforcement Learning from Human Feedback (RLHF), we train MusicRL-U, the first text-to-music model that incorporates human feedback at scale. Human evaluations show that both MusicRL-R and MusicRL-U are preferred to the baseline. Ultimately, MusicRL-RU combines the two approaches and results in the best model according to human raters. Ablation studies shed light on the musical attributes influencing human preferences, indicating that text adherence and quality only account for a part of it. This underscores the prevalence of subjectivity in musical appreciation and calls for further involvement of human listeners in the finetuning of music generation models.\n\n\nMusicrl: Aligning music generation to human preferences</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.04229v1</guid>
      <dc:creator>Geoffrey Cideron, Sertan Girgin, Mauro Verzetti, Damien Vincent, Matej Kastelic, Zalán Borsos, Brian McWilliams, Victor Ungureanu, Olivier Bachem, Olivier Pietquin, Matthieu Geist, Léonard Hussenot, Neil Zeghidour, Andrea Agostinelli</dc:creator>
      <pubDate>Tue, 06 Feb 2024 18:36:52 GMT</pubDate>
    </item>
    <item>
      <title>Weak-to-Strong Extrapolation Expedites Alignment</title>
      <link>http://arxiv.org/abs/2404.16792v2</link>
      <description>The open-source community is experiencing a surge in the release of large language models (LLMs) that are trained to follow instructions and align with human preference. However, further training to improve them still requires expensive computational resources and data annotations. Is it possible to bypass additional training and cost-effectively acquire better-aligned models? Inspired by the literature on model interpolation, we propose a simple method called ExPO to boost LLMs' alignment with human preference. Utilizing a model that has undergone alignment training (e.g., via DPO or RLHF) and its initial SFT checkpoint, ExPO directly obtains a better-aligned model by extrapolating from the weights of the initial and the aligned models, which implicitly optimizes the alignment objective via first-order approximation. Through experiments with twelve open-source LLMs on HuggingFace, we demonstrate that ExPO consistently improves off-the-shelf DPO/RLHF models, as evaluated on the mainstream LLM benchmarks AlpacaEval 2.0 and MT-Bench. Moreover, ExPO exhibits remarkable scalability across various model sizes (from 1.8B to 70B) and capabilities. Through controlled experiments and further empirical analyses, we shed light on the essence of ExPO amplifying the reward signal learned during alignment training. Our work demonstrates the efficacy of model extrapolation in expediting the alignment of LLMs with human preference, suggesting a promising direction for future research.\n\n\nWeak-to-strong extrapolation expedites alignment</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2404.16792v2</guid>
      <dc:creator>Chujie Zheng, Ziqi Wang, Heng Ji, Minlie Huang, Nanyun Peng</dc:creator>
      <pubDate>Wed, 22 May 2024 19:33:30 GMT</pubDate>
    </item>
    <item>
      <title>Large Language Model based Long-tail Query Rewriting in Taobao Search</title>
      <link>http://arxiv.org/abs/2311.03758v3</link>
      <description>In the realm of e-commerce search, the significance of semantic matching cannot be overstated, as it directly impacts both user experience and company revenue. Along this line, query rewriting, serving as an important technique to bridge the semantic gaps inherent in the semantic matching process, has attached wide attention from the industry and academia. However, existing query rewriting methods often struggle to effectively optimize long-tail queries and alleviate the phenomenon of &quot;few-recall&quot; caused by semantic gap. In this paper, we present BEQUE, a comprehensive framework that Bridges the sEmantic gap for long-tail QUEries. In detail, BEQUE comprises three stages: multi-instruction supervised fine tuning (SFT), offline feedback, and objective alignment. We first construct a rewriting dataset based on rejection sampling and auxiliary tasks mixing to fine-tune our large language model (LLM) in a supervised fashion. Subsequently, with the well-trained LLM, we employ beam search to generate multiple candidate rewrites, and feed them into Taobao offline system to obtain the partial order. Leveraging the partial order of rewrites, we introduce a contrastive learning method to highlight the distinctions between rewrites, and align the model with the Taobao online objectives. Offline experiments prove the effectiveness of our method in bridging semantic gap. Online A/B tests reveal that our method can significantly boost gross merchandise volume (GMV), number of transaction (#Trans) and unique visitor (UV) for long-tail queries. BEQUE has been deployed on Taobao, one of most popular online shopping platforms in China, since October 2023.\n\n\nLarge language model based long-tail query rewriting in taobao search</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2311.03758v3</guid>
      <dc:creator>Wenjun Peng, Guiyang Li, Yue Jiang, Zilong Wang, Dan Ou, Xiaoyi Zeng, Derong Xu, Tong Xu, Enhong Chen</dc:creator>
      <pubDate>Mon, 04 Mar 2024 14:35:15 GMT</pubDate>
    </item>
    <item>
      <title>CoGenesis: A Framework Collaborating Large and Small Language Models for Secure Context-Aware Instruction Following</title>
      <link>http://arxiv.org/abs/2403.03129v2</link>
      <description>With the advancement of language models (LMs), their exposure to private data is increasingly inevitable, and their deployment (especially for smaller ones) on personal devices, such as PCs and smartphones, has become a prevailing trend. In contexts laden with user information, enabling models to both safeguard user privacy and execute commands efficiently emerges as an essential research imperative. In this paper, we propose CoGenesis, a collaborative generation framework integrating large (hosted on cloud infrastructure) and small models (deployed on local devices) to address privacy concerns logically. Initially, we design a pipeline to create personalized writing instruction datasets enriched with extensive context details as the testbed of this research issue. Subsequently, we introduce two variants of CoGenesis based on sketch and logits respectively. Our experimental findings, based on our synthesized dataset and two additional open-source datasets, indicate that: 1) Large-scale models perform well when provided with user context but struggle in the absence of such context. 2) While specialized smaller models fine-tuned on the synthetic dataset show promise, they still lag behind their larger counterparts. 3) Our CoGenesis framework, utilizing mixed-scale models, showcases competitive performance, providing a feasible solution to privacy issues.\n\n\nCogenesis: A framework collaborating large and small language models for secure context-aware instruction following</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2403.03129v2</guid>
      <dc:creator>Kaiyan Zhang, Jianyu Wang, Ermo Hua, Biqing Qi, Ning Ding, Bowen Zhou</dc:creator>
      <pubDate>Thu, 06 Jun 2024 09:30:24 GMT</pubDate>
    </item>
    <item>
      <title>QuRating: Selecting High-Quality Data for Training Language Models</title>
      <link>http://arxiv.org/abs/2402.09739v2</link>
      <description>Selecting high-quality pre-training data is important for creating capable language models, but existing methods rely on simple heuristics. We introduce QuRating, a method for selecting pre-training data that can capture human intuitions about data quality. In this paper, we investigate four qualities - writing style, required expertise, facts &amp; trivia, and educational value - and find that LLMs are able to discern these qualities, especially when making pairwise judgments of texts. We train a QuRater model to learn scalar ratings from pairwise judgments, and use it to annotate a 260B training corpus with quality ratings for each of the four criteria. In our experiments, we select 30B tokens according to the different quality ratings and train 1.3B-parameter language models on the selected data. We find that it is important to balance quality and diversity. When we sample using quality ratings as logits over documents, our models obtain lower perplexity and stronger in-context learning performance than baselines. Our best model is based on educational value and performs similarly to a model trained with uniform sampling for 50% more steps. Beyond data selection, we use the quality ratings to construct a training curriculum which improves performance without changing the training dataset. We extensively analyze the quality ratings and discuss their characteristics, biases, and wider implications.\n\n\nQurating: Selecting high-quality data for training language models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.09739v2</guid>
      <dc:creator>Alexander Wettig, Aatmik Gupta, Saumya Malik, Danqi Chen</dc:creator>
      <pubDate>Thu, 13 Jun 2024 18:55:23 GMT</pubDate>
    </item>
    <item>
      <title>Reasons to Reject? Aligning Language Models with Judgments</title>
      <link>http://arxiv.org/abs/2312.14591v4</link>
      <description>As humans, we consistently interact with our peers and receive feedback in the form of natural language. This language feedback allows us to maintain appropriate behavior, and rectify potential errors. The question arises naturally: can we use language feedback to align large language models (LLMs)? In contrast to previous research that aligns LLMs with scalar rewards, we present the first systematic exploration of alignment through the lens of language feedback (i.e., judgment). We start with an in-depth investigation of potential methods that can be adapted for aligning LLMs with judgments, revealing that these methods cannot fully capitalize on judgments. To facilitate more effective utilization of judgments, we propose a novel framework, Contrastive Unlikelihood Training (CUT), that allows for fine-grained inappropriate content detection and correction based on judgments. Our results show that, with merely 1317 off-the-shelf judgment data, CUT (LLaMA2-13b) can beat the 175B DaVinci003 and surpass the best baseline by 50.84 points on AlpacaEval. CUT (LLaMA2-chat-13b) can also align LLMs in an iterative fashion using up-to-date model-specific judgments, improving performance from 81.09 to 91.68 points on AlpacaEval. Further analysis suggests that judgments hold greater potential than rewards in LLM alignment.\n\n\nReasons to reject? aligning language models with judgments</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2312.14591v4</guid>
      <dc:creator>Weiwen Xu, Deng Cai, Zhisong Zhang, Wai Lam, Shuming Shi</dc:creator>
      <pubDate>Thu, 06 Jun 2024 04:16:54 GMT</pubDate>
    </item>
    <item>
      <title>The PRISM Alignment Project: What Participatory, Representative and Individualised Human Feedback Reveals About the Subjective and Multicultural Alignment of Large Language Models</title>
      <link>http://arxiv.org/abs/2404.16019v1</link>
      <description>Human feedback plays a central role in the alignment of Large Language Models (LLMs). However, open questions remain about the methods (how), domains (where), people (who) and objectives (to what end) of human feedback collection. To navigate these questions, we introduce PRISM, a new dataset which maps the sociodemographics and stated preferences of 1,500 diverse participants from 75 countries, to their contextual preferences and fine-grained feedback in 8,011 live conversations with 21 LLMs. PRISM contributes (i) wide geographic and demographic participation in human feedback data; (ii) two census-representative samples for understanding collective welfare (UK and US); and (iii) individualised feedback where every rating is linked to a detailed participant profile, thus permitting exploration of personalisation and attribution of sample artefacts. We focus on collecting conversations that centre subjective and multicultural perspectives on value-laden and controversial topics, where we expect the most interpersonal and cross-cultural disagreement. We demonstrate the usefulness of PRISM via three case studies of dialogue diversity, preference diversity, and welfare outcomes, showing that it matters which humans set alignment norms. As well as offering a rich community resource, we advocate for broader participation in AI development and a more inclusive approach to technology design.\n\n\nThe PRISM Alignment Project: What Participatory, Representative and Individualised Human Feedback Reveals About the Subjective and Multicultural Alignment of …</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2404.16019v1</guid>
      <dc:creator>Hannah Rose Kirk, Alexander Whitefield, Paul Röttger, Andrew Bean, Katerina Margatina, Juan Ciro, Rafael Mosquera, Max Bartolo, Adina Williams, He He, Bertie Vidgen, Scott A. Hale</dc:creator>
      <pubDate>Wed, 24 Apr 2024 17:51:36 GMT</pubDate>
    </item>
    <item>
      <title>Evolution of Hierarchical Structure &amp; Reuse in iGEM Synthetic DNA Sequences</title>
      <link>http://arxiv.org/abs/1906.02446v1</link>
      <description>Many complex systems, both in technology and nature, exhibit hierarchical modularity: smaller modules, each of them providing a certain function, are used within larger modules that perform more complex functions. Previously, we have proposed a modeling framework, referred to as Evo-Lexis, that provides insight to some fundamental questions about evolving hierarchical systems.   The predictions of the Evo-Lexis model should be tested using real data from evolving systems in which the outputs can be well represented by sequences. In this paper, we investigate the time series of iGEM synthetic DNA dataset sequences, and whether the resulting iGEM hierarchies exhibit the qualitative properties predicted by the Evo-Lexis framework. Contrary to Evo-Lexis, in iGEM the amount of reuse decreases during the timeline of the dataset. Although this results in development of less cost-efficient and less deep Lexis-DAGs, the dataset exhibits a bias in reusing specific nodes more often than others. This results in the Lexis-DAGs to take the shape of an hourglass with relatively high H-score values and stable set of core nodes. Despite the reuse bias and stability of the core set, the dataset presents a high amount of diversity among the targets which is in line with modeling of Evo-Lexis.\n\n\nSequence modeling and design from molecular to genome scale with Evo</description>
      <guid isPermaLink="false">http://arxiv.org/abs/1906.02446v1</guid>
      <dc:creator>Payam Siyari, Bistra Dilkina, Constantine Dovrolis</dc:creator>
      <pubDate>Thu, 06 Jun 2019 07:15:33 GMT</pubDate>
    </item>
    <item>
      <title>Direct Preference Optimization with an Offset</title>
      <link>http://arxiv.org/abs/2402.10571v2</link>
      <description>Direct preference optimization (DPO) is a successful fine-tuning strategy for aligning large language models with human preferences without the need to train a reward model or employ reinforcement learning. DPO, as originally formulated, relies on binary preference data and fine-tunes a language model to increase the likelihood of a preferred response over a dispreferred response. However, not all preference pairs are equal. Sometimes, the preferred response is only slightly better than the dispreferred one. In other cases, the preference is much stronger. For instance, if a response contains harmful or toxic content, the annotator will have a strong preference for that response. In this paper, we propose a generalization of DPO, termed DPO with an offset (ODPO), that does not treat every preference pair equally during fine-tuning. Intuitively, ODPO requires the difference between the likelihood of the preferred and dispreferred response to be greater than an offset value. The offset is determined based on the extent to which one response is preferred over another. Our experiments on various tasks suggest that ODPO significantly outperforms DPO in aligning language models, especially when the number of preference pairs is limited.\n\n\nToken-level Direct Preference Optimization</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.10571v2</guid>
      <dc:creator>Afra Amini, Tim Vieira, Ryan Cotterell</dc:creator>
      <pubDate>Thu, 06 Jun 2024 12:02:37 GMT</pubDate>
    </item>
    <item>
      <title>From Matching to Generation: A Survey on Generative Information Retrieval</title>
      <link>http://arxiv.org/abs/2404.14851v3</link>
      <description>Information Retrieval (IR) systems are crucial tools for users to access information, widely applied in scenarios like search engines, question answering, and recommendation systems. Traditional IR methods, based on similarity matching to return ranked lists of documents, have been reliable means of information acquisition, dominating the IR field for years. With the advancement of pre-trained language models, generative information retrieval (GenIR) has emerged as a novel paradigm, gaining increasing attention in recent years. Currently, research in GenIR can be categorized into two aspects: generative document retrieval (GR) and reliable response generation. GR leverages the generative model's parameters for memorizing documents, enabling retrieval by directly generating relevant document identifiers without explicit indexing. Reliable response generation, on the other hand, employs language models to directly generate the information users seek, breaking the limitations of traditional IR in terms of document granularity and relevance matching, offering more flexibility, efficiency, and creativity, thus better meeting practical needs. This paper aims to systematically review the latest research progress in GenIR. We will summarize the advancements in GR regarding model training, document identifier, incremental learning, downstream tasks adaptation, multi-modal GR and generative recommendation, as well as progress in reliable response generation in aspects of internal knowledge memorization, external knowledge augmentation, generating response with citations and personal information assistant. We also review the evaluation, challenges and future prospects in GenIR systems. This review aims to offer a comprehensive reference for researchers in the GenIR field, encouraging further development in this area.\n\n\nFrom matching to generation: A survey on generative information retrieval</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2404.14851v3</guid>
      <dc:creator>Xiaoxi Li, Jiajie Jin, Yujia Zhou, Yuyao Zhang, Peitian Zhang, Yutao Zhu, Zhicheng Dou</dc:creator>
      <pubDate>Thu, 16 May 2024 03:28:28 GMT</pubDate>
    </item>
    <item>
      <title>MathChat: Benchmarking Mathematical Reasoning and Instruction Following in Multi-Turn Interactions</title>
      <link>http://arxiv.org/abs/2405.19444v1</link>
      <description>Large language models (LLMs) have demonstrated impressive capabilities in mathematical problem solving, particularly in single turn question answering formats. However, real world scenarios often involve mathematical question answering that requires multi turn or interactive information exchanges, and the performance of LLMs on these tasks is still underexplored. This paper introduces MathChat, a comprehensive benchmark specifically designed to evaluate LLMs across a broader spectrum of mathematical tasks. These tasks are structured to assess the models' abilities in multiturn interactions and open ended generation. We evaluate the performance of various SOTA LLMs on the MathChat benchmark, and we observe that while these models excel in single turn question answering, they significantly underperform in more complex scenarios that require sustained reasoning and dialogue understanding. To address the above limitations of existing LLMs when faced with multiturn and open ended tasks, we develop MathChat sync, a synthetic dialogue based math dataset for LLM finetuning, focusing on improving models' interaction and instruction following capabilities in conversations. Experimental results emphasize the need for training LLMs with diverse, conversational instruction tuning datasets like MathChatsync. We believe this work outlines one promising direction for improving the multiturn mathematical reasoning abilities of LLMs, thus pushing forward the development of LLMs that are more adept at interactive mathematical problem solving and real world applications.\n\n\nMt-bench-101: A fine-grained benchmark for evaluating large language models in multi-turn dialogues</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.19444v1</guid>
      <dc:creator>Zhenwen Liang, Dian Yu, Wenhao Yu, Wenlin Yao, Zhihan Zhang, Xiangliang Zhang, Dong Yu</dc:creator>
      <pubDate>Wed, 29 May 2024 18:45:55 GMT</pubDate>
    </item>
    <item>
      <title>Personalised Distillation: Empowering Open-Sourced LLMs with Adaptive Learning for Code Generation</title>
      <link>http://arxiv.org/abs/2310.18628v2</link>
      <description>With the rise of powerful closed-sourced LLMs (ChatGPT, GPT-4), there are increasing interests in distilling the capabilies of close-sourced LLMs to smaller open-sourced LLMs. Previous distillation methods usually prompt ChatGPT to generate a set of instructions and answers, for the student model to learn. However, such standard distillation approach neglects the merits and conditions of the student model. Inspired by modern teaching principles, we design a personalised distillation process, in which the student attempts to solve a task first, then the teacher provides an adaptive refinement for the student to improve. Instead of feeding the student with teacher's prior, personalised distillation enables personalised learning for the student model, as it only learns on examples it makes mistakes upon and learns to improve its own solution. On code generation, personalised distillation consistently outperforms standard distillation with only one third of the data. With only 2.5-3K personalised examples that incur a data-collection cost of 4-6$, we boost CodeGen-mono-16B by 7% to achieve 36.4% pass@1 and StarCoder by 12.2% to achieve 45.8% pass@1 on HumanEval.\n\n\nPersonalised distillation: Empowering open-sourced llms with adaptive learning for code generation</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2310.18628v2</guid>
      <dc:creator>Hailin Chen, Amrita Saha, Steven Hoi, Shafiq Joty</dc:creator>
      <pubDate>Fri, 26 Jan 2024 10:02:57 GMT</pubDate>
    </item>
    <item>
      <title>Aya 23: Open Weight Releases to Further Multilingual Progress</title>
      <link>http://arxiv.org/abs/2405.15032v2</link>
      <description>This technical report introduces Aya 23, a family of multilingual language models. Aya 23 builds on the recent release of the Aya model (\&quot;Ust\&quot;un et al., 2024), focusing on pairing a highly performant pre-trained model with the recently released Aya collection (Singh et al., 2024). The result is a powerful multilingual large language model serving 23 languages, expanding state-of-art language modeling capabilities to approximately half of the world's population. The Aya model covered 101 languages whereas Aya 23 is an experiment in depth vs breadth, exploring the impact of allocating more capacity to fewer languages that are included during pre-training. Aya 23 outperforms both previous massively multilingual models like Aya 101 for the languages it covers, as well as widely used models like Gemma, Mistral and Mixtral on an extensive range of discriminative and generative tasks. We release the open weights for both the 8B and 35B models as part of our continued commitment for expanding access to multilingual progress.\n\n\nAya 23: Open weight releases to further multilingual progress</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.15032v2</guid>
      <dc:creator>Viraat Aryabumi, John Dang, Dwarak Talupuru, Saurabh Dash, David Cairuz, Hangyu Lin, Bharat Venkitesh, Madeline Smith, Jon Ander Campos, Yi Chern Tan, Kelly Marchisio, Max Bartolo, Sebastian Ruder, Acyr Locatelli, Julia Kreutzer, Nick Frosst, Aidan Gomez, Phil Blunsom, Marzieh Fadaee, Ahmet Üstün, Sara Hooker</dc:creator>
      <pubDate>Fri, 31 May 2024 14:47:55 GMT</pubDate>
    </item>
    <item>
      <title>Rewards-in-Context: Multi-objective Alignment of Foundation Models with Dynamic Preference Adjustment</title>
      <link>http://arxiv.org/abs/2402.10207v5</link>
      <description>We consider the problem of multi-objective alignment of foundation models with human preferences, which is a critical step towards helpful and harmless AI systems. However, it is generally costly and unstable to fine-tune large foundation models using reinforcement learning (RL), and the multi-dimensionality, heterogeneity, and conflicting nature of human preferences further complicate the alignment process. In this paper, we introduce Rewards-in-Context (RiC), which conditions the response of a foundation model on multiple rewards in its prompt context and applies supervised fine-tuning for alignment. The salient features of RiC are simplicity and adaptivity, as it only requires supervised fine-tuning of a single foundation model and supports dynamic adjustment for user preferences during inference time. Inspired by the analytical solution of an abstracted convex optimization problem, our dynamic inference-time adjustment method approaches the Pareto-optimal solution for multiple objectives. Empirical evidence demonstrates the efficacy of our method in aligning both Large Language Models (LLMs) and diffusion models to accommodate diverse rewards with only around 10% GPU hours compared with multi-objective RL baseline.\n\n\nRewards-in-context: Multi-objective alignment of foundation models with dynamic preference adjustment</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.10207v5</guid>
      <dc:creator>Rui Yang, Xiaoman Pan, Feng Luo, Shuang Qiu, Han Zhong, Dong Yu, Jianshu Chen</dc:creator>
      <pubDate>Wed, 05 Jun 2024 09:25:41 GMT</pubDate>
    </item>
    <item>
      <title>Language Model Unalignment: Parametric Red-Teaming to Expose Hidden Harms and Biases</title>
      <link>http://arxiv.org/abs/2310.14303v2</link>
      <description>Red-teaming has been a widely adopted way to evaluate the harmfulness of Large Language Models (LLMs). It aims to jailbreak a model's safety behavior to make it act as a helpful agent disregarding the harmfulness of the query. Existing methods are primarily based on input text-based red-teaming such as adversarial prompts, low-resource prompts, or contextualized prompts to condition the model in a way to bypass its safe behavior. Bypassing the guardrails uncovers hidden harmful information and biases in the model that are left untreated or newly introduced by its safety training. However, prompt-based attacks fail to provide such a diagnosis owing to their low attack success rate, and applicability to specific models. In this paper, we present a new perspective on LLM safety research i.e., parametric red-teaming through Unalignment. It simply (instruction) tunes the model parameters to break model guardrails that are not deeply rooted in the model's behavior. Unalignment using as few as 100 examples can significantly bypass commonly referred to as CHATGPT, to the point where it responds with an 88% success rate to harmful queries on two safety benchmark datasets. On open-source models such as VICUNA-7B and LLAMA-2-CHAT 7B AND 13B, it shows an attack success rate of more than 91%. On bias evaluations, Unalignment exposes inherent biases in safety-aligned models such as CHATGPT and LLAMA- 2-CHAT where the model's responses are strongly biased and opinionated 64% of the time.\n\n\nLanguage model unalignment: Parametric red-teaming to expose hidden harms and biases</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2310.14303v2</guid>
      <dc:creator>Rishabh Bhardwaj, Soujanya Poria</dc:creator>
      <pubDate>Mon, 13 Nov 2023 05:28:47 GMT</pubDate>
    </item>
    <item>
      <title>WebLINX: Real-World Website Navigation with Multi-Turn Dialogue</title>
      <link>http://arxiv.org/abs/2402.05930v1</link>
      <description>We propose the problem of conversational web navigation, where a digital agent controls a web browser and follows user instructions to solve real-world tasks in a multi-turn dialogue fashion. To support this problem, we introduce WEBLINX - a large-scale benchmark of 100K interactions across 2300 expert demonstrations of conversational web navigation. Our benchmark covers a broad range of patterns on over 150 real-world websites and can be used to train and evaluate agents in diverse scenarios. Due to the magnitude of information present, Large Language Models (LLMs) cannot process entire web pages in real-time. To solve this bottleneck, we design a retrieval-inspired model that efficiently prunes HTML pages by ranking relevant elements. We use the selected elements, along with screenshots and action history, to assess a variety of models for their ability to replicate human behavior when navigating the web. Our experiments span from small text-only to proprietary multimodal LLMs. We find that smaller finetuned decoders surpass the best zero-shot LLMs (including GPT-4V), but also larger finetuned multimodal models which were explicitly pretrained on screenshots. However, all finetuned models struggle to generalize to unseen websites. Our findings highlight the need for large multimodal models that can generalize to novel settings. Our code, data and models are available for research: https://mcgill-nlp.github.io/weblinx\n\n\nWeblinx: Real-world website navigation with multi-turn dialogue</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.05930v1</guid>
      <dc:creator>Xing Han Lù, Zdeněk Kasner, Siva Reddy</dc:creator>
      <pubDate>Thu, 08 Feb 2024 18:58:02 GMT</pubDate>
    </item>
    <item>
      <title>Refining Decompiled C Code with Large Language Models</title>
      <link>http://arxiv.org/abs/2310.06530v2</link>
      <description>A C decompiler converts an executable into source code. The recovered C source code, once re-compiled, is expected to produce an executable with the same functionality as the original executable. With over twenty years of development, C decompilers have been widely used in production to support reverse engineering applications. Despite the prosperous development of C decompilers, it is widely acknowledged that decompiler outputs are mainly used for human consumption, and are not suitable for automatic recompilation. Often, a substantial amount of manual effort is required to fix the decompiler outputs before they can be recompiled and executed properly.   This paper is motived by the recent success of large language models (LLMs) in comprehending dense corpus of natural language. To alleviate the tedious, costly and often error-prone manual effort in fixing decompiler outputs, we investigate the feasibility of using LLMs to augment decompiler outputs, thus delivering recompilable decompilation. Note that different from previous efforts that focus on augmenting decompiler outputs with higher readability (e.g., recovering type/variable names), we focus on augmenting decompiler outputs with recompilability, meaning to generate code that can be recompiled into an executable with the same functionality as the original executable.   We conduct a pilot study to characterize the obstacles in recompiling the outputs of the de facto commercial C decompiler -- IDA-Pro. We then propose a two-step, hybrid approach to augmenting decompiler outputs with LLMs. We evaluate our approach on a set of popular C test cases, and show that our approach can deliver a high recompilation success rate to over 75% with moderate effort, whereas none of the IDA-Pro's original outputs can be recompiled. We conclude with a discussion on the limitations of our approach and promising future research directions.\n\n\nRefining decompiled c code with large language models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2310.06530v2</guid>
      <dc:creator>Wai Kin Wong, Huaijin Wang, Zongjie Li, Zhibo Liu, Shuai Wang, Qiyi Tang, Sen Nie, Shi Wu</dc:creator>
      <pubDate>Tue, 28 Nov 2023 19:09:54 GMT</pubDate>
    </item>
    <item>
      <title>Automatic Pair Construction for Contrastive Post-training</title>
      <link>http://arxiv.org/abs/2310.02263v2</link>
      <description>Alignment serves as an important step to steer large language models (LLMs) towards human preferences. In this paper, we propose an automatic way to construct contrastive data for LLM, using preference pairs from multiple models of varying strengths (e.g., InstructGPT, ChatGPT and GPT-4). We compare the contrastive techniques of SLiC and DPO to SFT baselines and find that DPO provides a step-function improvement even after continuing SFT saturates. We also explore a data curriculum learning scheme for contrastive post-training, which starts by learning from &quot;easier&quot; pairs and transitioning to &quot;harder&quot; ones, which further improves alignment. Finally, we scale up our experiments to train with more data and larger models like Orca. Remarkably, our automatic contrastive post-training further improves the performance of Orca, already a state-of-the-art instruction learning model tuned with GPT-4 outputs, to outperform ChatGPT.\n\n\nContrastive post-training large language models on data curriculum</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2310.02263v2</guid>
      <dc:creator>Canwen Xu, Corby Rosset, Ethan C. Chau, Luciano Del Corro, Shweti Mahajan, Julian McAuley, Jennifer Neville, Ahmed Hassan Awadallah, Nikhil Rao</dc:creator>
      <pubDate>Wed, 03 Apr 2024 00:16:19 GMT</pubDate>
    </item>
    <item>
      <title>Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?</title>
      <link>http://arxiv.org/abs/2405.05904v2</link>
      <description>When large language models are aligned via supervised fine-tuning, they may encounter new factual information that was not acquired through pre-training. It is often conjectured that this can teach the model the behavior of hallucinating factually incorrect responses, as the model is trained to generate facts that are not grounded in its pre-existing knowledge. In this work, we study the impact of such exposure to new knowledge on the capability of the fine-tuned model to utilize its pre-existing knowledge. To this end, we design a controlled setup, focused on closed-book QA, where we vary the proportion of the fine-tuning examples that introduce new knowledge. We demonstrate that large language models struggle to acquire new factual knowledge through fine-tuning, as fine-tuning examples that introduce new knowledge are learned significantly slower than those consistent with the model's knowledge. However, we also find that as the examples with new knowledge are eventually learned, they linearly increase the model's tendency to hallucinate. Taken together, our results highlight the risk in introducing new factual knowledge through fine-tuning, and support the view that large language models mostly acquire factual knowledge through pre-training, whereas fine-tuning teaches them to use it more efficiently.\n\n\nDoes Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.05904v2</guid>
      <dc:creator>Zorik Gekhman, Gal Yona, Roee Aharoni, Matan Eyal, Amir Feder, Roi Reichart, Jonathan Herzig</dc:creator>
      <pubDate>Mon, 13 May 2024 07:29:58 GMT</pubDate>
    </item>
    <item>
      <title>Don't throw away your value model! Generating more preferable text with Value-Guided Monte-Carlo Tree Search decoding</title>
      <link>http://arxiv.org/abs/2309.15028v3</link>
      <description>Inference-time search algorithms such as Monte-Carlo Tree Search (MCTS) may seem unnecessary when generating natural language text based on state-of-the-art reinforcement learning such as Proximal Policy Optimization (PPO). In this paper, we demonstrate that it is possible to get extra mileage out of PPO by integrating MCTS on top. The key idea is not to throw out the value network, a byproduct of PPO training for evaluating partial output sequences, when decoding text out of the policy network. More concretely, we present a novel value-guided decoding algorithm called PPO-MCTS, which can integrate the value network from PPO to work closely with the policy network during inference-time generation. Compared to prior approaches based on MCTS for controlled text generation, the key strength of our approach is to reduce the fundamental mismatch of the scoring mechanisms of the partial outputs between training and test. Evaluation on four text generation tasks demonstrate that PPO-MCTS greatly improves the preferability of generated text compared to the standard practice of using only the PPO policy. Our results demonstrate the promise of search algorithms even on top of the aligned language models from PPO, and the under-explored benefit of the value network.\n\n\nMaking ppo even better: Value-guided monte-carlo tree search decoding</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2309.15028v3</guid>
      <dc:creator>Jiacheng Liu, Andrew Cohen, Ramakanth Pasunuru, Yejin Choi, Hannaneh Hajishirzi, Asli Celikyilmaz</dc:creator>
      <pubDate>Tue, 02 Apr 2024 17:51:49 GMT</pubDate>
    </item>
    <item>
      <title>RLHF from Heterogeneous Feedback via Personalization and Preference Aggregation</title>
      <link>http://arxiv.org/abs/2405.00254v2</link>
      <description>Reinforcement learning from human feedback (RLHF) has been an effective technique for aligning AI systems with human values, with remarkable successes in fine-tuning large-language models recently. Most existing RLHF paradigms make the underlying assumption that human preferences are relatively homogeneous, and can be encoded by a single reward model. In this paper, we focus on addressing the issues due to the inherent heterogeneity in human preferences, as well as their potential strategic behavior in providing feedback. Specifically, we propose two frameworks to address heterogeneous human feedback in principled ways: personalization-based one and aggregation-based one. For the former, we propose two approaches based on representation learning and clustering, respectively, for learning multiple reward models that trades off the bias (due to preference heterogeneity) and variance (due to the use of fewer data for learning each model by personalization). We then establish sample complexity guarantees for both approaches. For the latter, we aim to adhere to the single-model framework, as already deployed in the current RLHF paradigm, by carefully aggregating diverse and truthful preferences from humans. We propose two approaches based on reward and preference aggregation, respectively: the former utilizes both utilitarianism and Leximin approaches to aggregate individual reward models, with sample complexity guarantees; the latter directly aggregates the human feedback in the form of probabilistic opinions. Under the probabilistic-opinion-feedback model, we also develop an approach to handle strategic human labelers who may bias and manipulate the aggregated preferences with untruthful feedback. Based on the ideas in mechanism design, our approach ensures truthful preference reporting, with the induced aggregation rule maximizing social welfare functions.\n\n\nPrincipled rlhf from heterogeneous feedback via personalization and preference aggregation</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.00254v2</guid>
      <dc:creator>Chanwoo Park, Mingyang Liu, Dingwen Kong, Kaiqing Zhang, Asuman Ozdaglar</dc:creator>
      <pubDate>Mon, 27 May 2024 14:08:40 GMT</pubDate>
    </item>
    <item>
      <title>Chinese Tiny LLM: Pretraining a Chinese-Centric Large Language Model</title>
      <link>http://arxiv.org/abs/2404.04167v4</link>
      <description>In this study, we introduce CT-LLM, a 2B large language model (LLM) that illustrates a pivotal shift towards prioritizing the Chinese language in developing LLMs. Uniquely initiated from scratch, CT-LLM diverges from the conventional methodology by primarily incorporating Chinese textual data, utilizing an extensive corpus of 1,200 billion tokens, including 800 billion Chinese tokens, 300 billion English tokens, and 100 billion code tokens. This strategic composition facilitates the model's exceptional proficiency in understanding and processing Chinese, a capability further enhanced through alignment techniques. Demonstrating remarkable performance on the CHC-Bench, CT-LLM excels in Chinese language tasks, and showcases its adeptness in English through SFT. This research challenges the prevailing paradigm of training LLMs predominantly on English corpora and then adapting them to other languages, broadening the horizons for LLM training methodologies. By open-sourcing the full process of training a Chinese LLM, including a detailed data processing procedure with the obtained Massive Appropriate Pretraining Chinese Corpus (MAP-CC), a well-chosen multidisciplinary Chinese Hard Case Benchmark (CHC-Bench), and the 2B-size Chinese Tiny LLM (CT-LLM), we aim to foster further exploration and innovation in both academia and industry, paving the way for more inclusive and versatile language models.\n\n\nChinese tiny llm: Pretraining a chinese-centric large language model</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2404.04167v4</guid>
      <dc:creator>Xinrun Du, Zhouliang Yu, Songyang Gao, Ding Pan, Yuyang Cheng, Ziyang Ma, Ruibin Yuan, Xingwei Qu, Jiaheng Liu, Tianyu Zheng, Xinchen Luo, Guorui Zhou, Wenhu Chen, Ge Zhang</dc:creator>
      <pubDate>Wed, 10 Jul 2024 16:51:17 GMT</pubDate>
    </item>
    <item>
      <title>Vaccine: Perturbation-aware Alignment for Large Language Model</title>
      <link>http://arxiv.org/abs/2402.01109v3</link>
      <description>The new paradigm of finetuning-as-a-service introduces a new attack surface for Large Language Models (LLMs): a few harmful data uploaded by users can easily trick the finetuning to produce an alignment-broken model. We conduct an empirical analysis and uncover a \textit{harmful embedding drift} phenomenon, showing a probable cause of the alignment-broken effect. Inspired by our findings, we propose Vaccine, a perturbation-aware alignment technique to mitigate the security risk of users finetuning. The core idea of Vaccine is to produce invariant hidden embeddings by progressively adding crafted perturbation to them in the alignment phase. This enables the embeddings to withstand harmful perturbation from un-sanitized user data in the finetuning phase. Our results on open source mainstream LLMs (e.g., Llama2, Opt, Vicuna) demonstrate that Vaccine can boost the robustness of alignment against harmful prompts induced embedding drift while reserving reasoning ability towards benign prompts. Our code is available at \url{https://github.com/git-disl/Vaccine}.\n\n\nVaccine: Perturbation-aware alignment for large language model</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.01109v3</guid>
      <dc:creator>Tiansheng Huang, Sihao Hu, Ling Liu</dc:creator>
      <pubDate>Thu, 29 Feb 2024 07:15:13 GMT</pubDate>
    </item>
    <item>
      <title>Stabilizing RLHF through Advantage Model and Selective Rehearsal</title>
      <link>http://arxiv.org/abs/2309.10202v1</link>
      <description>Large Language Models (LLMs) have revolutionized natural language processing, yet aligning these models with human values and preferences using RLHF remains a significant challenge. This challenge is characterized by various instabilities, such as reward hacking and catastrophic forgetting. In this technical report, we propose two innovations to stabilize RLHF training: 1) Advantage Model, which directly models advantage score i.e., extra reward compared to the expected rewards and regulates score distributions across tasks to prevent reward hacking. 2) Selective Rehearsal, which mitigates catastrophic forgetting by strategically selecting data for PPO training and knowledge rehearsing. Our experimental analysis on public and proprietary datasets reveals that the proposed methods not only increase stability in RLHF training but also achieve higher reward scores and win rates.\n\n\nStabilizing RLHF through advantage model and selective rehearsal</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2309.10202v1</guid>
      <dc:creator>Baolin Peng, Linfeng Song, Ye Tian, Lifeng Jin, Haitao Mi, Dong Yu</dc:creator>
      <pubDate>Mon, 18 Sep 2023 23:06:32 GMT</pubDate>
    </item>
    <item>
      <title>SALMON: Self-Alignment with Instructable Reward Models</title>
      <link>http://arxiv.org/abs/2310.05910v2</link>
      <description>Supervised Fine-Tuning (SFT) on response demonstrations combined with Reinforcement Learning from Human Feedback (RLHF) constitutes a powerful paradigm for aligning LLM-based AI agents. However, a significant limitation of such an approach is its dependency on high-quality human annotations, making its application to intricate tasks challenging due to difficulties in obtaining consistent response demonstrations and in-distribution response preferences. This paper presents a novel approach, namely SALMON, to align base language models with minimal human supervision, using only a small set of human-defined principles, yet achieving superior performance. Central to our approach is an instructable reward model. Trained on synthetic preference data, this model can generate reward scores based on arbitrary human-defined principles. By merely adjusting these principles during the RL training phase, we gain full control over the preferences with the instructable reward model, subsequently influencing the behavior of the RL-trained policy models, and reducing the reliance on the collection of online human preferences. Applying our method to the LLaMA-2-70b base language model, we developed an AI assistant named Dromedary-2. With only 6 exemplars for in-context learning and 31 human-defined principles, Dromedary-2 significantly surpasses the performance of several state-of-the-art AI systems, including LLaMA-2-Chat-70b, on various benchmark datasets. We have open-sourced the code and model weights to encourage further research into aligning LLM-based AI agents with enhanced supervision efficiency, improved controllability, and scalable oversight.\n\n\nSALMON: Self-Alignment with Instructable Reward Models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2310.05910v2</guid>
      <dc:creator>Zhiqing Sun, Yikang Shen, Hongxin Zhang, Qinhong Zhou, Zhenfang Chen, David Cox, Yiming Yang, Chuang Gan</dc:creator>
      <pubDate>Tue, 09 Apr 2024 23:21:45 GMT</pubDate>
    </item>
    <item>
      <title>ARGS: Alignment as Reward-Guided Search</title>
      <link>http://arxiv.org/abs/2402.01694v1</link>
      <description>Aligning large language models with human objectives is paramount, yet common approaches including RLHF suffer from unstable and resource-intensive training. In response to this challenge, we introduce ARGS, Alignment as Reward-Guided Search, a novel framework that integrates alignment into the decoding process, eliminating the need for expensive RL training. By adjusting the model's probabilistic predictions using a reward signal, ARGS generates texts with semantic diversity while being aligned with human preferences, offering a promising and flexible solution for aligning language models. Notably, ARGS demonstrates consistent enhancements in average reward compared to baselines across diverse alignment tasks and various model dimensions. For example, under the same greedy-based decoding strategy, our method improves the average reward by 19.56% relative to the baseline and secures a preference or tie score of 64.33% in GPT-4 evaluation. We believe that our framework, emphasizing decoding-time alignment, paves the way for more responsive language models in the future. Code is publicly available at: \url{https://github.com/deeplearning-wisc/args}.\n\n\nARGS: Alignment as reward-guided search</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.01694v1</guid>
      <dc:creator>Maxim Khanov, Jirayu Burapacheep, Yixuan Li</dc:creator>
      <pubDate>Tue, 23 Jan 2024 23:42:41 GMT</pubDate>
    </item>
    <item>
      <title>Reinforcement Learning in the Era of LLMs: What is Essential? What is needed? An RL Perspective on RLHF, Prompting, and Beyond</title>
      <link>http://arxiv.org/abs/2310.06147v1</link>
      <description>Recent advancements in Large Language Models (LLMs) have garnered wide attention and led to successful products such as ChatGPT and GPT-4. Their proficiency in adhering to instructions and delivering harmless, helpful, and honest (3H) responses can largely be attributed to the technique of Reinforcement Learning from Human Feedback (RLHF). In this paper, we aim to link the research in conventional RL to RL techniques used in LLM research. Demystify this technique by discussing why, when, and how RL excels. Furthermore, we explore potential future avenues that could either benefit from or contribute to RLHF research.   Highlighted Takeaways:   1. RLHF is Online Inverse RL with Offline Demonstration Data.   2. RLHF $&gt;$ SFT because Imitation Learning (and Inverse RL) $&gt;$ Behavior Cloning (BC) by alleviating the problem of compounding error.   3. The RM step in RLHF generates a proxy of the expensive human feedback, such an insight can be generalized to other LLM tasks such as prompting evaluation and optimization where feedback is also expensive.   4. The policy learning in RLHF is more challenging than conventional problems studied in IRL due to their high action dimensionality and feedback sparsity.   5. The main superiority of PPO over off-policy value-based methods is its stability gained from (almost) on-policy data and conservative policy updates.\n\n\nReinforcement learning in the era of llms: What is essential? what is needed? an rl perspective on rlhf, prompting, and beyond</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2310.06147v1</guid>
      <dc:creator>Hao Sun</dc:creator>
      <pubDate>Mon, 09 Oct 2023 20:49:42 GMT</pubDate>
    </item>
    <item>
      <title>TOFU: A Task of Fictitious Unlearning for LLMs</title>
      <link>http://arxiv.org/abs/2401.06121v1</link>
      <description>Large language models trained on massive corpora of data from the web can memorize and reproduce sensitive or private data raising both legal and ethical concerns. Unlearning, or tuning models to forget information present in their training data, provides us with a way to protect private data after training. Although several methods exist for such unlearning, it is unclear to what extent they result in models equivalent to those where the data to be forgotten was never learned in the first place. To address this challenge, we present TOFU, a Task of Fictitious Unlearning, as a benchmark aimed at helping deepen our understanding of unlearning. We offer a dataset of 200 diverse synthetic author profiles, each consisting of 20 question-answer pairs, and a subset of these profiles called the forget set that serves as the target for unlearning. We compile a suite of metrics that work together to provide a holistic picture of unlearning efficacy. Finally, we provide a set of baseline results from existing unlearning algorithms. Importantly, none of the baselines we consider show effective unlearning motivating continued efforts to develop approaches for unlearning that effectively tune models so that they truly behave as if they were never trained on the forget data at all.\n\n\nTofu: A task of fictitious unlearning for llms</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2401.06121v1</guid>
      <dc:creator>Pratyush Maini, Zhili Feng, Avi Schwarzschild, Zachary C. Lipton, J. Zico Kolter</dc:creator>
      <pubDate>Thu, 11 Jan 2024 18:57:12 GMT</pubDate>
    </item>
    <item>
      <title>Improving Large Language Models via Fine-grained Reinforcement Learning with Minimum Editing Constraint</title>
      <link>http://arxiv.org/abs/2401.06081v2</link>
      <description>Reinforcement learning (RL) has been widely used in training large language models (LLMs) for preventing unexpected outputs, eg reducing harmfulness and errors. However, existing RL methods mostly adopt the instance-level reward, which is unable to provide fine-grained supervision for complex reasoning tasks, and can not focus on the few key tokens that lead to the incorrectness. To address it, we propose a new RL method named RLMEC that incorporates a generative model as the reward model, which is trained by the erroneous solution rewriting task under the minimum editing constraint, and can produce token-level rewards for RL training. Based on the generative reward model, we design the token-level RL objective for training and an imitation-based regularization for stabilizing RL process. And the both objectives focus on the learning of the key tokens for the erroneous solution, reducing the effect of other unimportant tokens. The experiment results on mathematical tasks and question-answering tasks have demonstrated the effectiveness of our approach. Our code and data are available at https://github.com/RUCAIBox/RLMEC.\n\n\nImproving large language models via fine-grained reinforcement learning with minimum editing constraint</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2401.06081v2</guid>
      <dc:creator>Zhipeng Chen, Kun Zhou, Wayne Xin Zhao, Junchen Wan, Fuzheng Zhang, Di Zhang, Ji-Rong Wen</dc:creator>
      <pubDate>Mon, 17 Jun 2024 05:52:06 GMT</pubDate>
    </item>
    <item>
      <title>Learning to Trust Your Feelings: Leveraging Self-awareness in LLMs for Hallucination Mitigation</title>
      <link>http://arxiv.org/abs/2401.15449v1</link>
      <description>We evaluate the ability of Large Language Models (LLMs) to discern and express their internal knowledge state, a key factor in countering factual hallucination and ensuring reliable application of LLMs. We observe a robust self-awareness of internal knowledge state in LLMs, evidenced by over 85% accuracy in knowledge probing. However, LLMs often fail to express their internal knowledge during generation, leading to factual hallucinations. We develop an automated hallucination annotation tool, Dreamcatcher, which merges knowledge probing and consistency checking methods to rank factual preference data. Using knowledge preference as reward, We propose a Reinforcement Learning from Knowledge Feedback (RLKF) training framework, leveraging reinforcement learning to enhance the factuality and honesty of LLMs. Our experiments across multiple models show that RLKF training effectively enhances the ability of models to utilize their internal knowledge state, boosting performance in a variety of knowledge-based and honesty-related tasks.\n\n\nLearning to trust your feelings: Leveraging self-awareness in llms for hallucination mitigation</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2401.15449v1</guid>
      <dc:creator>Yuxin Liang, Zhuoyang Song, Hao Wang, Jiaxing Zhang</dc:creator>
      <pubDate>Sat, 27 Jan 2024 16:19:30 GMT</pubDate>
    </item>
    <item>
      <title>Information Theoretic Guarantees For Policy Alignment In Large Language Models</title>
      <link>http://arxiv.org/abs/2406.05883v1</link>
      <description>Policy alignment of large language models refers to constrained policy optimization, where the policy is optimized to maximize a reward while staying close to a reference policy with respect to an $f$-divergence such as the $\mathsf{KL}$ divergence. The best of $n$ alignment policy selects a sample from the reference policy that has the maximum reward among $n$ independent samples. For both cases (policy alignment and best of $n$), recent works showed empirically that the reward improvement of the aligned policy on the reference one scales like $\sqrt{\mathsf{KL}}$, with an explicit bound in $n$ on the $\mathsf{KL}$ for the best of $n$ policy. We show in this paper that the $\sqrt{\mathsf{KL}}$ information theoretic upper bound holds if the reward under the reference policy has sub-gaussian tails. Moreover, we prove for the best of $n$ policy, that the $\mathsf{KL}$ upper bound can be obtained for any $f$-divergence via a reduction to exponential order statistics owing to the R\'enyi representation of order statistics, and a data processing inequality. If additional information is known on the tails of the aligned policy we show that tighter control on the reward improvement can be obtained via the R\'enyi divergence. Finally we demonstrate how these upper bounds transfer from proxy rewards to golden rewards which results in a decrease in the golden reward improvement due to overestimation and approximation errors of the proxy reward.\n\n\nTheoretical guarantees on the best-of-n alignment policy</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.05883v1</guid>
      <dc:creator>Youssef Mroueh</dc:creator>
      <pubDate>Sun, 09 Jun 2024 18:41:50 GMT</pubDate>
    </item>
    <item>
      <title>Align on the Fly: Adapting Chatbot Behavior to Established Norms</title>
      <link>http://arxiv.org/abs/2312.15907v1</link>
      <description>In this paper, we aim to align large language models with the ever-changing, complex, and diverse human values (e.g., social norms) across time and locations. This presents a challenge to existing alignment techniques, such as supervised fine-tuning, which internalize values within model parameters. To overcome this, we propose an On-the-fly Preference Optimization (OPO) method, which is a real-time alignment that works in a streaming way. It employs an external memory to store established rules for alignment, which can constrain LLMs' behaviors without further training, allowing for convenient updates and customization of human values. We also introduce a scalable evaluation to assess the proposed method more effectively. Experimental results on both human-annotated and auto-generated questions from legal and moral domains indicate the effectiveness of the proposed OPO method. Our code and data are released at https://github.com/GAIR-NLP/OPO.\n\n\nAlign on the fly: Adapting chatbot behavior to established norms</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2312.15907v1</guid>
      <dc:creator>Chunpu Xu, Steffi Chern, Ethan Chern, Ge Zhang, Zekun Wang, Ruibo Liu, Jing Li, Jie Fu, Pengfei Liu</dc:creator>
      <pubDate>Tue, 26 Dec 2023 06:51:09 GMT</pubDate>
    </item>
    <item>
      <title>Enhancing Visual-Language Modality Alignment in Large Vision Language Models via Self-Improvement</title>
      <link>http://arxiv.org/abs/2405.15973v3</link>
      <description>Large vision-language models (LVLMs) have achieved impressive results in various visual question-answering and reasoning tasks through vision instruction tuning on specific datasets. However, there is still significant room for improvement in the alignment between visual and language modalities. Previous methods to enhance this alignment typically require external models or data, heavily depending on their capabilities and quality, which inevitably sets an upper bound on performance. In this paper, we propose SIMA, a framework that enhances visual and language modality alignment through self-improvement, eliminating the needs for external models or data. SIMA leverages prompts from existing vision instruction tuning datasets to self-generate responses and employs an in-context self-critic mechanism to select response pairs for preference tuning. The key innovation is the introduction of three vision metrics during the in-context self-critic process, which can guide the LVLM in selecting responses that enhance image comprehension. Through experiments across 14 hallucination and comprehensive benchmarks, we demonstrate that SIMA not only improves model performance across all benchmarks but also achieves superior modality alignment, outperforming previous approaches.\n\n\nEnhancing Visual-Language Modality Alignment in Large Vision Language Models via Self-Improvement</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.15973v3</guid>
      <dc:creator>Xiyao Wang, Jiuhai Chen, Zhaoyang Wang, Yuhang Zhou, Yiyang Zhou, Huaxiu Yao, Tianyi Zhou, Tom Goldstein, Parminder Bhatia, Furong Huang, Cao Xiao</dc:creator>
      <pubDate>Fri, 07 Jun 2024 20:15:24 GMT</pubDate>
    </item>
    <item>
      <title>Relative Preference Optimization: Enhancing LLM Alignment through Contrasting Responses across Identical and Diverse Prompts</title>
      <link>http://arxiv.org/abs/2402.10958v2</link>
      <description>In the field of large language models (LLMs), aligning models with the diverse preferences of users is a critical challenge. Direct Preference Optimization (DPO) has played a key role in this area. It works by using pairs of preferences derived from the same prompts, and it functions without needing an additional reward model. However, DPO does not fully reflect the complex nature of human learning, which often involves understanding contrasting responses to not only identical but also similar questions. To overcome this shortfall, we propose Relative Preference Optimization (RPO). RPO is designed to discern between more and less preferred responses derived from both identical and related prompts. It introduces a contrastive weighting mechanism, enabling the tuning of LLMs using a broader range of preference data, including both paired and unpaired sets. This approach expands the learning capabilities of the model, allowing it to leverage insights from a more varied set of prompts. Through empirical tests, including dialogue and summarization tasks, and evaluations using the AlpacaEval2.0 leaderboard, RPO has demonstrated a superior ability to align LLMs with user preferences and to improve their adaptability during the training process. Our code can be viewed at https://github.com/yinyueqin/relative-preference-optimization\n\n\nRelative preference optimization: Enhancing llm alignment through contrasting responses across identical and diverse prompts</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.10958v2</guid>
      <dc:creator>Yueqin Yin, Zhendong Wang, Yi Gu, Hai Huang, Weizhu Chen, Mingyuan Zhou</dc:creator>
      <pubDate>Mon, 27 May 2024 20:05:03 GMT</pubDate>
    </item>
    <item>
      <title>CityLLaVA: Efficient Fine-Tuning for VLMs in City Scenario</title>
      <link>http://arxiv.org/abs/2405.03194v1</link>
      <description>In the vast and dynamic landscape of urban settings, Traffic Safety Description and Analysis plays a pivotal role in applications ranging from insurance inspection to accident prevention. This paper introduces CityLLaVA, a novel fine-tuning framework for Visual Language Models (VLMs) designed for urban scenarios. CityLLaVA enhances model comprehension and prediction accuracy through (1) employing bounding boxes for optimal visual data preprocessing, including video best-view selection and visual prompt engineering during both training and testing phases; (2) constructing concise Question-Answer sequences and designing textual prompts to refine instruction comprehension; (3) implementing block expansion to fine-tune large VLMs efficiently; and (4) advancing prediction accuracy via a unique sequential questioning-based prediction augmentation. Demonstrating top-tier performance, our method achieved a benchmark score of 33.4308, securing the leading position on the leaderboard. The code can be found: https://github.com/alibaba/AICITY2024_Track2_AliOpenTrek_CityLLaVA\n\n\nCityllava: Efficient fine-tuning for vlms in city scenario</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.03194v1</guid>
      <dc:creator>Zhizhao Duan, Hao Cheng, Duo Xu, Xi Wu, Xiangxie Zhang, Xi Ye, Zhen Xie</dc:creator>
      <pubDate>Mon, 06 May 2024 06:38:49 GMT</pubDate>
    </item>
    <item>
      <title>Understanding and Mitigating Language Confusion in LLMs</title>
      <link>http://arxiv.org/abs/2406.20052v1</link>
      <description>We investigate a surprising limitation of LLMs: their inability to consistently generate text in a user's desired language. We create the Language Confusion Benchmark (LCB) to evaluate such failures, covering 15 typologically diverse languages with existing and newly-created English and multilingual prompts. We evaluate a range of LLMs on monolingual and cross-lingual generation reflecting practical use cases, finding that Llama Instruct and Mistral models exhibit high degrees of language confusion and even the strongest models fail to consistently respond in the correct language. We observe that base and English-centric instruct models are more prone to language confusion, which is aggravated by complex prompts and high sampling temperatures. We find that language confusion can be partially mitigated via few-shot prompting, multilingual SFT and preference tuning. We release our language confusion benchmark, which serves as a first layer of efficient, scalable multilingual evaluation at https://github.com/for-ai/language-confusion.\n\n\nUnderstanding and Mitigating Language Confusion in LLMs</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.20052v1</guid>
      <dc:creator>Kelly Marchisio, Wei-Yin Ko, Alexandre Bérard, Théo Dehaze, Sebastian Ruder</dc:creator>
      <pubDate>Fri, 28 Jun 2024 17:03:51 GMT</pubDate>
    </item>
    <item>
      <title>Large Language Models for Social Networks: Applications, Challenges, and Solutions</title>
      <link>http://arxiv.org/abs/2401.02575v1</link>
      <description>Large Language Models (LLMs) are transforming the way people generate, explore, and engage with content. We study how we can develop LLM applications for online social networks. Despite LLMs' successes in other domains, it is challenging to develop LLM-based products for social networks for numerous reasons, and it has been relatively under-reported in the research community. We categorize LLM applications for social networks into three categories. First is knowledge tasks where users want to find new knowledge and information, such as search and question-answering. Second is entertainment tasks where users want to consume interesting content, such as getting entertaining notification content. Third is foundational tasks that need to be done to moderate and operate the social networks, such as content annotation and LLM monitoring. For each task, we share the challenges we found, solutions we developed, and lessons we learned. To the best of our knowledge, this is the first comprehensive paper about developing LLM applications for social networks.\n\n\nLarge language models for social networks: Applications, challenges, and solutions</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2401.02575v1</guid>
      <dc:creator>Jingying Zeng, Richard Huang, Waleed Malik, Langxuan Yin, Bojan Babic, Danny Shacham, Xiao Yan, Jaewon Yang, Qi He</dc:creator>
      <pubDate>Thu, 04 Jan 2024 23:37:48 GMT</pubDate>
    </item>
    <item>
      <title>Open Arms: Open-Source Arms, Hands &amp; Control</title>
      <link>http://arxiv.org/abs/2205.12992v2</link>
      <description>Open Arms is a novel open-source platform of realistic human-like robotic hands and arms hardware with 28 Degree-of-Freedom (DoF), designed to extend the capabilities and accessibility of humanoid robotic grasping and manipulation. The Open Arms framework includes an open SDK and development environment, simulation tools, and application development tools to build and operate Open Arms. This paper describes these hands controls, sensing, mechanisms, aesthetic design, and manufacturing and their real-world applications with a teleoperated nursing robot. From 2015 to 2022, the authors have designed and established the manufacturing of Open Arms as a low-cost, high functionality robotic arms hardware and software framework to serve both humanoid robot applications and the urgent demand for low-cost prosthetics, as part of the Hanson Robotics Sophia Robot platform. Using the techniques of consumer product manufacturing, we set out to define modular, low-cost techniques for approximating the dexterity and sensitivity of human hands. To demonstrate the dexterity and control of our hands, we present a Generative Grasping Residual CNN (GGR-CNN) model that can generate robust antipodal grasps from input images of various objects in real-time speeds (22ms). We achieved state-of-the-art accuracy of 92.4% using our model architecture on a standard Cornell Grasping Dataset, which contains a diverse set of household objects.\n\n\nARM: Alignment with Residual Energy-Based Model</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2205.12992v2</guid>
      <dc:creator>David Hanson, Alishba Imran, Gerardo Morales, Vytas Krisciunas, Aditya Sagi, Aman Malali, Rushali Mohbe, Raviteja Upadrashta</dc:creator>
      <pubDate>Fri, 15 Jul 2022 23:27:06 GMT</pubDate>
    </item>
    <item>
      <title>Hummer: Towards Limited Competitive Preference Dataset</title>
      <link>http://arxiv.org/abs/2405.11647v2</link>
      <description>Preference datasets are essential for incorporating human preferences into pre-trained language models, playing a key role in the success of Reinforcement Learning from Human Feedback. However, these datasets often demonstrate conflicting alignment objectives, leading to increased vulnerability to jailbreak attacks and challenges in adapting downstream tasks to prioritize specific alignment objectives without negatively impacting others. In this work, we introduce a novel statistical metric, Alignment Dimension Conflict, to quantify the degree of conflict within preference datasets. We then present \texttt{Hummer} and its fine-grained variant, \texttt{Hummer-F}, as innovative pairwise preference datasets with reduced-conflict alignment objectives. \texttt{Hummer} is built based on UltraFeedback and is enhanced by AI feedback from GPT-4, marking as the first preference dataset aimed at reducing the competition between alignment objectives. Furthermore, we develop reward models, HummerRM and HummerRM-F, which employ a hybrid sampling approach to balance diverse alignment objectives effectively. This sampling method positions HummerRM as an ideal model for domain-specific further fine-tuning and reducing vulnerabilities to attacks.\n\n\nHummer: Towards limited competitive preference dataset</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.11647v2</guid>
      <dc:creator>Li Jiang, Yusen Wu, Junwu Xiong, Jingqing Ruan, Yichuan Ding, Qingpei Guo, Zujie Wen, Jun Zhou, Xiaotie Deng</dc:creator>
      <pubDate>Tue, 21 May 2024 02:01:42 GMT</pubDate>
    </item>
    <item>
      <title>PiSSA: Principal Singular Values and Singular Vectors Adaptation of Large Language Models</title>
      <link>http://arxiv.org/abs/2404.02948v3</link>
      <description>To parameter-efficiently fine-tune (PEFT) large language models (LLMs), the low-rank adaptation (LoRA) method approximates the model changes $\Delta W \in \mathbb{R}^{m \times n}$ through the product of two matrices $A \in \mathbb{R}^{m \times r}$ and $B \in \mathbb{R}^{r \times n}$, where $r \ll \min(m, n)$, $A$ is initialized with Gaussian noise, and $B$ with zeros. LoRA freezes the original model $W$ and updates the &quot;Noise &amp; Zero&quot; adapter, which may lead to slow convergence. To overcome this limitation, we introduce Principal Singular values and Singular vectors Adaptation (PiSSA). PiSSA shares the same architecture as LoRA, but initializes the adaptor matrices $A$ and $B$ with the principal components of the original matrix $W$, and put the remaining components into a residual matrix $W^{res} \in \mathbb{R}^{m \times n}$ which is frozen during fine-tuning. Compared to LoRA, PiSSA updates the principal components while freezing the &quot;residual&quot; parts, allowing faster convergence and enhanced performance. Comparative experiments of PiSSA and LoRA across 12 different models, ranging from 184M to 70B, encompassing 5 NLG and 8 NLU tasks, reveal that PiSSA consistently outperforms LoRA under identical experimental setups. On the GSM8K benchmark, Mistral-7B fine-tuned with PiSSA achieves an accuracy of 72.86%, surpassing LoRA's 67.7% by 5.16%. Due to the same architecture, PiSSA is also compatible with quantization to further reduce the memory requirement of fine-tuning. Compared to QLoRA, QPiSSA (PiSSA with 4-bit quantization) exhibits smaller quantization errors in the initial stages. Fine-tuning LLaMA-3-70B on GSM8K, QPiSSA attains an accuracy of 86.05%, exceeding the performances of QLoRA at 81.73%. Leveraging a fast SVD technique, PiSSA can be initialized in only a few seconds, presenting a negligible cost for transitioning from LoRA to PiSSA.\n\n\nPissa: Principal singular values and singular vectors adaptation of large language models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2404.02948v3</guid>
      <dc:creator>Fanxu Meng, Zhaohui Wang, Muhan Zhang</dc:creator>
      <pubDate>Tue, 28 May 2024 14:19:33 GMT</pubDate>
    </item>
    <item>
      <title>AI-TA: Towards an Intelligent Question-Answer Teaching Assistant using Open-Source LLMs</title>
      <link>http://arxiv.org/abs/2311.02775v3</link>
      <description>Responding to the thousands of student questions on online QA platforms each semester has a considerable human cost, particularly in computing courses with rapidly growing enrollments. To address the challenges of scalable and intelligent question-answering (QA), we introduce an innovative solution that leverages open-source Large Language Models (LLMs) from the LLaMA-2 family to ensure data privacy. Our approach combines augmentation techniques such as retrieval augmented generation (RAG), supervised fine-tuning (SFT), and learning from human preferences data using Direct Preference Optimization (DPO). Through extensive experimentation on a Piazza dataset from an introductory CS course, comprising 10,000 QA pairs and 1,500 pairs of preference data, we demonstrate a significant 30% improvement in the quality of answers, with RAG being a particularly impactful addition. Our contributions include the development of a novel architecture for educational QA, extensive evaluations of LLM performance utilizing both human assessments and LLM-based metrics, and insights into the challenges and future directions of educational data processing. This work paves the way for the development of AI-TA, an intelligent QA assistant customizable for courses with an online QA platform\n\n\nChaTA: Towards an Intelligent Question-Answer Teaching Assistant using Open-Source LLMs</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2311.02775v3</guid>
      <dc:creator>Yann Hicke, Anmol Agarwal, Qianou Ma, Paul Denny</dc:creator>
      <pubDate>Mon, 18 Dec 2023 23:23:06 GMT</pubDate>
    </item>
    <item>
      <title>Improving Machine Translation with Human Feedback: An Exploration of Quality Estimation as a Reward Model</title>
      <link>http://arxiv.org/abs/2401.12873v3</link>
      <description>Insufficient modeling of human preferences within the reward model is a major obstacle for leveraging human feedback to improve translation quality. Fortunately, quality estimation (QE), which predicts the quality of a given translation without reference, has achieved impressive alignment with human evaluations in the last two years. In this work, we investigate the potential of employing the QE model as the reward model to predict human preferences for feedback training. We first identify the overoptimization problem during QE-based feedback training, manifested as an increase in reward while translation quality declines. We examine the problem and argue that the vulnerability of the QE model might lead to high rewards for incorrect translations, resulting in overoptimization and error propagation. To address the problem, we adopt a simple yet effective method that uses heuristic rules to detect the incorrect translations and assigns a penalty term to the reward scores of them. Experimental results show that the proposed QE-based feedback training achieves consistent and significant improvements across various settings, further verified through human preference studies. Our subsequent analysis demonstrates the high data efficiency of the proposed QE-based feedback training: it outperforms systems using larger parallel corpora by a small amount of monolingual data. Our code is available at: https://github.com/zwhe99/FeedbackMT\n\n\nImproving machine translation with human feedback: An exploration of quality estimation as a reward model</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2401.12873v3</guid>
      <dc:creator>Zhiwei He, Xing Wang, Wenxiang Jiao, Zhuosheng Zhang, Rui Wang, Shuming Shi, Zhaopeng Tu</dc:creator>
      <pubDate>Mon, 18 Mar 2024 15:16:16 GMT</pubDate>
    </item>
    <item>
      <title>DistiLLM: Towards Streamlined Distillation for Large Language Models</title>
      <link>http://arxiv.org/abs/2402.03898v2</link>
      <description>Knowledge distillation (KD) is widely used for compressing a teacher model to a smaller student model, reducing its inference cost and memory footprint while preserving model capabilities. However, current KD methods for auto-regressive sequence models (e.g., large language models) suffer from missing a standardized objective function. Moreover, the recent use of student-generated outputs to address training-inference mismatches has significantly escalated computational costs. To tackle these issues, we introduce DistiLLM, a more effective and efficient KD framework for auto-regressive language models. DistiLLM comprises two components: (1) a novel skew Kullback-Leibler divergence loss, where we unveil and leverage its theoretical properties, and (2) an adaptive off-policy approach designed to enhance the efficiency in utilizing student-generated outputs. Extensive experiments, including instruction-following tasks, demonstrate the effectiveness of DistiLLM in building high-performing student models while achieving up to 4.3$\times$ speedup compared to recent KD methods.\n\n\nDistillm: Towards streamlined distillation for large language models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.03898v2</guid>
      <dc:creator>Jongwoo Ko, Sungnyun Kim, Tianyi Chen, Se-Young Yun</dc:creator>
      <pubDate>Wed, 03 Jul 2024 04:57:41 GMT</pubDate>
    </item>
    <item>
      <title>Distilling Large Language Models for Matching Patients to Clinical Trials</title>
      <link>http://arxiv.org/abs/2312.09958v1</link>
      <description>The recent success of large language models (LLMs) has paved the way for their adoption in the high-stakes domain of healthcare. Specifically, the application of LLMs in patient-trial matching, which involves assessing patient eligibility against clinical trial's nuanced inclusion and exclusion criteria, has shown promise. Recent research has shown that GPT-3.5, a widely recognized LLM developed by OpenAI, can outperform existing methods with minimal 'variable engineering' by simply comparing clinical trial information against patient summaries. However, there are significant challenges associated with using closed-source proprietary LLMs like GPT-3.5 in practical healthcare applications, such as cost, privacy and reproducibility concerns. To address these issues, this study presents the first systematic examination of the efficacy of both proprietary (GPT-3.5, and GPT-4) and open-source LLMs (LLAMA 7B,13B, and 70B) for the task of patient-trial matching. Employing a multifaceted evaluation framework, we conducted extensive automated and human-centric assessments coupled with a detailed error analysis for each model. To enhance the adaptability of open-source LLMs, we have created a specialized synthetic dataset utilizing GPT-4, enabling effective fine-tuning under constrained data conditions. Our findings reveal that open-source LLMs, when fine-tuned on this limited and synthetic dataset, demonstrate performance parity with their proprietary counterparts. This presents a massive opportunity for their deployment in real-world healthcare applications. To foster further research and applications in this field, we release both the annotated evaluation dataset along with the fine-tuned LLM -- Trial-LLAMA -- for public use.\n\n\nDistilling large language models for matching patients to clinical trials</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2312.09958v1</guid>
      <dc:creator>Mauro Nievas, Aditya Basu, Yanshan Wang, Hrituraj Singh</dc:creator>
      <pubDate>Fri, 15 Dec 2023 17:11:07 GMT</pubDate>
    </item>
    <item>
      <title>BiMediX: Bilingual Medical Mixture of Experts LLM</title>
      <link>http://arxiv.org/abs/2402.13253v1</link>
      <description>In this paper, we introduce BiMediX, the first bilingual medical mixture of experts LLM designed for seamless interaction in both English and Arabic. Our model facilitates a wide range of medical interactions in English and Arabic, including multi-turn chats to inquire about additional details such as patient symptoms and medical history, multiple-choice question answering, and open-ended question answering. We propose a semi-automated English-to-Arabic translation pipeline with human refinement to ensure high-quality translations. We also introduce a comprehensive evaluation benchmark for Arabic medical LLMs. Furthermore, we introduce BiMed1.3M, an extensive Arabic-English bilingual instruction set covering 1.3 Million diverse medical interactions, resulting in over 632 million healthcare specialized tokens for instruction tuning. Our BiMed1.3M dataset includes 250k synthesized multi-turn doctor-patient chats and maintains a 1:2 Arabic-to-English ratio. Our model outperforms state-of-the-art Med42 and Meditron by average absolute gains of 2.5% and 4.1%, respectively, computed across multiple medical evaluation benchmarks in English, while operating at 8-times faster inference. Moreover, our BiMediX outperforms the generic Arabic-English bilingual LLM, Jais-30B, by average absolute gains of 10% on our Arabic medical benchmark and 15% on bilingual evaluations across multiple datasets. Our project page with source code and trained model is available at https://github.com/mbzuai-oryx/BiMediX .\n\n\nBimedix: Bilingual medical mixture of experts llm</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.13253v1</guid>
      <dc:creator>Sara Pieri, Sahal Shaji Mullappilly, Fahad Shahbaz Khan, Rao Muhammad Anwer, Salman Khan, Timothy Baldwin, Hisham Cholakkal</dc:creator>
      <pubDate>Tue, 20 Feb 2024 18:59:26 GMT</pubDate>
    </item>
    <item>
      <title>ReFT: Reasoning with Reinforced Fine-Tuning</title>
      <link>http://arxiv.org/abs/2401.08967v2</link>
      <description>One way to enhance the reasoning capability of Large Language Models (LLMs) is to conduct Supervised Fine-Tuning (SFT) using Chain-of-Thought (CoT) annotations. This approach does not show sufficiently strong generalization ability, however, because the training only relies on the given CoT data. In math problem-solving, for example, there is usually only one annotated reasoning path for each question in the training data. Intuitively, it would be better for the algorithm to learn from multiple annotated reasoning paths given a question. To address this issue, we propose a simple yet effective approach called Reinforced Fine-Tuning (ReFT) to enhance the generalizability of learning LLMs for reasoning, with math problem-solving as an example. ReFT first warmups the model with SFT, and then employs on-line reinforcement learning, specifically the PPO algorithm in this paper, to further fine-tune the model, where an abundance of reasoning paths are automatically sampled given the question and the rewards are naturally derived from the ground-truth answers. Extensive experiments on GSM8K, MathQA, and SVAMP datasets show that ReFT significantly outperforms SFT, and the performance can be potentially further boosted by combining inference-time strategies such as majority voting and re-ranking. Note that ReFT obtains the improvement by learning from the same training questions as SFT, without relying on extra or augmented training questions. This indicates a superior generalization ability for ReFT.\n\n\nReft: Reasoning with reinforced fine-tuning</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2401.08967v2</guid>
      <dc:creator>Trung Quoc Luong, Xinbo Zhang, Zhanming Jie, Peng Sun, Xiaoran Jin, Hang Li</dc:creator>
      <pubDate>Thu, 27 Jun 2024 15:29:15 GMT</pubDate>
    </item>
    <item>
      <title>mFACE: Multilingual Summarization with Factual Consistency Evaluation</title>
      <link>http://arxiv.org/abs/2212.10622v2</link>
      <description>Abstractive summarization has enjoyed renewed interest in recent years, thanks to pre-trained language models and the availability of large-scale datasets. Despite promising results, current models still suffer from generating factually inconsistent summaries, reducing their utility for real-world application. Several recent efforts attempt to address this by devising models that automatically detect factual inconsistencies in machine generated summaries. However, they focus exclusively on English, a language with abundant resources. In this work, we leverage factual consistency evaluation models to improve multilingual summarization. We explore two intuitive approaches to mitigate hallucinations based on the signal provided by a multilingual NLI model, namely data filtering and controlled generation. Experimental results in the 45 languages from the XLSum dataset show gains over strong baselines in both automatic and human evaluation.\n\n\nFactuality of large language models in the year 2024</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2212.10622v2</guid>
      <dc:creator>Roee Aharoni, Shashi Narayan, Joshua Maynez, Jonathan Herzig, Elizabeth Clark, Mirella Lapata</dc:creator>
      <pubDate>Fri, 05 Jan 2024 12:13:55 GMT</pubDate>
    </item>
    <item>
      <title>Unmasking Transformers: A Theoretical Approach to Data Recovery via Attention Weights</title>
      <link>http://arxiv.org/abs/2310.12462v1</link>
      <description>In the realm of deep learning, transformers have emerged as a dominant architecture, particularly in natural language processing tasks. However, with their widespread adoption, concerns regarding the security and privacy of the data processed by these models have arisen. In this paper, we address a pivotal question: Can the data fed into transformers be recovered using their attention weights and outputs? We introduce a theoretical framework to tackle this problem. Specifically, we present an algorithm that aims to recover the input data $X \in \mathbb{R}^{d \times n}$ from given attention weights $W = QK^\top \in \mathbb{R}^{d \times d}$ and output $B \in \mathbb{R}^{n \times n}$ by minimizing the loss function $L(X)$. This loss function captures the discrepancy between the expected output and the actual output of the transformer. Our findings have significant implications for the Localized Layer-wise Mechanism (LLM), suggesting potential vulnerabilities in the model's design from a security and privacy perspective. This work underscores the importance of understanding and safeguarding the internal workings of transformers to ensure the confidentiality of processed data.\n\n\nUnmasking transformers: A theoretical approach to data recovery via attention weights</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2310.12462v1</guid>
      <dc:creator>Yichuan Deng, Zhao Song, Shenghao Xie, Chiwun Yang</dc:creator>
      <pubDate>Thu, 19 Oct 2023 04:41:01 GMT</pubDate>
    </item>
    <item>
      <title>ReMax: A Simple, Effective, and Efficient Reinforcement Learning Method for Aligning Large Language Models</title>
      <link>http://arxiv.org/abs/2310.10505v4</link>
      <description>Reinforcement Learning from Human Feedback (RLHF) is key to aligning Large Language Models (LLMs), typically paired with the Proximal Policy Optimization (PPO) algorithm. While PPO is a powerful method designed for general reinforcement learning tasks, it is overly sophisticated for LLMs, leading to laborious hyper-parameter tuning and significant computation burdens. To make RLHF efficient, we present ReMax, which leverages 3 properties of RLHF: fast simulation, deterministic transitions, and trajectory-level rewards. These properties are not exploited in PPO, making it less suitable for RLHF. Building on the renowned REINFORCE algorithm, ReMax does not require training an additional value model as in PPO and is further enhanced with a new variance reduction technique. ReMax offers several benefits over PPO: it is simpler to implement, eliminates more than 4 hyper-parameters in PPO, reduces GPU memory usage, and shortens training time. ReMax can save about 46% GPU memory than PPO when training a 7B model and enables training on A800-80GB GPUs without the memory-saving offloading technique needed by PPO. Applying ReMax to a Mistral-7B model resulted in a 94.78% win rate on the AlpacaEval leaderboard and a 7.739 score on MT-bench, setting a new SOTA for open-source 7B models. These results show the effectiveness of ReMax while addressing the limitations of PPO in LLMs.\n\n\nRemax: A simple, effective, and efficient reinforcement learning method for aligning large language models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2310.10505v4</guid>
      <dc:creator>Ziniu Li, Tian Xu, Yushun Zhang, Zhihang Lin, Yang Yu, Ruoyu Sun, Zhi-Quan Luo</dc:creator>
      <pubDate>Thu, 16 May 2024 02:22:23 GMT</pubDate>
    </item>
    <item>
      <title>COPR: Continual Learning Human Preference through Optimal Policy Regularization</title>
      <link>http://arxiv.org/abs/2310.15694v5</link>
      <description>The technique of Reinforcement Learning from Human Feedback (RLHF) is a commonly employed method to improve pre-trained Language Models (LM), enhancing their ability to conform to human preferences. Nevertheless, the current RLHF-based LMs necessitate full retraining each time novel queries or feedback are introduced, which becomes a challenging task because human preferences can vary between different domains or tasks. Retraining LMs poses practical difficulties in many real-world situations due to the significant time and computational resources required, along with concerns related to data privacy. To address this limitation, we propose a new method called Continual Optimal Policy Regularization (COPR), in which we compute the distribution of optimal policy bypassing the partition function and then regularize the current policy based on the historically optimal distribution to mitigate Catastrophic Forgetting (CF). COPR involves a single learning phase and doesn't necessitate complex reinforcement learning. Importantly, it shares the capability with RLHF to learn from unlabeled data by maintaining a scoring module, similar to reward model, making it flexible for continually learning without human feedback. Our experimental results show that COPR outperforms strong Continuous Learning (CL) baselines when it comes to consistently aligning with human preferences on incremental tasks and domains.\n\n\nCopf: Continual learning human preference through optimal policy fitting</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2310.15694v5</guid>
      <dc:creator>Han Zhang, Lin Gui, Yuanzhao Zhai, Hui Wang, Yu Lei, Ruifeng Xu</dc:creator>
      <pubDate>Tue, 26 Mar 2024 11:52:59 GMT</pubDate>
    </item>
    <item>
      <title>SOUL: Unlocking the Power of Second-Order Optimization for LLM Unlearning</title>
      <link>http://arxiv.org/abs/2404.18239v4</link>
      <description>Large Language Models (LLMs) have highlighted the necessity of effective unlearning mechanisms to comply with data regulations and ethical AI practices. LLM unlearning aims at removing undesired data influences and associated model capabilities without compromising utility beyond the scope of unlearning. While interest in studying LLM unlearning is growing, the impact of the optimizer choice for LLM unlearning remains unexplored. In this work, we shed light on the significance of optimizer selection in LLM unlearning for the first time, establishing a clear connection between second-order optimization and influence unlearning (a classical approach using influence functions to update the model for data influence removal). This insight propels us to develop a second-order optimization-based LLM unlearning framework, termed Second-Order UnLearning (SOUL), which extends the static, one-shot model update using influence unlearning to a dynamic, iterative unlearning process. Our extensive experiments show that SOUL consistently outperforms conventional first-order methods across various unlearning tasks, models, and metrics, indicating that second-order optimization offers an effective and broadly applicable solution for LLM unlearning. Codes are available at https://github.com/OPTML-Group/SOUL.\n\n\nSoul: Unlocking the power of second-order optimization for llm unlearning</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2404.18239v4</guid>
      <dc:creator>Jinghan Jia, Yihua Zhang, Yimeng Zhang, Jiancheng Liu, Bharat Runwal, James Diffenderfer, Bhavya Kailkhura, Sijia Liu</dc:creator>
      <pubDate>Mon, 24 Jun 2024 20:24:53 GMT</pubDate>
    </item>
    <item>
      <title>Robust Preference Optimization through Reward Model Distillation</title>
      <link>http://arxiv.org/abs/2405.19316v1</link>
      <description>Language model (LM) post-training (or alignment) involves maximizing a reward function that is derived from preference annotations. Direct Preference Optimization (DPO) is a popular offline alignment method that trains a policy directly on preference data without the need to train a reward model or apply reinforcement learning. However, typical preference datasets have only a single, or at most a few, annotation per preference pair, which causes DPO to overconfidently assign rewards that trend towards infinite magnitude. This frequently leads to degenerate policies, sometimes causing even the probabilities of the preferred generations to go to zero. In this work, we analyze this phenomenon and propose distillation to get a better proxy for the true preference distribution over generation pairs: we train the LM to produce probabilities that match the distribution induced by a reward model trained on the preference data. Moreover, to account for uncertainty in the reward model we are distilling from, we optimize against a family of reward models that, as a whole, is likely to include at least one reasonable proxy for the preference distribution. Our results show that distilling from such a family of reward models leads to improved robustness to distribution shift in preference annotations, while preserving the simple supervised nature of DPO.\n\n\nRobust preference optimization through reward model distillation</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.19316v1</guid>
      <dc:creator>Adam Fisch, Jacob Eisenstein, Vicky Zayats, Alekh Agarwal, Ahmad Beirami, Chirag Nagpal, Pete Shaw, Jonathan Berant</dc:creator>
      <pubDate>Wed, 29 May 2024 17:39:48 GMT</pubDate>
    </item>
    <item>
      <title>Exploration-Driven Policy Optimization in RLHF: Theoretical Insights on Efficient Data Utilization</title>
      <link>http://arxiv.org/abs/2402.10342v2</link>
      <description>Reinforcement Learning from Human Feedback (RLHF) has achieved impressive empirical successes while relying on a small amount of human feedback. However, there is limited theoretical justification for this phenomenon. Additionally, most recent studies focus on value-based algorithms despite the recent empirical successes of policy-based algorithms. In this work, we consider an RLHF algorithm based on policy optimization (PO-RLHF). The algorithm is based on the popular Policy Cover-Policy Gradient (PC-PG) algorithm, which assumes knowledge of the reward function. In PO-RLHF, knowledge of the reward function is not assumed, and the algorithm uses trajectory-based comparison feedback to infer the reward function. We provide performance bounds for PO-RLHF with low query complexity, which provides insight into why a small amount of human feedback may be sufficient to achieve good performance with RLHF. A key novelty is a trajectory-level elliptical potential analysis, which bounds the reward estimation error when comparison feedback (rather than numerical reward observation) is given. We provide and analyze algorithms PG-RLHF and NN-PG-RLHF for two settings: linear and neural function approximation, respectively.\n\n\nPolicy optimization in rlhf: The impact of out-of-preference data</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.10342v2</guid>
      <dc:creator>Yihan Du, Anna Winnicki, Gal Dalal, Shie Mannor, R. Srikant</dc:creator>
      <pubDate>Mon, 15 Jul 2024 04:19:50 GMT</pubDate>
    </item>
    <item>
      <title>Efficient Adversarial Training in LLMs with Continuous Attacks</title>
      <link>http://arxiv.org/abs/2405.15589v2</link>
      <description>Large language models (LLMs) are vulnerable to adversarial attacks that can bypass their safety guardrails. In many domains, adversarial training has proven to be one of the most promising methods to reliably improve robustness against such attacks. Yet, in the context of LLMs, current methods for adversarial training are hindered by the high computational costs required to perform discrete adversarial attacks at each training iteration. We address this problem by instead calculating adversarial attacks in the continuous embedding space of the LLM, which is orders of magnitudes more efficient. We propose a fast adversarial training algorithm (C-AdvUL) composed of two losses: the first makes the model robust on continuous embedding attacks computed on an adversarial behaviour dataset; the second ensures the usefulness of the final model by fine-tuning on utility data. Moreover, we introduce C-AdvIPO, an adversarial variant of IPO that does not require utility data for adversarially robust alignment. Our empirical evaluation on four models from different families (Gemma, Phi3, Mistral, Zephyr) and at different scales (2B, 3.8B, 7B) shows that both algorithms substantially enhance LLM robustness against discrete attacks (GCG, AutoDAN, PAIR), while maintaining utility. Our results demonstrate that robustness to continuous perturbations can extrapolate to discrete threat models. Thereby, we present a path toward scalable adversarial training algorithms for robustly aligning LLMs.\n\n\nEfficient adversarial training in llms with continuous attacks</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.15589v2</guid>
      <dc:creator>Sophie Xhonneux, Alessandro Sordoni, Stephan Günnemann, Gauthier Gidel, Leo Schwinn</dc:creator>
      <pubDate>Fri, 21 Jun 2024 19:59:31 GMT</pubDate>
    </item>
    <item>
      <title>Convergence of Two-Layer Regression with Nonlinear Units</title>
      <link>http://arxiv.org/abs/2308.08358v1</link>
      <description>Large language models (LLMs), such as ChatGPT and GPT4, have shown outstanding performance in many human life task. Attention computation plays an important role in training LLMs. Softmax unit and ReLU unit are the key structure in attention computation. Inspired by them, we put forward a softmax ReLU regression problem. Generally speaking, our goal is to find an optimal solution to the regression problem involving the ReLU unit. In this work, we calculate a close form representation for the Hessian of the loss function. Under certain assumptions, we prove the Lipschitz continuous and the PSDness of the Hessian. Then, we introduce an greedy algorithm based on approximate Newton method, which converges in the sense of the distance to optimal solution. Last, We relax the Lipschitz condition and prove the convergence in the sense of loss value.\n\n\nConvergence of two-layer regression with nonlinear units</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2308.08358v1</guid>
      <dc:creator>Yichuan Deng, Zhao Song, Shenghao Xie</dc:creator>
      <pubDate>Wed, 16 Aug 2023 13:30:45 GMT</pubDate>
    </item>
    <item>
      <title>Orca-Math: Unlocking the potential of SLMs in Grade School Math</title>
      <link>http://arxiv.org/abs/2402.14830v1</link>
      <description>Mathematical word problem-solving has long been recognized as a complex task for small language models (SLMs). A recent study hypothesized that the smallest model size, needed to achieve over 80% accuracy on the GSM8K benchmark, is 34 billion parameters. To reach this level of performance with smaller models, researcher often train SLMs to generate Python code or use tools to help avoid calculation errors. Additionally, they employ ensembling, where outputs of up to 100 model runs are combined to arrive at a more accurate result. Result selection is done using consensus, majority vote or a separate a verifier model used in conjunction with the SLM. Ensembling provides a substantial boost in accuracy but at a significant cost increase with multiple calls to the model (e.g., Phi-GSM uses top-48 to boost the performance from 68.2 to 81.5).   In this work, we present Orca-Math, a 7-billion-parameter SLM based on the Mistral-7B, which achieves 86.81% on GSM8k without the need for multiple model calls or the use of verifiers, code execution or any other external tools. Our approach has the following key elements: (1) A high quality synthetic dataset of 200K math problems created using a multi-agent setup where agents collaborate to create the data, (2) An iterative learning techniques that enables the SLM to practice solving problems, receive feedback on its solutions and learn from preference pairs incorporating the SLM solutions and the feedback. When trained with Supervised Fine-Tuning alone, Orca-Math achieves 81.50% on GSM8k pass@1 metric. With iterative preference learning, Orca-Math achieves 86.81% pass@1. Orca-Math surpasses the performance of significantly larger models such as LLAMA-2-70B, WizardMath-70B, Gemini-Pro, ChatGPT-3.5. It also significantly outperforms other smaller models while using much smaller data (hundreds of thousands vs. millions of problems).\n\n\nOrca-math: Unlocking the potential of slms in grade school math</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.14830v1</guid>
      <dc:creator>Arindam Mitra, Hamed Khanpour, Corby Rosset, Ahmed Awadallah</dc:creator>
      <pubDate>Fri, 16 Feb 2024 23:44:38 GMT</pubDate>
    </item>
    <item>
      <title>Self-Alignment of Large Language Models via Monopolylogue-based Social Scene Simulation</title>
      <link>http://arxiv.org/abs/2402.05699v3</link>
      <description>Aligning large language models (LLMs) with human values is imperative to mitigate potential adverse effects resulting from their misuse. Drawing from the sociological insight that acknowledging all parties' concerns is a key factor in shaping human values, this paper proposes a novel direction to align LLMs by themselves: social scene simulation. To achieve this, we present MATRIX, a novel social scene simulator that emulates realistic scenes around a user's input query, enabling the LLM to take social consequences into account before responding. MATRIX serves as a virtual rehearsal space, akin to a Monopolylogue, where the LLM performs diverse roles related to the query and practice by itself. To inject this alignment, we fine-tune the LLM with MATRIX-simulated data, ensuring adherence to human values without compromising inference speed. We theoretically show that the LLM with MATRIX outperforms Constitutional AI under mild assumptions. Finally, extensive experiments validate that our method outperforms over 10 baselines across 4 benchmarks. As evidenced by 875 user ratings, our tuned 13B-size LLM exceeds GPT-4 in aligning with human values. See our project page at https://shuotang123.github.io/MATRIX.\n\n\nSelf-alignment of large language models via monopolylogue-based social scene simulation</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.05699v3</guid>
      <dc:creator>Xianghe Pang, Shuo Tang, Rui Ye, Yuxin Xiong, Bolun Zhang, Yanfeng Wang, Siheng Chen</dc:creator>
      <pubDate>Sat, 08 Jun 2024 06:13:55 GMT</pubDate>
    </item>
    <item>
      <title>Trial and Error: Exploration-Based Trajectory Optimization for LLM Agents</title>
      <link>http://arxiv.org/abs/2403.02502v2</link>
      <description>Large Language Models (LLMs) have become integral components in various autonomous agent systems. In this study, we present an exploration-based trajectory optimization approach, referred to as ETO. This learning method is designed to enhance the performance of open LLM agents. Contrary to previous studies that exclusively train on successful expert trajectories, our method allows agents to learn from their exploration failures. This leads to improved performance through an iterative optimization framework. During the exploration phase, the agent interacts with the environment while completing given tasks, gathering failure trajectories to create contrastive trajectory pairs. In the subsequent training phase, the agent utilizes these trajectory preference pairs to update its policy using contrastive learning methods like DPO. This iterative cycle of exploration and training fosters continued improvement in the agents. Our experiments on three complex tasks demonstrate that ETO consistently surpasses baseline performance by a large margin. Furthermore, an examination of task-solving efficiency and potential in scenarios lacking expert trajectory underscores the effectiveness of our approach.\n\n\nTrial and error: Exploration-based trajectory optimization for llm agents</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2403.02502v2</guid>
      <dc:creator>Yifan Song, Da Yin, Xiang Yue, Jie Huang, Sujian Li, Bill Yuchen Lin</dc:creator>
      <pubDate>Wed, 10 Jul 2024 17:36:25 GMT</pubDate>
    </item>
    <item>
      <title>Scaling Laws for Reward Model Overoptimization in Direct Alignment Algorithms</title>
      <link>http://arxiv.org/abs/2406.02900v1</link>
      <description>Reinforcement Learning from Human Feedback (RLHF) has been crucial to the recent success of Large Language Models (LLMs), however, it is often a complex and brittle process. In the classical RLHF framework, a reward model is first trained to represent human preferences, which is in turn used by an online reinforcement learning (RL) algorithm to optimize the LLM. A prominent issue with such methods is \emph{reward over-optimization} or \emph{reward hacking}, where performance as measured by the learned proxy reward model increases, but true quality plateaus or even deteriorates. Direct Alignment Algorithms (DDAs) like Direct Preference Optimization have emerged as alternatives to the classical RLHF pipeline by circumventing the reward modeling phase. However, although DAAs do not use a separate proxy reward model, they still commonly deteriorate from over-optimization. While the so-called reward hacking phenomenon is not well-defined for DAAs, we still uncover similar trends: at higher KL budgets, DAA algorithms exhibit similar degradation patterns to their classic RLHF counterparts. In particular, we find that DAA methods deteriorate not only across a wide range of KL budgets but also often before even a single epoch of the dataset is completed. Through extensive empirical experimentation, this work formulates and formalizes the reward over-optimization or hacking problem for DAAs and explores its consequences across objectives, training regimes, and model scales.\n\n\nScaling laws for reward model overoptimization in direct alignment algorithms</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.02900v1</guid>
      <dc:creator>Rafael Rafailov, Yaswanth Chittepu, Ryan Park, Harshit Sikchi, Joey Hejna, Bradley Knox, Chelsea Finn, Scott Niekum</dc:creator>
      <pubDate>Wed, 05 Jun 2024 03:41:37 GMT</pubDate>
    </item>
    <item>
      <title>CultureBank: An Online Community-Driven Knowledge Base Towards Culturally Aware Language Technologies</title>
      <link>http://arxiv.org/abs/2404.15238v1</link>
      <description>To enhance language models' cultural awareness, we design a generalizable pipeline to construct cultural knowledge bases from different online communities on a massive scale. With the pipeline, we construct CultureBank, a knowledge base built upon users' self-narratives with 12K cultural descriptors sourced from TikTok and 11K from Reddit. Unlike previous cultural knowledge resources, CultureBank contains diverse views on cultural descriptors to allow flexible interpretation of cultural knowledge, and contextualized cultural scenarios to help grounded evaluation. With CultureBank, we evaluate different LLMs' cultural awareness, and identify areas for improvement. We also fine-tune a language model on CultureBank: experiments show that it achieves better performances on two downstream cultural tasks in a zero-shot setting. Finally, we offer recommendations based on our findings for future culturally aware language technologies. The project page is https://culturebank.github.io . The code and model is at https://github.com/SALT-NLP/CultureBank . The released CultureBank dataset is at https://huggingface.co/datasets/SALT-NLP/CultureBank .\n\n\nCulturebank: An online community-driven knowledge base towards culturally aware language technologies</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2404.15238v1</guid>
      <dc:creator>Weiyan Shi, Ryan Li, Yutong Zhang, Caleb Ziems, Chunhua yu, Raya Horesh, Rogério Abreu de Paula, Diyi Yang</dc:creator>
      <pubDate>Tue, 23 Apr 2024 17:16:08 GMT</pubDate>
    </item>
    <item>
      <title>Exploratory Preference Optimization: Harnessing Implicit Q*-Approximation for Sample-Efficient RLHF</title>
      <link>http://arxiv.org/abs/2405.21046v1</link>
      <description>Reinforcement learning from human feedback (RLHF) has emerged as a central tool for language model alignment. We consider online exploration in RLHF, which exploits interactive access to human or AI feedback by deliberately encouraging the model to produce diverse, maximally informative responses. By allowing RLHF to confidently stray from the pre-trained model, online exploration offers the possibility of novel, potentially super-human capabilities, but its full potential as a paradigm for language model training has yet to be realized, owing to computational and statistical bottlenecks in directly adapting existing reinforcement learning techniques. We propose a new algorithm for online exploration in RLHF, Exploratory Preference Optimization (XPO), which is simple and practical -- a one-line change to (online) Direct Preference Optimization (DPO; Rafailov et al., 2023) -- yet enjoys the strongest known provable guarantees and promising empirical performance. XPO augments the DPO objective with a novel and principled exploration bonus, empowering the algorithm to explore outside the support of the initial model and human feedback data. In theory, we show that XPO is provably sample-efficient and converges to a near-optimal language model policy under natural exploration conditions, irrespective of whether the initial model has good coverage. Our analysis, which builds on the observation that DPO implicitly performs a form of $Q^{\star}$-approximation (or, Bellman error minimization), combines previously disparate techniques from language modeling and theoretical reinforcement learning in a serendipitous fashion through the perspective of KL-regularized Markov decision processes. Empirically, we find that XPO is more sample-efficient than non-exploratory DPO variants in a preliminary evaluation.\n\n\nExploratory Preference Optimization: Harnessing Implicit Q*-Approximation for Sample-Efficient RLHF</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.21046v1</guid>
      <dc:creator>Tengyang Xie, Dylan J. Foster, Akshay Krishnamurthy, Corby Rosset, Ahmed Awadallah, Alexander Rakhlin</dc:creator>
      <pubDate>Fri, 31 May 2024 17:39:06 GMT</pubDate>
    </item>
    <item>
      <title>Looping in the Human Collaborative and Explainable Bayesian Optimization</title>
      <link>http://arxiv.org/abs/2310.17273v5</link>
      <description>Like many optimizers, Bayesian optimization often falls short of gaining user trust due to opacity. While attempts have been made to develop human-centric optimizers, they typically assume user knowledge is well-specified and error-free, employing users mainly as supervisors of the optimization process. We relax these assumptions and propose a more balanced human-AI partnership with our Collaborative and Explainable Bayesian Optimization (CoExBO) framework. Instead of explicitly requiring a user to provide a knowledge model, CoExBO employs preference learning to seamlessly integrate human insights into the optimization, resulting in algorithmic suggestions that resonate with user preference. CoExBO explains its candidate selection every iteration to foster trust, empowering users with a clearer grasp of the optimization. Furthermore, CoExBO offers a no-harm guarantee, allowing users to make mistakes; even with extreme adversarial interventions, the algorithm converges asymptotically to a vanilla Bayesian optimization. We validate CoExBO's efficacy through human-AI teaming experiments in lithium-ion battery design, highlighting substantial improvements over conventional methods. Code is available https://github.com/ma921/CoExBO.\n\n\nLooping in the human: Collaborative and explainable Bayesian optimization</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2310.17273v5</guid>
      <dc:creator>Masaki Adachi, Brady Planden, David A. Howey, Michael A. Osborne, Sebastian Orbell, Natalia Ares, Krikamol Muandet, Siu Lun Chau</dc:creator>
      <pubDate>Thu, 29 Feb 2024 14:39:52 GMT</pubDate>
    </item>
    <item>
      <title>Constructive Large Language Models Alignment with Diverse Feedback</title>
      <link>http://arxiv.org/abs/2310.06450v2</link>
      <description>In recent research on large language models (LLMs), there has been a growing emphasis on aligning these models with human values to reduce the impact of harmful content. However, current alignment methods often rely solely on singular forms of human feedback, such as preferences, annotated labels, or natural language critiques, overlooking the potential advantages of combining these feedback types. This limitation leads to suboptimal performance, even when ample training data is available. In this paper, we introduce Constructive and Diverse Feedback (CDF) as a novel method to enhance LLM alignment, inspired by constructivist learning theory. Our approach involves collecting three distinct types of feedback tailored to problems of varying difficulty levels within the training dataset. Specifically, we exploit critique feedback for easy problems, refinement feedback for medium problems, and preference feedback for hard problems. By training our model with this diversified feedback, we achieve enhanced alignment performance while using less training data. To assess the effectiveness of CDF, we evaluate it against previous methods in three downstream tasks: question answering, dialog generation, and text summarization. Experimental results demonstrate that CDF achieves superior performance even with a smaller training dataset.\n\n\nConstructive large language models alignment with diverse feedback</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2310.06450v2</guid>
      <dc:creator>Tianshu Yu, Ting-En Lin, Yuchuan Wu, Min Yang, Fei Huang, Yongbin Li</dc:creator>
      <pubDate>Wed, 11 Oct 2023 07:04:04 GMT</pubDate>
    </item>
    <item>
      <title>Eyes Closed, Safety On: Protecting Multimodal LLMs via Image-to-Text Transformation</title>
      <link>http://arxiv.org/abs/2403.09572v3</link>
      <description>Multimodal large language models (MLLMs) have shown impressive reasoning abilities. However, they are also more vulnerable to jailbreak attacks than their LLM predecessors. Although still capable of detecting the unsafe responses, we observe that safety mechanisms of the pre-aligned LLMs in MLLMs can be easily bypassed with the introduction of image features. To construct robust MLLMs, we propose ECSO (Eyes Closed, Safety On), a novel training-free protecting approach that exploits the inherent safety awareness of MLLMs, and generates safer responses via adaptively transforming unsafe images into texts to activate the intrinsic safety mechanism of pre-aligned LLMs in MLLMs. Experiments on five state-of-the-art (SoTA) MLLMs demonstrate that ECSO enhances model safety significantly (e.g.,, 37.6% improvement on the MM-SafetyBench (SD+OCR) and 71.3% on VLSafe with LLaVA-1.5-7B), while consistently maintaining utility results on common MLLM benchmarks. Furthermore, we show that ECSO can be used as a data engine to generate supervised-finetuning (SFT) data for MLLM alignment without extra human intervention.\n\n\nEyes closed, safety on: Protecting multimodal llms via image-to-text transformation</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2403.09572v3</guid>
      <dc:creator>Yunhao Gou, Kai Chen, Zhili Liu, Lanqing Hong, Hang Xu, Zhenguo Li, Dit-Yan Yeung, James T. Kwok, Yu Zhang</dc:creator>
      <pubDate>Mon, 15 Jul 2024 07:03:53 GMT</pubDate>
    </item>
    <item>
      <title>Do We Really Need a Complex Agent System? Distill Embodied Agent into a Single Model</title>
      <link>http://arxiv.org/abs/2404.04619v1</link>
      <description>With the power of large language models (LLMs), open-ended embodied agents can flexibly understand human instructions, generate interpretable guidance strategies, and output executable actions. Nowadays, Multi-modal Language Models~(MLMs) integrate multi-modal signals into LLMs, further bringing richer perception to entity agents and allowing embodied agents to perceive world-understanding tasks more delicately. However, existing works: 1) operate independently by agents, each containing multiple LLMs, from perception to action, resulting in gaps between complex tasks and execution; 2) train MLMs on static data, struggling with dynamics in open-ended scenarios; 3) input prior knowledge directly as prompts, suppressing application flexibility. We propose STEVE-2, a hierarchical knowledge distillation framework for open-ended embodied tasks, characterized by 1) a hierarchical system for multi-granular task division, 2) a mirrored distillation method for parallel simulation data, and 3) an extra expert model for bringing additional knowledge into parallel simulation. After distillation, embodied agents can complete complex, open-ended tasks without additional expert guidance, utilizing the performance and knowledge of a versatile MLM. Extensive evaluations on navigation and creation tasks highlight the superior performance of STEVE-2 in open-ended tasks, with $1.4 \times$ - $7.3 \times$ in performance.\n\n\nDo we really need a complex agent system? distill embodied agent into a single model</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2404.04619v1</guid>
      <dc:creator>Zhonghan Zhao, Ke Ma, Wenhao Chai, Xuan Wang, Kewei Chen, Dongxu Guo, Yanting Zhang, Hongwei Wang, Gaoang Wang</dc:creator>
      <pubDate>Sat, 06 Apr 2024 12:51:00 GMT</pubDate>
    </item>
    <item>
      <title>Reinforcement Learning from Human Feedback with Active Queries</title>
      <link>http://arxiv.org/abs/2402.09401v1</link>
      <description>Aligning large language models (LLM) with human preference plays a key role in building modern generative models and can be achieved by reinforcement learning from human feedback (RLHF). Despite their superior performance, current RLHF approaches often require a large amount of human-labelled preference data, which is expensive to collect. In this paper, inspired by the success of active learning, we address this problem by proposing query-efficient RLHF methods. We first formalize the alignment problem as a contextual dueling bandit problem and design an active-query-based proximal policy optimization (APPO) algorithm with an $\tilde{O}(d^2/\Delta)$ regret bound and an $\tilde{O}(d^2/\Delta^2)$ query complexity, where $d$ is the dimension of feature space and $\Delta$ is the sub-optimality gap over all the contexts. We then propose ADPO, a practical version of our algorithm based on direct preference optimization (DPO) and apply it to fine-tuning LLMs. Our experiments show that ADPO, while only making about half of queries for human preference, matches the performance of the state-of-the-art DPO method.\n\n\nReinforcement learning from human feedback with active queries</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.09401v1</guid>
      <dc:creator>Kaixuan Ji, Jiafan He, Quanquan Gu</dc:creator>
      <pubDate>Wed, 14 Feb 2024 18:58:40 GMT</pubDate>
    </item>
    <item>
      <title>Aligning Synthetic Medical Images with Clinical Knowledge using Human Feedback</title>
      <link>http://arxiv.org/abs/2306.12438v1</link>
      <description>Generative models capable of capturing nuanced clinical features in medical images hold great promise for facilitating clinical data sharing, enhancing rare disease datasets, and efficiently synthesizing annotated medical images at scale. Despite their potential, assessing the quality of synthetic medical images remains a challenge. While modern generative models can synthesize visually-realistic medical images, the clinical validity of these images may be called into question. Domain-agnostic scores, such as FID score, precision, and recall, cannot incorporate clinical knowledge and are, therefore, not suitable for assessing clinical sensibility. Additionally, there are numerous unpredictable ways in which generative models may fail to synthesize clinically plausible images, making it challenging to anticipate potential failures and manually design scores for their detection. To address these challenges, this paper introduces a pathologist-in-the-loop framework for generating clinically-plausible synthetic medical images. Starting with a diffusion model pretrained using real images, our framework comprises three steps: (1) evaluating the generated images by expert pathologists to assess whether they satisfy clinical desiderata, (2) training a reward model that predicts the pathologist feedback on new samples, and (3) incorporating expert knowledge into the diffusion model by using the reward model to inform a finetuning objective. We show that human feedback significantly improves the quality of synthetic images in terms of fidelity, diversity, utility in downstream applications, and plausibility as evaluated by experts.\n\n\nFeedback efficient online fine-tuning of diffusion models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2306.12438v1</guid>
      <dc:creator>Shenghuan Sun, Gregory M. Goldgof, Atul Butte, Ahmed M. Alaa</dc:creator>
      <pubDate>Fri, 16 Jun 2023 21:54:20 GMT</pubDate>
    </item>
    <item>
      <title>Provably Mitigating Overoptimization in RLHF: Your SFT Loss is Implicitly an Adversarial Regularizer</title>
      <link>http://arxiv.org/abs/2405.16436v1</link>
      <description>Aligning generative models with human preference via RLHF typically suffers from overoptimization, where an imperfectly learned reward model can misguide the generative model to output undesired responses. We investigate this problem in a principled manner by identifying the source of the misalignment as a form of distributional shift and uncertainty in learning human preferences. To mitigate overoptimization, we first propose a theoretical algorithm that chooses the best policy for an adversarially chosen reward model; one that simultaneously minimizes the maximum likelihood estimation of the loss and a reward penalty term. Here, the reward penalty term is introduced to prevent the policy from choosing actions with spurious high proxy rewards, resulting in provable sample efficiency of the algorithm under a partial coverage style condition. Moving from theory to practice, the proposed algorithm further enjoys an equivalent but surprisingly easy-to-implement reformulation. Using the equivalence between reward models and the corresponding optimal policy, the algorithm features a simple objective that combines: (i) a preference optimization loss that directly aligns the policy with human preference, and (ii) a supervised learning loss that explicitly imitates the policy with a (suitable) baseline distribution. In the context of aligning large language models (LLM), this objective fuses the direct preference optimization (DPO) loss with the supervised fune-tuning (SFT) loss to help mitigate the overoptimization towards undesired responses, for which we name the algorithm Regularized Preference Optimization (RPO). Experiments of aligning LLMs demonstrate the improved performance of RPO compared with DPO baselines. Our work sheds light on the interplay between preference optimization and SFT in tuning LLMs with both theoretical guarantees and empirical evidence.\n\n\nProvably mitigating overoptimization in rlhf: Your sft loss is implicitly an adversarial regularizer</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.16436v1</guid>
      <dc:creator>Zhihan Liu, Miao Lu, Shenao Zhang, Boyi Liu, Hongyi Guo, Yingxiang Yang, Jose Blanchet, Zhaoran Wang</dc:creator>
      <pubDate>Sun, 26 May 2024 05:38:50 GMT</pubDate>
    </item>
    <item>
      <title>Overcoming Reward Overoptimization via Adversarial Policy Optimization with Lightweight Uncertainty Estimation</title>
      <link>http://arxiv.org/abs/2403.05171v2</link>
      <description>We introduce Adversarial Policy Optimization (AdvPO), a novel solution to the pervasive issue of reward over-optimization in Reinforcement Learning from Human Feedback (RLHF) for Large Language Models (LLMs). Over-optimization occurs when a reward model serves as an imperfect proxy for human preference, and RL-driven policy optimization erroneously exploits reward inaccuracies. In this paper, we begin by introducing a lightweight way to quantify uncertainties in rewards, relying solely on the last layer embeddings of the reward model, without the need for computationally expensive reward ensembles. AdvPO then addresses a distributionally robust optimization problem centred around the confidence interval of the reward model's predictions for policy improvement. Through comprehensive experiments on the Anthropic HH and TL;DR summarization datasets, we illustrate the efficacy of AdvPO in mitigating the overoptimization issue, consequently resulting in enhanced performance as evaluated through human-assisted evaluation.\n\n\nOvercoming reward overoptimization via adversarial policy optimization with lightweight uncertainty estimation</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2403.05171v2</guid>
      <dc:creator>Xiaoying Zhang, Jean-Francois Ton, Wei Shen, Hongning Wang, Yang Liu</dc:creator>
      <pubDate>Tue, 09 Jul 2024 13:17:36 GMT</pubDate>
    </item>
    <item>
      <title>Tastle: Distract Large Language Models for Automatic Jailbreak Attack</title>
      <link>http://arxiv.org/abs/2403.08424v1</link>
      <description>Large language models (LLMs) have achieved significant advances in recent days. Extensive efforts have been made before the public release of LLMs to align their behaviors with human values. The primary goal of alignment is to ensure their helpfulness, honesty and harmlessness. However, even meticulously aligned LLMs remain vulnerable to malicious manipulations such as jailbreaking, leading to unintended behaviors. The jailbreak is to intentionally develop a malicious prompt that escapes from the LLM security restrictions to produce uncensored detrimental contents. Previous works explore different jailbreak methods for red teaming LLMs, yet they encounter challenges regarding to effectiveness and scalability. In this work, we propose Tastle, a novel black-box jailbreak framework for automated red teaming of LLMs. We designed malicious content concealing and memory reframing with an iterative optimization algorithm to jailbreak LLMs, motivated by the research about the distractibility and over-confidence phenomenon of LLMs. Extensive experiments of jailbreaking both open-source and proprietary LLMs demonstrate the superiority of our framework in terms of effectiveness, scalability and transferability. We also evaluate the effectiveness of existing jailbreak defense methods against our attack and highlight the crucial need to develop more effective and practical defense strategies.\n\n\nTastle: Distract large language models for automatic jailbreak attack</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2403.08424v1</guid>
      <dc:creator>Zeguan Xiao, Yan Yang, Guanhua Chen, Yun Chen</dc:creator>
      <pubDate>Wed, 13 Mar 2024 11:16:43 GMT</pubDate>
    </item>
    <item>
      <title>Selecting Large Language Model to Fine-tune via Rectified Scaling Law</title>
      <link>http://arxiv.org/abs/2402.02314v3</link>
      <description>The ever-growing ecosystem of LLMs has posed a challenge in selecting the most appropriate pre-trained model to fine-tune amidst a sea of options. Given constrained resources, fine-tuning all models and making selections afterward is unrealistic. In this work, we formulate this resource-constrained selection task into predicting fine-tuning performance and illustrate its natural connection with Scaling Law. Unlike pre-training, we find that the fine-tuning scaling curve includes not just the well-known &quot;power phase&quot; but also the previously unobserved &quot;pre-power phase&quot;. We also explain why existing Scaling Law fails to capture this phase transition phenomenon both theoretically and empirically. To address this, we introduce the concept of &quot;pre-learned data size&quot; into our Rectified Scaling Law, which overcomes theoretical limitations and fits experimental results much better. By leveraging our law, we propose a novel LLM selection algorithm that selects the near-optimal model with hundreds of times less resource consumption, while other methods may provide negatively correlated selection. The project page is available at rectified-scaling-law.github.io.\n\n\nSelecting large language model to fine-tune via rectified scaling law</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.02314v3</guid>
      <dc:creator>Haowei Lin, Baizhou Huang, Haotian Ye, Qinyu Chen, Zihao Wang, Sujian Li, Jianzhu Ma, Xiaojun Wan, James Zou, Yitao Liang</dc:creator>
      <pubDate>Tue, 28 May 2024 16:16:42 GMT</pubDate>
    </item>
    <item>
      <title>Human vs. Machine: Behavioral Differences Between Expert Humans and Language Models in Wargame Simulations</title>
      <link>http://arxiv.org/abs/2403.03407v2</link>
      <description>To some, the advent of artificial intelligence (AI) promises better decision-making and increased military effectiveness while reducing the influence of human error and emotions. However, there is still debate about how AI systems, especially large language models (LLMs), behave compared to humans in high-stakes military decision-making scenarios with the potential for increased risks towards escalation and unnecessary conflicts. To test this potential and scrutinize the use of LLMs for such purposes, we use a new wargame experiment with 107 national security experts designed to look at crisis escalation in a fictional US-China scenario and compare human players to LLM-simulated responses in separate simulations. Wargames have a long history in the development of military strategy and the response of nations to threats or attacks. Here, we show a considerable high-level agreement in the LLM and human responses and significant quantitative and qualitative differences in individual actions and strategic tendencies. These differences depend on intrinsic biases in LLMs regarding the appropriate level of violence following strategic instructions, the choice of LLM, and whether the LLMs are tasked to decide for a team of players directly or first to simulate dialog between players. When simulating the dialog, the discussions lack quality and maintain a farcical harmony. The LLM simulations cannot account for human player characteristics, showing no significant difference even for extreme traits, such as &quot;pacifist&quot; or &quot;aggressive sociopath&quot;. Our results motivate policymakers to be cautious before granting autonomy or following AI-based strategy recommendations.\n\n\nHuman vs. machine: Language models and wargames</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2403.03407v2</guid>
      <dc:creator>Max Lamparth, Anthony Corso, Jacob Ganz, Oriana Skylar Mastro, Jacquelyn Schneider, Harold Trinkunas</dc:creator>
      <pubDate>Mon, 03 Jun 2024 15:00:47 GMT</pubDate>
    </item>
    <item>
      <title>The Empty Signifier Problem: Towards Clearer Paradigms for Operationalising &quot;Alignment&quot; in Large Language Models</title>
      <link>http://arxiv.org/abs/2310.02457v2</link>
      <description>In this paper, we address the concept of &quot;alignment&quot; in large language models (LLMs) through the lens of post-structuralist socio-political theory, specifically examining its parallels to empty signifiers. To establish a shared vocabulary around how abstract concepts of alignment are operationalised in empirical datasets, we propose a framework that demarcates: 1) which dimensions of model behaviour are considered important, then 2) how meanings and definitions are ascribed to these dimensions, and by whom. We situate existing empirical literature and provide guidance on deciding which paradigm to follow. Through this framework, we aim to foster a culture of transparency and critical evaluation, aiding the community in navigating the complexities of aligning LLMs with human populations.\n\n\nThe empty signifier problem: Towards clearer paradigms for operationalising&quot; alignment&quot; in large language models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2310.02457v2</guid>
      <dc:creator>Hannah Rose Kirk, Bertie Vidgen, Paul Röttger, Scott A. Hale</dc:creator>
      <pubDate>Wed, 15 Nov 2023 18:02:03 GMT</pubDate>
    </item>
    <item>
      <title>Selfsupervised learning for pathological speech detection</title>
      <link>http://arxiv.org/abs/2406.02572v1</link>
      <description>Speech production is a complex phenomenon, wherein the brain orchestrates a sequence of processes involving thought processing, motor planning, and the execution of articulatory movements. However, this intricate execution of various processes is susceptible to influence and disruption by various neurodegenerative pathological speech disorders, such as Parkinsons' disease, resulting in dysarthria, apraxia, and other conditions. These disorders lead to pathological speech characterized by abnormal speech patterns and imprecise articulation. Diagnosing these speech disorders in clinical settings typically involves auditory perceptual tests, which are time-consuming, and the diagnosis can vary among clinicians based on their experiences, biases, and cognitive load during the diagnosis. Additionally, unlike neurotypical speakers, patients with speech pathologies or impairments are unable to access various virtual assistants such as Alexa, Siri, etc. To address these challenges, several automatic pathological speech detection (PSD) approaches have been proposed. These approaches aim to provide efficient and accurate detection of speech disorders, thereby facilitating timely intervention and support for individuals affected by these conditions. These approaches mainly vary in two aspects: the input representations utilized and the classifiers employed. Due to the limited availability of data, the performance of detection remains subpar. Self-supervised learning (SSL) embeddings, such as wav2vec2, and their multilingual versions, are being explored as a promising avenue to improve performance. These embeddings leverage self-supervised learning techniques to extract rich representations from audio data, thereby offering a potential solution to address the limitations posed by the scarcity of labeled data.\n\n\nSelf-supervised alignment with mutual information: Learning to follow principles without preference labels</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.02572v1</guid>
      <dc:creator>Shakeel Ahmad Sheikh</dc:creator>
      <pubDate>Thu, 16 May 2024 07:12:47 GMT</pubDate>
    </item>
    <item>
      <title>Learning Interpretable Concepts: Unifying Causal Representation Learning and Foundation Models</title>
      <link>http://arxiv.org/abs/2402.09236v1</link>
      <description>To build intelligent machine learning systems, there are two broad approaches. One approach is to build inherently interpretable models, as endeavored by the growing field of causal representation learning. The other approach is to build highly-performant foundation models and then invest efforts into understanding how they work. In this work, we relate these two approaches and study how to learn human-interpretable concepts from data. Weaving together ideas from both fields, we formally define a notion of concepts and show that they can be provably recovered from diverse data. Experiments on synthetic data and large language models show the utility of our unified approach.\n\n\nLearning interpretable concepts: Unifying causal representation learning and foundation models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.09236v1</guid>
      <dc:creator>Goutham Rajendran, Simon Buchholz, Bryon Aragam, Bernhard Schölkopf, Pradeep Ravikumar</dc:creator>
      <pubDate>Wed, 14 Feb 2024 15:23:59 GMT</pubDate>
    </item>
    <item>
      <title>Towards Tracing Trustworthiness Dynamics: Revisiting Pre-training Period of Large Language Models</title>
      <link>http://arxiv.org/abs/2402.19465v1</link>
      <description>Ensuring the trustworthiness of large language models (LLMs) is crucial. Most studies concentrate on fully pre-trained LLMs to better understand and improve LLMs' trustworthiness. In this paper, to reveal the untapped potential of pre-training, we pioneer the exploration of LLMs' trustworthiness during this period, focusing on five key dimensions: reliability, privacy, toxicity, fairness, and robustness. To begin with, we apply linear probing to LLMs. The high probing accuracy suggests that \textit{LLMs in early pre-training can already distinguish concepts in each trustworthiness dimension}. Therefore, to further uncover the hidden possibilities of pre-training, we extract steering vectors from a LLM's pre-training checkpoints to enhance the LLM's trustworthiness. Finally, inspired by~\citet{choi2023understanding} that mutual information estimation is bounded by linear probing accuracy, we also probe LLMs with mutual information to investigate the dynamics of trustworthiness during pre-training. We are the first to observe a similar two-phase phenomenon: fitting and compression~\citep{shwartz2017opening}. This research provides an initial exploration of trustworthiness modeling during LLM pre-training, seeking to unveil new insights and spur further developments in the field. We will make our code publicly accessible at \url{https://github.com/ChnQ/TracingLLM}.\n\n\nTowards tracing trustworthiness dynamics: Revisiting pre-training period of large language models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.19465v1</guid>
      <dc:creator>Chen Qian, Jie Zhang, Wei Yao, Dongrui Liu, Zhenfei Yin, Yu Qiao, Yong Liu, Jing Shao</dc:creator>
      <pubDate>Thu, 29 Feb 2024 18:55:06 GMT</pubDate>
    </item>
    <item>
      <title>Safety Fine-Tuning at (Almost) No Cost: A Baseline for Vision Large Language Models</title>
      <link>http://arxiv.org/abs/2402.02207v2</link>
      <description>Current vision large language models (VLLMs) exhibit remarkable capabilities yet are prone to generate harmful content and are vulnerable to even the simplest jailbreaking attacks. Our initial analysis finds that this is due to the presence of harmful data during vision-language instruction fine-tuning, and that VLLM fine-tuning can cause forgetting of safety alignment previously learned by the underpinning LLM. To address this issue, we first curate a vision-language safe instruction-following dataset VLGuard covering various harmful categories. Our experiments demonstrate that integrating this dataset into standard vision-language fine-tuning or utilizing it for post-hoc fine-tuning effectively safety aligns VLLMs. This alignment is achieved with minimal impact on, or even enhancement of, the models' helpfulness. The versatility of our safety fine-tuning dataset makes it a valuable resource for safety-testing existing VLLMs, training new models or safeguarding pre-trained VLLMs. Empirical results demonstrate that fine-tuned VLLMs effectively reject unsafe instructions and substantially reduce the success rates of several black-box adversarial attacks, which approach zero in many cases. The code and dataset are available at https://github.com/ys-zong/VLGuard.\n\n\nParameter-efficient tuning helps language model alignment</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.02207v2</guid>
      <dc:creator>Yongshuo Zong, Ondrej Bohdal, Tingyang Yu, Yongxin Yang, Timothy Hospedales</dc:creator>
      <pubDate>Mon, 17 Jun 2024 22:26:32 GMT</pubDate>
    </item>
    <item>
      <title>The Alignment Ceiling: Objective Mismatch in Reinforcement Learning from Human Feedback</title>
      <link>http://arxiv.org/abs/2311.00168v2</link>
      <description>Reinforcement learning from human feedback (RLHF) has emerged as a powerful technique to make large language models (LLMs) more capable in complex settings. RLHF proceeds as collecting human preference data, training a reward model on said data, and optimizing a base ML model with respect to said reward for extrinsic evaluation metrics (e.g. MMLU, GSM8k). RLHF relies on many assumptions about how the various pieces fit together, such as a reward model capturing human preferences and an RL optimizer extracting the right signal from a reward model. As the RLHF process involves many distinct design decisions, it is easy to assume that multiple processes are correlated and therefore numerically linked. This apparent correlation is often not true, where reward models are easily overoptimized or RL optimizers can reduce performance on tasks not modeled in the data. Notable manifestations of models trained with imperfect RLHF systems are those that are prone to refusing basic requests for safety reasons or appearing lazy in generations. As chat model evaluation becomes increasingly nuanced, the reliance on a perceived link between reward model training, RL scores, and downstream performance drives these issues, which we describe as an objective mismatch. In this paper, we illustrate the causes of this issue, reviewing relevant literature from model-based reinforcement learning, and argue for solutions. By solving objective mismatch in RLHF, the ML models of the future will be more precisely aligned to user instructions for both safety and helpfulness.\n\n\nThe alignment ceiling: Objective mismatch in reinforcement learning from human feedback</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2311.00168v2</guid>
      <dc:creator>Nathan Lambert, Roberto Calandra</dc:creator>
      <pubDate>Fri, 02 Feb 2024 03:41:50 GMT</pubDate>
    </item>
    <item>
      <title>Canvil: Designerly Adaptation for LLM-Powered User Experiences</title>
      <link>http://arxiv.org/abs/2401.09051v1</link>
      <description>Advancements in large language models (LLMs) are poised to spark a proliferation of LLM-powered user experiences. In product teams, designers are often tasked with crafting user experiences that align with user needs. To involve designers and leverage their user-centered perspectives to create effective and responsible LLM-powered products, we introduce the practice of designerly adaptation for engaging with LLMs as an adaptable design material. We first identify key characteristics of designerly adaptation through a formative study with designers experienced in designing for LLM-powered products (N=12). These characteristics are 1) have a low technical barrier to entry, 2) leverage designers' unique perspectives bridging users and technology, and 3) encourage model tinkering. Based on this characterization, we build Canvil, a Figma widget that operationalizes designerly adaptation. Canvil supports structured authoring of system prompts to adapt LLM behavior, testing of adapted models on diverse user inputs, and integration of model outputs into interface designs. We use Canvil as a technology probe in a group-based design study (6 groups, N=17) to investigate the implications of integrating designerly adaptation into design workflows. We find that designers are able to iteratively tinker with different adaptation approaches and reason about interface affordances to enhance end-user interaction with LLMs. Furthermore, designers identified promising collaborative workflows for designerly adaptation. Our work opens new avenues for collaborative processes and tools that foreground designers' user-centered expertise in the crafting and deployment of LLM-powered user experiences.\n\n\nCanvil: Designerly Adaptation for LLM-Powered User Experiences</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2401.09051v1</guid>
      <dc:creator>K. J. Kevin Feng, Q. Vera Liao, Ziang Xiao, Jennifer Wortman Vaughan, Amy X. Zhang, David W. McDonald</dc:creator>
      <pubDate>Wed, 17 Jan 2024 08:32:17 GMT</pubDate>
    </item>
    <item>
      <title>Countering Reward Over-optimization in LLM with Demonstration-Guided Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2404.19409v1</link>
      <description>While Reinforcement Learning (RL) has been proven essential for tuning large language models (LLMs), it can lead to reward over-optimization (ROO). Existing approaches address ROO by adding KL regularization, requiring computationally expensive hyperparameter tuning. Additionally, KL regularization focuses solely on regularizing the language policy, neglecting a potential source of regularization: the reward function itself. Inspired by demonstration-guided RL, we here introduce the Reward Calibration from Demonstration (RCfD), which leverages human demonstrations and a reward model to recalibrate the reward objective. Formally, given a prompt, the RCfD objective minimizes the distance between the demonstrations' and LLM's rewards rather than directly maximizing the reward function. This objective shift avoids incentivizing the LLM to exploit the reward model and promotes more natural and diverse language generation. We show the effectiveness of RCfD on three language tasks, which achieves comparable performance to carefully tuned baselines while mitigating ROO.\n\n\nCountering reward over-optimization in llm with demonstration-guided reinforcement learning</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2404.19409v1</guid>
      <dc:creator>Mathieu Rita, Florian Strub, Rahma Chaabouni, Paul Michel, Emmanuel Dupoux, Olivier Pietquin</dc:creator>
      <pubDate>Tue, 30 Apr 2024 09:57:21 GMT</pubDate>
    </item>
    <item>
      <title>The History and Risks of Reinforcement Learning and Human Feedback</title>
      <link>http://arxiv.org/abs/2310.13595v2</link>
      <description>Reinforcement learning from human feedback (RLHF) has emerged as a powerful technique to make large language models (LLMs) easier to use and more effective. A core piece of the RLHF process is the training and utilization of a model of human preferences that acts as a reward function for optimization. This approach, which operates at the intersection of many stakeholders and academic disciplines, remains poorly understood. RLHF reward models are often cited as being central to achieving performance, yet very few descriptors of capabilities, evaluations, training methods, or open-source models exist. Given this lack of information, further study and transparency is needed for learned RLHF reward models. In this paper, we illustrate the complex history of optimizing preferences, and articulate lines of inquiry to understand the sociotechnical context of reward models. In particular, we highlight the ontological differences between costs, rewards, and preferences at stake in RLHF's foundations, related methodological tensions, and possible research directions to improve general understanding of how reward models function.\n\n\nEntangled preferences: The history and risks of reinforcement learning and human feedback</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2310.13595v2</guid>
      <dc:creator>Nathan Lambert, Thomas Krendl Gilbert, Tom Zick</dc:creator>
      <pubDate>Tue, 28 Nov 2023 18:16:11 GMT</pubDate>
    </item>
    <item>
      <title>Aligning to Thousands of Preferences via System Message Generalization</title>
      <link>http://arxiv.org/abs/2405.17977v1</link>
      <description>Although humans inherently have diverse values, current large language model (LLM) alignment methods often assume that aligning LLMs with the general public's preferences is optimal. A major challenge in adopting a more individualized approach to LLM alignment is its lack of scalability, as it involves repeatedly acquiring preference data and training new reward models and LLMs for each individual's preferences. To address these challenges, we propose a new paradigm where users specify what they value most within the system message, steering the LLM's generation behavior to better align with the user's intentions. However, a naive application of such an approach is non-trivial since LLMs are typically trained on a uniform system message (e.g., &quot;You are a helpful assistant&quot;) which limits their ability to generalize to diverse, unseen system messages. To improve this generalization, we create the Multifaceted Collection, a preference dataset with 192k combinations of values beyond generic helpfulness and harmlessness, spanning 65k user instructions. Using this dataset, we train a 7B LLM called Janus and test it on 921 prompts from 5 benchmarks (AlpacaEval 2.0, FLASK, Koala, MT-Bench, and Self-Instruct) by adding various unseen system messages that reflect user preferences. Janus achieves tie+win rate of 75.2%, 72.4%, and 66.4% against Mistral 7B Instruct v0.2, GPT-3.5 Turbo, and GPT-4, respectively. Unexpectedly, on three benchmarks focused on response helpfulness (AlpacaEval 2.0, MT-Bench, Arena Hard Auto v0.1), Janus also outperforms LLaMA 3 8B Instruct by a +4.0%, +0.1%, +3.0% margin, underscoring that training with a vast array of system messages could also enhance alignment to the general public's preference as well. Our code, dataset, benchmark, and models are available at https://github.com/kaistAI/Janus.\n\n\nAligning to thousands of preferences via system message generalization</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.17977v1</guid>
      <dc:creator>Seongyun Lee, Sue Hyun Park, Seungone Kim, Minjoon Seo</dc:creator>
      <pubDate>Tue, 28 May 2024 09:06:18 GMT</pubDate>
    </item>
    <item>
      <title>OpenELM: An Efficient Language Model Family with Open Training and Inference Framework</title>
      <link>http://arxiv.org/abs/2404.14619v2</link>
      <description>The reproducibility and transparency of large language models are crucial for advancing open research, ensuring the trustworthiness of results, and enabling investigations into data and model biases, as well as potential risks. To this end, we release OpenELM, a state-of-the-art open language model. OpenELM uses a layer-wise scaling strategy to efficiently allocate parameters within each layer of the transformer model, leading to enhanced accuracy. For example, with a parameter budget of approximately one billion parameters, OpenELM exhibits a 2.36% improvement in accuracy compared to OLMo while requiring $2\times$ fewer pre-training tokens.   Diverging from prior practices that only provide model weights and inference code, and pre-train on private datasets, our release includes the complete framework for training and evaluation of the language model on publicly available datasets, including training logs, multiple checkpoints, and pre-training configurations. We also release code to convert models to MLX library for inference and fine-tuning on Apple devices. This comprehensive release aims to empower and strengthen the open research community, paving the way for future open research endeavors.   Our source code along with pre-trained model weights and training recipes is available at \url{https://github.com/apple/corenet}. Additionally, \model models can be found on HuggingFace at: \url{https://huggingface.co/apple/OpenELM}.\n\n\nOpenELM: An Efficient Language Model Family with Open-source Training and Inference Framework</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2404.14619v2</guid>
      <dc:creator>Sachin Mehta, Mohammad Hossein Sekhavat, Qingqing Cao, Maxwell Horton, Yanzi Jin, Chenfan Sun, Iman Mirzadeh, Mahyar Najibi, Dmitry Belenko, Peter Zatloukal, Mohammad Rastegari</dc:creator>
      <pubDate>Thu, 02 May 2024 00:30:57 GMT</pubDate>
    </item>
    <item>
      <title>Curriculum Direct Preference Optimization for Diffusion and Consistency Models</title>
      <link>http://arxiv.org/abs/2405.13637v2</link>
      <description>Direct Preference Optimization (DPO) has been proposed as an effective and efficient alternative to reinforcement learning from human feedback (RLHF). In this paper, we propose a novel and enhanced version of DPO based on curriculum learning for text-to-image generation. Our method is divided into two training stages. First, a ranking of the examples generated for each prompt is obtained by employing a reward model. Then, increasingly difficult pairs of examples are sampled and provided to a text-to-image generative (diffusion or consistency) model. Generated samples that are far apart in the ranking are considered to form easy pairs, while those that are close in the ranking form hard pairs. In other words, we use the rank difference between samples as a measure of difficulty. The sampled pairs are split into batches according to their difficulty levels, which are gradually used to train the generative model. Our approach, Curriculum DPO, is compared against state-of-the-art fine-tuning approaches on three benchmarks, outperforming the competing methods in terms of text alignment, aesthetics and human preference. Our code is available at https://anonymous.4open.science/r/Curriculum-DPO-EE14.\n\n\nCurry-dpo: Enhancing alignment using curriculum learning &amp; ranked preferences</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.13637v2</guid>
      <dc:creator>Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu, Nicu Sebe, Mubarak Shah</dc:creator>
      <pubDate>Fri, 24 May 2024 13:14:40 GMT</pubDate>
    </item>
    <item>
      <title>ICDPO: Effectively Borrowing Alignment Capability of Others via In-context Direct Preference Optimization</title>
      <link>http://arxiv.org/abs/2402.09320v1</link>
      <description>Large Language Models (LLMs) rely on Human Preference Alignment (HPA) to ensure the generation of safe content. Due to the heavy cost associated with fine-tuning, fine-tuning-free methods have emerged, typically modifying LLM decoding with external auxiliary methods. However, these methods do not essentially enhance the LLM itself. In this paper, we rethink the derivation procedures of DPO, based on which we conversely build an instant scorer using the states of the LLM before and after In-context Learning (ICL). Accordingly, we propose a novel approach called In-Context Direct Preference Optimization (ICDPO). It enables LLMs to borrow the HPA capabilities from superior LLMs with ICL, generating well-aligned responses as estimated by the aforementioned instant scorer, thereby enhancing the final performance. ICDPO can be further enhanced with a two-stage retriever and an upgraded scorer, both offering benefits. Extensive experiments show its effectiveness, particularly in outperforming two fine-tuning-free baselines, and it exhibits competitiveness with SFT + LoRA. We also conduct detailed analyses to offer comprehensive insights into ICDPO.\n\n\nIcdpo: Effectively borrowing alignment capability of others via in-context direct preference optimization</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.09320v1</guid>
      <dc:creator>Feifan Song, Yuxuan Fan, Xin Zhang, Peiyi Wang, Houfeng Wang</dc:creator>
      <pubDate>Wed, 14 Feb 2024 17:14:34 GMT</pubDate>
    </item>
    <item>
      <title>Detecting and Mitigating Hallucination in Large Vision Language Models via Fine-Grained AI Feedback</title>
      <link>http://arxiv.org/abs/2404.14233v1</link>
      <description>The rapidly developing Large Vision Language Models (LVLMs) have shown notable capabilities on a range of multi-modal tasks, but still face the hallucination phenomena where the generated texts do not align with the given contexts, significantly restricting the usages of LVLMs. Most previous work detects and mitigates hallucination at the coarse-grained level or requires expensive annotation (e.g., labeling by proprietary models or human experts). To address these issues, we propose detecting and mitigating hallucinations in LVLMs via fine-grained AI feedback. The basic idea is that we generate a small-size sentence-level hallucination annotation dataset by proprietary models, whereby we train a hallucination detection model which can perform sentence-level hallucination detection, covering primary hallucination types (i.e., object, attribute, and relationship). Then, we propose a detect-then-rewrite pipeline to automatically construct preference dataset for training hallucination mitigating model. Furthermore, we propose differentiating the severity of hallucinations, and introducing a Hallucination Severity-Aware Direct Preference Optimization (HSA-DPO) for mitigating hallucination in LVLMs by incorporating the severity of hallucinations into preference learning. Extensive experiments demonstrate the effectiveness of our method.\n\n\nDetecting and mitigating hallucination in large vision language models via fine-grained ai feedback</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2404.14233v1</guid>
      <dc:creator>Wenyi Xiao, Ziwei Huang, Leilei Gan, Wanggui He, Haoyuan Li, Zhelun Yu, Hao Jiang, Fei Wu, Linchao Zhu</dc:creator>
      <pubDate>Mon, 22 Apr 2024 14:46:10 GMT</pubDate>
    </item>
    <item>
      <title>Leveraging Implicit Feedback from Deployment Data in Dialogue</title>
      <link>http://arxiv.org/abs/2307.14117v2</link>
      <description>We study improving social conversational agents by learning from natural dialogue between users and a deployed model, without extra annotations. To implicitly measure the quality of a machine-generated utterance, we leverage signals like user response length, sentiment and reaction of the future human utterances in the collected dialogue episodes. Our experiments use the publicly released deployment data from BlenderBot (Xu et al., 2023). Human evaluation indicates improvements in our new models over baseline responses; however, we find that some proxy signals can lead to more generations with undesirable properties as well. For example, optimizing for conversation length can lead to more controversial or unfriendly generations compared to the baseline, whereas optimizing for positive sentiment or reaction can decrease these behaviors.\n\n\nLeveraging implicit feedback from deployment data in dialogue</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2307.14117v2</guid>
      <dc:creator>Richard Yuanzhe Pang, Stephen Roller, Kyunghyun Cho, He He, Jason Weston</dc:creator>
      <pubDate>Thu, 01 Feb 2024 04:30:38 GMT</pubDate>
    </item>
    <item>
      <title>Protecting Your LLMs with Information Bottleneck</title>
      <link>http://arxiv.org/abs/2404.13968v2</link>
      <description>The advent of large language models (LLMs) has revolutionized the field of natural language processing, yet they might be attacked to produce harmful content. Despite efforts to ethically align LLMs, these are often fragile and can be circumvented by jailbreaking attacks through optimized or manual adversarial prompts. To address this, we introduce the Information Bottleneck Protector (IBProtector), a defense mechanism grounded in the information bottleneck principle, and we modify the objective to avoid trivial solutions. The IBProtector selectively compresses and perturbs prompts, facilitated by a lightweight and trainable extractor, preserving only essential information for the target LLMs to respond with the expected answer. Moreover, we further consider a situation where the gradient is not visible to be compatible with any LLM. Our empirical evaluations show that IBProtector outperforms current defense methods in mitigating jailbreak attempts, without overly affecting response quality or inference speed. Its effectiveness and adaptability across various attack methods and target LLMs underscore the potential of IBProtector as a novel, transferable defense that bolsters the security of LLMs without requiring modifications to the underlying models.\n\n\nProtecting your llms with information bottleneck</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2404.13968v2</guid>
      <dc:creator>Zichuan Liu, Zefan Wang, Linjie Xu, Jinyu Wang, Lei Song, Tianchun Wang, Chunlin Chen, Wei Cheng, Jiang Bian</dc:creator>
      <pubDate>Thu, 16 May 2024 13:26:57 GMT</pubDate>
    </item>
    <item>
      <title>Panacea: Pareto Alignment via Preference Adaptation for LLMs</title>
      <link>http://arxiv.org/abs/2402.02030v2</link>
      <description>Current methods for large language model alignment typically use scalar human preference labels. However, this convention tends to oversimplify the multi-dimensional and heterogeneous nature of human preferences, leading to reduced expressivity and even misalignment. This paper presents Panacea, an innovative approach that reframes alignment as a multi-dimensional preference optimization problem. Panacea trains a single model capable of adapting online and Pareto-optimally to diverse sets of preferences without the need for further tuning. A major challenge here is using a low-dimensional preference vector to guide the model's behavior, despite it being governed by an overwhelmingly large number of parameters. To address this, Panacea is designed to use singular value decomposition (SVD)-based low-rank adaptation, which allows the preference vector to be simply injected online as singular values. Theoretically, we prove that Panacea recovers the entire Pareto front with common loss aggregation methods under mild conditions. Moreover, our experiments demonstrate, for the first time, the feasibility of aligning a single LLM to represent an exponentially vast spectrum of human preferences through various optimization methods. Our work marks a step forward in effectively and efficiently aligning models to diverse and intricate human preferences in a controllable and Pareto-optimal manner.\n\n\nPanacea: Pareto Alignment via Preference Adaptation for LLMs</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.02030v2</guid>
      <dc:creator>Yifan Zhong, Chengdong Ma, Xiaoyuan Zhang, Ziran Yang, Haojun Chen, Qingfu Zhang, Siyuan Qi, Yaodong Yang</dc:creator>
      <pubDate>Thu, 23 May 2024 13:49:25 GMT</pubDate>
    </item>
    <item>
      <title>TigerBot: An Open Multilingual Multitask LLM</title>
      <link>http://arxiv.org/abs/2312.08688v2</link>
      <description>We release and introduce the TigerBot family of large language models (LLMs), consisting of base and chat models, sized from 7, 13, 70 and 180 billion parameters. We develop our models embarking from Llama-2 and BLOOM, and push the boundary further in data, training algorithm, infrastructure, and application tools. Our models yield meaningful performance gain over SOTA open-source models, e.g., Llama-2, specifically 6% gain in English and 20% gain in Chinese. TigerBot model family also achieves leading performance in major academic and industrial benchmarks and leaderboards. We believe that TigerBot represents just a snapshot of lightning-fast progression in LLM open-source community. Therefore, we are thrilled to give back by publicly releasing our models and reporting our approach behind, with additional emphases on building SOTA LLMs in a democratized way and making LLMs of use in real-world applications.\n\n\nTigerbot: An open multilingual multitask llm</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2312.08688v2</guid>
      <dc:creator>Ye Chen, Wei Cai, Liangmin Wu, Xiaowei Li, Zhanxuan Xin, Cong Fu</dc:creator>
      <pubDate>Fri, 15 Dec 2023 01:42:20 GMT</pubDate>
    </item>
    <item>
      <title>Empowering Biomedical Discovery with AI Agents</title>
      <link>http://arxiv.org/abs/2404.02831v1</link>
      <description>We envision 'AI scientists' as systems capable of skeptical learning and reasoning that empower biomedical research through collaborative agents that integrate machine learning tools with experimental platforms. Rather than taking humans out of the discovery process, biomedical AI agents combine human creativity and expertise with AI's ability to analyze large datasets, navigate hypothesis spaces, and execute repetitive tasks. AI agents are proficient in a variety of tasks, including self-assessment and planning of discovery workflows. These agents use large language models and generative models to feature structured memory for continual learning and use machine learning tools to incorporate scientific knowledge, biological principles, and theories. AI agents can impact areas ranging from hybrid cell simulation, programmable control of phenotypes, and the design of cellular circuits to the development of new therapies.\n\n\nEmpowering biomedical discovery with ai agents</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2404.02831v1</guid>
      <dc:creator>Shanghua Gao, Ada Fang, Yepeng Huang, Valentina Giunchiglia, Ayush Noori, Jonathan Richard Schwarz, Yasha Ektefaie, Jovana Kondic, Marinka Zitnik</dc:creator>
      <pubDate>Wed, 03 Apr 2024 16:08:01 GMT</pubDate>
    </item>
    <item>
      <title>Ask Optimal Questions: Aligning Large Language Models with Retriever's Preference in Conversational Search</title>
      <link>http://arxiv.org/abs/2402.11827v1</link>
      <description>Conversational search, unlike single-turn retrieval tasks, requires understanding the current question within a dialogue context. The common approach of rewrite-then-retrieve aims to decontextualize questions to be self-sufficient for off-the-shelf retrievers, but most existing methods produce sub-optimal query rewrites due to the limited ability to incorporate signals from the retrieval results. To overcome this limitation, we present a novel framework RetPO (Retriever's Preference Optimization), which is designed to optimize a language model (LM) for reformulating search queries in line with the preferences of the target retrieval systems. The process begins by prompting a large LM to produce various potential rewrites and then collects retrieval performance for these rewrites as the retrievers' preferences. Through the process, we construct a large-scale dataset called RF collection, containing Retrievers' Feedback on over 410K query rewrites across 12K conversations. Furthermore, we fine-tune a smaller LM using this dataset to align it with the retrievers' preferences as feedback. The resulting model achieves state-of-the-art performance on two recent conversational search benchmarks, significantly outperforming existing baselines, including GPT-3.5.\n\n\nAsk Optimal Questions: Aligning Large Language Models with Retriever's Preference in Conversational Search</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.11827v1</guid>
      <dc:creator>Chanwoong Yoon, Gangwoo Kim, Byeongguk Jeon, Sungdong Kim, Yohan Jo, Jaewoo Kang</dc:creator>
      <pubDate>Mon, 19 Feb 2024 04:41:31 GMT</pubDate>
    </item>
    <item>
      <title>Direct Preference Optimization for Neural Machine Translation with Minimum Bayes Risk Decoding</title>
      <link>http://arxiv.org/abs/2311.08380v2</link>
      <description>Minimum Bayes Risk (MBR) decoding can significantly improve translation performance of Multilingual Large Language Models (MLLMs). However, MBR decoding is computationally expensive. We show how the recently developed Reinforcement Learning technique, Direct Preference Optimization (DPO), can fine-tune MLLMs to get the gains of MBR without any additional computation in inference. Our method uses only a small monolingual fine-tuning set and yields significantly improved performance on multiple NMT test sets compared to MLLMs without DPO.\n\n\nDirect preference optimization for neural machine translation with minimum bayes risk decoding</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2311.08380v2</guid>
      <dc:creator>Guangyu Yang, Jinghong Chen, Weizhe Lin, Bill Byrne</dc:creator>
      <pubDate>Fri, 12 Apr 2024 14:07:38 GMT</pubDate>
    </item>
    <item>
      <title>Improving Generalization of Alignment with Human Preferences through Group Invariant Learning</title>
      <link>http://arxiv.org/abs/2310.11971v3</link>
      <description>The success of AI assistants based on language models (LLMs) hinges crucially on Reinforcement Learning from Human Feedback (RLHF), which enables the generation of responses more aligned with human preferences. As universal AI assistants, there's a growing expectation for them to perform consistently across various domains. However, previous work shows that Reinforcement Learning (RL) often exploits shortcuts to attain high rewards and overlooks challenging samples. This focus on quick reward gains undermines both the stability in training and the model's ability to generalize to new, unseen data. In this work, we propose a novel approach that can learn a consistent policy via RL across various data groups or domains. Given the challenges associated with acquiring group annotations, our method automatically classifies data into different groups, deliberately maximizing performance variance. Then, we optimize the policy to perform well on challenging groups. Lastly, leveraging the established groups, our approach adaptively adjusts the exploration space, allocating more learning capacity to more challenging data and preventing the model from over-optimizing on simpler data. Experimental results indicate that our approach significantly enhances training stability and model generalization.\n\n\nImproving generalization of alignment with human preferences through group invariant learning</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2310.11971v3</guid>
      <dc:creator>Rui Zheng, Wei Shen, Yuan Hua, Wenbin Lai, Shihan Dou, Yuhao Zhou, Zhiheng Xi, Xiao Wang, Haoran Huang, Tao Gui, Qi Zhang, Xuanjing Huang</dc:creator>
      <pubDate>Tue, 26 Dec 2023 02:31:57 GMT</pubDate>
    </item>
    <item>
      <title>Principled Penalty-based Methods for Bilevel Reinforcement Learning and RLHF</title>
      <link>http://arxiv.org/abs/2402.06886v3</link>
      <description>Bilevel optimization has been recently applied to many machine learning tasks. However, their applications have been restricted to the supervised learning setting, where static objective functions with benign structures are considered. But bilevel problems such as incentive design, inverse reinforcement learning (RL), and RL from human feedback (RLHF) are often modeled as dynamic objective functions that go beyond the simple static objective structures, which pose significant challenges of using existing bilevel solutions. To tackle this new class of bilevel problems, we introduce the first principled algorithmic framework for solving bilevel RL problems through the lens of penalty formulation. We provide theoretical studies of the problem landscape and its penalty-based (policy) gradient algorithms. We demonstrate the effectiveness of our algorithms via simulations in the Stackelberg Markov game, RL from human feedback and incentive design.\n\n\nPrincipled penalty-based methods for bilevel reinforcement learning and rlhf</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.06886v3</guid>
      <dc:creator>Han Shen, Zhuoran Yang, Tianyi Chen</dc:creator>
      <pubDate>Sat, 01 Jun 2024 02:41:07 GMT</pubDate>
    </item>
    <item>
      <title>GenAudit: Fixing Factual Errors in Language Model Outputs with Evidence</title>
      <link>http://arxiv.org/abs/2402.12566v2</link>
      <description>LLMs can generate factually incorrect statements even when provided access to reference documents. Such errors can be dangerous in high-stakes applications (e.g., document-grounded QA for healthcare or finance). We present GenAudit -- a tool intended to assist fact-checking LLM responses for document-grounded tasks. GenAudit suggests edits to the LLM response by revising or removing claims that are not supported by the reference document, and also presents evidence from the reference for facts that do appear to have support. We train models to execute these tasks, and design an interactive interface to present suggested edits and evidence to users. Comprehensive evaluation by human raters shows that GenAudit can detect errors in 8 different LLM outputs when summarizing documents from diverse domains. To ensure that most errors are flagged by the system, we propose a method that can increase the error recall while minimizing impact on precision. We release our tool (GenAudit) and fact-checking model for public use.\n\n\nGenAudit: Fixing Factual Errors in Language Model Outputs with Evidence</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.12566v2</guid>
      <dc:creator>Kundan Krishna, Sanjana Ramprasad, Prakhar Gupta, Byron C. Wallace, Zachary C. Lipton, Jeffrey P. Bigham</dc:creator>
      <pubDate>Sat, 16 Mar 2024 21:14:16 GMT</pubDate>
    </item>
    <item>
      <title>Towards Analyzing and Understanding the Limitations of DPO: A Theoretical Perspective</title>
      <link>http://arxiv.org/abs/2404.04626v1</link>
      <description>Direct Preference Optimization (DPO), which derives reward signals directly from pairwise preference data, has shown its effectiveness on aligning Large Language Models (LLMs) with human preferences. Despite its widespread use across various tasks, DPO has been criticized for its sensitivity to the SFT's effectiveness and its hindrance to the learning capacity towards human-preferred responses, leading to less satisfactory performance. To overcome those limitations, the theoretical understanding of DPO are indispensable but still lacking. To this end, we take a step towards theoretically analyzing and understanding the limitations of DPO. Specifically, we provide an analytical framework using the field theory to analyze the optimization process of DPO. By analyzing the gradient vector field of the DPO loss function, we find that the DPO loss function decreases the probability of producing human dispreferred data at a faster rate than it increases the probability of producing preferred data. This provides theoretical insights for understanding the limitations of DPO discovered in the related research experiments, thereby setting the foundation for its improvement.\n\n\nTowards analyzing and understanding the limitations of dpo: A theoretical perspective</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2404.04626v1</guid>
      <dc:creator>Duanyu Feng, Bowen Qin, Chen Huang, Zheng Zhang, Wenqiang Lei</dc:creator>
      <pubDate>Sat, 06 Apr 2024 13:24:37 GMT</pubDate>
    </item>
    <item>
      <title>RLVF: Learning from Verbal Feedback without Overgeneralization</title>
      <link>http://arxiv.org/abs/2402.10893v1</link>
      <description>The diversity of contexts in which large language models (LLMs) are deployed requires the ability to modify or customize default model behaviors to incorporate nuanced requirements and preferences. A convenient interface to specify such model adjustments is high-level verbal feedback, such as &quot;Don't use emojis when drafting emails to my boss.&quot; However, while writing high-level feedback is far simpler than collecting annotations for reinforcement learning from human feedback (RLHF), we find that simply prompting a model with such feedback leads to overgeneralization of the feedback to contexts where it is not relevant. We study the problem of incorporating verbal feedback without such overgeneralization, inspiring a new method Contextualized Critiques with Constrained Preference Optimization (C3PO). C3PO uses a piece of high-level feedback to generate a small synthetic preference dataset specifying how the feedback should (and should not) be applied. It then fine-tunes the model in accordance with the synthetic preference data while minimizing the divergence from the original model for prompts where the feedback does not apply. Our experimental results indicate that our approach effectively applies verbal feedback to relevant scenarios while preserving existing behaviors for other contexts. For both human- and GPT-4-generated high-level feedback, C3PO effectively adheres to the given feedback comparably to in-context baselines while reducing overgeneralization by 30%.\n\n\nRlvf: Learning from verbal feedback without overgeneralization</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.10893v1</guid>
      <dc:creator>Moritz Stephan, Alexander Khazatsky, Eric Mitchell, Annie S Chen, Sheryl Hsu, Archit Sharma, Chelsea Finn</dc:creator>
      <pubDate>Fri, 16 Feb 2024 18:50:24 GMT</pubDate>
    </item>
    <item>
      <title>Safety</title>
      <link>http://arxiv.org/abs/1705.09512v1</link>
      <description>Chapter 17 in High-Luminosity Large Hadron Collider (HL-LHC) : Preliminary Design Report. The Large Hadron Collider (LHC) is one of the largest scientific instruments ever built. Since opening up a new energy frontier for exploration in 2010, it has gathered a global user community of about 7,000 scientists working in fundamental particle physics and the physics of hadronic matter at extreme temperature and density. To sustain and extend its discovery potential, the LHC will need a major upgrade in the 2020s. This will increase its luminosity (rate of collisions) by a factor of five beyond the original design value and the integrated luminosity (total collisions created) by a factor ten. The LHC is already a highly complex and exquisitely optimised machine so this upgrade must be carefully conceived and will require about ten years to implement. The new configuration, known as High Luminosity LHC (HL-LHC), will rely on a number of key innovations that push accelerator technology beyond its present limits. Among these are cutting-edge 11-12 tesla superconducting magnets, compact superconducting cavities for beam rotation with ultra-precise phase control, new technology and physical processes for beam collimation and 300 metre-long high-power superconducting links with negligible energy dissipation. The present document describes the technologies and components that will be used to realise the project and is intended to serve as the basis for the detailed engineering design of HL-LHC.\n\n\nSafety of Multimodal Large Language Models on Images and Text</description>
      <guid isPermaLink="false">http://arxiv.org/abs/1705.09512v1</guid>
      <dc:creator>C. Adorisio, I. Bejar Alonso, J. C. Gascon, T. Otto, S. Roesler</dc:creator>
      <pubDate>Fri, 26 May 2017 10:28:38 GMT</pubDate>
    </item>
    <item>
      <title>CAT: Enhancing Multimodal Large Language Model to Answer Questions in Dynamic Audio-Visual Scenarios</title>
      <link>http://arxiv.org/abs/2403.04640v1</link>
      <description>This paper focuses on the challenge of answering questions in scenarios that are composed of rich and complex dynamic audio-visual components. Although existing Multimodal Large Language Models (MLLMs) can respond to audio-visual content, these responses are sometimes ambiguous and fail to describe specific audio-visual events. To overcome this limitation, we introduce the CAT, which enhances MLLM in three ways: 1) besides straightforwardly bridging audio and video, we design a clue aggregator that aggregates question-related clues in dynamic audio-visual scenarios to enrich the detailed knowledge required for large language models. 2) CAT is trained on a mixed multimodal dataset, allowing direct application in audio-visual scenarios. Notably, we collect an audio-visual joint instruction dataset named AVinstruct, to further enhance the capacity of CAT to model cross-semantic correlations. 3) we propose AI-assisted ambiguity-aware direct preference optimization, a strategy specialized in retraining the model to favor the non-ambiguity response and improve the ability to localize specific audio-visual objects. Extensive experimental results demonstrate that CAT outperforms existing methods on multimodal tasks, especially in Audio-Visual Question Answering (AVQA) tasks. The codes and the collected instructions are released at https://github.com/rikeilong/Bay-CAT.\n\n\nCAT: enhancing multimodal large language model to answer questions in dynamic audio-visual scenarios</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2403.04640v1</guid>
      <dc:creator>Qilang Ye, Zitong Yu, Rui Shao, Xinyu Xie, Philip Torr, Xiaochun Cao</dc:creator>
      <pubDate>Thu, 07 Mar 2024 16:31:02 GMT</pubDate>
    </item>
    <item>
      <title>Learn Your Reference Model for Real Good Alignment</title>
      <link>http://arxiv.org/abs/2404.09656v2</link>
      <description>The complexity of the alignment problem stems from the fact that existing methods are considered unstable. Reinforcement Learning from Human Feedback (RLHF) addresses this issue by minimizing the KL divergence between the trained policy and the initial supervised fine-tuned policy (SFT) to avoid generating out-of-domain samples for the reward model (RM). Recently, many methods have emerged that shift from online to offline optimization, reformulating the RLHF objective and removing the reward model (DPO, IPO, KTO). Despite eliminating the reward model and the challenges it posed, these algorithms are still constrained in terms of closeness of the trained policy to the SFT one. In our paper, we argue that this implicit limitation in the offline optimization methods leads to suboptimal results. To address this issue, we propose a class of new methods called Trust Region (TR-DPO, TR-IPO, TR-KTO), which update the reference policy during training. With this straightforward update approach, we demonstrate the effectiveness of the new paradigm of language model alignment against the classical one on the Anthropic-HH and Reddit TL;DR datasets. Most notably, when automatically comparing TR methods and baselines side by side using pretrained Pythia 6.9B models on the Reddit TL;DR task, the difference in win rates reaches 8.4% for DPO, 14.3% for IPO, and 15% for KTO. Finally, by assessing model response ratings grounded on criteria such as coherence, correctness, helpfulness, and harmlessness, we demonstrate that our proposed methods significantly outperform existing techniques.\n\n\nLearn your reference model for real good alignment</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2404.09656v2</guid>
      <dc:creator>Alexey Gorbatovski, Boris Shaposhnikov, Alexey Malakhov, Nikita Surnachev, Yaroslav Aksenov, Ian Maksimov, Nikita Balagansky, Daniil Gavrilov</dc:creator>
      <pubDate>Tue, 21 May 2024 15:04:12 GMT</pubDate>
    </item>
    <item>
      <title>Online DPO: Online Direct Preference Optimization with Fast-Slow Chasing</title>
      <link>http://arxiv.org/abs/2406.05534v1</link>
      <description>Direct Preference Optimization (DPO) improves the alignment of large language models (LLMs) with human values by training directly on human preference datasets, eliminating the need for reward models. However, due to the presence of cross-domain human preferences, direct continual training can lead to catastrophic forgetting, limiting DPO's performance and efficiency. Inspired by intraspecific competition driving species evolution, we propose a Online Fast-Slow chasing DPO (OFS-DPO) for preference alignment, simulating competition through fast and slow chasing among models to facilitate rapid adaptation. Specifically, we first derive the regret upper bound for online learning, validating our motivation with a min-max optimization pattern. Based on this, we introduce two identical modules using Low-rank Adaptive (LoRA) with different optimization speeds to simulate intraspecific competition, and propose a new regularization term to guide their learning. To further mitigate catastrophic forgetting in cross-domain scenarios, we extend the OFS-DPO with LoRA modules combination strategy, resulting in the Cross domain Online Fast-Slow chasing DPO (COFS-DPO). This method leverages linear combinations of fast modules parameters from different task domains, fully utilizing historical information to achive continual value alignment. Experimental results show that OFS-DPO outperforms DPO in in-domain alignment, while COFS-DPO excels in cross-domain continual learning scenarios.\n\n\nOnline DPO: Online Direct Preference Optimization with Fast-Slow Chasing</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.05534v1</guid>
      <dc:creator>Biqing Qi, Pengfei Li, Fangyuan Li, Junqi Gao, Kaiyan Zhang, Bowen Zhou</dc:creator>
      <pubDate>Sat, 08 Jun 2024 17:30:54 GMT</pubDate>
    </item>
    <item>
      <title>Knowledge-to-SQL: Enhancing SQL Generation with Data Expert LLM</title>
      <link>http://arxiv.org/abs/2402.11517v3</link>
      <description>Generating accurate SQL queries for user questions (text-to-SQL) has been a long-standing challenge since it requires a deep understanding of both the user's question and the corresponding database schema in order to retrieve the desired content accurately. Existing methods rely on the comprehensive capability of large language models (LLMs) to generate the SQL. However, some necessary knowledge is not explicitly included in the database schema and user question or has been learned by LLMs. Thus, the generated SQL of the knowledge-insufficient questions may be inaccurate, negatively influencing the text-to-SQL models' performance and robustness. To address this challenge, we propose the Knowledge-to-SQL framework, which employs tailored Data Expert LLM (DELLM) to provide helpful knowledge for all text-to-SQL models. Specifically, we introduce the detailed implementation of DELLM regarding table reading and the basic fine-tuning process. We further propose a Preference Learning via Database Feedback (PLDBF) strategy, refining the DELLM to generate more helpful knowledge for LLMs. Extensive experiments verify that DELLM can enhance the state-of-the-art approaches for text-to-SQL tasks. The corresponding code of DELLM is released for further research.\n\n\nKnowledge-to-sql: Enhancing sql generation with data expert llm</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.11517v3</guid>
      <dc:creator>Zijin Hong, Zheng Yuan, Hao Chen, Qinggang Zhang, Feiran Huang, Xiao Huang</dc:creator>
      <pubDate>Thu, 06 Jun 2024 13:56:59 GMT</pubDate>
    </item>
    <item>
      <title>More RLHF, More Trust? On The Impact of Human Preference Alignment On Language Model Trustworthiness</title>
      <link>http://arxiv.org/abs/2404.18870v1</link>
      <description>The surge in Large Language Models (LLMs) development has led to improved performance on cognitive tasks as well as an urgent need to align these models with human values in order to safely exploit their power. Despite the effectiveness of preference learning algorithms like Reinforcement Learning From Human Feedback (RLHF) in aligning human preferences, their assumed improvements on model trustworthiness haven't been thoroughly testified. Toward this end, this study investigates how models that have been aligned with general-purpose preference data on helpfulness and harmlessness perform across five trustworthiness verticals: toxicity, stereotypical bias, machine ethics, truthfulness, and privacy. For model alignment, we focus on three widely used RLHF variants: Supervised Finetuning (SFT), Proximal Policy Optimization (PPO), and Direct Preference Optimization (DPO). Through extensive empirical investigations, we discover that the improvement in trustworthiness by RLHF is far from guaranteed, and there exists a complex interplay between preference data, alignment algorithms, and specific trustworthiness aspects. Together, our results underscore the need for more nuanced approaches for model alignment. By shedding light on the intricate dynamics of these components within model alignment, we hope this research will guide the community towards developing language models that are both capable and trustworthy.\n\n\nOn diverse preferences for large language model alignment</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2404.18870v1</guid>
      <dc:creator>Aaron J. Li, Satyapriya Krishna, Himabindu Lakkaraju</dc:creator>
      <pubDate>Mon, 29 Apr 2024 17:00:53 GMT</pubDate>
    </item>
    <item>
      <title>From Complex to Simple: Enhancing Multi-Constraint Complex Instruction Following Ability of Large Language Models</title>
      <link>http://arxiv.org/abs/2404.15846v2</link>
      <description>It is imperative for Large language models (LLMs) to follow instructions with elaborate requirements (i.e. Complex Instructions Following). Yet, it remains under-explored how to enhance the ability of LLMs to follow complex instructions with multiple constraints. To bridge the gap, we initially study what training data is effective in enhancing complex constraints following abilities. We found that training LLMs with instructions containing multiple constraints enhances their understanding of complex instructions, especially those with lower complexity levels. The improvement can even generalize to compositions of out-of-domain constraints. Additionally, we further propose methods addressing how to obtain and utilize the effective training data. Finally, we conduct extensive experiments to prove the effectiveness of our methods in terms of overall performance and training efficiency. We also demonstrate that our methods improve models' ability to follow instructions generally and generalize effectively across out-of-domain, in-domain, and adversarial settings, while maintaining general capabilities.\n\n\nFrom Complex to Simple: Enhancing Multi-Constraint Complex Instruction Following Ability of Large Language Models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2404.15846v2</guid>
      <dc:creator>Qianyu He, Jie Zeng, Qianxi He, Jiaqing Liang, Yanghua Xiao</dc:creator>
      <pubDate>Tue, 18 Jun 2024 13:16:36 GMT</pubDate>
    </item>
    <item>
      <title>Mapping Social Choice Theory to RLHF</title>
      <link>http://arxiv.org/abs/2404.13038v1</link>
      <description>Recent work on the limitations of using reinforcement learning from human feedback (RLHF) to incorporate human preferences into model behavior often raises social choice theory as a reference point. Social choice theory's analysis of settings such as voting mechanisms provides technical infrastructure that can inform how to aggregate human preferences amid disagreement. We analyze the problem settings of social choice and RLHF, identify key differences between them, and discuss how these differences may affect the RLHF interpretation of well-known technical results in social choice.\n\n\nMapping social choice theory to RLHF</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2404.13038v1</guid>
      <dc:creator>Jessica Dai, Eve Fleisig</dc:creator>
      <pubDate>Fri, 19 Apr 2024 17:49:56 GMT</pubDate>
    </item>
    <item>
      <title>EMO: Earth Mover Distance Optimization for Auto-Regressive Language Modeling</title>
      <link>http://arxiv.org/abs/2310.04691v7</link>
      <description>Neural language models are probabilistic models of human text. They are predominantly trained using maximum likelihood estimation (MLE), which is equivalent to minimizing the forward cross-entropy between the empirical data distribution and the model distribution. However, various degeneration phenomena are still widely observed when decoding from the distributions learned by such models. We establish that the forward cross-entropy is suboptimal as a distance metric for aligning human and model distribution due to its (1) recall-prioritization (2) negative diversity ignorance and (3) train-test mismatch. In this paper, we propose Earth Mover Distance Optimization (EMO) for auto-regressive language modeling. EMO capitalizes on the inherent properties of earth mover distance to address the aforementioned challenges. Due to the high complexity of direct computation, we further introduce a feasible upper bound for EMO to ease end-to-end training. Upon extensive evaluation of language models trained using EMO and MLE. We find that EMO demonstrates a consistently better language modeling performance than MLE across domains. Moreover, EMO demonstrates noteworthy enhancements in downstream performance with minimal fine-tuning on merely 25,000 sentences. This highlights the tremendous potential of EMO as a lightweight calibration method for enhancing large-scale pre-trained language models.\n\n\nEMO: Earth Mover Distance Optimization for Auto-Regressive Language Modeling</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2310.04691v7</guid>
      <dc:creator>Siyu Ren, Zhiyong Wu, Kenny Q. Zhu</dc:creator>
      <pubDate>Fri, 02 Feb 2024 04:25:02 GMT</pubDate>
    </item>
    <item>
      <title>Online Merging Optimizers for Boosting Rewards and Mitigating Tax in Alignment</title>
      <link>http://arxiv.org/abs/2405.17931v1</link>
      <description>Effectively aligning Large Language Models (LLMs) with human-centric values while preventing the degradation of abilities acquired through Pre-training and Supervised Fine-tuning (SFT) poses a central challenge in Reinforcement Learning from Human Feedback (RLHF). In this paper, we first discover that interpolating RLHF and SFT model parameters can adjust the trade-off between human preference and basic capabilities, thereby reducing the alignment tax at the cost of alignment reward. Inspired by this, we propose integrating the RL policy and SFT models at each optimization step in RLHF to continuously regulate the training direction, introducing the Online Merging Optimizer. Specifically, we merge gradients with the parameter differences between SFT and pretrained models, effectively steering the gradient towards maximizing rewards in the direction of SFT optimization. We demonstrate that our optimizer works well with different LLM families, such as Qwen and LLaMA, across various model sizes ranging from 1.8B to 8B, various RLHF algorithms like DPO and KTO, and existing model merging methods. It significantly enhances alignment reward while mitigating alignment tax, achieving higher overall performance across 14 benchmarks.\n\n\nOnline merging optimizers for boosting rewards and mitigating tax in alignment</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.17931v1</guid>
      <dc:creator>Keming Lu, Bowen Yu, Fei Huang, Yang Fan, Runji Lin, Chang Zhou</dc:creator>
      <pubDate>Tue, 28 May 2024 07:53:40 GMT</pubDate>
    </item>
    <item>
      <title>Enhancing Large Vision Language Models with Self-Training on Image Comprehension</title>
      <link>http://arxiv.org/abs/2405.19716v1</link>
      <description>Large vision language models (LVLMs) integrate large language models (LLMs) with pre-trained vision encoders, thereby activating the perception capability of the model to understand image inputs for different queries and conduct subsequent reasoning. Improving this capability requires high-quality vision-language data, which is costly and labor-intensive to acquire. Self-training approaches have been effective in single-modal settings to alleviate the need for labeled data by leveraging model's own generation. However, effective self-training remains a challenge regarding the unique visual perception and reasoning capability of LVLMs. To address this, we introduce Self-Training on Image Comprehension (STIC), which emphasizes a self-training approach specifically for image comprehension. First, the model self-constructs a preference dataset for image descriptions using unlabeled images. Preferred responses are generated through a step-by-step prompt, while dis-preferred responses are generated from either corrupted images or misleading prompts. To further self-improve reasoning on the extracted visual information, we let the model reuse a small portion of existing instruction-tuning data and append its self-generated image descriptions to the prompts. We validate the effectiveness of STIC across seven different benchmarks, demonstrating substantial performance gains of 4.0% on average while using 70% less supervised fine-tuning data than the current method. Further studies investigate various components of STIC and highlight its potential to leverage vast quantities of unlabeled images for self-training. Code and data are made publicly available.\n\n\nEnhancing Large Vision Language Models with Self-Training on Image Comprehension</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.19716v1</guid>
      <dc:creator>Yihe Deng, Pan Lu, Fan Yin, Ziniu Hu, Sheng Shen, James Zou, Kai-Wei Chang, Wei Wang</dc:creator>
      <pubDate>Thu, 30 May 2024 05:53:49 GMT</pubDate>
    </item>
    <item>
      <title>The JCMT BISTRO Survey: An 850/450$μ$m Polarization Study of NGC 2071IR in OrionB</title>
      <link>http://arxiv.org/abs/2109.13543v1</link>
      <description>We present the results of simultaneous 450 $\mu$m and 850 $\mu$m polarization observations toward the massive star forming region NGC 2071IR, a target of the BISTRO (B-fields in Star-Forming Region Observations) Survey, using the POL-2 polarimeter and SCUBA-2 camera mounted on the James Clerk Maxwell Telescope. We find a pinched magnetic field morphology in the central dense core region, which could be due to a rotating toroidal disk-like structure and a bipolar outflow originating from the central young stellar object, IRS 3. Using the modified Davis-Chandrasekhar-Fermi method, we obtain a plane-of-sky magnetic field strength of 563$\pm$421 $\mu$G in the central $\sim$0.12 pc region from 850 $\mu$m polarization data. The corresponding magnetic energy density of 2.04$\times$10$^{-8}$ erg cm$^{-3}$ is comparable to the turbulent and gravitational energy densities in the region. We find that the magnetic field direction is very well aligned with the whole of the IRS 3 bipolar outflow structure. We find that the median value of polarization fractions, 3.0 \%, at 450 $\mu$m in the central 3 arcminute region, which is larger than the median value of 1.2 \% at 850 $\mu$m. The trend could be due to the better alignment of warmer dust in the strong radiation environment. We also find that polarization fractions decrease with intensity at both wavelengths, with slopes, determined by fitting a Rician noise model, of $0.59 \pm 0.03$ at 450 $\mu$m and $0.36 \pm 0.04$ at 850 $\mu$m, respectively. We think that the shallow slope at 850 $\mu$m is due to grain alignment at the center being assisted by strong radiation from the central young stellar objects.\n\n\nOrion-14b: Open-source multilingual large language models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2109.13543v1</guid>
      <dc:creator>A-Ran Lyo, Jongsoo Kim, Sarah Sadavoy, Doug Johnstone, David Berry, Kate Pattle, Woojin Kwon, Pierre Bastien, Takashi Onaka, James Di Francesco, Ji-Hyun Kang, Ray Furuya, Charles L. H. Hull, Motohide Tamura, Patrick M. Koch, Derek Ward-Thompson, Tetsuo Hasegawa, Thiem Hoang, Doris Arzoumanian, Chang Won Lee, Chin-Fei Lee, Do-Young Byun, Florian Kirchschlager, Yasuo Doi, Kee-Tae Kim, Jihye Hwang, Pham Ngoc Diep, Lapo Fanciullo, Sang-Sung Lee, Geumsook Park, Hyunju Yoo, Eun Jung Chung, Anthony Whitworth, Steve Mairs, Archana Soam, Tie Liu, Xindi Tang, Simon Coudé, Philippe André, Tyler L. Bourke, Huei-Ru Vivien Chen, Zhiwei Chen, Wen Ping Chen, Mike Chen, Tao-Chung Ching, Jungyeon Cho, Minho Choi, Yunhee Choi, Antonio Chrysostomou, Sophia Dai, C. Darren Dowell, Hao-Yuan Duan, Yan Duan, David Eden, Chakali Eswaraiah, Stewart Eyres, Jason Fiege, Laura M. Fisse, Erica Franzmann, Per Friberg, Rachel Friesen, Gary Fuller, Tim Gledhill, Sarah Graves, Jane Greaves, Matt Griffin, Qilao Gu, Ilseung Han, Jannifer Hatchell, Saeko Hayashi, Martin Houde, Tsuyoshi Inoue, Shu-ichiro Inutsuka, Kazunari Iwasaki, Il-Gyo Jeong, Miju Kang, Akimasa Kataoka, Koji Kawabata, Francisca Kemper, Gwanjeong Kim, Mi-Ryang Kim, Shinyoung Kim, Kyoung Hee Kim, Jason Kirk, Masato I. N. Kobayashi, Vera Könyves, Takayoshi Kusune, Jungmi Kwon, Kevin Lacaille, Shih-Ping Lai, Chi-Yan Law, Jeong-Eun Lee, Yong-Hee Lee, Hyeseung Lee, Dalei Li, Di Li, Hua-Bai Li, Hong-Li Liu, Junhao Liu, Sheng-Yuan Liu, Xing Lu, Masafumi Matsumura, Brenda Matthews, Gerald Moriarty-Schieven, Tetsuya Nagata, Fumitaka Nakamura, Hiroyuki Nakanishi, Nguyen Bich Ngoc, Nagayoshi Ohashi, Harriet Parsons, Nicolas Peretto, Felix Priestley, Tae-soo Pyo, Lei Qian, Keping Qiu, Ramprasad Rao, Jonathan Rawlings, Mark G. Rawlings, Brendan Retter, John Richer, Andrew Rigby, Hiro Saito, Giorgio Savini, Anna Scaife, Masumichi Seta, Yoshito Shimajiri, Hiroko Shinnaga, Mehrnoosh Tahani, Ya-Wen Tang, Kohji Tomisaka, Le Ngoc Tram, Yusuke Tsukamoto, Serena Viti, Jia-Wei Wang, Hongchi Wang, Jinjin Xie, Hsi-Wei Yen, Jinghua Yuan, Hyeong-Sik Yun, Tetsuya Zenko, Guoyin Zhang, Chuan-Peng Zhang, Yapeng Zhang, Jianjun Zhou, Lei Zhu, Ilse de Looze</dc:creator>
      <pubDate>Tue, 28 Sep 2021 07:50:29 GMT</pubDate>
    </item>
    <item>
      <title>Improving Factual Consistency of Text Summarization by Adversarially Decoupling Comprehension and Embellishment Abilities of LLMs</title>
      <link>http://arxiv.org/abs/2310.19347v3</link>
      <description>Despite the recent progress in text summarization made by large language models (LLMs), they often generate summaries that are factually inconsistent with original articles, known as &quot;hallucinations&quot; in text generation. Unlike previous small models (e.g., BART, T5), current LLMs make fewer silly mistakes but more sophisticated ones, such as imposing cause and effect, adding false details, overgeneralizing, etc. These hallucinations are challenging to detect through traditional methods, which poses great challenges for improving the factual consistency of text summarization. In this paper, we propose an adversarially DEcoupling method to disentangle the Comprehension and EmbellishmeNT abilities of LLMs (DECENT). Furthermore, we adopt a probing-based efficient training to cover the shortage of sensitivity for true and false in the training process of LLMs. In this way, LLMs are less confused about embellishing and understanding; thus, they can execute the instructions more accurately and have enhanced abilities to distinguish hallucinations. Experimental results show that DECENT significantly improves the reliability of text summarization based on LLMs.\n\n\nImproving factual consistency of text summarization by adversarially decoupling comprehension and embellishment abilities of llms</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2310.19347v3</guid>
      <dc:creator>Huawen Feng, Yan Fan, Xiong Liu, Ting-En Lin, Zekun Yao, Yuchuan Wu, Fei Huang, Yongbin Li, Qianli Ma</dc:creator>
      <pubDate>Tue, 14 Nov 2023 06:55:56 GMT</pubDate>
    </item>
    <item>
      <title>Direct Large Language Model Alignment Through Self-Rewarding Contrastive Prompt Distillation</title>
      <link>http://arxiv.org/abs/2402.11907v1</link>
      <description>Aligning large language models (LLMs) with human expectations without human-annotated preference data is an important problem. In this paper, we propose a method to evaluate the response preference by using the output probabilities of response pairs under contrastive prompt pairs, which could achieve better performance on LLaMA2-7B and LLaMA2-13B compared to RLAIF. Based on this, we propose an automatic alignment method, Direct Large Model Alignment (DLMA). First, we use contrastive prompt pairs to automatically generate preference data. Then, we continue to evaluate the generated preference data using contrastive prompt pairs and calculate a self-rewarding score. Finally, we use the DPO algorithm to effectively align LLMs by combining this self-rewarding score. In the experimental stage, our DLMA method could surpass the \texttt{RLHF} method without relying on human-annotated preference data.\n\n\nDirect large language model alignment through self-rewarding contrastive prompt distillation</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.11907v1</guid>
      <dc:creator>Aiwei Liu, Haoping Bai, Zhiyun Lu, Xiang Kong, Simon Wang, Jiulong Shan, Meng Cao, Lijie Wen</dc:creator>
      <pubDate>Mon, 19 Feb 2024 07:46:40 GMT</pubDate>
    </item>
    <item>
      <title>CodeUltraFeedback: An LLM-as-a-Judge Dataset for Aligning Large Language Models to Coding Preferences</title>
      <link>http://arxiv.org/abs/2403.09032v1</link>
      <description>Evaluating the alignment of large language models (LLMs) with user-defined coding preferences is a challenging endeavour that requires assessing intricate textual LLMs' outputs. By relying on automated metrics and static analysis tools, existing benchmarks fail to assess nuances in user instructions and LLM outputs, highlighting the need for large-scale datasets and benchmarks for LLM preference alignment. In this paper, we introduce CodeUltraFeedback, a preference dataset of 10,000 complex instructions to tune and align LLMs to coding preferences through AI feedback. We generate responses to the instructions using a pool of 14 diverse LLMs, which we then annotate according to their alignment with five coding preferences using the LLM-as-a-Judge approach with GPT-3.5, producing both numerical and textual feedback. We also present CODAL-Bench, a benchmark for assessing LLM alignment with these coding preferences. Our results show that CodeLlama-7B-Instruct, aligned through reinforcement learning from AI feedback (RLAIF) with direct preference optimization (DPO) using CodeUltraFeedback's AI feedback data, outperforms 34B LLMs on CODAL-Bench, validating the utility of CodeUltraFeedback for preference tuning. Furthermore, we show our DPO-aligned CodeLlama model improves functional correctness on HumanEval+ compared to the unaligned base model. Therefore, our contributions bridge the gap in preference tuning of LLMs for code and set the stage for further advancements in model alignment and RLAIF for code intelligence. Our code and data are available at https://github.com/martin-wey/CodeUltraFeedback.\n\n\nCodeUltraFeedback: An LLM-as-a-Judge Dataset for Aligning Large Language Models to Coding Preferences</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2403.09032v1</guid>
      <dc:creator>Martin Weyssow, Aton Kamanda, Houari Sahraoui</dc:creator>
      <pubDate>Thu, 14 Mar 2024 01:51:35 GMT</pubDate>
    </item>
    <item>
      <title>Provably Robust DPO: Aligning Language Models with Noisy Feedback</title>
      <link>http://arxiv.org/abs/2403.00409v2</link>
      <description>Learning from preference-based feedback has recently gained traction as a promising approach to align language models with human interests. While these aligned generative models have demonstrated impressive capabilities across various tasks, their dependence on high-quality human preference data poses a bottleneck in practical applications. Specifically, noisy (incorrect and ambiguous) preference pairs in the dataset might restrict the language models from capturing human intent accurately. While practitioners have recently proposed heuristics to mitigate the effect of noisy preferences, a complete theoretical understanding of their workings remain elusive.   In this work, we aim to bridge this gap by by introducing a general framework for policy optimization in the presence of random preference flips. We focus on the direct preference optimization (DPO) algorithm in particular since it assumes that preferences adhere to the Bradley-Terry-Luce (BTL) model, raising concerns about the impact of noisy data on the learned policy. We design a novel loss function, which de-bias the effect of noise on average, making a policy trained by minimizing that loss robust to the noise. Under log-linear parameterization of the policy class and assuming good feature coverage of the SFT policy, we prove that the sub-optimality gap of the proposed robust DPO (rDPO) policy compared to the optimal policy is of the order $O(\frac{1}{1-2\epsilon}\sqrt{\frac{d}{n}})$, where $\epsilon &lt; 1/2$ is flip rate of labels, $d$ is policy parameter dimension and $n$ is size of dataset. Our experiments on IMDb sentiment generation and Anthropic's helpful-harmless dataset show that rDPO is robust to noise in preference labels compared to vanilla DPO and other heuristics proposed by practitioners.\n\n\nProvably robust dpo: Aligning language models with noisy feedback</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2403.00409v2</guid>
      <dc:creator>Sayak Ray Chowdhury, Anush Kini, Nagarajan Natarajan</dc:creator>
      <pubDate>Fri, 12 Apr 2024 01:09:37 GMT</pubDate>
    </item>
    <item>
      <title>OLAPH: Improving Factuality in Biomedical Long-form Question Answering</title>
      <link>http://arxiv.org/abs/2405.12701v1</link>
      <description>In the medical domain, numerous scenarios necessitate the long-form generation ability of large language models (LLMs). Specifically, when addressing patients' questions, it is essential that the model's response conveys factual claims, highlighting the need for an automated method to evaluate those claims. Thus, we introduce MedLFQA, a benchmark dataset reconstructed using long-form question-answering datasets related to the biomedical domain. We use MedLFQA to facilitate the automatic evaluations of factuality. We also propose OLAPH, a simple and novel framework that enables the improvement of factuality through automatic evaluations. The OLAPH framework iteratively trains LLMs to mitigate hallucinations using sampling predictions and preference optimization. In other words, we iteratively set the highest-scoring response as a preferred response derived from sampling predictions and train LLMs to align with the preferred response that improves factuality. We highlight that, even on evaluation metrics not used during training, LLMs trained with our OLAPH framework demonstrate significant performance improvement in factuality. Our findings reveal that a 7B LLM trained with our OLAPH framework can provide long answers comparable to the medical experts' answers in terms of factuality. We believe that our work could shed light on gauging the long-text generation ability of LLMs in the medical domain. Our code and datasets are available at https://github.com/dmis-lab/OLAPH}{https://github.com/dmis-lab/OLAPH.\n\n\nOLAPH: Improving Factuality in Biomedical Long-form Question Answering</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.12701v1</guid>
      <dc:creator>Minbyul Jeong, Hyeon Hwang, Chanwoong Yoon, Taewhoo Lee, Jaewoo Kang</dc:creator>
      <pubDate>Tue, 21 May 2024 11:50:16 GMT</pubDate>
    </item>
    <item>
      <title>Red Teaming Game: A Game-Theoretic Framework for Red Teaming Language Models</title>
      <link>http://arxiv.org/abs/2310.00322v4</link>
      <description>Deployable Large Language Models (LLMs) must conform to the criterion of helpfulness and harmlessness, thereby achieving consistency between LLMs outputs and human values. Red-teaming techniques constitute a critical way towards this criterion. Existing work rely solely on manual red team designs and heuristic adversarial prompts for vulnerability detection and optimization. These approaches lack rigorous mathematical formulation, thus limiting the exploration of diverse attack strategy within quantifiable measure and optimization of LLMs under convergence guarantees. In this paper, we present Red-teaming Game (RTG), a general game-theoretic framework without manual annotation. RTG is designed for analyzing the multi-turn attack and defense interactions between Red-team language Models (RLMs) and Blue-team Language Model (BLM). Within the RTG, we propose Gamified Red-teaming Solver (GRTS) with diversity measure of the semantic space. GRTS is an automated red teaming technique to solve RTG towards Nash equilibrium through meta-game analysis, which corresponds to the theoretically guaranteed optimization direction of both RLMs and BLM. Empirical results in multi-turn attacks with RLMs show that GRTS autonomously discovered diverse attack strategies and effectively improved security of LLMs, outperforming existing heuristic red-team designs. Overall, RTG has established a foundational framework for red teaming tasks and constructed a new scalable oversight technique for alignment.\n\n\nGradient-based language model red teaming</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2310.00322v4</guid>
      <dc:creator>Chengdong Ma, Ziran Yang, Minquan Gao, Hai Ci, Jun Gao, Xuehai Pan, Yaodong Yang</dc:creator>
      <pubDate>Sat, 06 Apr 2024 16:48:20 GMT</pubDate>
    </item>
    <item>
      <title>sDPO: Don't Use Your Data All at Once</title>
      <link>http://arxiv.org/abs/2403.19270v1</link>
      <description>As development of large language models (LLM) progresses, aligning them with human preferences has become increasingly important. We propose stepwise DPO (sDPO), an extension of the recently popularized direct preference optimization (DPO) for alignment tuning. This approach involves dividing the available preference datasets and utilizing them in a stepwise manner, rather than employing it all at once. We demonstrate that this method facilitates the use of more precisely aligned reference models within the DPO training framework. Furthermore, sDPO trains the final model to be more performant, even outperforming other popular LLMs with more parameters.\n\n\nsDPO: Don't Use Your Data All at Once</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2403.19270v1</guid>
      <dc:creator>Dahyun Kim, Yungi Kim, Wonho Song, Hyeonwoo Kim, Yunsu Kim, Sanghoon Kim, Chanjun Park</dc:creator>
      <pubDate>Thu, 28 Mar 2024 09:56:04 GMT</pubDate>
    </item>
    <item>
      <title>Impact of Preference Noise on the Alignment Performance of Generative Language Models</title>
      <link>http://arxiv.org/abs/2404.09824v1</link>
      <description>A key requirement in developing Generative Language Models (GLMs) is to have their values aligned with human values. Preference-based alignment is a widely used paradigm for this purpose, in which preferences over generation pairs are first elicited from human annotators or AI systems, and then fed into some alignment techniques, e.g., Direct Preference Optimization. However, a substantial percent (20 - 40%) of the preference pairs used in GLM alignment are noisy, and it remains unclear how the noise affects the alignment performance and how to mitigate its negative impact. In this paper, we propose a framework to inject desirable amounts and types of noise to the preferences, and systematically study the impact of preference noise on the alignment performance in two tasks (summarization and dialogue generation). We find that the alignment performance can be highly sensitive to the noise rates in the preference data: e.g., a 10 percentage points (pp) increase of the noise rate can lead to 30 pp drop in the alignment performance (in win rate). To mitigate the impact of noise, confidence-based data filtering shows significant benefit when certain types of noise are present. We hope our work can help the community better understand and mitigate the impact of preference noise in GLM alignment.\n\n\nImpact of preference noise on the alignment performance of generative language models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2404.09824v1</guid>
      <dc:creator>Yang Gao, Dana Alon, Donald Metzler</dc:creator>
      <pubDate>Mon, 15 Apr 2024 14:21:53 GMT</pubDate>
    </item>
    <item>
      <title>Recent Technical Improvements to the HAYSTAC Experiment</title>
      <link>http://arxiv.org/abs/1706.03676v2</link>
      <description>We report here several technical improvements to the HAYSTAC (Haloscope at Yale Sensitive To Axion Cold dark matter) that have improved operational efficiency, sensitivity, and stability.\n\n\nH2o-danube-1.8 b technical report</description>
      <guid isPermaLink="false">http://arxiv.org/abs/1706.03676v2</guid>
      <dc:creator>L. Zhong, B. M. Brubaker, S. B. Cahn, S. K. Lamoreaux</dc:creator>
      <pubDate>Mon, 07 Aug 2017 13:13:09 GMT</pubDate>
    </item>
    <item>
      <title>Diffusion Model Alignment Using Direct Preference Optimization</title>
      <link>http://arxiv.org/abs/2311.12908v1</link>
      <description>Large language models (LLMs) are fine-tuned using human comparison data with Reinforcement Learning from Human Feedback (RLHF) methods to make them better aligned with users' preferences. In contrast to LLMs, human preference learning has not been widely explored in text-to-image diffusion models; the best existing approach is to fine-tune a pretrained model using carefully curated high quality images and captions to improve visual appeal and text alignment. We propose Diffusion-DPO, a method to align diffusion models to human preferences by directly optimizing on human comparison data. Diffusion-DPO is adapted from the recently developed Direct Preference Optimization (DPO), a simpler alternative to RLHF which directly optimizes a policy that best satisfies human preferences under a classification objective. We re-formulate DPO to account for a diffusion model notion of likelihood, utilizing the evidence lower bound to derive a differentiable objective. Using the Pick-a-Pic dataset of 851K crowdsourced pairwise preferences, we fine-tune the base model of the state-of-the-art Stable Diffusion XL (SDXL)-1.0 model with Diffusion-DPO. Our fine-tuned base model significantly outperforms both base SDXL-1.0 and the larger SDXL-1.0 model consisting of an additional refinement model in human evaluation, improving visual appeal and prompt alignment. We also develop a variant that uses AI feedback and has comparable performance to training on human preferences, opening the door for scaling of diffusion model alignment methods.\n\n\nAligning diffusion models by optimizing human utility</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2311.12908v1</guid>
      <dc:creator>Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming Xiong, Shafiq Joty, Nikhil Naik</dc:creator>
      <pubDate>Tue, 21 Nov 2023 15:24:05 GMT</pubDate>
    </item>
    <item>
      <title>HFT: Half Fine-Tuning for Large Language Models</title>
      <link>http://arxiv.org/abs/2404.18466v1</link>
      <description>Large language models (LLMs) with one or more fine-tuning phases have become a necessary step to unlock various capabilities, enabling LLMs to follow natural language instructions or align with human preferences. However, it carries the risk of catastrophic forgetting during sequential training, the parametric knowledge or the ability learned in previous stages may be overwhelmed by incoming training data. In this paper, we find that by regularly resetting partial parameters, LLMs can restore some of the original knowledge. Inspired by this, we introduce Half Fine-Tuning (HFT) for LLMs, as a substitute for full fine-tuning (FFT), to mitigate the forgetting issues, where half of the parameters are selected to learn new tasks while the other half are frozen to remain previous knowledge. We provide a feasibility analysis from the perspective of optimization and interpret the parameter selection operation as a regularization term. Without changing the model architecture, HFT could be seamlessly integrated into existing fine-tuning frameworks. Extensive experiments and analysis on supervised fine-tuning, direct preference optimization, and continual learning consistently demonstrate the effectiveness, robustness, and efficiency of HFT. Compared with FFT, HFT not only significantly alleviates the forgetting problem, but also achieves the best performance in a series of downstream benchmarks, with an approximately 30% reduction in training time.\n\n\nHFT: Half Fine-Tuning for Large Language Models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2404.18466v1</guid>
      <dc:creator>Tingfeng Hui, Zhenyu Zhang, Shuohuan Wang, Weiran Xu, Yu Sun, Hua Wu</dc:creator>
      <pubDate>Mon, 29 Apr 2024 07:07:58 GMT</pubDate>
    </item>
    <item>
      <title>Explaining Bayesian Optimization by Shapley Values Facilitates Human-AI Collaboration</title>
      <link>http://arxiv.org/abs/2403.04629v2</link>
      <description>Bayesian optimization (BO) with Gaussian processes (GP) has become an indispensable algorithm for black box optimization problems. Not without a dash of irony, BO is often considered a black box itself, lacking ways to provide reasons as to why certain parameters are proposed to be evaluated. This is particularly relevant in human-in-the-loop applications of BO, such as in robotics. We address this issue by proposing ShapleyBO, a framework for interpreting BO's proposals by game-theoretic Shapley values.They quantify each parameter's contribution to BO's acquisition function. Exploiting the linearity of Shapley values, we are further able to identify how strongly each parameter drives BO's exploration and exploitation for additive acquisition functions like the confidence bound. We also show that ShapleyBO can disentangle the contributions to exploration into those that explore aleatoric and epistemic uncertainty. Moreover, our method gives rise to a ShapleyBO-assisted human machine interface (HMI), allowing users to interfere with BO in case proposals do not align with human reasoning. We demonstrate this HMI's benefits for the use case of personalizing wearable robotic devices (assistive back exosuits) by human-in-the-loop BO. Results suggest human-BO teams with access to ShapleyBO can achieve lower regret than teams without.\n\n\nExplaining Bayesian Optimization by Shapley Values Facilitates Human-AI Collaboration</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2403.04629v2</guid>
      <dc:creator>Julian Rodemann, Federico Croppi, Philipp Arens, Yusuf Sale, Julia Herbinger, Bernd Bischl, Eyke Hüllermeier, Thomas Augustin, Conor J. Walsh, Giuseppe Casalicchio</dc:creator>
      <pubDate>Fri, 08 Mar 2024 07:52:32 GMT</pubDate>
    </item>
    <item>
      <title>RS-DPO: A Hybrid Rejection Sampling and Direct Preference Optimization Method for Alignment of Large Language Models</title>
      <link>http://arxiv.org/abs/2402.10038v2</link>
      <description>Reinforcement learning from human feedback (RLHF) has been extensively employed to align large language models with user intent. However, proximal policy optimization (PPO) based RLHF is occasionally unstable requiring significant hyperparameter finetuning, and computationally expensive to maximize the estimated reward during alignment. Recently, direct preference optimization (DPO) is proposed to address those challenges. However, DPO relies on contrastive responses generated from human annotator and alternative LLM, instead of the policy model, limiting the effectiveness of the RLHF. In this paper, we addresses both challenges by systematically combining rejection sampling (RS) and DPO. Our proposed method, RS-DPO, initiates with the development of a supervised fine-tuned policy model (SFT). A varied set of k responses per prompt are sampled directly from the SFT model. RS-DPO identifies pairs of contrastive samples based on their reward distribution. Finally, we apply DPO with the contrastive samples to align the model to human preference. Our experiments indicate that our proposed method effectively fine-tunes LLMs with limited resource environments, leading to improved alignment with user intent. Furthermore, it outperforms existing methods, including RS, PPO, and DPO.\n\n\nRs-dpo: A hybrid rejection sampling and direct preference optimization method for alignment of large language models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.10038v2</guid>
      <dc:creator>Saeed Khaki, JinJin Li, Lan Ma, Liu Yang, Prathap Ramachandra</dc:creator>
      <pubDate>Sat, 30 Mar 2024 16:10:47 GMT</pubDate>
    </item>
    <item>
      <title>Tango 2: Aligning Diffusion-based Text-to-Audio Generations through Direct Preference Optimization</title>
      <link>http://arxiv.org/abs/2404.09956v4</link>
      <description>Generative multimodal content is increasingly prevalent in much of the content creation arena, as it has the potential to allow artists and media personnel to create pre-production mockups by quickly bringing their ideas to life. The generation of audio from text prompts is an important aspect of such processes in the music and film industry. Many of the recent diffusion-based text-to-audio models focus on training increasingly sophisticated diffusion models on a large set of datasets of prompt-audio pairs. These models do not explicitly focus on the presence of concepts or events and their temporal ordering in the output audio with respect to the input prompt. Our hypothesis is focusing on how these aspects of audio generation could improve audio generation performance in the presence of limited data. As such, in this work, using an existing text-to-audio model Tango, we synthetically create a preference dataset where each prompt has a winner audio output and some loser audio outputs for the diffusion model to learn from. The loser outputs, in theory, have some concepts from the prompt missing or in an incorrect order. We fine-tune the publicly available Tango text-to-audio model using diffusion-DPO (direct preference optimization) loss on our preference dataset and show that it leads to improved audio output over Tango and AudioLDM2, in terms of both automatic- and manual-evaluation metrics.\n\n\nTango 2: Aligning diffusion-based text-to-audio generations through direct preference optimization</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2404.09956v4</guid>
      <dc:creator>Navonil Majumder, Chia-Yu Hung, Deepanway Ghosal, Wei-Ning Hsu, Rada Mihalcea, Soujanya Poria</dc:creator>
      <pubDate>Wed, 17 Jul 2024 16:17:50 GMT</pubDate>
    </item>
    <item>
      <title>LLMs Meet Multimodal Generation and Editing: A Survey</title>
      <link>http://arxiv.org/abs/2405.19334v2</link>
      <description>With the recent advancement in large language models (LLMs), there is a growing interest in combining LLMs with multimodal learning. Previous surveys of multimodal large language models (MLLMs) mainly focus on multimodal understanding. This survey elaborates on multimodal generation and editing across various domains, comprising image, video, 3D, and audio. Specifically, we summarize the notable advancements with milestone works in these fields and categorize these studies into LLM-based and CLIP/T5-based methods. Then, we summarize the various roles of LLMs in multimodal generation and exhaustively investigate the critical technical components behind these methods and the multimodal datasets utilized in these studies. Additionally, we dig into tool-augmented multimodal agents that can leverage existing generative models for human-computer interaction. Lastly, we discuss the advancements in the generative AI safety field, investigate emerging applications, and discuss future prospects. Our work provides a systematic and insightful overview of multimodal generation and processing, which is expected to advance the development of Artificial Intelligence for Generative Content (AIGC) and world models. A curated list of all related papers can be found at https://github.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation\n\n\nLLMs Meet Multimodal Generation and Editing: A Survey</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.19334v2</guid>
      <dc:creator>Yingqing He, Zhaoyang Liu, Jingye Chen, Zeyue Tian, Hongyu Liu, Xiaowei Chi, Runtao Liu, Ruibin Yuan, Yazhou Xing, Wenhai Wang, Jifeng Dai, Yong Zhang, Wei Xue, Qifeng Liu, Yike Guo, Qifeng Chen</dc:creator>
      <pubDate>Sun, 09 Jun 2024 11:34:12 GMT</pubDate>
    </item>
    <item>
      <title>PediatricsGPT: Large Language Models as Chinese Medical Assistants for Pediatric Applications</title>
      <link>http://arxiv.org/abs/2405.19266v2</link>
      <description>Developing intelligent pediatric consultation systems offers promising prospects for improving diagnostic efficiency, especially in China, where healthcare resources are scarce. Despite recent advances in Large Language Models (LLMs) for Chinese medicine, their performance is sub-optimal in pediatric applications due to inadequate instruction data and vulnerable training procedures. To address the above issues, this paper builds PedCorpus, a high-quality dataset of over 300,000 multi-task instructions from pediatric textbooks, guidelines, and knowledge graph resources to fulfil diverse diagnostic demands. Upon well-designed PedCorpus, we propose PediatricsGPT, the first Chinese pediatric LLM assistant built on a systematic and robust training pipeline. In the continuous pre-training phase, we introduce a hybrid instruction pre-training mechanism to mitigate the internal-injected knowledge inconsistency of LLMs for medical domain adaptation. Immediately, the full-parameter Supervised Fine-Tuning (SFT) is utilized to incorporate the general medical knowledge schema into the models. After that, we devise a direct following preference optimization to enhance the generation of pediatrician-like humanistic responses. In the parameter-efficient secondary SFT phase, a mixture of universal-specific experts strategy is presented to resolve the competency conflict between medical generalist and pediatric expertise mastery. Extensive results based on the metrics, GPT-4, and doctor evaluations on distinct doctor downstream tasks show that PediatricsGPT consistently outperforms previous Chinese medical LLMs. Our model and dataset will be open-source for community development.\n\n\nPediatricsGPT: Large Language Models as Chinese Medical Assistants for Pediatric Applications</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.19266v2</guid>
      <dc:creator>Dingkang Yang, Jinjie Wei, Dongling Xiao, Shunli Wang, Tong Wu, Gang Li, Mingcheng Li, Shuaibing Wang, Jiawei Chen, Yue Jiang, Qingyao Xu, Ke Li, Peng Zhai, Lihua Zhang</dc:creator>
      <pubDate>Mon, 03 Jun 2024 15:27:10 GMT</pubDate>
    </item>
    <item>
      <title>Pedagogical Alignment of Large Language Models</title>
      <link>http://arxiv.org/abs/2402.05000v2</link>
      <description>In this paper, we introduce the novel concept of pedagogically aligned Large Language Models (LLMs) that signifies a transformative shift in the application of LLMs within educational contexts. Rather than providing direct responses to user queries, pedagogically-aligned LLMs function as scaffolding tools, breaking complex problems into manageable subproblems and guiding students towards the final answer through constructive feedback and hints. The objective is to equip learners with problem-solving strategies that deepen their understanding and internalization of the subject matter. Previous research in this field has primarily applied the supervised finetuning approach without framing the objective as an alignment problem, hence not employing reinforcement learning through human feedback (RLHF) methods. This study reinterprets the narrative by viewing the task through the lens of alignment and demonstrates how RLHF methods emerge naturally as a superior alternative for aligning LLM behaviour. Building on this perspective, we propose a novel approach for constructing a reward dataset specifically designed for the pedagogical alignment of LLMs. We apply three state-of-the-art RLHF algorithms and find that they outperform SFT significantly. Our qualitative analyses across model differences and hyperparameter sensitivity further validate the superiority of RLHF over SFT. Also, our study sheds light on the potential of online feedback for enhancing the performance of pedagogically-aligned LLMs, thus providing valuable insights for the advancement of these models in educational settings.\n\n\nPedagogical alignment of large language models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.05000v2</guid>
      <dc:creator>Shashank Sonkar, Kangqi Ni, Sapana Chaudhary, Richard G. Baraniuk</dc:creator>
      <pubDate>Fri, 12 Jul 2024 20:02:03 GMT</pubDate>
    </item>
    <item>
      <title>Eliciting Human Preferences with Language Models</title>
      <link>http://arxiv.org/abs/2310.11589v1</link>
      <description>Language models (LMs) can be directed to perform target tasks by using labeled examples or natural language prompts. But selecting examples or writing prompts for can be challenging--especially in tasks that involve unusual edge cases, demand precise articulation of nebulous preferences, or require an accurate mental model of LM behavior. We propose to use *LMs themselves* to guide the task specification process. In this paper, we introduce **Generative Active Task Elicitation (GATE)**: a learning framework in which models elicit and infer intended behavior through free-form, language-based interaction with users. We study GATE in three domains: email validation, content recommendation, and moral reasoning. In preregistered experiments, we show that LMs prompted to perform GATE (e.g., by generating open-ended questions or synthesizing informative edge cases) elicit responses that are often more informative than user-written prompts or labels. Users report that interactive task elicitation requires less effort than prompting or example labeling and surfaces novel considerations not initially anticipated by users. Our findings suggest that LM-driven elicitation can be a powerful tool for aligning models to complex human preferences and values.\n\n\nSelf-Exploring Language Models: Active Preference Elicitation for Online Alignment</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2310.11589v1</guid>
      <dc:creator>Belinda Z. Li, Alex Tamkin, Noah Goodman, Jacob Andreas</dc:creator>
      <pubDate>Tue, 17 Oct 2023 21:11:21 GMT</pubDate>
    </item>
    <item>
      <title>Transforming and Combining Rewards for Aligning Large Language Models</title>
      <link>http://arxiv.org/abs/2402.00742v1</link>
      <description>A common approach for aligning language models to human preferences is to first learn a reward model from preference data, and then use this reward model to update the language model. We study two closely related problems that arise in this approach. First, any monotone transformation of the reward model preserves preference ranking; is there a choice that is ``better'' than others? Second, we often wish to align language models to multiple properties: how should we combine multiple reward models? Using a probabilistic interpretation of the alignment procedure, we identify a natural choice for transformation for (the common case of) rewards learned from Bradley-Terry preference models. This derived transformation has two important properties. First, it emphasizes improving poorly-performing outputs, rather than outputs that already score well. This mitigates both underfitting (where some prompts are not improved) and reward hacking (where the model learns to exploit misspecification of the reward model). Second, it enables principled aggregation of rewards by linking summation to logical conjunction: the sum of transformed rewards corresponds to the probability that the output is ``good'' in all measured properties, in a sense we make precise. Experiments aligning language models to be both helpful and harmless using RLHF show substantial improvements over the baseline (non-transformed) approach.\n\n\nTransforming and Combining Rewards for Aligning Large Language Models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.00742v1</guid>
      <dc:creator>Zihao Wang, Chirag Nagpal, Jonathan Berant, Jacob Eisenstein, Alex D'Amour, Sanmi Koyejo, Victor Veitch</dc:creator>
      <pubDate>Thu, 01 Feb 2024 16:39:28 GMT</pubDate>
    </item>
    <item>
      <title>Your Large Language Model is Secretly a Fairness Proponent and You Should Prompt it Like One</title>
      <link>http://arxiv.org/abs/2402.12150v1</link>
      <description>The widespread adoption of large language models (LLMs) underscores the urgent need to ensure their fairness. However, LLMs frequently present dominant viewpoints while ignoring alternative perspectives from minority parties, resulting in potential biases. We hypothesize that these fairness-violating behaviors occur because LLMs express their viewpoints using a human personality that represents the majority of training data. In response to this, we validate that prompting LLMs with specific roles can allow LLMs to express diverse viewpoints. Building on this insight and observation, we develop FairThinking, a pipeline designed to automatically generate roles that enable LLMs to articulate diverse perspectives for fair expressions. To evaluate FairThinking, we create a dataset with a thousand items covering three fairness-related topics and conduct experiments on GPT-3.5, GPT-4, Llama2, and Mistral to demonstrate its superior performance.\n\n\nYour Large Language Model is Secretly a Fairness Proponent and You Should Prompt it Like One</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.12150v1</guid>
      <dc:creator>Tianlin Li, Xiaoyu Zhang, Chao Du, Tianyu Pang, Qian Liu, Qing Guo, Chao Shen, Yang Liu</dc:creator>
      <pubDate>Mon, 19 Feb 2024 14:02:22 GMT</pubDate>
    </item>
    <item>
      <title>Alpaca against Vicuna: Using LLMs to Uncover Memorization of LLMs</title>
      <link>http://arxiv.org/abs/2403.04801v2</link>
      <description>In this paper, we introduce a black-box prompt optimization method that uses an attacker LLM agent to uncover higher levels of memorization in a victim agent, compared to what is revealed by prompting the target model with the training data directly, which is the dominant approach of quantifying memorization in LLMs. We use an iterative rejection-sampling optimization process to find instruction-based prompts with two main characteristics: (1) minimal overlap with the training data to avoid presenting the solution directly to the model, and (2) maximal overlap between the victim model's output and the training data, aiming to induce the victim to spit out training data. We observe that our instruction-based prompts generate outputs with 23.7% higher overlap with training data compared to the baseline prefix-suffix measurements. Our findings show that (1) instruction-tuned models can expose pre-training data as much as their base-models, if not more so, (2) contexts other than the original training data can lead to leakage, and (3) using instructions proposed by other LLMs can open a new avenue of automated attacks that we should further study and explore. The code can be found at https://github.com/Alymostafa/Instruction_based_attack .\n\n\nAlpaca against Vicuna: Using LLMs to Uncover Memorization of LLMs</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2403.04801v2</guid>
      <dc:creator>Aly M. Kassem, Omar Mahmoud, Niloofar Mireshghallah, Hyunwoo Kim, Yulia Tsvetkov, Yejin Choi, Sherif Saad, Santu Rana</dc:creator>
      <pubDate>Sun, 31 Mar 2024 04:33:56 GMT</pubDate>
    </item>
    <item>
      <title>A Comprehensive Survey of Large Language Models and Multimodal Large Language Models in Medicine</title>
      <link>http://arxiv.org/abs/2405.08603v1</link>
      <description>Since the release of ChatGPT and GPT-4, large language models (LLMs) and multimodal large language models (MLLMs) have garnered significant attention due to their powerful and general capabilities in understanding, reasoning, and generation, thereby offering new paradigms for the integration of artificial intelligence with medicine. This survey comprehensively overviews the development background and principles of LLMs and MLLMs, as well as explores their application scenarios, challenges, and future directions in medicine. Specifically, this survey begins by focusing on the paradigm shift, tracing the evolution from traditional models to LLMs and MLLMs, summarizing the model structures to provide detailed foundational knowledge. Subsequently, the survey details the entire process from constructing and evaluating to using LLMs and MLLMs with a clear logic. Following this, to emphasize the significant value of LLMs and MLLMs in healthcare, we survey and summarize 6 promising applications in healthcare. Finally, the survey discusses the challenges faced by medical LLMs and MLLMs and proposes a feasible approach and direction for the subsequent integration of artificial intelligence with medicine. Thus, this survey aims to provide researchers with a valuable and comprehensive reference guide from the perspectives of the background, principles, and clinical applications of LLMs and MLLMs.\n\n\nA comprehensive survey of large language models and multimodal large language models in medicine</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.08603v1</guid>
      <dc:creator>Hanguang Xiao, Feizhong Zhou, Xingyue Liu, Tianqi Liu, Zhipeng Li, Xin Liu, Xiaoxuan Huang</dc:creator>
      <pubDate>Tue, 14 May 2024 13:42:05 GMT</pubDate>
    </item>
    <item>
      <title>Language Models Trained to do Arithmetic Predict Human Risky and Intertemporal Choice</title>
      <link>http://arxiv.org/abs/2405.19313v1</link>
      <description>The observed similarities in the behavior of humans and Large Language Models (LLMs) have prompted researchers to consider the potential of using LLMs as models of human cognition. However, several significant challenges must be addressed before LLMs can be legitimately regarded as cognitive models. For instance, LLMs are trained on far more data than humans typically encounter, and may have been directly trained on human data in specific cognitive tasks or aligned with human preferences. Consequently, the origins of these behavioral similarities are not well understood. In this paper, we propose a novel way to enhance the utility of LLMs as cognitive models. This approach involves (i) leveraging computationally equivalent tasks that both an LLM and a rational agent need to master for solving a cognitive problem and (ii) examining the specific task distributions required for an LLM to exhibit human-like behaviors. We apply this approach to decision-making -- specifically risky and intertemporal choice -- where the key computationally equivalent task is the arithmetic of expected value calculations. We show that an LLM pretrained on an ecologically valid arithmetic dataset, which we call Arithmetic-GPT, predicts human behavior better than many traditional cognitive models. Pretraining LLMs on ecologically valid arithmetic datasets is sufficient to produce a strong correspondence between these models and human decision-making. Our results also suggest that LLMs used as cognitive models should be carefully investigated via ablation studies of the pretraining data.\n\n\nLanguage Models Trained to do Arithmetic Predict Human Risky and Intertemporal Choice</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.19313v1</guid>
      <dc:creator>Jian-Qiao Zhu, Haijiang Yan, Thomas L. Griffiths</dc:creator>
      <pubDate>Wed, 29 May 2024 17:37:14 GMT</pubDate>
    </item>
    <item>
      <title>AgentOhana: Design Unified Data and Training Pipeline for Effective Agent Learning</title>
      <link>http://arxiv.org/abs/2402.15506v3</link>
      <description>Autonomous agents powered by large language models (LLMs) have garnered significant research attention. However, fully harnessing the potential of LLMs for agent-based tasks presents inherent challenges due to the heterogeneous nature of diverse data sources featuring multi-turn trajectories. In this paper, we introduce \textbf{AgentOhana} as a comprehensive solution to address these challenges. \textit{AgentOhana} aggregates agent trajectories from distinct environments, spanning a wide array of scenarios. It meticulously standardizes and unifies these trajectories into a consistent format, streamlining the creation of a generic data loader optimized for agent training. Leveraging the data unification, our training pipeline maintains equilibrium across different data sources and preserves independent randomness across devices during dataset partitioning and model training. Additionally, we present \textbf{xLAM-v0.1}, a large action model tailored for AI agents, which demonstrates exceptional performance across various benchmarks. Begin the exploration at \url{https://github.com/SalesforceAIResearch/xLAM}.\n\n\nAgentOhana: Design Unified Data and Training Pipeline for Effective Agent Learning</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.15506v3</guid>
      <dc:creator>Jianguo Zhang, Tian Lan, Rithesh Murthy, Zhiwei Liu, Weiran Yao, Juntao Tan, Thai Hoang, Liangwei Yang, Yihao Feng, Zuxin Liu, Tulika Awalgaonkar, Juan Carlos Niebles, Silvio Savarese, Shelby Heinecke, Huan Wang, Caiming Xiong</dc:creator>
      <pubDate>Wed, 20 Mar 2024 06:00:14 GMT</pubDate>
    </item>
    <item>
      <title>Collective Constitutional AI: Aligning a Language Model with Public Input</title>
      <link>http://arxiv.org/abs/2406.07814v1</link>
      <description>There is growing consensus that language model (LM) developers should not be the sole deciders of LM behavior, creating a need for methods that enable the broader public to collectively shape the behavior of LM systems that affect them. To address this need, we present Collective Constitutional AI (CCAI): a multi-stage process for sourcing and integrating public input into LMs-from identifying a target population to sourcing principles to training and evaluating a model. We demonstrate the real-world practicality of this approach by creating what is, to our knowledge, the first LM fine-tuned with collectively sourced public input and evaluating this model against a baseline model trained with established principles from a LM developer. Our quantitative evaluations demonstrate several benefits of our approach: the CCAI-trained model shows lower bias across nine social dimensions compared to the baseline model, while maintaining equivalent performance on language, math, and helpful-harmless evaluations. Qualitative comparisons of the models suggest that the models differ on the basis of their respective constitutions, e.g., when prompted with contentious topics, the CCAI-trained model tends to generate responses that reframe the matter positively instead of a refusal. These results demonstrate a promising, tractable pathway toward publicly informed development of language models.\n\n\nCollective Constitutional AI: Aligning a Language Model with Public Input</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.07814v1</guid>
      <dc:creator>Saffron Huang, Divya Siddarth, Liane Lovitt, Thomas I. Liao, Esin Durmus, Alex Tamkin, Deep Ganguli</dc:creator>
      <pubDate>Wed, 12 Jun 2024 02:20:46 GMT</pubDate>
    </item>
    <item>
      <title>HYDRA: Model Factorization Framework for Black-Box LLM Personalization</title>
      <link>http://arxiv.org/abs/2406.02888v2</link>
      <description>Personalization has emerged as a critical research area in modern intelligent systems, focusing on mining users' behavioral history and adapting to their preferences for delivering tailored experiences. Despite the remarkable few-shot capabilities exhibited by black-box large language models (LLMs), the inherent opacity of their model parameters presents significant challenges in aligning the generated output with individual expectations. Existing solutions have primarily focused on prompt design to incorporate user-specific profiles and behaviors; however, such approaches often struggle to generalize effectively due to their inability to capture shared knowledge among all users. To address these challenges, we propose HYDRA, a model factorization framework that captures both user-specific behavior patterns from historical data and shared general knowledge among all users to deliver personalized generation. In order to capture user-specific behavior patterns, we first train a reranker to prioritize the most useful information from top-retrieved relevant historical records. By combining the prioritized history with the corresponding query, we train an adapter to align the output with individual user-specific preferences, eliminating the reliance on access to inherent model parameters of black-box LLMs. Both the reranker and the adapter can be decomposed into a base model with multiple user-specific heads, resembling a hydra. The base model maintains shared knowledge across users, while the multiple personal heads capture user-specific preferences. Experimental results demonstrate that HYDRA outperforms existing state-of-the-art prompt-based methods by an average relative improvement of 9.01% across five diverse personalization tasks in the LaMP benchmark. Our implementation is available at https://github.com/night-chen/HYDRA.\n\n\nHYDRA: Model Factorization Framework for Black-Box LLM Personalization</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.02888v2</guid>
      <dc:creator>Yuchen Zhuang, Haotian Sun, Yue Yu, Rushi Qiang, Qifan Wang, Chao Zhang, Bo Dai</dc:creator>
      <pubDate>Tue, 11 Jun 2024 01:51:57 GMT</pubDate>
    </item>
    <item>
      <title>TELLER: A Trustworthy Framework for Explainable, Generalizable and Controllable Fake News Detection</title>
      <link>http://arxiv.org/abs/2402.07776v2</link>
      <description>The proliferation of fake news has emerged as a severe societal problem, raising significant interest from industry and academia. While existing deep-learning based methods have made progress in detecting fake news accurately, their reliability may be compromised caused by the non-transparent reasoning processes, poor generalization abilities and inherent risks of integration with large language models (LLMs). To address this challenge, we propose {\methodname}, a novel framework for trustworthy fake news detection that prioritizes explainability, generalizability and controllability of models. This is achieved via a dual-system framework that integrates cognition and decision systems, adhering to the principles above. The cognition system harnesses human expertise to generate logical predicates, which guide LLMs in generating human-readable logic atoms. Meanwhile, the decision system deduces generalizable logic rules to aggregate these atoms, enabling the identification of the truthfulness of the input news across diverse domains and enhancing transparency in the decision-making process. Finally, we present comprehensive evaluation results on four datasets, demonstrating the feasibility and trustworthiness of our proposed framework. Our implementation is available at \url{https://github.com/less-and-less-bugs/Trust_TELLER}.\n\n\nTeller: A trustworthy framework for explainable, generalizable and controllable fake news detection</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.07776v2</guid>
      <dc:creator>Hui Liu, Wenya Wang, Haoru Li, Haoliang Li</dc:creator>
      <pubDate>Tue, 28 May 2024 06:14:34 GMT</pubDate>
    </item>
    <item>
      <title>Filtered Direct Preference Optimization</title>
      <link>http://arxiv.org/abs/2404.13846v3</link>
      <description>Reinforcement learning from human feedback (RLHF) plays a crucial role in aligning language models with human preferences. While the significance of dataset quality is generally recognized, explicit investigations into its impact within the RLHF framework, to our knowledge, have been limited. This paper addresses the issue of text quality within the preference dataset by focusing on direct preference optimization (DPO), an increasingly adopted reward-model-free RLHF method. We confirm that text quality significantly influences the performance of models optimized with DPO more than those optimized with reward-model-based RLHF. Building on this new insight, we propose an extension of DPO, termed filtered direct preference optimization (fDPO). fDPO uses a trained reward model to monitor the quality of texts within the preference dataset during DPO training. Samples of lower quality are discarded based on comparisons with texts generated by the model being optimized, resulting in a more accurate dataset. Experimental results demonstrate that fDPO enhances the final model performance. Our code is available at https://github.com/CyberAgentAILab/filtered-dpo.\n\n\nFiltered direct preference optimization</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2404.13846v3</guid>
      <dc:creator>Tetsuro Morimura, Mitsuki Sakamoto, Yuu Jinnai, Kenshi Abe, Kaito Ariu</dc:creator>
      <pubDate>Thu, 04 Jul 2024 07:40:53 GMT</pubDate>
    </item>
    <item>
      <title>Large Language Models Meet NL2Code: A Survey</title>
      <link>http://arxiv.org/abs/2212.09420v2</link>
      <description>The task of generating code from a natural language description, or NL2Code, is considered a pressing and significant challenge in code intelligence. Thanks to the rapid development of pre-training techniques, surging large language models are being proposed for code, sparking the advances in NL2Code. To facilitate further research and applications in this field, in this paper, we present a comprehensive survey of 27 existing large language models for NL2Code, and also review benchmarks and metrics. We provide an intuitive comparison of all existing models on the HumanEval benchmark. Through in-depth observation and analysis, we provide some insights and conclude that the key factors contributing to the success of large language models for NL2Code are &quot;Large Size, Premium Data, Expert Tuning&quot;. In addition, we discuss challenges and opportunities regarding the gap between models and humans. We also create a website https://nl2code.github.io to track the latest progress through crowd-sourcing. To the best of our knowledge, this is the first survey of large language models for NL2Code, and we believe it will contribute to the ongoing development of the field.\n\n\nA Survey on Large Language Models for Code Generation</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2212.09420v2</guid>
      <dc:creator>Daoguang Zan, Bei Chen, Fengji Zhang, Dianjie Lu, Bingchao Wu, Bei Guan, Yongji Wang, Jian-Guang Lou</dc:creator>
      <pubDate>Mon, 08 May 2023 10:25:41 GMT</pubDate>
    </item>
    <item>
      <title>Mitigating Object Hallucination via Data Augmented Contrastive Tuning</title>
      <link>http://arxiv.org/abs/2405.18654v1</link>
      <description>Despite their remarkable progress, Multimodal Large Language Models (MLLMs) tend to hallucinate factually inaccurate information. In this work, we address object hallucinations in MLLMs, where information is offered about an object that is not present in the model input. We introduce a contrastive tuning method that can be applied to a pretrained off-the-shelf MLLM for mitigating hallucinations while preserving its general vision-language capabilities. For a given factual token, we create a hallucinated token through generative data augmentation by selectively altering the ground-truth information. The proposed contrastive tuning is applied at the token level to improve the relative likelihood of the factual token compared to the hallucinated one. Our thorough evaluation confirms the effectiveness of contrastive tuning in mitigating hallucination. Moreover, the proposed contrastive tuning is simple, fast, and requires minimal training with no additional overhead at inference.\n\n\nMitigating Object Hallucination via Data Augmented Contrastive Tuning</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.18654v1</guid>
      <dc:creator>Pritam Sarkar, Sayna Ebrahimi, Ali Etemad, Ahmad Beirami, Sercan Ö. Arık, Tomas Pfister</dc:creator>
      <pubDate>Tue, 28 May 2024 23:36:00 GMT</pubDate>
    </item>
    <item>
      <title>ReST-MCTS*: LLM Self-Training via Process Reward Guided Tree Search</title>
      <link>http://arxiv.org/abs/2406.03816v1</link>
      <description>Recent methodologies in LLM self-training mostly rely on LLM generating responses and filtering those with correct output answers as training data. This approach often yields a low-quality fine-tuning training set (e.g., incorrect plans or intermediate reasoning). In this paper, we develop a reinforced self-training approach, called ReST-MCTS*, based on integrating process reward guidance with tree search MCTS* for collecting higher-quality reasoning traces as well as per-step value to train policy and reward models. ReST-MCTS* circumvents the per-step manual annotation typically used to train process rewards by tree-search-based reinforcement learning: Given oracle final correct answers, ReST-MCTS* is able to infer the correct process rewards by estimating the probability this step can help lead to the correct answer. These inferred rewards serve dual purposes: they act as value targets for further refining the process reward model and also facilitate the selection of high-quality traces for policy model self-training. We first show that the tree-search policy in ReST-MCTS* achieves higher accuracy compared with prior LLM reasoning baselines such as Best-of-N and Tree-of-Thought, within the same search budget. We then show that by using traces searched by this tree-search policy as training data, we can continuously enhance the three language models for multiple iterations, and outperform other self-training algorithms such as ReST$^\text{EM}$ and Self-Rewarding LM.\n\n\nReST-MCTS*: LLM Self-Training via Process Reward Guided Tree Search</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.03816v1</guid>
      <dc:creator>Dan Zhang, Sining Zhoubian, Yisong Yue, Yuxiao Dong, Jie Tang</dc:creator>
      <pubDate>Thu, 06 Jun 2024 07:40:00 GMT</pubDate>
    </item>
    <item>
      <title>BabyStories: Can Reinforcement Learning Teach Baby Language Models to Write Better Stories?</title>
      <link>http://arxiv.org/abs/2310.16681v1</link>
      <description>Language models have seen significant growth in the size of their corpus, leading to notable performance improvements. Yet, there has been limited progress in developing models that handle smaller, more human-like datasets. As part of the BabyLM shared task, this study explores the impact of reinforcement learning from human feedback (RLHF) on language models pretrained from scratch with a limited training corpus. Comparing two GPT-2 variants, the larger model performs better in storytelling tasks after RLHF fine-tuning. These findings suggest that RLHF techniques may be more advantageous for larger models due to their higher learning and adaptation capacity, though more experiments are needed to confirm this finding. These insights highlight the potential benefits of RLHF fine-tuning for language models within limited data, enhancing their ability to maintain narrative focus and coherence while adhering better to initial instructions in storytelling tasks. The code for this work is publicly at https://github.com/Zephyr1022/BabyStories-UTSA.\n\n\nBabyStories: Can Reinforcement Learning Teach Baby Language Models to Write Better Stories?</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2310.16681v1</guid>
      <dc:creator>Xingmeng Zhao, Tongnian Wang, Sheri Osborn, Anthony Rios</dc:creator>
      <pubDate>Wed, 25 Oct 2023 14:45:48 GMT</pubDate>
    </item>
    <item>
      <title>Prompt Optimization with Human Feedback</title>
      <link>http://arxiv.org/abs/2405.17346v1</link>
      <description>Large language models (LLMs) have demonstrated remarkable performances in various tasks. However, the performance of LLMs heavily depends on the input prompt, which has given rise to a number of recent works on prompt optimization. However, previous works often require the availability of a numeric score to assess the quality of every prompt. Unfortunately, when a human user interacts with a black-box LLM, attaining such a score is often infeasible and unreliable. Instead, it is usually significantly easier and more reliable to obtain preference feedback from a human user, i.e., showing the user the responses generated from a pair of prompts and asking the user which one is preferred. Therefore, in this paper, we study the problem of prompt optimization with human feedback (POHF), in which we aim to optimize the prompt for a black-box LLM using only human preference feedback. Drawing inspiration from dueling bandits, we design a theoretically principled strategy to select a pair of prompts to query for preference feedback in every iteration, and hence introduce our algorithm named automated POHF (APOHF). We apply our APOHF algorithm to various tasks, including optimizing user instructions, prompt optimization for text-to-image generative models, and response optimization with human feedback (i.e., further refining the response using a variant of our APOHF). The results demonstrate that our APOHF can efficiently find a good prompt using a small number of preference feedback instances. Our code can be found at \url{https://github.com/xqlin98/APOHF}.\n\n\nPrompt Optimization with Human Feedback</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.17346v1</guid>
      <dc:creator>Xiaoqiang Lin, Zhongxiang Dai, Arun Verma, See-Kiong Ng, Patrick Jaillet, Bryan Kian Hsiang Low</dc:creator>
      <pubDate>Mon, 27 May 2024 16:49:29 GMT</pubDate>
    </item>
    <item>
      <title>Negating Negatives: Alignment without Human Positive Samples via Distributional Dispreference Optimization</title>
      <link>http://arxiv.org/abs/2403.03419v1</link>
      <description>Large language models (LLMs) have revolutionized the role of AI, yet also pose potential risks of propagating unethical content. Alignment technologies have been introduced to steer LLMs towards human preference, gaining increasing attention. Despite notable breakthroughs in this direction, existing methods heavily rely on high-quality positive-negative training pairs, suffering from noisy labels and the marginal distinction between preferred and dispreferred response data. Given recent LLMs' proficiency in generating helpful responses, this work pivots towards a new research focus: achieving alignment using solely human-annotated negative samples, preserving helpfulness while reducing harmfulness. For this purpose, we propose Distributional Dispreference Optimization (D$^2$O), which maximizes the discrepancy between the generated responses and the dispreferred ones to effectively eschew harmful information. We theoretically demonstrate that D$^2$O is equivalent to learning a distributional instead of instance-level preference model reflecting human dispreference against the distribution of negative responses. Besides, D$^2$O integrates an implicit Jeffrey Divergence regularization to balance the exploitation and exploration of reference policies and converges to a non-negative one during training. Extensive experiments demonstrate that our method achieves comparable generation quality and surpasses the latest baselines in producing less harmful and more informative responses with better training stability and faster convergence.\n\n\nNegating negatives: Alignment without human positive samples via distributional dispreference optimization</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2403.03419v1</guid>
      <dc:creator>Shitong Duan, Xiaoyuan Yi, Peng Zhang, Tun Lu, Xing Xie, Ning Gu</dc:creator>
      <pubDate>Wed, 06 Mar 2024 03:02:38 GMT</pubDate>
    </item>
    <item>
      <title>Value-Incentivized Preference Optimization: A Unified Approach to Online and Offline RLHF</title>
      <link>http://arxiv.org/abs/2405.19320v3</link>
      <description>Reinforcement learning from human feedback (RLHF) has demonstrated great promise in aligning large language models (LLMs) with human preference. Depending on the availability of preference data, both online and offline RLHF are active areas of investigation. A key bottleneck is understanding how to incorporate uncertainty estimation in the reward function learned from the preference data for RLHF, regardless of how the preference data is collected. While the principles of optimism or pessimism under uncertainty are well-established in standard reinforcement learning (RL), a practically-implementable and theoretically-grounded form amenable to large language models is not yet available, as standard techniques for constructing confidence intervals become intractable under arbitrary policy parameterizations.   In this paper, we introduce a unified approach to online and offline RLHF -- value-incentivized preference optimization (VPO) -- which regularizes the maximum-likelihood estimate of the reward function with the corresponding value function, modulated by a $\textit{sign}$ to indicate whether the optimism or pessimism is chosen. VPO also directly optimizes the policy with implicit reward modeling, and therefore shares a simpler RLHF pipeline similar to direct preference optimization. Theoretical guarantees of VPO are provided for both online and offline settings, matching the rates of their standard RL counterparts. Moreover, experiments on text summarization and dialog verify the practicality and effectiveness of VPO.\n\n\nValue-Incentivized Preference Optimization: A Unified Approach to Online and Offline RLHF</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.19320v3</guid>
      <dc:creator>Shicong Cen, Jincheng Mei, Katayoon Goshvadi, Hanjun Dai, Tong Yang, Sherry Yang, Dale Schuurmans, Yuejie Chi, Bo Dai</dc:creator>
      <pubDate>Fri, 05 Jul 2024 04:59:42 GMT</pubDate>
    </item>
    <item>
      <title>T2V-Turbo: Breaking the Quality Bottleneck of Video Consistency Model with Mixed Reward Feedback</title>
      <link>http://arxiv.org/abs/2405.18750v1</link>
      <description>Diffusion-based text-to-video (T2V) models have achieved significant success but continue to be hampered by the slow sampling speed of their iterative sampling processes. To address the challenge, consistency models have been proposed to facilitate fast inference, albeit at the cost of sample quality. In this work, we aim to break the quality bottleneck of a video consistency model (VCM) to achieve $\textbf{both fast and high-quality video generation}$. We introduce T2V-Turbo, which integrates feedback from a mixture of differentiable reward models into the consistency distillation (CD) process of a pre-trained T2V model. Notably, we directly optimize rewards associated with single-step generations that arise naturally from computing the CD loss, effectively bypassing the memory constraints imposed by backpropagating gradients through an iterative sampling process. Remarkably, the 4-step generations from our T2V-Turbo achieve the highest total score on VBench, even surpassing Gen-2 and Pika. We further conduct human evaluations to corroborate the results, validating that the 4-step generations from our T2V-Turbo are preferred over the 50-step DDIM samples from their teacher models, representing more than a tenfold acceleration while improving video generation quality.\n\n\nT2V-Turbo: Breaking the Quality Bottleneck of Video Consistency Model with Mixed Reward Feedback</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.18750v1</guid>
      <dc:creator>Jiachen Li, Weixi Feng, Tsu-Jui Fu, Xinyi Wang, Sugato Basu, Wenhu Chen, William Yang Wang</dc:creator>
      <pubDate>Wed, 29 May 2024 04:26:17 GMT</pubDate>
    </item>
    <item>
      <title>Comparing Bad Apples to Good Oranges: Aligning Large Language Models via Joint Preference Optimization</title>
      <link>http://arxiv.org/abs/2404.00530v1</link>
      <description>A common technique for aligning large language models (LLMs) relies on acquiring human preferences by comparing multiple generations conditioned on a fixed context. This only leverages the pairwise comparisons when the generations are placed in an identical context. However, such conditional rankings often fail to capture the complex and multidimensional aspects of human preferences. In this work, we revisit the traditional paradigm of preference acquisition and propose a new axis that is based on eliciting preferences jointly over the instruction-response pairs. While prior preference optimizations are designed for conditional ranking protocols (e.g., DPO), our proposed preference acquisition protocol introduces DOVE, a new preference optimization objective that upweights the joint probability of the chosen instruction-response pair over the rejected instruction-response pair. Interestingly, we find that the LLM trained with joint instruction-response preference data using DOVE outperforms the LLM trained with DPO by 5.2% and 3.3% win-rate for the summarization and open-ended dialogue datasets, respectively. Our findings reveal that joint preferences over instruction and response pairs can significantly enhance the alignment of LLMs by tapping into a broader spectrum of human preference elicitation. The data and code is available at https://github.com/Hritikbansal/dove.\n\n\nComparing Bad Apples to Good Oranges: Aligning Large Language Models via Joint Preference Optimization</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2404.00530v1</guid>
      <dc:creator>Hritik Bansal, Ashima Suvarna, Gantavya Bhatt, Nanyun Peng, Kai-Wei Chang, Aditya Grover</dc:creator>
      <pubDate>Sun, 31 Mar 2024 02:05:40 GMT</pubDate>
    </item>
    <item>
      <title>Supervised Fine-Tuning as Inverse Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2403.12017v1</link>
      <description>The prevailing approach to aligning Large Language Models (LLMs) typically relies on human or AI feedback and assumes access to specific types of preference datasets. In our work, we question the efficacy of such datasets and explore various scenarios where alignment with expert demonstrations proves more realistic. We build a sequential decision-making framework to formulate the problem of aligning LLMs using demonstration datasets. Drawing insights from inverse reinforcement learning and imitation learning, we introduce various approaches for divergence minimization in the LLM alignment tasks. Our analysis highlights the mass-covering and mode-seeking behaviors of these different approaches. Inclusively, we examine the pros and cons of the classical supervised fine-tuning method, elaborating on scenarios where different methods shine.\n\n\nInverse-RLignment: Inverse Reinforcement Learning from Demonstrations for LLM Alignment</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2403.12017v1</guid>
      <dc:creator>Hao Sun</dc:creator>
      <pubDate>Mon, 18 Mar 2024 17:52:57 GMT</pubDate>
    </item>
    <item>
      <title>OpenToM: A Comprehensive Benchmark for Evaluating Theory-of-Mind Reasoning Capabilities of Large Language Models</title>
      <link>http://arxiv.org/abs/2402.06044v3</link>
      <description>Neural Theory-of-Mind (N-ToM), machine's ability to understand and keep track of the mental states of others, is pivotal in developing socially intelligent agents. However, prevalent N-ToM benchmarks have several shortcomings, including the presence of ambiguous and artificial narratives, absence of personality traits and preferences, a lack of questions addressing characters' psychological mental states, and limited diversity in the questions posed. In response to these issues, we construct OpenToM, a new benchmark for assessing N-ToM with (1) longer and clearer narrative stories, (2) characters with explicit personality traits, (3) actions that are triggered by character intentions, and (4) questions designed to challenge LLMs' capabilities of modeling characters' mental states of both the physical and psychological world. Using OpenToM, we reveal that state-of-the-art LLMs thrive at modeling certain aspects of mental states in the physical world but fall short when tracking characters' mental states in the psychological world.\n\n\nOpenToM: A Comprehensive Benchmark for Evaluating Theory-of-Mind Reasoning Capabilities of Large Language Models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.06044v3</guid>
      <dc:creator>Hainiu Xu, Runcong Zhao, Lixing Zhu, Jinhua Du, Yulan He</dc:creator>
      <pubDate>Mon, 03 Jun 2024 10:48:16 GMT</pubDate>
    </item>
    <item>
      <title>Binary Classifier Optimization for Large Language Model Alignment</title>
      <link>http://arxiv.org/abs/2404.04656v1</link>
      <description>Aligning Large Language Models (LLMs) to human preferences through preference optimization has been crucial but labor-intensive, necessitating for each prompt a comparison of both a chosen and a rejected text completion by evaluators. Recently, Kahneman-Tversky Optimization (KTO) has demonstrated that LLMs can be aligned using merely binary &quot;thumbs-up&quot; or &quot;thumbs-down&quot; signals on each prompt-completion pair. In this paper, we present theoretical foundations to explain the successful alignment achieved through these binary signals. Our analysis uncovers a new perspective: optimizing a binary classifier, whose logit is a reward, implicitly induces minimizing the Direct Preference Optimization (DPO) loss. In the process of this discovery, we identified two techniques for effective alignment: reward shift and underlying distribution matching. Consequently, we propose a new algorithm, \textit{Binary Classifier Optimization}, that integrates the techniques. We validate our methodology in two settings: first, on a paired preference dataset, where our method performs on par with DPO and KTO; and second, on binary signal datasets simulating real-world conditions with divergent underlying distributions between thumbs-up and thumbs-down data. Our model consistently demonstrates effective and robust alignment across two base LLMs and three different binary signal datasets, showcasing the strength of our approach to learning from binary feedback.\n\n\nBinary classifier optimization for large language model alignment</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2404.04656v1</guid>
      <dc:creator>Seungjae Jung, Gunsoo Han, Daniel Wontae Nam, Kyoung-Woon On</dc:creator>
      <pubDate>Sat, 06 Apr 2024 15:20:59 GMT</pubDate>
    </item>
    <item>
      <title>Immunization against harmful fine-tuning attacks</title>
      <link>http://arxiv.org/abs/2402.16382v1</link>
      <description>Approaches to aligning large language models (LLMs) with human values has focused on correcting misalignment that emerges from pretraining. However, this focus overlooks another source of misalignment: bad actors might purposely fine-tune LLMs to achieve harmful goals. In this paper, we present an emerging threat model that has arisen from alignment circumvention and fine-tuning attacks. However, lacking in previous works is a clear presentation of the conditions for effective defence. We propose a set of conditions for effective defence against harmful fine-tuning in LLMs called &quot;Immunization conditions,&quot; which help us understand how we would construct and measure future defences. Using this formal framework for defence, we offer a synthesis of different research directions that might be persued to prevent harmful fine-tuning attacks and provide a demonstration of how to use these conditions experimentally showing early results of using an adversarial loss to immunize LLama2-7b-chat.\n\n\nImmunization against harmful fine-tuning attacks</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.16382v1</guid>
      <dc:creator>Domenic Rosati, Jan Wehner, Kai Williams, Łukasz Bartoszcze, Jan Batzner, Hassan Sajjad, Frank Rudzicz</dc:creator>
      <pubDate>Mon, 26 Feb 2024 08:08:03 GMT</pubDate>
    </item>
    <item>
      <title>Conformal Alignment: Knowing When to Trust Foundation Models with Guarantees</title>
      <link>http://arxiv.org/abs/2405.10301v2</link>
      <description>Before deploying outputs from foundation models in high-stakes tasks, it is imperative to ensure that they align with human values. For instance, in radiology report generation, reports generated by a vision-language model must align with human evaluations before their use in medical decision-making. This paper presents Conformal Alignment, a general framework for identifying units whose outputs meet a user-specified alignment criterion. It is guaranteed that on average, a prescribed fraction of selected units indeed meet the alignment criterion, regardless of the foundation model or the data distribution. Given any pre-trained model and new units with model-generated outputs, Conformal Alignment leverages a set of reference data with ground-truth alignment status to train an alignment predictor. It then selects new units whose predicted alignment scores surpass a data-dependent threshold, certifying their corresponding outputs as trustworthy. Through applications to question answering and radiology report generation, we demonstrate that our method is able to accurately identify units with trustworthy outputs via lightweight training over a moderate amount of reference data. En route, we investigate the informativeness of various features in alignment prediction and combine them with standard models to construct the alignment predictor.\n\n\nConformal Alignment: Knowing When to Trust Foundation Models with Guarantees</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.10301v2</guid>
      <dc:creator>Yu Gui, Ying Jin, Zhimei Ren</dc:creator>
      <pubDate>Tue, 21 May 2024 21:49:47 GMT</pubDate>
    </item>
    <item>
      <title>RaFe: Ranking Feedback Improves Query Rewriting for RAG</title>
      <link>http://arxiv.org/abs/2405.14431v1</link>
      <description>As Large Language Models (LLMs) and Retrieval Augmentation Generation (RAG) techniques have evolved, query rewriting has been widely incorporated into the RAG system for downstream tasks like open-domain QA. Many works have attempted to utilize small models with reinforcement learning rather than costly LLMs to improve query rewriting. However, current methods require annotations (e.g., labeled relevant documents or downstream answers) or predesigned rewards for feedback, which lack generalization, and fail to utilize signals tailored for query rewriting. In this paper, we propose ours, a framework for training query rewriting models free of annotations. By leveraging a publicly available reranker, ours~provides feedback aligned well with the rewriting objectives. Experimental results demonstrate that ours~can obtain better performance than baselines.\n\n\nRaFe: Ranking Feedback Improves Query Rewriting for RAG</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.14431v1</guid>
      <dc:creator>Shengyu Mao, Yong Jiang, Boli Chen, Xiao Li, Peng Wang, Xinyu Wang, Pengjun Xie, Fei Huang, Huajun Chen, Ningyu Zhang</dc:creator>
      <pubDate>Thu, 23 May 2024 11:00:19 GMT</pubDate>
    </item>
    <item>
      <title>An Empirical Study on Large Language Models in Accuracy and Robustness under Chinese Industrial Scenarios</title>
      <link>http://arxiv.org/abs/2402.01723v1</link>
      <description>Recent years have witnessed the rapid development of large language models (LLMs) in various domains. To better serve the large number of Chinese users, many commercial vendors in China have adopted localization strategies, training and providing local LLMs specifically customized for Chinese users. Furthermore, looking ahead, one of the key future applications of LLMs will be practical deployment in industrial production by enterprises and users in those sectors. However, the accuracy and robustness of LLMs in industrial scenarios have not been well studied. In this paper, we present a comprehensive empirical study on the accuracy and robustness of LLMs in the context of the Chinese industrial production area. We manually collected 1,200 domain-specific problems from 8 different industrial sectors to evaluate LLM accuracy. Furthermore, we designed a metamorphic testing framework containing four industrial-specific stability categories with eight abilities, totaling 13,631 questions with variants to evaluate LLM robustness. In total, we evaluated 9 different LLMs developed by Chinese vendors, as well as four different LLMs developed by global vendors. Our major findings include: (1) Current LLMs exhibit low accuracy in Chinese industrial contexts, with all LLMs scoring less than 0.6. (2) The robustness scores vary across industrial sectors, and local LLMs overall perform worse than global ones. (3) LLM robustness differs significantly across abilities. Global LLMs are more robust under logical-related variants, while advanced local LLMs perform better on problems related to understanding Chinese industrial terminology. Our study results provide valuable guidance for understanding and promoting the industrial domain capabilities of LLMs from both development and industrial enterprise perspectives. The results further motivate possible research directions and tooling support.\n\n\nAn empirical study on large language models in accuracy and robustness under chinese industrial scenarios</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.01723v1</guid>
      <dc:creator>Zongjie Li, Wenying Qiu, Pingchuan Ma, Yichen Li, You Li, Sijia He, Baozheng Jiang, Shuai Wang, Weixi Gu</dc:creator>
      <pubDate>Sat, 27 Jan 2024 03:37:55 GMT</pubDate>
    </item>
    <item>
      <title>Loose LIPS Sink Ships: Asking Questions in Battleship with Language-Informed Program Sampling</title>
      <link>http://arxiv.org/abs/2402.19471v2</link>
      <description>Questions combine our mastery of language with our remarkable facility for reasoning about uncertainty. How do people navigate vast hypothesis spaces to pose informative questions given limited cognitive resources? We study these tradeoffs in a classic grounded question-asking task based on the board game Battleship. Our language-informed program sampling (LIPS) model uses large language models (LLMs) to generate natural language questions, translate them into symbolic programs, and evaluate their expected information gain. We find that with a surprisingly modest resource budget, this simple Monte Carlo optimization strategy yields informative questions that mirror human performance across varied Battleship board scenarios. In contrast, LLM-only baselines struggle to ground questions in the board state; notably, GPT-4V provides no improvement over non-visual baselines. Our results illustrate how Bayesian models of question-asking can leverage the statistics of language to capture human priors, while highlighting some shortcomings of pure LLMs as grounded reasoners.\n\n\nLoose LIPS Sink Ships: Asking Questions in Battleship with Language-Informed Program Sampling</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.19471v2</guid>
      <dc:creator>Gabriel Grand, Valerio Pepe, Jacob Andreas, Joshua B. Tenenbaum</dc:creator>
      <pubDate>Wed, 01 May 2024 19:00:06 GMT</pubDate>
    </item>
    <item>
      <title>Improving Socratic Question Generation using Data Augmentation and Preference Optimization</title>
      <link>http://arxiv.org/abs/2403.00199v3</link>
      <description>The Socratic method is a way of guiding students toward solving a problem independently without directly revealing the solution to the problem. Although this method has been shown to significantly improve student learning outcomes, it remains a complex labor-intensive task for instructors. Large language models (LLMs) can be used to augment human effort by automatically generating Socratic questions for students. However, existing methods that involve prompting these LLMs sometimes produce invalid outputs, e.g., those that directly reveal the solution to the problem or provide irrelevant or premature questions. To alleviate this problem, inspired by reinforcement learning with AI feedback (RLAIF), we first propose a data augmentation method to enrich existing Socratic questioning datasets with questions that are invalid in specific ways. Next, we propose a method to optimize open-source LLMs such as LLama 2 to prefer ground-truth questions over generated invalid ones, using direct preference optimization (DPO). Our experiments on a Socratic questions dataset for student code debugging show that a DPO-optimized 7B LLama 2 model can effectively avoid generating invalid questions, and as a result, outperforms existing state-of-the-art prompting methods.\n\n\nImproving socratic question generation using data augmentation and preference optimization</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2403.00199v3</guid>
      <dc:creator>Nischal Ashok Kumar, Andrew Lan</dc:creator>
      <pubDate>Fri, 19 Apr 2024 02:36:26 GMT</pubDate>
    </item>
    <item>
      <title>PlanGPT: Enhancing Urban Planning with Tailored Language Model and Efficient Retrieval</title>
      <link>http://arxiv.org/abs/2402.19273v1</link>
      <description>In the field of urban planning, general-purpose large language models often struggle to meet the specific needs of planners. Tasks like generating urban planning texts, retrieving related information, and evaluating planning documents pose unique challenges. To enhance the efficiency of urban professionals and overcome these obstacles, we introduce PlanGPT, the first specialized Large Language Model tailored for urban and spatial planning. Developed through collaborative efforts with institutions like the Chinese Academy of Urban Planning, PlanGPT leverages a customized local database retrieval framework, domain-specific fine-tuning of base models, and advanced tooling capabilities. Empirical tests demonstrate that PlanGPT has achieved advanced performance, delivering responses of superior quality precisely tailored to the intricacies of urban planning.\n\n\nPlanGPT: Enhancing urban planning with tailored language model and efficient retrieval</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.19273v1</guid>
      <dc:creator>He Zhu, Wenjia Zhang, Nuoxian Huang, Boyang Li, Luyao Niu, Zipei Fan, Tianle Lun, Yicheng Tao, Junyou Su, Zhaoya Gong, Chenyu Fang, Xing Liu</dc:creator>
      <pubDate>Thu, 29 Feb 2024 15:41:20 GMT</pubDate>
    </item>
    <item>
      <title>Recovering Structured Probability Matrices</title>
      <link>http://arxiv.org/abs/1602.06586v6</link>
      <description>We consider the problem of accurately recovering a matrix B of size M by M , which represents a probability distribution over M2 outcomes, given access to an observed matrix of &quot;counts&quot; generated by taking independent samples from the distribution B. How can structural properties of the underlying matrix B be leveraged to yield computationally efficient and information theoretically optimal reconstruction algorithms? When can accurate reconstruction be accomplished in the sparse data regime? This basic problem lies at the core of a number of questions that are currently being considered by different communities, including building recommendation systems and collaborative filtering in the sparse data regime, community detection in sparse random graphs, learning structured models such as topic models or hidden Markov models, and the efforts from the natural language processing community to compute &quot;word embeddings&quot;.   Our results apply to the setting where B has a low rank structure. For this setting, we propose an efficient algorithm that accurately recovers the underlying M by M matrix using Theta(M) samples. This result easily translates to Theta(M) sample algorithms for learning topic models and learning hidden Markov Models. These linear sample complexities are optimal, up to constant factors, in an extremely strong sense: even testing basic properties of the underlying matrix (such as whether it has rank 1 or 2) requires Omega(M) samples. We provide an even stronger lower bound where distinguishing whether a sequence of observations were drawn from the uniform distribution over M observations versus being generated by an HMM with two hidden states requires Omega(M) observations. This precludes sublinear-sample hypothesis tests for basic properties, such as identity or uniformity, as well as sublinear sample estimators for quantities such as the entropy rate of HMMs.\n\n\nRecovering the Pre-Fine-Tuning Weights of Generative Models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/1602.06586v6</guid>
      <dc:creator>Qingqing Huang, Sham M. Kakade, Weihao Kong, Gregory Valiant</dc:creator>
      <pubDate>Tue, 06 Feb 2018 02:25:08 GMT</pubDate>
    </item>
    <item>
      <title>ConPro: Learning Severity Representation for Medical Images using Contrastive Learning and Preference Optimization</title>
      <link>http://arxiv.org/abs/2404.18831v1</link>
      <description>Understanding the severity of conditions shown in images in medical diagnosis is crucial, serving as a key guide for clinical assessment, treatment, as well as evaluating longitudinal progression. This paper proposes Con- PrO: a novel representation learning method for severity assessment in medical images using Contrastive learningintegrated Preference Optimization. Different from conventional contrastive learning methods that maximize the distance between classes, ConPrO injects into the latent vector the distance preference knowledge between various severity classes and the normal class. We systematically examine the key components of our framework to illuminate how contrastive prediction tasks acquire valuable representations. We show that our representation learning framework offers valuable severity ordering in the feature space while outperforming previous state-of-the-art methods on classification tasks. We achieve a 6% and 20% relative improvement compared to a supervised and a self-supervised baseline, respectively. In addition, we derived discussions on severity indicators and related applications of preference comparison in the medical domain.\n\n\nConPro: Learning Severity Representation for Medical Images using Contrastive Learning and Preference Optimization</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2404.18831v1</guid>
      <dc:creator>Hong Nguyen, Hoang Nguyen, Melinda Chang, Hieu Pham, Shrikanth Narayanan, Michael Pazzani</dc:creator>
      <pubDate>Mon, 29 Apr 2024 16:16:42 GMT</pubDate>
    </item>
    <item>
      <title>A Closer Look at the Limitations of Instruction Tuning</title>
      <link>http://arxiv.org/abs/2402.05119v5</link>
      <description>Instruction Tuning (IT), the process of training large language models (LLMs) using instruction-response pairs, has emerged as the predominant method for transforming base pre-trained LLMs into open-domain conversational agents. While IT has achieved notable success and widespread adoption, its limitations and shortcomings remain underexplored. In this paper, through rigorous experiments and an in-depth analysis of the changes LLMs undergo through IT, we reveal various limitations of IT. In particular, we show that (1) IT fails to enhance knowledge or skills in LLMs. LoRA fine-tuning is limited to learning response initiation and style tokens, and full-parameter fine-tuning leads to knowledge degradation. (2) Copying response patterns from IT datasets derived from knowledgeable sources leads to a decline in response quality. (3) Full-parameter fine-tuning increases hallucination by inaccurately borrowing tokens from conceptually similar instances in the IT dataset for generating responses. (4) Popular methods to improve IT do not lead to performance improvements over a simple LoRA fine-tuned model. Our findings reveal that responses generated solely from pre-trained knowledge consistently outperform responses by models that learn any form of new knowledge from IT on open-source datasets. We hope the insights and challenges revealed in this paper inspire future work in related directions.\n\n\nA Closer Look at the Limitations of Instruction Tuning</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.05119v5</guid>
      <dc:creator>Sreyan Ghosh, Chandra Kiran Reddy Evuru, Sonal Kumar, Ramaneswaran S, Deepali Aneja, Zeyu Jin, Ramani Duraiswami, Dinesh Manocha</dc:creator>
      <pubDate>Sun, 14 Jul 2024 18:14:57 GMT</pubDate>
    </item>
    <item>
      <title>Intuitive Fine-Tuning: Towards Simplifying Alignment into a Single Process</title>
      <link>http://arxiv.org/abs/2405.11870v2</link>
      <description>Supervised Fine-Tuning (SFT) and Preference Optimization (PO) are two fundamental processes for enhancing the capabilities of Language Models (LMs) post pre-training, aligning them better with human preferences. Although SFT advances in training efficiency, PO delivers better alignment, thus they are often combined. However, common practices simply apply them sequentially without integrating their optimization objectives, ignoring the opportunities to bridge their paradigm gap and take the strengths from both. To obtain a unified understanding, we interpret SFT and PO with two sub-processes -- Preference Estimation and Transition Optimization -- defined at token level within the Markov Decision Process (MDP) framework. This modeling shows that SFT is only a specialized case of PO with inferior estimation and optimization. PO evaluates the quality of model's entire generated answer, whereas SFT only scores predicted tokens based on preceding tokens from target answers. Therefore, SFT overestimates the ability of model, leading to inferior optimization. Building on this view, we introduce Intuitive Fine-Tuning (IFT) to integrate SFT and Preference Optimization into a single process. IFT captures LMs' intuitive sense of the entire answers through a temporal residual connection, but it solely relies on a single policy and the same volume of non-preference-labeled data as SFT. Our experiments show that IFT performs comparably or even superiorly to sequential recipes of SFT and some typical Preference Optimization methods across several tasks, particularly those requires generation, reasoning, and fact-following abilities. An explainable Frozen Lake game further validates the effectiveness of IFT for getting competitive policy.\n\n\nIntuitive Fine-Tuning: Towards Unifying SFT and RLHF into a Single Process</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.11870v2</guid>
      <dc:creator>Ermo Hua, Biqing Qi, Kaiyan Zhang, Yue Yu, Ning Ding, Xingtai Lv, Kai Tian, Bowen Zhou</dc:creator>
      <pubDate>Tue, 28 May 2024 16:14:58 GMT</pubDate>
    </item>
    <item>
      <title>Diffusion Language Models Are Versatile Protein Learners</title>
      <link>http://arxiv.org/abs/2402.18567v1</link>
      <description>This paper introduces diffusion protein language model (DPLM), a versatile protein language model that demonstrates strong generative and predictive capabilities for protein sequences. We first pre-train scalable DPLMs from evolutionary-scale protein sequences within a generative self-supervised discrete diffusion probabilistic framework, which generalizes language modeling for proteins in a principled way. After pre-training, DPLM exhibits the ability to generate structurally plausible, novel, and diverse protein sequences for unconditional generation. We further demonstrate the proposed diffusion generative pre-training makes DPLM possess a better understanding of proteins, making it a superior representation learner, which can be fine-tuned for various predictive tasks, comparing favorably to ESM2 (Lin et al., 2022). Moreover, DPLM can be tailored for various needs, which showcases its prowess of conditional generation in several ways: (1) conditioning on partial peptide sequences, e.g., generating scaffolds for functional motifs with high success rate; (2) incorporating other modalities as conditioner, e.g., structure-conditioned generation for inverse folding; and (3) steering sequence generation towards desired properties, e.g., satisfying specified secondary structures, through a plug-and-play classifier guidance.\n\n\nDiffusion Language Models Are Versatile Protein Learners</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.18567v1</guid>
      <dc:creator>Xinyou Wang, Zaixiang Zheng, Fei Ye, Dongyu Xue, Shujian Huang, Quanquan Gu</dc:creator>
      <pubDate>Wed, 28 Feb 2024 18:57:56 GMT</pubDate>
    </item>
    <item>
      <title>Exploring Text-to-Motion Generation with Human Preference</title>
      <link>http://arxiv.org/abs/2404.09445v1</link>
      <description>This paper presents an exploration of preference learning in text-to-motion generation. We find that current improvements in text-to-motion generation still rely on datasets requiring expert labelers with motion capture systems. Instead, learning from human preference data does not require motion capture systems; a labeler with no expertise simply compares two generated motions. This is particularly efficient because evaluating the model's output is easier than gathering the motion that performs a desired task (e.g. backflip). To pioneer the exploration of this paradigm, we annotate 3,528 preference pairs generated by MotionGPT, marking the first effort to investigate various algorithms for learning from preference data. In particular, our exploration highlights important design choices when using preference data. Additionally, our experimental results show that preference learning has the potential to greatly improve current text-to-motion generative models. Our code and dataset are publicly available at https://github.com/THU-LYJ-Lab/InstructMotion}{https://github.com/THU-LYJ-Lab/InstructMotion to further facilitate research in this area.\n\n\nExploring Text-to-Motion Generation with Human Preference</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2404.09445v1</guid>
      <dc:creator>Jenny Sheng, Matthieu Lin, Andrew Zhao, Kevin Pruvost, Yu-Hui Wen, Yangguang Li, Gao Huang, Yong-Jin Liu</dc:creator>
      <pubDate>Mon, 15 Apr 2024 04:14:42 GMT</pubDate>
    </item>
    <item>
      <title>Detoxifying Large Language Models via Knowledge Editing</title>
      <link>http://arxiv.org/abs/2403.14472v5</link>
      <description>This paper investigates using knowledge editing techniques to detoxify Large Language Models (LLMs). We construct a benchmark, SafeEdit, which covers nine unsafe categories with various powerful attack prompts and equips comprehensive metrics for systematic evaluation. We conduct experiments with several knowledge editing approaches, indicating that knowledge editing has the potential to detoxify LLMs with a limited impact on general performance efficiently. Then, we propose a simple yet effective baseline, dubbed Detoxifying with Intraoperative Neural Monitoring (DINM), to diminish the toxicity of LLMs within a few tuning steps via only one instance. We further provide an in-depth analysis of the internal mechanism for various detoxifying approaches, demonstrating that previous methods like SFT and DPO may merely suppress the activations of toxic parameters, while DINM mitigates the toxicity of the toxic parameters to a certain extent, making permanent adjustments. We hope that these insights could shed light on future work of developing detoxifying approaches and the underlying knowledge mechanisms of LLMs. Code and benchmark are available at https://github.com/zjunlp/EasyEdit.\n\n\nDetoxifying Large Language Models via Knowledge Editing</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2403.14472v5</guid>
      <dc:creator>Mengru Wang, Ningyu Zhang, Ziwen Xu, Zekun Xi, Shumin Deng, Yunzhi Yao, Qishen Zhang, Linyi Yang, Jindong Wang, Huajun Chen</dc:creator>
      <pubDate>Tue, 28 May 2024 09:11:25 GMT</pubDate>
    </item>
    <item>
      <title>Safe LoRA: the Silver Lining of Reducing Safety Risks when Fine-tuning Large Language Models</title>
      <link>http://arxiv.org/abs/2405.16833v1</link>
      <description>While large language models (LLMs) such as Llama-2 or GPT-4 have shown impressive zero-shot performance, fine-tuning is still necessary to enhance their performance for customized datasets, domain-specific tasks, or other private needs. However, fine-tuning all parameters of LLMs requires significant hardware resources, which can be impractical for typical users. Therefore, parameter-efficient fine-tuning such as LoRA have emerged, allowing users to fine-tune LLMs without the need for considerable computing resources, with little performance degradation compared to fine-tuning all parameters. Unfortunately, recent studies indicate that fine-tuning can increase the risk to the safety of LLMs, even when data does not contain malicious content. To address this challenge, we propose Safe LoRA, a simple one-liner patch to the original LoRA implementation by introducing the projection of LoRA weights from selected layers to the safety-aligned subspace, effectively reducing the safety risks in LLM fine-tuning while maintaining utility. It is worth noting that Safe LoRA is a training-free and data-free approach, as it only requires the knowledge of the weights from the base and aligned LLMs. Our extensive experiments demonstrate that when fine-tuning on purely malicious data, Safe LoRA retains similar safety performance as the original aligned model. Moreover, when the fine-tuning dataset contains a mixture of both benign and malicious data, Safe LoRA mitigates the negative effect made by malicious data while preserving performance on downstream tasks.\n\n\nSafe LoRA: the Silver Lining of Reducing Safety Risks when Fine-tuning Large Language Models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.16833v1</guid>
      <dc:creator>Chia-Yi Hsu, Yu-Lin Tsai, Chih-Hsun Lin, Pin-Yu Chen, Chia-Mu Yu, Chun-Ying Huang</dc:creator>
      <pubDate>Mon, 27 May 2024 05:04:05 GMT</pubDate>
    </item>
    <item>
      <title>Dissecting Human and LLM Preferences</title>
      <link>http://arxiv.org/abs/2402.11296v1</link>
      <description>As a relative quality comparison of model responses, human and Large Language Model (LLM) preferences serve as common alignment goals in model fine-tuning and criteria in evaluation. Yet, these preferences merely reflect broad tendencies, resulting in less explainable and controllable models with potential safety risks. In this work, we dissect the preferences of human and 32 different LLMs to understand their quantitative composition, using annotations from real-world user-model conversations for a fine-grained, scenario-wise analysis. We find that humans are less sensitive to errors, favor responses that support their stances, and show clear dislike when models admit their limits. On the contrary, advanced LLMs like GPT-4-Turbo emphasize correctness, clarity, and harmlessness more. Additionally, LLMs of similar sizes tend to exhibit similar preferences, regardless of their training methods, and fine-tuning for alignment does not significantly alter the preferences of pretrained-only LLMs. Finally, we show that preference-based evaluation can be intentionally manipulated. In both training-free and training-based settings, aligning a model with the preferences of judges boosts scores, while injecting the least preferred properties lowers them. This results in notable score shifts: up to 0.59 on MT-Bench (1-10 scale) and 31.94 on AlpacaEval 2.0 (0-100 scale), highlighting the significant impact of this strategic adaptation. Interactive Demo: https://huggingface.co/spaces/GAIR/Preference-Dissection-Visualization Dataset: https://huggingface.co/datasets/GAIR/preference-dissection Code: https://github.com/GAIR-NLP/Preference-Dissection\n\n\nDissecting Human and LLM Preferences</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.11296v1</guid>
      <dc:creator>Junlong Li, Fan Zhou, Shichao Sun, Yikai Zhang, Hai Zhao, Pengfei Liu</dc:creator>
      <pubDate>Sat, 17 Feb 2024 14:34:31 GMT</pubDate>
    </item>
    <item>
      <title>SymbolicAI: A framework for logic-based approaches combining generative models and solvers</title>
      <link>http://arxiv.org/abs/2402.00854v3</link>
      <description>We introduce SymbolicAI, a versatile and modular framework employing a logic-based approach to concept learning and flow management in generative processes. SymbolicAI enables the seamless integration of generative models with a diverse range of solvers by treating large language models (LLMs) as semantic parsers that execute tasks based on both natural and formal language instructions, thus bridging the gap between symbolic reasoning and generative AI. We leverage probabilistic programming principles to tackle complex tasks, and utilize differentiable and classical programming paradigms with their respective strengths. The framework introduces a set of polymorphic, compositional, and self-referential operations for multi-modal data that connects multi-step generative processes and aligns their outputs with user objectives in complex workflows. As a result, we can transition between the capabilities of various foundation models with in-context learning capabilities and specialized, fine-tuned models or solvers proficient in addressing specific problems. Through these operations based on in-context learning our framework enables the creation and evaluation of explainable computational graphs. Finally, we introduce a quality measure and its empirical score for evaluating these computational graphs, and propose a benchmark that compares various state-of-the-art LLMs across a set of complex workflows. We refer to the empirical score as the &quot;Vector Embedding for Relational Trajectory Evaluation through Cross-similarity&quot;, or VERTEX score for short. The framework codebase and benchmark are linked below.\n\n\nSymbolicAI: A framework for logic-based approaches combining generative models and solvers</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.00854v3</guid>
      <dc:creator>Marius-Constantin Dinu, Claudiu Leoveanu-Condrei, Markus Holzleitner, Werner Zellinger, Sepp Hochreiter</dc:creator>
      <pubDate>Mon, 27 May 2024 13:05:13 GMT</pubDate>
    </item>
    <item>
      <title>Batch Reinforcement Learning from Crowds</title>
      <link>http://arxiv.org/abs/2111.04279v2</link>
      <description>A shortcoming of batch reinforcement learning is its requirement for rewards in data, thus not applicable to tasks without reward functions. Existing settings for lack of reward, such as behavioral cloning, rely on optimal demonstrations collected from humans. Unfortunately, extensive expertise is required for ensuring optimality, which hinder the acquisition of large-scale data for complex tasks. This paper addresses the lack of reward in a batch reinforcement learning setting by learning a reward function from preferences. Generating preferences only requires a basic understanding of a task. Being a mental process, generating preferences is faster than performing demonstrations. So preferences can be collected at scale from non-expert humans using crowdsourcing. This paper tackles a critical challenge that emerged when collecting data from non-expert humans: the noise in preferences. A novel probabilistic model is proposed for modelling the reliability of labels, which utilizes labels collaboratively. Moreover, the proposed model smooths the estimation with a learned reward function. Evaluation on Atari datasets demonstrates the effectiveness of the proposed model, followed by an ablation study to analyze the relative importance of the proposed ideas.\n\n\nBatch Active Learning of Reward Functions from Human Preferences</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2111.04279v2</guid>
      <dc:creator>Guoxi Zhang, Hisashi Kashima</dc:creator>
      <pubDate>Tue, 29 Nov 2022 09:23:36 GMT</pubDate>
    </item>
    <item>
      <title>Exploring the Relationship between Alignment and Cross-lingual Transfer in Multilingual Transformers</title>
      <link>http://arxiv.org/abs/2306.02790v1</link>
      <description>Without any explicit cross-lingual training data, multilingual language models can achieve cross-lingual transfer. One common way to improve this transfer is to perform realignment steps before fine-tuning, i.e., to train the model to build similar representations for pairs of words from translated sentences. But such realignment methods were found to not always improve results across languages and tasks, which raises the question of whether aligned representations are truly beneficial for cross-lingual transfer. We provide evidence that alignment is actually significantly correlated with cross-lingual transfer across languages, models and random seeds. We show that fine-tuning can have a significant impact on alignment, depending mainly on the downstream task and the model. Finally, we show that realignment can, in some instances, improve cross-lingual transfer, and we identify conditions in which realignment methods provide significant improvements. Namely, we find that realignment works better on tasks for which alignment is correlated with cross-lingual transfer when generalizing to a distant language and with smaller models, as well as when using a bilingual dictionary rather than FastAlign to extract realignment pairs. For example, for POS-tagging, between English and Arabic, realignment can bring a +15.8 accuracy improvement on distilmBERT, even outperforming XLM-R Large by 1.7. We thus advocate for further research on realignment methods for smaller multilingual models as an alternative to scaling.\n\n\nDecoding-time Realignment of Language Models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2306.02790v1</guid>
      <dc:creator>Félix Gaschi, Patricio Cerda, Parisa Rastin, Yannick Toussaint</dc:creator>
      <pubDate>Mon, 05 Jun 2023 11:35:40 GMT</pubDate>
    </item>
    <item>
      <title>Participation in the age of foundation models</title>
      <link>http://arxiv.org/abs/2405.19479v1</link>
      <description>Growing interest and investment in the capabilities of foundation models has positioned such systems to impact a wide array of public services. Alongside these opportunities is the risk that these systems reify existing power imbalances and cause disproportionate harm to marginalized communities. Participatory approaches hold promise to instead lend agency and decision-making power to marginalized stakeholders. But existing approaches in participatory AI/ML are typically deeply grounded in context - how do we apply these approaches to foundation models, which are, by design, disconnected from context? Our paper interrogates this question.   First, we examine existing attempts at incorporating participation into foundation models. We highlight the tension between participation and scale, demonstrating that it is intractable for impacted communities to meaningfully shape a foundation model that is intended to be universally applicable. In response, we develop a blueprint for participatory foundation models that identifies more local, application-oriented opportunities for meaningful participation. In addition to the &quot;foundation&quot; layer, our framework proposes the &quot;subfloor'' layer, in which stakeholders develop shared technical infrastructure, norms and governance for a grounded domain, and the &quot;surface'' layer, in which affected communities shape the use of a foundation model for a specific downstream task. The intermediate &quot;subfloor'' layer scopes the range of potential harms to consider, and affords communities more concrete avenues for deliberation and intervention. At the same time, it avoids duplicative effort by scaling input across relevant use cases. Through three case studies in clinical care, financial services, and journalism, we illustrate how this multi-layer model can create more meaningful opportunities for participation than solely intervening at the foundation layer.\n\n\nParticipation in the age of foundation models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.19479v1</guid>
      <dc:creator>Harini Suresh, Emily Tseng, Meg Young, Mary L. Gray, Emma Pierson, Karen Levy</dc:creator>
      <pubDate>Wed, 29 May 2024 19:53:23 GMT</pubDate>
    </item>
    <item>
      <title>It HAS to be Subjective: Human Annotator Simulation via Zero-shot Density Estimation</title>
      <link>http://arxiv.org/abs/2310.00486v1</link>
      <description>Human annotator simulation (HAS) serves as a cost-effective substitute for human evaluation such as data annotation and system assessment. Human perception and behaviour during human evaluation exhibit inherent variability due to diverse cognitive processes and subjective interpretations, which should be taken into account in modelling to better mimic the way people perceive and interact with the world. This paper introduces a novel meta-learning framework that treats HAS as a zero-shot density estimation problem, which incorporates human variability and allows for the efficient generation of human-like annotations for unlabelled test inputs. Under this framework, we propose two new model classes, conditional integer flows and conditional softmax flows, to account for ordinal and categorical annotations, respectively. The proposed method is evaluated on three real-world human evaluation tasks and shows superior capability and efficiency to predict the aggregated behaviours of human annotators, match the distribution of human annotations, and simulate the inter-annotator disagreements.\n\n\nIt HAS to be Subjective: Human Annotator Simulation via Zero-shot Density Estimation</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2310.00486v1</guid>
      <dc:creator>Wen Wu, Wenlin Chen, Chao Zhang, Philip C. Woodland</dc:creator>
      <pubDate>Sat, 30 Sep 2023 20:54:59 GMT</pubDate>
    </item>
    <item>
      <title>Efficient and Effective Vocabulary Expansion Towards Multilingual Large Language Models</title>
      <link>http://arxiv.org/abs/2402.14714v1</link>
      <description>This report introduces \texttt{EEVE-Korean-v1.0}, a Korean adaptation of large language models that exhibit remarkable capabilities across English and Korean text understanding. Building on recent highly capable but English-centric LLMs, such as SOLAR-10.7B and Phi-2, where non-English texts are inefficiently processed with English-centric tokenizers, we present an efficient and effective vocabulary expansion (EEVE) method, which encompasses parameter freezing and subword initialization. In contrast to previous efforts that believe new embeddings require trillions of training tokens, we show that our method can significantly boost non-English proficiency within just 2 billion tokens. Surpassing most instruction-tuned LLMs on the Open Ko-LLM Leaderboard, as of January 2024, our model \texttt{EEVE-Korean-10.8B-v1.0} ranks as the leading Korean pre-trained model in the open-source community, according to Hugging Face's leaderboard. We open-source our models on Huggingface to empower the open research community in various languages.\n\n\nEfficient and effective vocabulary expansion towards multilingual large language models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.14714v1</guid>
      <dc:creator>Seungduk Kim, Seungtaek Choi, Myeongho Jeong</dc:creator>
      <pubDate>Thu, 22 Feb 2024 17:12:39 GMT</pubDate>
    </item>
    <item>
      <title>The Life Cycle of Large Language Models: A Review of Biases in Education</title>
      <link>http://arxiv.org/abs/2407.11203v1</link>
      <description>Large Language Models (LLMs) are increasingly adopted in educational contexts to provide personalized support to students and teachers. The unprecedented capacity of LLM-based applications to understand and generate natural language can potentially improve instructional effectiveness and learning outcomes, but the integration of LLMs in education technology has renewed concerns over algorithmic bias which may exacerbate educational inequities. In this review, building on prior work on mapping the traditional machine learning life cycle, we provide a holistic map of the LLM life cycle from the initial development of LLMs to customizing pre-trained models for various applications in educational settings. We explain each step in the LLM life cycle and identify potential sources of bias that may arise in the context of education. We discuss why current measures of bias from traditional machine learning fail to transfer to LLM-generated content in education, such as tutoring conversations because the text is high-dimensional, there can be multiple correct responses, and tailoring responses may be pedagogically desirable rather than unfair. This review aims to clarify the complex nature of bias in LLM applications and provide practical guidance for their evaluation to promote educational equity.\n\n\nThe life cycle of large language models in education: A framework for understanding sources of bias</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2407.11203v1</guid>
      <dc:creator>Jinsook Lee, Yann Hicke, Renzhe Yu, Christopher Brooks, René F. Kizilcec</dc:creator>
      <pubDate>Mon, 03 Jun 2024 18:00:28 GMT</pubDate>
    </item>
    <item>
      <title>Towards Safety and Helpfulness Balanced Responses via Controllable Large Language Models</title>
      <link>http://arxiv.org/abs/2404.01295v1</link>
      <description>As large language models (LLMs) become easily accessible nowadays, the trade-off between safety and helpfulness can significantly impact user experience. A model that prioritizes safety will cause users to feel less engaged and assisted while prioritizing helpfulness will potentially cause harm. Possible harms include teaching people how to build a bomb, exposing youth to inappropriate content, and hurting users' mental health. In this work, we propose to balance safety and helpfulness in diverse use cases by controlling both attributes in LLM. We explore training-free and fine-tuning methods that do not require extra human annotations and analyze the challenges of controlling safety and helpfulness in LLMs. Our experiments demonstrate that our method can rewind a learned model and unlock its controllability.\n\n\nTowards Safety and Helpfulness Balanced Responses via Controllable Large Language Models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2404.01295v1</guid>
      <dc:creator>Yi-Lin Tuan, Xilun Chen, Eric Michael Smith, Louis Martin, Soumya Batra, Asli Celikyilmaz, William Yang Wang, Daniel M. Bikel</dc:creator>
      <pubDate>Mon, 01 Apr 2024 17:59:06 GMT</pubDate>
    </item>
    <item>
      <title>Authorship Style Transfer with Policy Optimization</title>
      <link>http://arxiv.org/abs/2403.08043v1</link>
      <description>Authorship style transfer aims to rewrite a given text into a specified target while preserving the original meaning in the source. Existing approaches rely on the availability of a large number of target style exemplars for model training. However, these overlook cases where a limited number of target style examples are available. The development of parameter-efficient transfer learning techniques and policy optimization (PO) approaches suggest lightweight PO is a feasible approach to low-resource style transfer. In this work, we propose a simple two step tune-and-optimize technique for low-resource textual style transfer. We apply our technique to authorship transfer as well as a larger-data native language style task and in both cases find it outperforms state-of-the-art baseline models.\n\n\nAuthorship Style Transfer with Policy Optimization</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2403.08043v1</guid>
      <dc:creator>Shuai Liu, Shantanu Agarwal, Jonathan May</dc:creator>
      <pubDate>Tue, 12 Mar 2024 19:34:54 GMT</pubDate>
    </item>
    <item>
      <title>Aligners: Decoupling LLMs and Alignment</title>
      <link>http://arxiv.org/abs/2403.04224v3</link>
      <description>Large Language Models (LLMs) need to be aligned with human expectations to ensure their safety and utility in most applications. Alignment is challenging, costly, and needs to be repeated for every LLM and alignment criterion. We propose to decouple LLMs and alignment by training aligner models that can be used to align any LLM for a given criteria on an as-needed basis, thus also reducing the potential negative impacts of alignment on performance. Our recipe for training the aligner models solely relies on synthetic data generated with a (prompted) LLM and can be easily adjusted for a variety of alignment criteria. We use the same synthetic data to train inspectors, binary miss-alignment classification models to guide a &quot;squad&quot; of multiple aligners. Our empirical results demonstrate consistent improvements when applying aligner squad to various LLMs, including chat-aligned models, across several instruction-following and red-teaming datasets.\n\n\nAligning Large Language Models by On-Policy Self-Judgment</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2403.04224v3</guid>
      <dc:creator>Lilian Ngweta, Mayank Agarwal, Subha Maity, Alex Gittens, Yuekai Sun, Mikhail Yurochkin</dc:creator>
      <pubDate>Sun, 16 Jun 2024 15:59:11 GMT</pubDate>
    </item>
    <item>
      <title>Human Alignment of Large Language Models through Online Preference Optimisation</title>
      <link>http://arxiv.org/abs/2403.08635v1</link>
      <description>Ensuring alignment of language models' outputs with human preferences is critical to guarantee a useful, safe, and pleasant user experience. Thus, human alignment has been extensively studied recently and several methods such as Reinforcement Learning from Human Feedback (RLHF), Direct Policy Optimisation (DPO) and Sequence Likelihood Calibration (SLiC) have emerged. In this paper, our contribution is two-fold. First, we show the equivalence between two recent alignment methods, namely Identity Policy Optimisation (IPO) and Nash Mirror Descent (Nash-MD). Second, we introduce a generalisation of IPO, named IPO-MD, that leverages the regularised sampling approach proposed by Nash-MD.   This equivalence may seem surprising at first sight, since IPO is an offline method whereas Nash-MD is an online method using a preference model. However, this equivalence can be proven when we consider the online version of IPO, that is when both generations are sampled by the online policy and annotated by a trained preference model. Optimising the IPO loss with such a stream of data becomes then equivalent to finding the Nash equilibrium of the preference model through self-play. Building on this equivalence, we introduce the IPO-MD algorithm that generates data with a mixture policy (between the online and reference policy) similarly as the general Nash-MD algorithm. We compare online-IPO and IPO-MD to different online versions of existing losses on preference data such as DPO and SLiC on a summarisation task.\n\n\nHuman alignment of large language models through online preference optimisation</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2403.08635v1</guid>
      <dc:creator>Daniele Calandriello, Daniel Guo, Remi Munos, Mark Rowland, Yunhao Tang, Bernardo Avila Pires, Pierre Harvey Richemond, Charline Le Lan, Michal Valko, Tianqi Liu, Rishabh Joshi, Zeyu Zheng, Bilal Piot</dc:creator>
      <pubDate>Wed, 13 Mar 2024 15:47:26 GMT</pubDate>
    </item>
    <item>
      <title>Improving the Robustness of Large Language Models via Consistency Alignment</title>
      <link>http://arxiv.org/abs/2403.14221v2</link>
      <description>Large language models (LLMs) have shown tremendous success in following user instructions and generating helpful responses. Nevertheless, their robustness is still far from optimal, as they may generate significantly inconsistent responses due to minor changes in the verbalized instructions. Recent literature has explored this inconsistency issue, highlighting the importance of continued improvement in the robustness of response generation. However, systematic analysis and solutions are still lacking. In this paper, we quantitatively define the inconsistency problem and propose a two-stage training framework consisting of instruction-augmented supervised fine-tuning and consistency alignment training. The first stage helps a model generalize on following instructions via similar instruction augmentations. In the second stage, we improve the diversity and help the model understand which responses are more aligned with human expectations by differentiating subtle differences in similar responses. The training process is accomplished by self-rewards inferred from the trained model at the first stage without referring to external human preference resources. We conduct extensive experiments on recent publicly available LLMs on instruction-following tasks and demonstrate the effectiveness of our training framework.\n\n\nImproving the robustness of large language models via consistency alignment</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2403.14221v2</guid>
      <dc:creator>Yukun Zhao, Lingyong Yan, Weiwei Sun, Guoliang Xing, Shuaiqiang Wang, Chong Meng, Zhicong Cheng, Zhaochun Ren, Dawei Yin</dc:creator>
      <pubDate>Fri, 22 Mar 2024 12:34:47 GMT</pubDate>
    </item>
    <item>
      <title>Improving In-context Learning via Bidirectional Alignment</title>
      <link>http://arxiv.org/abs/2312.17055v2</link>
      <description>Large language models (LLMs) have shown impressive few-shot generalization on many tasks via in-context learning (ICL). Despite their success in showing such emergent abilities, the scale and complexity of larger models also lead to unprecedentedly high computational demands and deployment challenges. In reaction, researchers explore transferring the powerful capabilities of larger models to more efficient and compact models by typically aligning the output of smaller (student) models with that of larger (teacher) models. Existing methods either train student models on the generated outputs of teacher models or imitate their token-level probability distributions. However, these distillation methods pay little to no attention to the input, which also plays a crucial role in ICL. Based on the finding that the performance of ICL is highly sensitive to the selection of demonstration examples, we propose Bidirectional Alignment (BiAlign) to fully leverage the models' preferences for ICL examples to improve the ICL abilities of student models. Specifically, we introduce the alignment of input preferences between student and teacher models by incorporating a novel ranking loss, in addition to aligning the token-level output distribution. With extensive experiments and analysis, we demonstrate that BiAlign can consistently outperform existing baselines on a variety of tasks involving language understanding, reasoning, and coding.\n\n\nImproving in-context learning via bidirectional alignment</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2312.17055v2</guid>
      <dc:creator>Chengwei Qin, Wenhan Xia, Fangkai Jiao, Chen Chen, Yuchen Hu, Bosheng Ding, Shafiq Joty</dc:creator>
      <pubDate>Mon, 24 Jun 2024 08:34:18 GMT</pubDate>
    </item>
    <item>
      <title>Mixed Preference Optimization: Reinforcement Learning with Data Selection and Better Reference Model</title>
      <link>http://arxiv.org/abs/2403.19443v1</link>
      <description>Large Language Models (LLMs) have become increasingly popular due to their ability to process and generate natural language. However, as they are trained on massive datasets of text, LLMs can inherit harmful biases and produce outputs that are not aligned with human values. This paper studies two main approaches to LLM alignment: Reinforcement Learning with Human Feedback (RLHF) and contrastive learning-based methods like Direct Preference Optimization (DPO). By analyzing the stability and robustness of RLHF and DPO, we propose MPO (Mixed Preference Optimization), a novel method that mitigates the weaknesses of both approaches. Specifically, we propose a two-stage training procedure: first train DPO on an easy dataset, and then perform RLHF on a difficult set with DPO model being the reference model. Here, the easy and difficult sets are constructed by a well-trained reward model that splits response pairs into those with large gaps of reward (easy), and those with small gaps (difficult). The first stage allows us to obtain a relatively optimal policy (LLM) model quickly, whereas the second stage refines LLM with online RLHF, thus mitigating the distribution shift issue associated with DPO. Experiments are conducted on two public alignment datasets, namely HH-RLHF and TLDR, demonstrating the effectiveness of MPO, both in terms of GPT4 and human evaluation.\n\n\nMixed preference optimization: Reinforcement learning with data selection and better reference model</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2403.19443v1</guid>
      <dc:creator>Qi Gou, Cam-Tu Nguyen</dc:creator>
      <pubDate>Thu, 28 Mar 2024 14:15:10 GMT</pubDate>
    </item>
    <item>
      <title>Can Large Language Models be Good Emotional Supporter? Mitigating Preference Bias on Emotional Support Conversation</title>
      <link>http://arxiv.org/abs/2402.13211v2</link>
      <description>Emotional Support Conversation (ESC) is a task aimed at alleviating individuals' emotional distress through daily conversation. Given its inherent complexity and non-intuitive nature, ESConv dataset incorporates support strategies to facilitate the generation of appropriate responses. Recently, despite the remarkable conversational ability of large language models (LLMs), previous studies have suggested that they often struggle with providing useful emotional support. Hence, this work initially analyzes the results of LLMs on ESConv, revealing challenges in selecting the correct strategy and a notable preference for a specific strategy. Motivated by these, we explore the impact of the inherent preference in LLMs on providing emotional support, and consequently, we observe that exhibiting high preference for specific strategies hinders effective emotional support, aggravating its robustness in predicting the appropriate strategy. Moreover, we conduct a methodological study to offer insights into the necessary approaches for LLMs to serve as proficient emotional supporters. Our findings emphasize that (1) low preference for specific strategies hinders the progress of emotional support, (2) external assistance helps reduce preference bias, and (3) existing LLMs alone cannot become good emotional supporters. These insights suggest promising avenues for future research to enhance the emotional intelligence of LLMs.\n\n\nCan Large Language Models be Good Emotional Supporter? Mitigating Preference Bias on Emotional Support Conversation</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.13211v2</guid>
      <dc:creator>Dongjin Kang, Sunghwan Kim, Taeyoon Kwon, Seungjun Moon, Hyunsouk Cho, Youngjae Yu, Dongha Lee, Jinyoung Yeo</dc:creator>
      <pubDate>Wed, 05 Jun 2024 13:39:59 GMT</pubDate>
    </item>
    <item>
      <title>Prefix Text as a Yarn: Eliciting Non-English Alignment in Foundation Language Model</title>
      <link>http://arxiv.org/abs/2404.16766v1</link>
      <description>While supervised fine-tuning (SFT) has been a straightforward approach for tailoring the output of foundation large language model (LLM) to specific preferences, concerns have been raised about the depth of this alignment, with some critiques suggesting it is merely &quot;superficial&quot;. We critically examine this hypothesis within the scope of cross-lingual generation tasks, proposing that the effectiveness of SFT may be constrained by its reliance on prior tokens to guide cross-lingual generation. Based on this crucial insight, and in response to the challenges posed by the costly and limited availability of non-English data for SFT, we introduce a novel training-free alignment method named PreTTY, which employs minimal task-related prior tokens to bridge the foundation LLM and the SFT LLM, achieving comparable performance without training. Experiments on machine translation and part-of-speech tagging across eight languages demonstrate the efficacy of PreTTY in cross-lingual settings. Remarkably, by initiating the decoding process with only one or two prior tokens, foundation LLMs can achieve performance comparable to their SFT counterparts. This method presents a cost-effective alternative to SFT and advances the democratization of multilingual LLMs.\n\n\nPrefix Text as a Yarn: Eliciting Non-English Alignment in Foundation Language Model</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2404.16766v1</guid>
      <dc:creator>Runzhe Zhan, Xinyi Yang, Derek F. Wong, Lidia S. Chao, Yue Zhang</dc:creator>
      <pubDate>Thu, 25 Apr 2024 17:19:36 GMT</pubDate>
    </item>
    <item>
      <title>Compress to Impress: Unleashing the Potential of Compressive Memory in Real-World Long-Term Conversations</title>
      <link>http://arxiv.org/abs/2402.11975v2</link>
      <description>Existing retrieval-based methods have made significant strides in maintaining long-term conversations. However, these approaches face challenges in memory database management and accurate memory retrieval, hindering their efficacy in dynamic, real-world interactions. This study introduces a novel framework, COmpressive Memory-Enhanced Dialogue sYstems (COMEDY), which eschews traditional retrieval modules and memory databases. Instead, COMEDY adopts a &quot;One-for-All&quot; approach, utilizing a single language model to manage memory generation, compression, and response generation. Central to this framework is the concept of compressive memory, which intergrates session-specific summaries, user-bot dynamics, and past events into a concise memory format. To support COMEDY, we curated a large-scale Chinese instruction-tuning dataset, Dolphin, derived from real user-chatbot interactions. Comparative evaluations demonstrate COMEDY's superiority over traditional retrieval-based methods in producing more nuanced and human-like conversational experiences. Our codes are available at https://github.com/nuochenpku/COMEDY.\n\n\nCompress to impress: Unleashing the potential of compressive memory in real-world long-term conversations</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.11975v2</guid>
      <dc:creator>Nuo Chen, Hongguang Li, Juhua Huang, Baoyuan Wang, Jia Li</dc:creator>
      <pubDate>Mon, 01 Jul 2024 09:38:06 GMT</pubDate>
    </item>
    <item>
      <title>MoDiPO: text-to-motion alignment via AI-feedback-driven Direct Preference Optimization</title>
      <link>http://arxiv.org/abs/2405.03803v1</link>
      <description>Diffusion Models have revolutionized the field of human motion generation by offering exceptional generation quality and fine-grained controllability through natural language conditioning. Their inherent stochasticity, that is the ability to generate various outputs from a single input, is key to their success. However, this diversity should not be unrestricted, as it may lead to unlikely generations. Instead, it should be confined within the boundaries of text-aligned and realistic generations. To address this issue, we propose MoDiPO (Motion Diffusion DPO), a novel methodology that leverages Direct Preference Optimization (DPO) to align text-to-motion models. We streamline the laborious and expensive process of gathering human preferences needed in DPO by leveraging AI feedback instead. This enables us to experiment with novel DPO strategies, using both online and offline generated motion-preference pairs. To foster future research we contribute with a motion-preference dataset which we dub Pick-a-Move. We demonstrate, both qualitatively and quantitatively, that our proposed method yields significantly more realistic motions. In particular, MoDiPO substantially improves Frechet Inception Distance (FID) while retaining the same RPrecision and Multi-Modality performances.\n\n\nMoDiPO: text-to-motion alignment via AI-feedback-driven Direct Preference Optimization</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.03803v1</guid>
      <dc:creator>Massimiliano Pappa, Luca Collorone, Giovanni Ficarra, Indro Spinelli, Fabio Galasso</dc:creator>
      <pubDate>Mon, 06 May 2024 19:19:20 GMT</pubDate>
    </item>
    <item>
      <title>Submodular Participatory Budgeting</title>
      <link>http://arxiv.org/abs/2406.13586v1</link>
      <description>Participatory budgeting refers to the practice of allocating public resources by collecting and aggregating individual preferences. Most existing studies in this field often assume an additive utility function, where each individual holds a private utility for each candidate project, and the total utility of a set of funded projects is simply the sum of the utilities of all projects. We argue that this assumption does not always hold in reality. For example, building two playgrounds in the same neighborhood does not necessarily lead to twice the utility of building a single playground.   To address this, we extend the existing study by proposing a submodular participatory budgeting problem, assuming that the utility function of each individual is a monotone and submodular function over funded projects. We propose and examine three preference elicitation methods, including \emph{ranking-by-marginal-values}, \emph{ranking-by-values} and \emph{threshold approval votes}, and analyze their performances in terms of distortion. Notably, if the utility function is addicative, our aggregation rule designed for threshold approval votes achieves a better distortion than the state-of-the-art approach.\n\n\nParticipatory Objective Design via Preference Elicitation</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.13586v1</guid>
      <dc:creator>Jing Yuan, Shaojie Tang</dc:creator>
      <pubDate>Wed, 19 Jun 2024 14:22:54 GMT</pubDate>
    </item>
    <item>
      <title>Strengthened Symbol Binding Makes Large Language Models Reliable Multiple-Choice Selectors</title>
      <link>http://arxiv.org/abs/2406.01026v2</link>
      <description>Multiple-Choice Questions (MCQs) constitute a critical area of research in the study of Large Language Models (LLMs). Previous works have investigated the selection bias problem in MCQs within few-shot scenarios, in which the LLM's performance may be influenced by the presentation of answer choices, leaving the selection bias during Supervised Fine-Tuning (SFT) unexplored. In this paper, we reveal that selection bias persists in the SFT phase , primarily due to the LLM's inadequate Multiple Choice Symbol Binding (MCSB) ability. This limitation implies that the model struggles to associate the answer options with their corresponding symbols (e.g., A/B/C/D) effectively. To enhance the model's MCSB capability, we first incorporate option contents into the loss function and subsequently adjust the weights of the option symbols and contents, guiding the model to understand the option content of the current symbol. Based on this, we introduce an efficient SFT algorithm for MCQs, termed Point-wise Intelligent Feedback (PIF). PIF constructs negative instances by randomly combining the incorrect option contents with all candidate symbols, and proposes a point-wise loss to provide feedback on these negative samples into LLMs. Our experimental results demonstrate that PIF significantly reduces the model's selection bias by improving its MCSB capability. Remarkably, PIF exhibits a substantial enhancement in the accuracy for MCQs.\n\n\nStrengthened Symbol Binding Makes Large Language Models Reliable Multiple-Choice Selectors</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.01026v2</guid>
      <dc:creator>Mengge Xue, Zhenyu Hu, Liqun Liu, Kuo Liao, Shuang Li, Honglin Han, Meng Zhao, Chengguo Yin</dc:creator>
      <pubDate>Thu, 06 Jun 2024 06:32:45 GMT</pubDate>
    </item>
    <item>
      <title>MART: Improving LLM Safety with Multi-round Automatic Red-Teaming</title>
      <link>http://arxiv.org/abs/2311.07689v1</link>
      <description>Red-teaming is a common practice for mitigating unsafe behaviors in Large Language Models (LLMs), which involves thoroughly assessing LLMs to identify potential flaws and addressing them with responsible and accurate responses. While effective, manual red-teaming is costly, and existing automatic red-teaming typically discovers safety risks without addressing them. In this paper, we propose a Multi-round Automatic Red-Teaming (MART) method, which incorporates both automatic adversarial prompt writing and safe response generation, significantly increasing red-teaming scalability and the safety of the target LLM. Specifically, an adversarial LLM and a target LLM interplay with each other in an iterative manner, where the adversarial LLM aims to generate challenging prompts that elicit unsafe responses from the target LLM, while the target LLM is fine-tuned with safety aligned data on these adversarial prompts. In each round, the adversarial LLM crafts better attacks on the updated target LLM, while the target LLM also improves itself through safety fine-tuning. On adversarial prompt benchmarks, the violation rate of an LLM with limited safety alignment reduces up to 84.7% after 4 rounds of MART, achieving comparable performance to LLMs with extensive adversarial prompt writing. Notably, model helpfulness on non-adversarial prompts remains stable throughout iterations, indicating the target LLM maintains strong performance on instruction following.\n\n\nXwin-LM: Strong and Scalable Alignment Practice for LLMs</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2311.07689v1</guid>
      <dc:creator>Suyu Ge, Chunting Zhou, Rui Hou, Madian Khabsa, Yi-Chia Wang, Qifan Wang, Jiawei Han, Yuning Mao</dc:creator>
      <pubDate>Mon, 13 Nov 2023 19:13:29 GMT</pubDate>
    </item>
    <item>
      <title>Stepwise Alignment for Constrained Language Model Policy Optimization</title>
      <link>http://arxiv.org/abs/2404.11049v2</link>
      <description>Safety and trustworthiness are indispensable requirements for real-world applications of AI systems using large language models (LLMs). This paper formulates human value alignment as an optimization problem of the language model policy to maximize reward under a safety constraint, and then proposes an algorithm, Stepwise Alignment for Constrained Policy Optimization (SACPO). One key idea behind SACPO, supported by theory, is that the optimal policy incorporating reward and safety can be directly obtained from a reward-aligned policy. Building on this key idea, SACPO aligns LLMs step-wise with each metric while leveraging simple yet powerful alignment algorithms such as direct preference optimization (DPO). SACPO offers several advantages, including simplicity, stability, computational efficiency, and flexibility of algorithms and datasets. Under mild assumptions, our theoretical analysis provides the upper bounds on optimality and safety constraint violation. Our experimental results show that SACPO can fine-tune Alpaca-7B better than the state-of-the-art method in terms of both helpfulness and harmlessness.\n\n\nStepwise Alignment for Constrained Language Model Policy Optimization</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2404.11049v2</guid>
      <dc:creator>Akifumi Wachi, Thien Q. Tran, Rei Sato, Takumi Tanabe, Youhei Akimoto</dc:creator>
      <pubDate>Thu, 23 May 2024 01:13:41 GMT</pubDate>
    </item>
    <item>
      <title>Reward Model Learning vs. Direct Policy Optimization: A Comparative Analysis of Learning from Human Preferences</title>
      <link>http://arxiv.org/abs/2403.01857v2</link>
      <description>In this paper, we take a step towards a deeper understanding of learning from human preferences by systematically comparing the paradigm of reinforcement learning from human feedback (RLHF) with the recently proposed paradigm of direct preference optimization (DPO). We focus our attention on the class of loglinear policy parametrization and linear reward functions. In order to compare the two paradigms, we first derive minimax statistical bounds on the suboptimality gap induced by both RLHF and DPO, assuming access to an oracle that exactly solves the optimization problems. We provide a detailed discussion on the relative comparison between the two paradigms, simultaneously taking into account the sample size, policy and reward class dimensions, and the regularization temperature. Moreover, we extend our analysis to the approximate optimization setting and derive exponentially decaying convergence rates for both RLHF and DPO. Next, we analyze the setting where the ground-truth reward is not realizable and find that, while RLHF incurs a constant additional error, DPO retains its asymptotically decaying gap by just tuning the temperature accordingly. Finally, we extend our comparison to the Markov decision process setting, where we generalize our results with exact optimization. To the best of our knowledge, we are the first to provide such a comparative analysis for RLHF and DPO.\n\n\nReward Model Learning vs. Direct Policy Optimization: A Comparative Analysis of Learning from Human Preferences</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2403.01857v2</guid>
      <dc:creator>Andi Nika, Debmalya Mandal, Parameswaran Kamalaruban, Georgios Tzannetos, Goran Radanović, Adish Singla</dc:creator>
      <pubDate>Wed, 05 Jun 2024 09:00:36 GMT</pubDate>
    </item>
    <item>
      <title>DialogGen: Multi-modal Interactive Dialogue System for Multi-turn Text-to-Image Generation</title>
      <link>http://arxiv.org/abs/2403.08857v2</link>
      <description>Text-to-image (T2I) generation models have significantly advanced in recent years. However, effective interaction with these models is challenging for average users due to the need for specialized prompt engineering knowledge and the inability to perform multi-turn image generation, hindering a dynamic and iterative creation process. Recent attempts have tried to equip Multi-modal Large Language Models (MLLMs) with T2I models to bring the user's natural language instructions into reality. Hence, the output modality of MLLMs is extended, and the multi-turn generation quality of T2I models is enhanced thanks to the strong multi-modal comprehension ability of MLLMs. However, many of these works face challenges in identifying correct output modalities and generating coherent images accordingly as the number of output modalities increases and the conversations go deeper. Therefore, we propose DialogGen, an effective pipeline to align off-the-shelf MLLMs and T2I models to build a Multi-modal Interactive Dialogue System (MIDS) for multi-turn Text-to-Image generation. It is composed of drawing prompt alignment, careful training data curation, and error correction. Moreover, as the field of MIDS flourishes, comprehensive benchmarks are urgently needed to evaluate MIDS fairly in terms of output modality correctness and multi-modal output coherence. To address this issue, we introduce the Multi-modal Dialogue Benchmark (DialogBen), a comprehensive bilingual benchmark designed to assess the ability of MLLMs to generate accurate and coherent multi-modal content that supports image editing. It contains two evaluation metrics to measure the model's ability to switch modalities and the coherence of the output images. Our extensive experiments on DialogBen and user study demonstrate the effectiveness of DialogGen compared with other State-of-the-Art models.\n\n\nDialogGen: Multi-modal Interactive Dialogue System for Multi-turn Text-to-Image Generation</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2403.08857v2</guid>
      <dc:creator>Minbin Huang, Yanxin Long, Xinchi Deng, Ruihang Chu, Jiangfeng Xiong, Xiaodan Liang, Hong Cheng, Qinglin Lu, Wei Liu</dc:creator>
      <pubDate>Wed, 03 Jul 2024 13:09:53 GMT</pubDate>
    </item>
    <item>
      <title>Aligning Crowd Feedback via Distributional Preference Reward Modeling</title>
      <link>http://arxiv.org/abs/2402.09764v3</link>
      <description>Deep Reinforcement Learning is widely used for aligning Large Language Models (LLM) with human preference. However, the conventional reward modelling is predominantly dependent on human annotations provided by a select cohort of individuals. Such dependence may unintentionally result in skewed models that reflect the inclinations of these annotators, thereby failing to adequately represent the wider population's expectations. We propose the Distributional Preference Reward Model (DPRM), a simple yet effective framework to align large language models with diverse human preferences. To this end, we characterize multiple preferences by a categorical distribution and introduce a Bayesian updater to accommodate shifted or new preferences. On top of that, we design an optimal-transportation-based loss to calibrate DPRM to align with the preference distribution. Finally, the expected reward is utilized to fine-tune an LLM policy to generate responses favoured by the population. Our experiments show that DPRM significantly enhances the alignment of LLMs with population preference, yielding more accurate, unbiased, and contextually appropriate responses.\n\n\nAligning crowd feedback via distributional preference reward modeling</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.09764v3</guid>
      <dc:creator>Dexun Li, Cong Zhang, Kuicai Dong, Derrick Goh Xin Deik, Ruiming Tang, Yong Liu</dc:creator>
      <pubDate>Thu, 30 May 2024 15:39:17 GMT</pubDate>
    </item>
    <item>
      <title>Antigen-Specific Antibody Design via Direct Energy-based Preference Optimization</title>
      <link>http://arxiv.org/abs/2403.16576v2</link>
      <description>Antibody design, a crucial task with significant implications across various disciplines such as therapeutics and biology, presents considerable challenges due to its intricate nature. In this paper, we tackle antigen-specific antibody sequence-structure co-design as an optimization problem towards specific preferences, considering both rationality and functionality. Leveraging a pre-trained conditional diffusion model that jointly models sequences and structures of antibodies with equivariant neural networks, we propose direct energy-based preference optimization to guide the generation of antibodies with both rational structures and considerable binding affinities to given antigens. Our method involves fine-tuning the pre-trained diffusion model using a residue-level decomposed energy preference. Additionally, we employ gradient surgery to address conflicts between various types of energy, such as attraction and repulsion. Experiments on RAbD benchmark show that our approach effectively optimizes the energy of generated antibodies and achieves state-of-the-art performance in designing high-quality antibodies with low total energy and high binding affinity simultaneously, demonstrating the superiority of our approach.\n\n\nAntigen-Specific Antibody Design via Direct Energy-based Preference Optimization</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2403.16576v2</guid>
      <dc:creator>Xiangxin Zhou, Dongyu Xue, Ruizhe Chen, Zaixiang Zheng, Liang Wang, Quanquan Gu</dc:creator>
      <pubDate>Wed, 26 Jun 2024 03:06:42 GMT</pubDate>
    </item>
    <item>
      <title>Benchmarking LLM-based Machine Translation on Cultural Awareness</title>
      <link>http://arxiv.org/abs/2305.14328v2</link>
      <description>Translating cultural-specific content is crucial for effective cross-cultural communication. However, many MT systems still struggle to translate sentences containing cultural-specific entities accurately and understandably. Recent advancements in in-context learning utilize lightweight prompts to guide large language models (LLMs) in machine translation tasks. Nevertheless, the effectiveness of this approach in enhancing machine translation with cultural awareness remains uncertain. To address this gap, we introduce a new data curation pipeline to construct a culturally relevant parallel corpus, enriched with annotations of cultural-specific items. Furthermore, we devise a novel evaluation metric to assess the understandability of translations in a reference-free manner by GPT-4. We evaluate a variety of neural machine translation (NMT) and LLM-based MT systems using our dataset. Additionally, we propose several prompting strategies for LLMs to incorporate external and internal cultural knowledge into the translation process. Our results demonstrate that eliciting explanations can significantly enhance the understandability of cultural-specific entities, especially those without well-known translations.\n\n\nBenchmarking llm-based machine translation on cultural awareness</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2305.14328v2</guid>
      <dc:creator>Binwei Yao, Ming Jiang, Diyi Yang, Junjie Hu</dc:creator>
      <pubDate>Sat, 23 Mar 2024 02:20:02 GMT</pubDate>
    </item>
    <item>
      <title>MetaRM: Shifted Distributions Alignment via Meta-Learning</title>
      <link>http://arxiv.org/abs/2405.00438v1</link>
      <description>The success of Reinforcement Learning from Human Feedback (RLHF) in language model alignment is critically dependent on the capability of the reward model (RM). However, as the training process progresses, the output distribution of the policy model shifts, leading to the RM's reduced ability to distinguish between responses. This issue is further compounded when the RM, trained on a specific data distribution, struggles to generalize to examples outside of that distribution. These two issues can be united as a challenge posed by the shifted distribution of the environment. To surmount this challenge, we introduce MetaRM, a method leveraging meta-learning to align the RM with the shifted environment distribution. MetaRM is designed to train the RM by minimizing data loss, particularly for data that can improve the differentiation ability to examples of the shifted target distribution. Extensive experiments demonstrate that MetaRM significantly improves the RM's distinguishing ability in iterative RLHF optimization, and also provides the capacity to identify subtle differences in out-of-distribution samples.\n\n\nMetaRM: Shifted Distributions Alignment via Meta-Learning</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.00438v1</guid>
      <dc:creator>Shihan Dou, Yan Liu, Enyu Zhou, Tianlong Li, Haoxiang Jia, Limao Xiong, Xin Zhao, Junjie Ye, Rui Zheng, Tao Gui, Qi Zhang, Xuanjing Huang</dc:creator>
      <pubDate>Wed, 01 May 2024 10:43:55 GMT</pubDate>
    </item>
    <item>
      <title>Investigating Regularization of Self-Play Language Models</title>
      <link>http://arxiv.org/abs/2404.04291v1</link>
      <description>This paper explores the effects of various forms of regularization in the context of language model alignment via self-play. While both reinforcement learning from human feedback (RLHF) and direct preference optimization (DPO) require to collect costly human-annotated pairwise preferences, the self-play fine-tuning (SPIN) approach replaces the rejected answers by data generated from the previous iterate. However, the SPIN method presents a performance instability issue in the learning phase, which can be mitigated by playing against a mixture of the two previous iterates. In the same vein, we propose in this work to address this issue from two perspectives: first, by incorporating an additional Kullback-Leibler (KL) regularization to stay at the proximity of the reference policy; second, by using the idea of fictitious play which smoothens the opponent policy across all previous iterations. In particular, we show that the KL-based regularizer boils down to replacing the previous policy by its geometric mixture with the base policy inside of the SPIN loss function. We finally discuss empirical results on MT-Bench as well as on the Hugging Face Open LLM Leaderboard.\n\n\nInvestigating Regularization of Self-Play Language Models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2404.04291v1</guid>
      <dc:creator>Reda Alami, Abdalgader Abubaker, Mastane Achab, Mohamed El Amine Seddik, Salem Lahlou</dc:creator>
      <pubDate>Thu, 04 Apr 2024 05:38:44 GMT</pubDate>
    </item>
    <item>
      <title>Agent Alignment in Evolving Social Norms</title>
      <link>http://arxiv.org/abs/2401.04620v4</link>
      <description>Agents based on Large Language Models (LLMs) are increasingly permeating various domains of human production and life, highlighting the importance of aligning them with human values. The current alignment of AI systems primarily focuses on passively aligning LLMs through human intervention. However, agents possess characteristics like receiving environmental feedback and self-evolution, rendering the LLM alignment methods inadequate. In response, we propose an evolutionary framework for agent evolution and alignment, named EvolutionaryAgent, which transforms agent alignment into a process of evolution and selection under the principle of survival of the fittest. In an environment where social norms continuously evolve, agents better adapted to the current social norms will have a higher probability of survival and proliferation, while those inadequately aligned dwindle over time. Experimental results assessing the agents from multiple perspectives in aligning with social norms demonstrate that EvolutionaryAgent can align progressively better with the evolving social norms while maintaining its proficiency in general tasks. Effectiveness tests conducted on various open and closed-source LLMs as the foundation for agents also prove the applicability of our approach.\n\n\nAgent Alignment in Evolving Social Norms</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2401.04620v4</guid>
      <dc:creator>Shimin Li, Tianxiang Sun, Qinyuan Cheng, Xipeng Qiu</dc:creator>
      <pubDate>Tue, 20 Feb 2024 03:24:55 GMT</pubDate>
    </item>
    <item>
      <title>Oil &amp; Water? Diffusion of AI Within and Across Scientific Fields</title>
      <link>http://arxiv.org/abs/2405.15828v1</link>
      <description>This study empirically investigates claims of the increasing ubiquity of artificial intelligence (AI) within roughly 80 million research publications across 20 diverse scientific fields, by examining the change in scholarly engagement with AI from 1985 through 2022. We observe exponential growth, with AI-engaged publications increasing approximately thirteenfold (13x) across all fields, suggesting a dramatic shift from niche to mainstream. Moreover, we provide the first empirical examination of the distribution of AI-engaged publications across publication venues within individual fields, with results that reveal a broadening of AI engagement within disciplines. While this broadening engagement suggests a move toward greater disciplinary integration in every field, increased ubiquity is associated with a semantic tension between AI-engaged research and more traditional disciplinary research. Through an analysis of tens of millions of document embeddings, we observe a complex interplay between AI-engaged and non-AI-engaged research within and across fields, suggesting that increasing ubiquity is something of an oil-and-water phenomenon -- AI-engaged work is spreading out over fields, but not mixing well with non-AI-engaged work.\n\n\nOil &amp; water? diffusion of ai within and across scientific fields</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.15828v1</guid>
      <dc:creator>Eamon Duede, William Dolan, André Bauer, Ian Foster, Karim Lakhani</dc:creator>
      <pubDate>Fri, 24 May 2024 00:39:32 GMT</pubDate>
    </item>
    <item>
      <title>Rethinking LLM Language Adaptation: A Case Study on Chinese Mixtral</title>
      <link>http://arxiv.org/abs/2403.01851v1</link>
      <description>Mixtral, a representative sparse mixture of experts (SMoE) language model, has received significant attention due to its unique model design and superior performance. Based on Mixtral-8x7B-v0.1, in this paper, we propose Chinese-Mixtral and Chinese-Mixtral-Instruct with improved Chinese language abilities by adopting further pre-training and instruction fine-tuning. Experimental results show that our Chinese-Mixtral and Chinese-Mixtral-Instruct successfully improve Chinese understanding and generation performance while retaining the original English abilities. Then, we discuss several key questions when performing language adaptation on large language models, including the necessity of extending the language-specific vocabulary and the choice of the initialization model (foundation model v.s. instruction model), by providing empirical results and analysis. We also present the visualizations of each expert to examine their importance on downstream tasks. Our resources are publicly available through \url{https://github.com/ymcui/Chinese-Mixtral}.\n\n\nRethinking LLM Language Adaptation: A Case Study on Chinese Mixtral</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2403.01851v1</guid>
      <dc:creator>Yiming Cui, Xin Yao</dc:creator>
      <pubDate>Mon, 04 Mar 2024 09:01:10 GMT</pubDate>
    </item>
    <item>
      <title>BRAIn: Bayesian Reward-conditioned Amortized Inference for natural language generation from feedback</title>
      <link>http://arxiv.org/abs/2402.02479v2</link>
      <description>Distribution matching methods for language model alignment such as Generation with Distributional Control (GDC) and Distributional Policy Gradient (DPG) have not received the same level of attention in reinforcement learning from human feedback (RLHF) as contrastive methods such as Sequence Likelihood Calibration (SLiC), Direct Preference Optimization (DPO) and its variants. We identify high variance of the gradient estimate as the primary reason for the lack of success of these methods and propose a self-normalized baseline to reduce the variance. We further generalize the target distribution in DPG, GDC and DPO by using Bayes' rule to define the reward-conditioned posterior. The resulting approach, referred to as BRAIn - Bayesian Reward-conditioned Amortized Inference acts as a bridge between distribution matching methods and DPO and significantly outperforms prior art in summarization and Antropic HH tasks.\n\n\nBRAIn: Bayesian Reward-conditioned Amortized Inference for natural language generation from feedback</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.02479v2</guid>
      <dc:creator>Gaurav Pandey, Yatin Nandwani, Tahira Naseem, Mayank Mishra, Guangxuan Xu, Dinesh Raghu, Sachindra Joshi, Asim Munawar, Ramón Fernandez Astudillo</dc:creator>
      <pubDate>Mon, 10 Jun 2024 10:18:46 GMT</pubDate>
    </item>
    <item>
      <title>FeB4RAG: Evaluating Federated Search in the Context of Retrieval Augmented Generation</title>
      <link>http://arxiv.org/abs/2402.11891v1</link>
      <description>Federated search systems aggregate results from multiple search engines, selecting appropriate sources to enhance result quality and align with user intent. With the increasing uptake of Retrieval-Augmented Generation (RAG) pipelines, federated search can play a pivotal role in sourcing relevant information across heterogeneous data sources to generate informed responses. However, existing datasets, such as those developed in the past TREC FedWeb tracks, predate the RAG paradigm shift and lack representation of modern information retrieval challenges. To bridge this gap, we present FeB4RAG, a novel dataset specifically designed for federated search within RAG frameworks. This dataset, derived from 16 sub-collections of the widely used \beir benchmarking collection, includes 790 information requests (akin to conversational queries) tailored for chatbot applications, along with top results returned by each resource and associated LLM-derived relevance judgements. Additionally, to support the need for this collection, we demonstrate the impact on response generation of a high quality federated search system for RAG compared to a naive approach to federated search. We do so by comparing answers generated through the RAG pipeline through a qualitative side-by-side comparison. Our collection fosters and supports the development and evaluation of new federated search methods, especially in the context of RAG pipelines.\n\n\nFeb4rag: Evaluating federated search in the context of retrieval augmented generation</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.11891v1</guid>
      <dc:creator>Shuai Wang, Ekaterina Khramtsova, Shengyao Zhuang, Guido Zuccon</dc:creator>
      <pubDate>Mon, 19 Feb 2024 07:06:52 GMT</pubDate>
    </item>
    <item>
      <title>Understanding Sinusoidal Neural Networks</title>
      <link>http://arxiv.org/abs/2212.01833v2</link>
      <description>In this work, we investigate the structure and representation capacity of sinusoidal MLPs - multilayer perceptron networks that use sine as the activation function. These neural networks (known as neural fields) have become fundamental in representing common signals in computer graphics, such as images, signed distance functions, and radiance fields. This success can be primarily attributed to two key properties of sinusoidal MLPs: smoothness and compactness. These functions are smooth because they arise from the composition of affine maps with the sine function. This work provides theoretical results to justify the compactness property of sinusoidal MLPs and provides control mechanisms in the definition and training of these networks.   We propose to study a sinusoidal MLP by expanding it as a harmonic sum. First, we observe that its first layer can be seen as a harmonic dictionary, which we call the input sinusoidal neurons. Then, a hidden layer combines this dictionary using an affine map and modulates the outputs using the sine, this results in a special dictionary of sinusoidal neurons. We prove that each of these sinusoidal neurons expands as a harmonic sum producing a large number of new frequencies expressed as integer linear combinations of the input frequencies. Thus, each hidden neuron produces the same frequencies, and the corresponding amplitudes are completely determined by the hidden affine map. We also provide an upper bound and a way of sorting these amplitudes that can control the resulting approximation, allowing us to truncate the corresponding series. Finally, we present applications for training and initialization of sinusoidal MLPs. Additionally, we show that if the input neurons are periodic, then the entire network will be periodic with the same period. We relate these periodic networks with the Fourier series representation.\n\n\nUnderstanding and Controlling a Maze-Solving Policy Network</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2212.01833v2</guid>
      <dc:creator>Tiago Novello</dc:creator>
      <pubDate>Mon, 11 Sep 2023 17:02:33 GMT</pubDate>
    </item>
    <item>
      <title>Genetic-guided GFlowNets for Sample Efficient Molecular Optimization</title>
      <link>http://arxiv.org/abs/2402.05961v2</link>
      <description>The challenge of discovering new molecules with desired properties is crucial in domains like drug discovery and material design. Recent advances in deep learning-based generative methods have shown promise but face the issue of sample efficiency due to the computational expense of evaluating the reward function. This paper proposes a novel algorithm for sample-efficient molecular optimization by distilling a powerful genetic algorithm into deep generative policy using GFlowNets training, the off-policy method for amortized inference. This approach enables the deep generative policy to learn from domain knowledge, which has been explicitly integrated into the genetic algorithm. Our method achieves state-of-the-art performance in the official molecular optimization benchmark, significantly outperforming previous methods. It also demonstrates effectiveness in designing inhibitors against SARS-CoV-2 with substantially fewer reward calls.\n\n\nGenetic-guided GFlowNets: Advancing in Practical Molecular Optimization Benchmark</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.05961v2</guid>
      <dc:creator>Hyeonah Kim, Minsu Kim, Sanghyeok Choi, Jinkyoo Park</dc:creator>
      <pubDate>Sat, 25 May 2024 14:20:39 GMT</pubDate>
    </item>
    <item>
      <title>Predicting Text Preference Via Structured Comparative Reasoning</title>
      <link>http://arxiv.org/abs/2311.08390v2</link>
      <description>Comparative reasoning plays a crucial role in text preference prediction; however, large language models (LLMs) often demonstrate inconsistencies in their reasoning. While approaches like Chain-of-Thought improve accuracy in many other settings, they struggle to consistently distinguish the similarities and differences of complex texts. We introduce SC, a prompting approach that predicts text preferences by generating structured intermediate comparisons. SC begins by proposing aspects of comparison, followed by generating textual comparisons under each aspect. We select consistent comparisons with a pairwise consistency comparator that ensures each aspect's comparisons clearly distinguish differences between texts, significantly reducing hallucination and improving consistency. Our comprehensive evaluations across various NLP tasks, including summarization, retrieval, and automatic rating, demonstrate that SC equips LLMs to achieve state-of-the-art performance in text preference prediction.\n\n\nOn what basis? predicting text preference via structured comparative reasoning</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2311.08390v2</guid>
      <dc:creator>Jing Nathan Yan, Tianqi Liu, Justin T Chiu, Jiaming Shen, Zhen Qin, Yue Yu, Yao Zhao, Charu Lakshmanan, Yair Kurzion, Alexander M. Rush, Jialu Liu, Michael Bendersky</dc:creator>
      <pubDate>Mon, 01 Jul 2024 16:57:56 GMT</pubDate>
    </item>
    <item>
      <title>Learning Dynamics of LLM Finetuning</title>
      <link>http://arxiv.org/abs/2407.10490v1</link>
      <description>Learning dynamics, which describes how the learning of specific training examples influences the model's prediction of other examples, give us a powerful tool for understanding the behavior of deep learning systems. We study the learning dynamics of large language models during finetuning, by analyzing the step-wise decomposition and accumulated influence among different responses. Our framework allows a uniform interpretation of many interesting observations about the training of popular algorithms for both instruction tuning and preference tuning. The analysis not only explains where the benefits of these methods come from but also inspires a simple, effective method to further improve the alignment performance. Code for experiments is available at https://github.com/Joshua-Ren/Learning_dynamics_LLM.\n\n\nFine-tuning language models using formal methods feedback</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2407.10490v1</guid>
      <dc:creator>Yi Ren, Danica J. Sutherland</dc:creator>
      <pubDate>Mon, 15 Jul 2024 07:30:28 GMT</pubDate>
    </item>
    <item>
      <title>Humans are Still Better than ChatGPT: Case of the IEEEXtreme Competition</title>
      <link>http://arxiv.org/abs/2305.06934v1</link>
      <description>Since the release of ChatGPT, numerous studies have highlighted the remarkable performance of ChatGPT, which often rivals or even surpasses human capabilities in various tasks and domains. However, this paper presents a contrasting perspective by demonstrating an instance where human performance excels in typical tasks suited for ChatGPT, specifically in the domain of computer programming. We utilize the IEEExtreme Challenge competition as a benchmark, a prestigious, annual international programming contest encompassing a wide range of problems with different complexities. To conduct a thorough evaluation, we selected and executed a diverse set of 102 challenges, drawn from five distinct IEEExtreme editions, using three major programming languages: Python, Java, and C++. Our empirical analysis provides evidence that contrary to popular belief, human programmers maintain a competitive edge over ChatGPT in certain aspects of problem-solving within the programming context. In fact, we found that the average score obtained by ChatGPT on the set of IEEExtreme programming problems is 3.9 to 5.8 times lower than the average human score, depending on the programming language. This paper elaborates on these findings, offering critical insights into the limitations and potential areas of improvement for AI-based language models like ChatGPT.\n\n\nLaboratory-Scale AI: Open-Weight Models are Competitive with ChatGPT Even in Low-Resource Settings</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2305.06934v1</guid>
      <dc:creator>Anis Koubaa, Basit Qureshi, Adel Ammar, Zahid Khan, Wadii Boulila, Lahouari Ghouti</dc:creator>
      <pubDate>Wed, 10 May 2023 08:16:46 GMT</pubDate>
    </item>
    <item>
      <title>Attention is Naturally Sparse with Gaussian Distributed Input</title>
      <link>http://arxiv.org/abs/2404.02690v1</link>
      <description>The computational intensity of Large Language Models (LLMs) is a critical bottleneck, primarily due to the $O(n^2)$ complexity of the attention mechanism in transformer architectures. Addressing this, sparse attention emerges as a key innovation, aiming to reduce computational load while maintaining model performance. This study presents a rigorous theoretical analysis of the sparsity in attention scores within LLMs, particularly under the framework of Gaussian inputs. By establishing a set of foundational assumptions and employing a methodical theoretical approach, we unravel the intrinsic characteristics of attention score sparsity and its implications on computational efficiency. Our main contribution lies in providing a detailed theoretical examination of how sparsity manifests in attention mechanisms, offering insights into the potential trade-offs between computational savings and model effectiveness. This work not only advances our understanding of sparse attention but also provides a scaffold for future research in optimizing the computational frameworks of LLMs, paving the way for more scalable and efficient AI systems.\n\n\nAttention is Naturally Sparse with Gaussian Distributed Input</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2404.02690v1</guid>
      <dc:creator>Yichuan Deng, Zhao Song, Chiwun Yang</dc:creator>
      <pubDate>Wed, 03 Apr 2024 12:37:34 GMT</pubDate>
    </item>
    <item>
      <title>A density estimation perspective on learning from pairwise human preferences</title>
      <link>http://arxiv.org/abs/2311.14115v3</link>
      <description>Learning from human feedback (LHF) -- and in particular learning from pairwise preferences -- has recently become a crucial ingredient in training large language models (LLMs), and has been the subject of much research. Most recent works frame it as a reinforcement learning problem, where a reward function is learned from pairwise preference data and the LLM is treated as a policy which is adapted to maximize the rewards, often under additional regularization constraints. We propose an alternative interpretation which centers on the generative process for pairwise preferences and treats LHF as a density estimation problem. We provide theoretical and empirical results showing that for a family of generative processes defined via preference behavior distribution equations, training a reward function on pairwise preferences effectively models an annotator's implicit preference distribution. Finally, we discuss and present findings on &quot;annotator misspecification&quot; -- failure cases where wrong modeling assumptions are made about annotator behavior, resulting in poorly-adapted models -- suggesting that approaches that learn from pairwise human preferences could have trouble learning from a population of annotators with diverse viewpoints.\n\n\nA density estimation perspective on learning from pairwise human preferences</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2311.14115v3</guid>
      <dc:creator>Vincent Dumoulin, Daniel D. Johnson, Pablo Samuel Castro, Hugo Larochelle, Yann Dauphin</dc:creator>
      <pubDate>Wed, 10 Jan 2024 16:11:32 GMT</pubDate>
    </item>
    <item>
      <title>Making Reasoning Matter: Measuring and Improving Faithfulness of Chain-of-Thought Reasoning</title>
      <link>http://arxiv.org/abs/2402.13950v2</link>
      <description>Large language models (LLMs) have been shown to perform better when asked to reason step-by-step before answering a question. However, it is unclear to what degree the model's final answer is faithful to the stated reasoning steps. In this paper, we perform a causal mediation analysis on twelve LLMs to examine how intermediate reasoning steps generated by the LLM influence the final outcome and find that LLMs do not reliably use their intermediate reasoning steps when generating an answer. To address this issue, we introduce FRODO, a framework to tailor small-sized LMs to generate correct reasoning steps and robustly reason over these steps. FRODO consists of an inference module that learns to generate correct reasoning steps using an implicit causal reward function and a reasoning module that learns to faithfully reason over these intermediate inferences using a counterfactual and causal preference objective. Our experiments show that FRODO significantly outperforms four competitive baselines. Furthermore, FRODO improves the robustness and generalization ability of the reasoning LM, yielding higher performance on out-of-distribution test sets. Finally, we find that FRODO's rationales are more faithful to its final answer predictions than standard supervised fine-tuning.\n\n\nMaking Reasoning Matter: Measuring and Improving Faithfulness of Chain-of-Thought Reasoning</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.13950v2</guid>
      <dc:creator>Debjit Paul, Robert West, Antoine Bosselut, Boi Faltings</dc:creator>
      <pubDate>Fri, 23 Feb 2024 18:01:48 GMT</pubDate>
    </item>
    <item>
      <title>Online Personalizing White-box LLMs Generation with Neural Bandits</title>
      <link>http://arxiv.org/abs/2404.16115v1</link>
      <description>The advent of personalized content generation by LLMs presents a novel challenge: how to efficiently adapt text to meet individual preferences without the unsustainable demand of creating a unique model for each user. This study introduces an innovative online method that employs neural bandit algorithms to dynamically optimize soft instruction embeddings based on user feedback, enhancing the personalization of open-ended text generation by white-box LLMs. Through rigorous experimentation on various tasks, we demonstrate significant performance improvements over baseline strategies. NeuralTS, in particular, leads to substantial enhancements in personalized news headline generation, achieving up to a 62.9% improvement in terms of best ROUGE scores and up to 2.76% increase in LLM-agent evaluation against the baseline.\n\n\nOnline personalizing white-box llms generation with neural bandits</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2404.16115v1</guid>
      <dc:creator>Zekai Chen, Weeden Daniel, Po-yu Chen, Francois Buet-Golfouse</dc:creator>
      <pubDate>Wed, 24 Apr 2024 18:13:12 GMT</pubDate>
    </item>
    <item>
      <title>Autonomous Data Selection with Language Models for Mathematical Texts</title>
      <link>http://arxiv.org/abs/2402.07625v2</link>
      <description>To improve language models' proficiency in mathematical reasoning via continual pretraining, we introduce a novel strategy that leverages base language models for autonomous data selection. Departing from conventional supervised fine-tuning or trained classifiers with human-annotated data, our approach Autonomous Data Selection (AutoDS) utilizes meta-prompted language models as zero-shot verifiers to evaluate and select high-quality mathematical content autonomously. To demonstrate the efficacy of our method, we continuously pretrained a 7B-parameter language model on our curated dataset, achieving substantial improvements in downstream performance on the MATH, GSM8K, and BIG-Bench Hard (BBH) tasks with a token amount reduced by orders of magnitude compared to previous continual pretraining works. Our method showcases a 2 times increase in pretraining token efficiency compared to state-of-the-art baselines, underscoring the potential of our approach in enhancing models' mathematical reasoning capabilities. The AutoMathText dataset is available at https://huggingface.co/datasets/math-ai/AutoMathText. The code is available at https://github.com/yifanzhang-pro/AutoMathText.\n\n\nAutonomous data selection with language models for mathematical texts</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.07625v2</guid>
      <dc:creator>Yifan Zhang, Yifan Luo, Yang Yuan, Andrew Chi-Chih Yao</dc:creator>
      <pubDate>Tue, 02 Apr 2024 04:17:30 GMT</pubDate>
    </item>
    <item>
      <title>Online Structured Prediction via Coactive Learning</title>
      <link>http://arxiv.org/abs/1205.4213v2</link>
      <description>We propose Coactive Learning as a model of interaction between a learning system and a human user, where both have the common goal of providing results of maximum utility to the user. At each step, the system (e.g. search engine) receives a context (e.g. query) and predicts an object (e.g. ranking). The user responds by correcting the system if necessary, providing a slightly improved -- but not necessarily optimal -- object as feedback. We argue that such feedback can often be inferred from observable user behavior, for example, from clicks in web-search. Evaluating predictions by their cardinal utility to the user, we propose efficient learning algorithms that have ${\cal O}(\frac{1}{\sqrt{T}})$ average regret, even though the learning algorithm never observes cardinal utility values as in conventional online learning. We demonstrate the applicability of our model and learning algorithms on a movie recommendation task, as well as ranking for web-search.\n\n\nCoactive Learning for Large Language Models using Implicit User Feedback</description>
      <guid isPermaLink="false">http://arxiv.org/abs/1205.4213v2</guid>
      <dc:creator>Pannaga Shivaswamy, Thorsten Joachims</dc:creator>
      <pubDate>Wed, 27 Jun 2012 16:25:02 GMT</pubDate>
    </item>
    <item>
      <title>Importance-Aware Learning for Neural Headline Editing</title>
      <link>http://arxiv.org/abs/1912.01114v1</link>
      <description>Many social media news writers are not professionally trained. Therefore, social media platforms have to hire professional editors to adjust amateur headlines to attract more readers. We propose to automate this headline editing process through neural network models to provide more immediate writing support for these social media news writers. To train such a neural headline editing model, we collected a dataset which contains articles with original headlines and professionally edited headlines. However, it is expensive to collect a large number of professionally edited headlines. To solve this low-resource problem, we design an encoder-decoder model which leverages large scale pre-trained language models. We further improve the pre-trained model's quality by introducing a headline generation task as an intermediate task before the headline editing task. Also, we propose Self Importance-Aware (SIA) loss to address the different levels of editing in the dataset by down-weighting the importance of easily classified tokens and sentences. With the help of Pre-training, Adaptation, and SIA, the model learns to generate headlines in the professional editor's style. Experimental results show that our method significantly improves the quality of headline editing comparing against previous methods.\n\n\nNCL_NLP at SemEval-2024 Task 7: CoT-NumHG: A CoT-Based SFT Training Strategy with Large Language Models for Number-Focused Headline Generation</description>
      <guid isPermaLink="false">http://arxiv.org/abs/1912.01114v1</guid>
      <dc:creator>Qingyang Wu, Lei Li, Hao Zhou, Ying Zeng, Zhou Yu</dc:creator>
      <pubDate>Mon, 25 Nov 2019 06:42:02 GMT</pubDate>
    </item>
    <item>
      <title>GraphWiz: An Instruction-Following Language Model for Graph Problems</title>
      <link>http://arxiv.org/abs/2402.16029v5</link>
      <description>Large language models (LLMs) have achieved impressive success across several fields, but their proficiency in understanding and resolving complex graph problems is less explored. To bridge this gap, we introduce GraphInstruct, a novel and comprehensive instruction-tuning dataset designed to equip language models with the ability to tackle a broad spectrum of graph problems using explicit reasoning paths. Utilizing GraphInstruct, we build GraphWiz, an open-source language model capable of resolving various graph problem types while generating clear reasoning processes. To enhance the model's capability and reliability, we incorporate the Direct Preference Optimization (DPO) framework into the graph problem-solving context. The enhanced model, GraphWiz-DPO, achieves an average accuracy of 65% across nine tasks with different complexity levels, surpassing GPT-4 which has an average accuracy of 43.8%. Moreover, our research delves into the delicate balance between training data volume and model performance, highlighting the potential for overfitting with increased data. We also explore the transferability of the model's reasoning ability across different graph tasks, indicating the model's adaptability and practical application potential. Our investigation offers a new blueprint and valuable insights for developing LLMs specialized in graph reasoning and problem-solving.\n\n\nGraphWiz: An Instruction-Following Language Model for Graph Problems</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.16029v5</guid>
      <dc:creator>Nuo Chen, Yuhan Li, Jianheng Tang, Jia Li</dc:creator>
      <pubDate>Wed, 03 Jul 2024 06:39:59 GMT</pubDate>
    </item>
    <item>
      <title>Contrastive Perplexity for Controlled Generation: An Application in Detoxifying Large Language Models</title>
      <link>http://arxiv.org/abs/2401.08491v2</link>
      <description>The generation of undesirable and factually incorrect content of large language models poses a significant challenge and remains largely an unsolved issue. This paper studies the integration of a contrastive learning objective for fine-tuning LLMs for implicit knowledge editing and controlled text generation. Optimizing the training objective entails aligning text perplexities in a contrastive fashion. To facilitate training the model in a self-supervised fashion, we leverage an off-the-shelf LLM for training data generation. We showcase applicability in the domain of detoxification. Herein, the proposed approach leads to a significant decrease in the generation of toxic content while preserving general utility for downstream tasks such as commonsense reasoning and reading comprehension. The proposed approach is conceptually simple but empirically powerful.\n\n\nContrastive Perplexity for Controlled Generation: An Application in Detoxifying Large Language Models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2401.08491v2</guid>
      <dc:creator>Tassilo Klein, Moin Nabi</dc:creator>
      <pubDate>Wed, 24 Jan 2024 23:04:02 GMT</pubDate>
    </item>
    <item>
      <title>An Information Bottleneck Characterization of the Understanding-Workload Tradeoff</title>
      <link>http://arxiv.org/abs/2310.07802v1</link>
      <description>Recent advances in artificial intelligence (AI) have underscored the need for explainable AI (XAI) to support human understanding of AI systems. Consideration of human factors that impact explanation efficacy, such as mental workload and human understanding, is central to effective XAI design. Existing work in XAI has demonstrated a tradeoff between understanding and workload induced by different types of explanations. Explaining complex concepts through abstractions (hand-crafted groupings of related problem features) has been shown to effectively address and balance this workload-understanding tradeoff. In this work, we characterize the workload-understanding balance via the Information Bottleneck method: an information-theoretic approach which automatically generates abstractions that maximize informativeness and minimize complexity. In particular, we establish empirical connections between workload and complexity and between understanding and informativeness through human-subject experiments. This empirical link between human factors and information-theoretic concepts provides an important mathematical characterization of the workload-understanding tradeoff which enables user-tailored XAI design.\n\n\nAn Information Bottleneck Characterization of the Understanding-Workload Tradeoff in Human-Centered Explainable AI</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2310.07802v1</guid>
      <dc:creator>Lindsay Sanneman, Mycal Tucker, Julie Shah</dc:creator>
      <pubDate>Wed, 11 Oct 2023 18:35:26 GMT</pubDate>
    </item>
    <item>
      <title>Dont Add, dont Miss: Effective Content Preserving Generation from Pre-Selected Text Spans</title>
      <link>http://arxiv.org/abs/2310.09017v3</link>
      <description>The recently introduced Controlled Text Reduction (CTR) task isolates the text generation step within typical summarization-style tasks. It does so by challenging models to generate coherent text conforming to pre-selected content within the input text (``highlights''). This framing enables increased modularity in summarization-like tasks, allowing to couple a single CTR model with various content-selection setups and modules. However, there are currently no reliable CTR models, while the performance of the existing baseline for the task is mediocre, falling short of practical utility. Here, we address this gap by introducing a high-quality, open-source CTR model that tackles two prior key limitations: inadequate enforcement of the content-preservation constraint, and suboptimal silver training data. Addressing these, we amplify the content-preservation constraint in both training, via RL, and inference, via a controlled decoding strategy. Further, we substantially improve the silver training data quality via GPT-4 distillation. Overall, pairing the distilled dataset with the highlight-adherence strategies yields marked gains over the current baseline, of up to 30 ROUGE-L points, providing a reliable CTR model for downstream use.\n\n\nDon't Forget Your Reward Values: Language Model Alignment via Value-based Calibration</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2310.09017v3</guid>
      <dc:creator>Aviv Slobodkin, Avi Caciularu, Eran Hirsch, Ido Dagan</dc:creator>
      <pubDate>Sun, 25 Feb 2024 14:45:00 GMT</pubDate>
    </item>
    <item>
      <title>PICLe: Eliciting Diverse Behaviors from Large Language Models with Persona In-Context Learning</title>
      <link>http://arxiv.org/abs/2405.02501v2</link>
      <description>Large Language Models (LLMs) are trained on massive text corpora, which are encoded with diverse personality traits. This triggers an interesting goal of eliciting a desired personality trait from the LLM, and probing its behavioral preferences. Accordingly, we formalize the persona elicitation task, aiming to customize LLM behaviors to align with a target persona. We present Persona In-Context Learning (PICLe), a novel persona elicitation framework grounded in Bayesian inference. At the core, PICLe introduces a new ICL example selection criterion based on likelihood ratio, which is designed to optimally guide the model in eliciting a specific target persona. We demonstrate the effectiveness of PICLe through extensive comparisons against baseline methods across three contemporary LLMs. Code is available at https://github.com/deeplearning-wisc/picle.\n\n\nBeyond Helpfulness and Harmlessness: Eliciting Diverse Behaviors from Large Language Models with Persona In-Context Learning</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.02501v2</guid>
      <dc:creator>Hyeong Kyu Choi, Yixuan Li</dc:creator>
      <pubDate>Tue, 14 May 2024 05:53:07 GMT</pubDate>
    </item>
    <item>
      <title>Construction of Domain-specified Japanese Large Language Model for Finance through Continual Pre-training</title>
      <link>http://arxiv.org/abs/2404.10555v1</link>
      <description>Large language models (LLMs) are now widely used in various fields, including finance. However, Japanese financial-specific LLMs have not been proposed yet. Hence, this study aims to construct a Japanese financial-specific LLM through continual pre-training. Before tuning, we constructed Japanese financial-focused datasets for continual pre-training. As a base model, we employed a Japanese LLM that achieved state-of-the-art performance on Japanese financial benchmarks among the 10-billion-class parameter models. After continual pre-training using the datasets and the base model, the tuned model performed better than the original model on the Japanese financial benchmarks. Moreover, the outputs comparison results reveal that the tuned model's outputs tend to be better than the original model's outputs in terms of the quality and length of the answers. These findings indicate that domain-specific continual pre-training is also effective for LLMs. The tuned model is publicly available on Hugging Face.\n\n\nConstruction of Domain-specified Japanese Large Language Model for Finance through Continual Pre-training</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2404.10555v1</guid>
      <dc:creator>Masanori Hirano, Kentaro Imajo</dc:creator>
      <pubDate>Tue, 16 Apr 2024 13:26:32 GMT</pubDate>
    </item>
    <item>
      <title>Risks from Language Models for Automated Mental Healthcare: Ethics and Structure for Implementation</title>
      <link>http://arxiv.org/abs/2406.11852v1</link>
      <description>Amidst the growing interest in developing task-autonomous AI for automated mental health care, this paper addresses the ethical and practical challenges associated with the issue and proposes a structured framework that delineates levels of autonomy, outlines ethical requirements, and defines beneficial default behaviors for AI agents in the context of mental health support. We also evaluate ten state-of-the-art language models using 16 mental health-related questions designed to reflect various mental health conditions, such as psychosis, mania, depression, suicidal thoughts, and homicidal tendencies. The question design and response evaluations were conducted by mental health clinicians (M.D.s). We find that existing language models are insufficient to match the standard provided by human professionals who can navigate nuances and appreciate context. This is due to a range of issues, including overly cautious or sycophantic responses and the absence of necessary safeguards. Alarmingly, we find that most of the tested models could cause harm if accessed in mental health emergencies, failing to protect users and potentially exacerbating existing symptoms. We explore solutions to enhance the safety of current models. Before the release of increasingly task-autonomous AI systems in mental health, it is crucial to ensure that these models can reliably detect and manage symptoms of common psychiatric disorders to prevent harm to users. This involves aligning with the ethical framework and default behaviors outlined in our study. We contend that model developers are responsible for refining their systems per these guidelines to safeguard against the risks posed by current AI technologies to user mental health and safety.   Trigger warning: Contains and discusses examples of sensitive mental health topics, including suicide and self-harm.\n\n\nRisks from Language Models for Automated Mental Healthcare: Ethics and Structure for Implementation</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.11852v1</guid>
      <dc:creator>Declan Grabb, Max Lamparth, Nina Vasan</dc:creator>
      <pubDate>Tue, 02 Apr 2024 15:05:06 GMT</pubDate>
    </item>
    <item>
      <title>AMOR: A Recipe for Building Adaptable Modular Knowledge Agents Through Process Feedback</title>
      <link>http://arxiv.org/abs/2402.01469v1</link>
      <description>The notable success of large language models (LLMs) has sparked an upsurge in building language agents to complete various complex tasks. We present AMOR, an agent framework based on open-source LLMs, which reasons with external knowledge bases and adapts to specific domains through human supervision to the reasoning process. AMOR builds reasoning logic over a finite state machine (FSM) that solves problems through autonomous executions and transitions over disentangled modules. This allows humans to provide direct feedback to the individual modules, and thus naturally forms process supervision. Based on this reasoning and feedback framework, we develop AMOR through two-stage fine-tuning: warm-up and adaptation. The former fine-tunes the LLM with examples automatically constructed from various public datasets and enables AMOR to generalize across different knowledge environments, while the latter tailors AMOR to specific domains using process feedback. Extensive experiments across multiple domains demonstrate the advantage of AMOR to strong baselines, thanks to its FSM-based reasoning and process feedback mechanism.\n\n\nAMOR: A Recipe for Building Adaptable Modular Knowledge Agents Through Process Feedback</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.01469v1</guid>
      <dc:creator>Jian Guan, Wei Wu, Zujie Wen, Peng Xu, Hongning Wang, Minlie Huang</dc:creator>
      <pubDate>Fri, 02 Feb 2024 14:56:48 GMT</pubDate>
    </item>
    <item>
      <title>Interpretability-Aware Vision Transformer</title>
      <link>http://arxiv.org/abs/2309.08035v1</link>
      <description>Vision Transformers (ViTs) have become prominent models for solving various vision tasks. However, the interpretability of ViTs has not kept pace with their promising performance. While there has been a surge of interest in developing {\it post hoc} solutions to explain ViTs' outputs, these methods do not generalize to different downstream tasks and various transformer architectures. Furthermore, if ViTs are not properly trained with the given data and do not prioritize the region of interest, the {\it post hoc} methods would be less effective. Instead of developing another {\it post hoc} approach, we introduce a novel training procedure that inherently enhances model interpretability. Our interpretability-aware ViT (IA-ViT) draws inspiration from a fresh insight: both the class patch and image patches consistently generate predicted distributions and attention maps. IA-ViT is composed of a feature extractor, a predictor, and an interpreter, which are trained jointly with an interpretability-aware training objective. Consequently, the interpreter simulates the behavior of the predictor and provides a faithful explanation through its single-head self-attention mechanism. Our comprehensive experimental results demonstrate the effectiveness of IA-ViT in several image classification tasks, with both qualitative and quantitative evaluations of model performance and interpretability. Source code is available from: https://github.com/qiangyao1988/IA-ViT.\n\n\nTransformers in source code generation: A comprehensive survey</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2309.08035v1</guid>
      <dc:creator>Yao Qiang, Chengyin Li, Prashant Khanduri, Dongxiao Zhu</dc:creator>
      <pubDate>Thu, 14 Sep 2023 21:50:49 GMT</pubDate>
    </item>
    <item>
      <title>Let AI Entertain You: Increasing User Engagement with Generative AI and Rejection Sampling</title>
      <link>http://arxiv.org/abs/2312.12457v1</link>
      <description>While generative AI excels in content generation, it does not always increase user engagement. This can be attributed to two main factors. First, generative AI generates content without incorporating explicit or implicit feedback about user interactions. Even if the generated content seems to be more informative or well-written, it does not necessarily lead to an increase in user activities, such as clicks. Second, there is a concern with the quality of the content generative AI produces, which often lacks the distinctiveness and authenticity that human-created content possesses. These two factors can lead to content that fails to meet specific needs and preferences of users, ultimately reducing its potential to be engaging.   This paper presents a generic framework of how to improve user engagement with generative AI by leveraging user feedback. Our solutions employ rejection sampling, a technique used in reinforcement learning, to boost engagement metrics. We leveraged the framework in the context of email notification subject lines generation for an online social network, and achieved significant engagement metric lift including +1% Session and +0.4% Weekly Active Users. We believe our work offers a universal framework that enhances user engagement with generative AI, particularly when standard generative AI reaches its limits in terms of enhancing content to be more captivating. To the best of our knowledge, this represents an early milestone in the industry's successful use of generative AI to enhance user engagement.\n\n\nLet AI Entertain You: Increasing User Engagement with Generative AI and Rejection Sampling</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2312.12457v1</guid>
      <dc:creator>Jingying Zeng, Jaewon Yang, Waleed Malik, Xiao Yan, Richard Huang, Qi He</dc:creator>
      <pubDate>Sat, 16 Dec 2023 08:06:12 GMT</pubDate>
    </item>
    <item>
      <title>A safety realignment framework via subspace-oriented model fusion for large language models</title>
      <link>http://arxiv.org/abs/2405.09055v1</link>
      <description>The current safeguard mechanisms for large language models (LLMs) are indeed susceptible to jailbreak attacks, making them inherently fragile. Even the process of fine-tuning on apparently benign data for downstream tasks can jeopardize safety. One potential solution is to conduct safety fine-tuning subsequent to downstream fine-tuning. However, there's a risk of catastrophic forgetting during safety fine-tuning, where LLMs may regain safety measures but lose the task-specific knowledge acquired during downstream fine-tuning. In this paper, we introduce a safety realignment framework through subspace-oriented model fusion (SOMF), aiming to combine the safeguard capabilities of initially aligned model and the current fine-tuned model into a realigned model. Our approach begins by disentangling all task vectors from the weights of each fine-tuned model. We then identify safety-related regions within these vectors by subspace masking techniques. Finally, we explore the fusion of the initial safely aligned LLM with all task vectors based on the identified safety subspace. We validate that our safety realignment framework satisfies the safety requirements of a single fine-tuned model as well as multiple models during their fusion. Our findings confirm that SOMF preserves safety without notably compromising performance on downstream tasks, including instruction following in Chinese, English, and Hindi, as well as problem-solving capabilities in Code and Math.\n\n\nA safety realignment framework via subspace-oriented model fusion for large language models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.09055v1</guid>
      <dc:creator>Xin Yi, Shunfan Zheng, Linlin Wang, Xiaoling Wang, Liang He</dc:creator>
      <pubDate>Wed, 15 May 2024 03:04:05 GMT</pubDate>
    </item>
    <item>
      <title>Best-of-Venom: Attacking RLHF by Injecting Poisoned Preference Data</title>
      <link>http://arxiv.org/abs/2404.05530v1</link>
      <description>Reinforcement Learning from Human Feedback (RLHF) is a popular method for aligning Language Models (LM) with human values and preferences. RLHF requires a large number of preference pairs as training data, which are often used in both the Supervised Fine-Tuning and Reward Model training, and therefore publicly available datasets are commonly used. In this work, we study to what extent a malicious actor can manipulate the LMs generations by poisoning the preferences, i.e., injecting poisonous preference pairs into these datasets and the RLHF training process. We propose strategies to build poisonous preference pairs and test their performance by poisoning two widely used preference datasets. Our results show that preference poisoning is highly effective: by injecting a small amount of poisonous data (1-5% of the original dataset), we can effectively manipulate the LM to generate a target entity in a target sentiment (positive or negative). The findings from our experiments also shed light on strategies to defend against the preference poisoning attack.\n\n\nBest-of-Venom: Attacking RLHF by Injecting Poisoned Preference Data</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2404.05530v1</guid>
      <dc:creator>Tim Baumgärtner, Yang Gao, Dana Alon, Donald Metzler</dc:creator>
      <pubDate>Mon, 08 Apr 2024 13:59:02 GMT</pubDate>
    </item>
    <item>
      <title>Exploring Autonomous Agents through the Lens of Large Language Models: A Review</title>
      <link>http://arxiv.org/abs/2404.04442v1</link>
      <description>Large Language Models (LLMs) are transforming artificial intelligence, enabling autonomous agents to perform diverse tasks across various domains. These agents, proficient in human-like text comprehension and generation, have the potential to revolutionize sectors from customer service to healthcare. However, they face challenges such as multimodality, human value alignment, hallucinations, and evaluation. Techniques like prompting, reasoning, tool utilization, and in-context learning are being explored to enhance their capabilities. Evaluation platforms like AgentBench, WebArena, and ToolLLM provide robust methods for assessing these agents in complex scenarios. These advancements are leading to the development of more resilient and capable autonomous agents, anticipated to become integral in our digital lives, assisting in tasks from email responses to disease diagnosis. The future of AI, with LLMs at the forefront, is promising.\n\n\nExploring autonomous agents through the lens of large language models: A review</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2404.04442v1</guid>
      <dc:creator>Saikat Barua</dc:creator>
      <pubDate>Fri, 05 Apr 2024 22:59:02 GMT</pubDate>
    </item>
    <item>
      <title>Break the Breakout: Reinventing LM Defense Against Jailbreak Attacks with Self-Refinement</title>
      <link>http://arxiv.org/abs/2402.15180v2</link>
      <description>Caution: This paper includes offensive words that could potentially cause unpleasantness. Language models (LMs) are vulnerable to exploitation for adversarial misuse. Training LMs for safety alignment is extensive and makes it hard to respond to fast-developing attacks immediately, such as jailbreaks. We propose self-refine with formatting that achieves outstanding safety even in non-safety-aligned LMs and evaluate our method alongside several defense baselines, demonstrating that it is the safest training-free method against jailbreak attacks. Additionally, we proposed a formatting method that improves the efficiency of the self-refine process while reducing attack success rates in fewer iterations. We've also observed that non-safety-aligned LMs outperform safety-aligned LMs in safety tasks by giving more helpful and safe responses. In conclusion, our findings can achieve less safety risk with fewer computational costs, allowing non-safety LM to be easily utilized in real-world service.\n\n\nBreak the Breakout: Reinventing LM Defense Against Jailbreak Attacks with Self-Refinement</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.15180v2</guid>
      <dc:creator>Heegyu Kim, Sehyun Yuk, Hyunsouk Cho</dc:creator>
      <pubDate>Tue, 27 Feb 2024 01:39:20 GMT</pubDate>
    </item>
    <item>
      <title>Conditions on Preference Relations that Guarantee the Existence of Optimal Policies</title>
      <link>http://arxiv.org/abs/2311.01990v2</link>
      <description>Learning from Preferential Feedback (LfPF) plays an essential role in training Large Language Models, as well as certain types of interactive learning agents. However, a substantial gap exists between the theory and application of LfPF algorithms. Current results guaranteeing the existence of optimal policies in LfPF problems assume that both the preferences and transition dynamics are determined by a Markov Decision Process. We introduce the Direct Preference Process, a new framework for analyzing LfPF problems in partially-observable, non-Markovian environments. Within this framework, we establish conditions that guarantee the existence of optimal policies by considering the ordinal structure of the preferences. We show that a decision-making problem can have optimal policies -- that are characterized by recursive optimality equations -- even when no reward function can express the learning goal. These findings underline the need to explore preference-based learning strategies which do not assume that preferences are generated by reward.\n\n\nConditions on Preference Relations that Guarantee the Existence of Optimal Policies</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2311.01990v2</guid>
      <dc:creator>Jonathan Colaço Carr, Prakash Panangaden, Doina Precup</dc:creator>
      <pubDate>Wed, 27 Mar 2024 22:03:46 GMT</pubDate>
    </item>
    <item>
      <title>NOTE: Notable generation Of patient Text summaries through Efficient approach based on direct preference optimization</title>
      <link>http://arxiv.org/abs/2402.11882v1</link>
      <description>The discharge summary is a one of critical documents in the patient journey, encompassing all events experienced during hospitalization, including multiple visits, medications, tests, surgery/procedures, and admissions/discharge. Providing a summary of the patient's progress is crucial, as it significantly influences future care and planning. Consequently, clinicians face the laborious and resource-intensive task of manually collecting, organizing, and combining all the necessary data for a discharge summary. Therefore, we propose &quot;NOTE&quot;, which stands for &quot;Notable generation Of patient Text summaries through an Efficient approach based on direct preference optimization&quot;. NOTE is based on Medical Information Mart for Intensive Care- III dataset and summarizes a single hospitalization of a patient. Patient events are sequentially combined and used to generate a discharge summary for each hospitalization. In the present circumstances, large language models' application programming interfaces (LLMs' APIs) are widely available, but importing and exporting medical data presents significant challenges due to privacy protection policies in healthcare institutions. Moreover, to ensure optimal performance, it is essential to implement a lightweight model for internal server or program within the hospital. Therefore, we utilized DPO and parameter efficient fine tuning (PEFT) techniques to apply a fine-tuning method that guarantees superior performance. To demonstrate the practical application of the developed NOTE, we provide a webpage-based demonstration software. In the future, we will aim to deploy the software available for actual use by clinicians in hospital. NOTE can be utilized to generate various summaries not only discharge summaries but also throughout a patient's journey, thereby alleviating the labor-intensive workload of clinicians and aiming for increased efficiency.\n\n\nNOTE: Notable generation Of patient Text summaries through Efficient approach based on direct preference optimization</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.11882v1</guid>
      <dc:creator>Imjin Ahn, Hansle Gwon, Young-Hak Kim, Tae Joon Jun, Sanghyun Park</dc:creator>
      <pubDate>Mon, 19 Feb 2024 06:43:25 GMT</pubDate>
    </item>
    <item>
      <title>An Improved Traditional Chinese Evaluation Suite for Foundation Model</title>
      <link>http://arxiv.org/abs/2403.01858v3</link>
      <description>We present TMMLU+, a new benchmark designed for Traditional Chinese language understanding. TMMLU+ is a multi-choice question-answering dataset with 66 subjects from elementary to professional level. It is six times larger and boasts a more balanced subject distribution than its predecessor, Taiwan Massive Multitask Language Understanding (TMMLU). We also benchmark closed-source models and 26 open-weight Chinese large language models (LLMs) of parameters ranging from 1.8B to 72B on the proposed TMMLU+. Our findings reveal that (1.) Traditional Chinese models still trail behind their Simplified Chinese counterparts, highlighting a need for more focused advancements in LLMs catering to Traditional Chinese. (2.) Current LLMs still fall short of human performance in average scores, indicating a potential need for future research to delve deeper into social science and humanities subjects. (3.) Among all the tokenization compression metrics examined, we identify that only the fertility score uniquely demonstrates strong correlations with our benchmark results. We foresee that TMMLU+ will pinpoint areas for future model improvement, thereby narrowing the gap between machine and human linguistic capabilities and supporting researchers in developing Traditional Chinese LLMs. Our dataset, along with the benchmark source code, is accessible at huggingface.co/datasets/ikala/tmmluplus.\n\n\nHyacinth6B: A large language model for Traditional Chinese</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2403.01858v3</guid>
      <dc:creator>Zhi-Rui Tam, Ya-Ting Pai, Yen-Wei Lee, Jun-Da Chen, Wei-Min Chu, Sega Cheng, Hong-Han Shuai</dc:creator>
      <pubDate>Thu, 11 Jul 2024 14:41:14 GMT</pubDate>
    </item>
    <item>
      <title>Aligning Vision Models with Human Aesthetics in Retrieval: Benchmarks and Algorithms</title>
      <link>http://arxiv.org/abs/2406.09397v1</link>
      <description>Modern vision models are trained on very large noisy datasets. While these models acquire strong capabilities, they may not follow the user's intent to output the desired results in certain aspects, e.g., visual aesthetic, preferred style, and responsibility. In this paper, we target the realm of visual aesthetics and aim to align vision models with human aesthetic standards in a retrieval system. Advanced retrieval systems usually adopt a cascade of aesthetic models as re-rankers or filters, which are limited to low-level features like saturation and perform poorly when stylistic, cultural or knowledge contexts are involved. We find that utilizing the reasoning ability of large language models (LLMs) to rephrase the search query and extend the aesthetic expectations can make up for this shortcoming. Based on the above findings, we propose a preference-based reinforcement learning method that fine-tunes the vision models to distill the knowledge from both LLMs reasoning and the aesthetic models to better align the vision models with human aesthetics. Meanwhile, with rare benchmarks designed for evaluating retrieval systems, we leverage large multi-modality model (LMM) to evaluate the aesthetic performance with their strong abilities. As aesthetic assessment is one of the most subjective tasks, to validate the robustness of LMM, we further propose a novel dataset named HPIR to benchmark the alignment with human aesthetics. Experiments demonstrate that our method significantly enhances the aesthetic behaviors of the vision models, under several metrics. We believe the proposed algorithm can be a general practice for aligning vision models with human values.\n\n\nAligning Vision Models with Human Aesthetics in Retrieval: Benchmarks and Algorithms</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.09397v1</guid>
      <dc:creator>Miaosen Zhang, Yixuan Wei, Zhen Xing, Yifei Ma, Zuxuan Wu, Ji Li, Zheng Zhang, Qi Dai, Chong Luo, Xin Geng, Baining Guo</dc:creator>
      <pubDate>Thu, 13 Jun 2024 17:59:20 GMT</pubDate>
    </item>
    <item>
      <title>Linear Alignment: A Closed-form Solution for Aligning Human Preferences without Tuning and Feedback</title>
      <link>http://arxiv.org/abs/2401.11458v3</link>
      <description>The success of AI assistants based on Language Models (LLMs) hinges on Reinforcement Learning from Human Feedback (RLHF) to comprehend and align with user intentions. However, traditional alignment algorithms, such as PPO, are hampered by complex annotation and training requirements. This reliance limits the applicability of RLHF and hinders the development of professional assistants tailored to diverse human preferences. In this work, we introduce \textit{Linear Alignment}, a novel algorithm that aligns language models with human preferences in one single inference step, eliminating the reliance on data annotation and model training. Linear alignment incorporates a new parameterization for policy optimization under divergence constraints, which enables the extraction of optimal policy in a closed-form manner and facilitates the direct estimation of the aligned response. Extensive experiments on both general and personalized preference datasets demonstrate that linear alignment significantly enhances the performance and efficiency of LLM alignment across diverse scenarios. Our code and dataset is published on \url{https://github.com/Wizardcoast/Linear_Alignment.git}.\n\n\nLinear Alignment: A Closed-form Solution for Aligning Human Preferences without Tuning and Feedback</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2401.11458v3</guid>
      <dc:creator>Songyang Gao, Qiming Ge, Wei Shen, Shihan Dou, Junjie Ye, Xiao Wang, Rui Zheng, Yicheng Zou, Zhi Chen, Hang Yan, Qi Zhang, Dahua Lin</dc:creator>
      <pubDate>Tue, 02 Jul 2024 03:24:29 GMT</pubDate>
    </item>
    <item>
      <title>Train Global, Tailor Local: Minimalist Multilingual Translation into Endangered Languages</title>
      <link>http://arxiv.org/abs/2305.03873v1</link>
      <description>In many humanitarian scenarios, translation into severely low resource languages often does not require a universal translation engine, but a dedicated text-specific translation engine. For example, healthcare records, hygienic procedures, government communication, emergency procedures and religious texts are all limited texts. While generic translation engines for all languages do not exist, translation of multilingually known limited texts into new, endangered languages may be possible and reduce human translation effort. We attempt to leverage translation resources from many rich resource languages to efficiently produce best possible translation quality for a well known text, which is available in multiple languages, in a new, severely low resource language. We examine two approaches: 1. best selection of seed sentences to jump start translations in a new language in view of best generalization to the remainder of a larger targeted text(s), and 2. we adapt large general multilingual translation engines from many other languages to focus on a specific text in a new, unknown language. We find that adapting large pretrained multilingual models to the domain/text first and then to the severely low resource language works best. If we also select a best set of seed sentences, we can improve average chrF performance on new test languages from a baseline of 21.9 to 50.7, while reducing the number of seed sentences to only around 1,000 in the new, unknown language.\n\n\nInto the Unknown: Self-Learning Large Language Models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2305.03873v1</guid>
      <dc:creator>Zhong Zhou, Jan Niehues, Alex Waibel</dc:creator>
      <pubDate>Fri, 05 May 2023 23:22:16 GMT</pubDate>
    </item>
    <item>
      <title>Label Supervised LLaMA Finetuning</title>
      <link>http://arxiv.org/abs/2310.01208v1</link>
      <description>The recent success of Large Language Models (LLMs) has gained significant attention in both academia and industry. Substantial efforts have been made to enhance the zero- and few-shot generalization capabilities of open-source LLMs through finetuning. Currently, the prevailing approach is instruction-tuning, which trains LLMs to complete real-world tasks by generating responses guided by natural language instructions. It is worth noticing that such an approach may underperform in sequence and token classification tasks. Unlike text generation tasks, classification tasks have a limited label space, where precise label prediction is more appreciated than generating diverse and human-like responses. Prior research has unveiled that instruction-tuned LLMs cannot outperform BERT, prompting us to explore the potential of leveraging latent representations from LLMs for supervised label prediction. In this paper, we introduce a label-supervised adaptation for LLMs, which aims to finetuning the model with discriminant labels. We evaluate this approach with Label Supervised LLaMA (LS-LLaMA), based on LLaMA-2-7B, a relatively small-scale LLM, and can be finetuned on a single GeForce RTX4090 GPU. We extract latent representations from the final LLaMA layer and project them into the label space to compute the cross-entropy loss. The model is finetuned by Low-Rank Adaptation (LoRA) to minimize this loss. Remarkably, without intricate prompt engineering or external knowledge, LS-LLaMA substantially outperforms LLMs ten times its size in scale and demonstrates consistent improvements compared to robust baselines like BERT-Large and RoBERTa-Large in text classification. Moreover, by removing the causal mask from decoders, LS-unLLaMA achieves the state-of-the-art performance in named entity recognition (NER). Our work will shed light on a novel approach to adapting LLMs for various downstream tasks.\n\n\nFine-tuning of diffusion models via stochastic control: entropy regularization and beyond</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2310.01208v1</guid>
      <dc:creator>Zongxi Li, Xianming Li, Yuzhang Liu, Haoran Xie, Jing Li, Fu-lee Wang, Qing Li, Xiaoqin Zhong</dc:creator>
      <pubDate>Mon, 02 Oct 2023 13:53:03 GMT</pubDate>
    </item>
    <item>
      <title>uTeBC-NLP at SemEval-2024 Task 9: Can LLMs be Lateral Thinkers?</title>
      <link>http://arxiv.org/abs/2404.02474v1</link>
      <description>Inspired by human cognition, Jiang et al.(2023c) create a benchmark for assessing LLMs' lateral thinking-thinking outside the box. Building upon this benchmark, we investigate how different prompting methods enhance LLMs' performance on this task to reveal their inherent power for outside-the-box thinking ability. Through participating in SemEval-2024, task 9, Sentence Puzzle sub-task, we explore prompt engineering methods: chain of thoughts (CoT) and direct prompting, enhancing with informative descriptions, and employing contextualizing prompts using a retrieval augmented generation (RAG) pipeline. Our experiments involve three LLMs including GPT-3.5, GPT-4, and Zephyr-7B-beta. We generate a dataset of thinking paths between riddles and options using GPT-4, validated by humans for quality. Findings indicate that compressed informative prompts enhance performance. Dynamic in-context learning enhances model performance significantly. Furthermore, fine-tuning Zephyr on our dataset enhances performance across other commonsense datasets, underscoring the value of innovative thinking.\n\n\nuTeBC-NLP at SemEval-2024 Task 9: Can LLMs be Lateral Thinkers?</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2404.02474v1</guid>
      <dc:creator>Pouya Sadeghi, Amirhossein Abaskohi, Yadollah Yaghoobzadeh</dc:creator>
      <pubDate>Wed, 03 Apr 2024 05:31:59 GMT</pubDate>
    </item>
    <item>
      <title>Vision-Flan: Scaling Human-Labeled Tasks in Visual Instruction Tuning</title>
      <link>http://arxiv.org/abs/2402.11690v1</link>
      <description>Despite vision-language models' (VLMs) remarkable capabilities as versatile visual assistants, two substantial challenges persist within the existing VLM frameworks: (1) lacking task diversity in pretraining and visual instruction tuning, and (2) annotation error and bias in GPT-4 synthesized instruction tuning data. Both challenges lead to issues such as poor generalizability, hallucination, and catastrophic forgetting. To address these challenges, we construct Vision-Flan, the most diverse publicly available visual instruction tuning dataset to date, comprising 187 diverse tasks and 1,664,261 instances sourced from academic datasets, and each task is accompanied by an expert-written instruction. In addition, we propose a two-stage instruction tuning framework, in which VLMs are firstly finetuned on Vision-Flan and further tuned on GPT-4 synthesized data. We find this two-stage tuning framework significantly outperforms the traditional single-stage visual instruction tuning framework and achieves the state-of-the-art performance across a wide range of multi-modal evaluation benchmarks. Finally, we conduct in-depth analyses to understand visual instruction tuning and our findings reveal that: (1) GPT-4 synthesized data does not substantially enhance VLMs' capabilities but rather modulates the model's responses to human-preferred formats; (2) A minimal quantity (e.g., 1,000) of GPT-4 synthesized data can effectively align VLM responses with human-preference; (3) Visual instruction tuning mainly helps large-language models (LLMs) to understand visual features.\n\n\nScaling Data Diversity for Fine-Tuning Language Models in Human Alignment</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.11690v1</guid>
      <dc:creator>Zhiyang Xu, Chao Feng, Rulin Shao, Trevor Ashby, Ying Shen, Di Jin, Yu Cheng, Qifan Wang, Lifu Huang</dc:creator>
      <pubDate>Sun, 18 Feb 2024 19:38:44 GMT</pubDate>
    </item>
    <item>
      <title>Conifer: Improving Complex Constrained Instruction-Following Ability of Large Language Models</title>
      <link>http://arxiv.org/abs/2404.02823v1</link>
      <description>The ability of large language models (LLMs) to follow instructions is crucial to real-world applications. Despite recent advances, several studies have highlighted that LLMs struggle when faced with challenging instructions, especially those that include complex constraints, hindering their effectiveness in various tasks. To address this challenge, we introduce Conifer, a novel instruction tuning dataset, designed to enhance LLMs to follow multi-level instructions with complex constraints. Utilizing GPT-4, we curate the dataset by a series of LLM-driven refinement processes to ensure high quality. We also propose a progressive learning scheme that emphasizes an easy-to-hard progression, and learning from process feedback. Models trained with Conifer exhibit remarkable improvements in instruction-following abilities, especially for instructions with complex constraints. On several instruction-following benchmarks, our 7B model outperforms the state-of-the-art open-source 7B models, even exceeds the performance of models 10 times larger on certain metrics. All the code and Conifer dataset are available at https://www.github.com/ConiferLM/Conifer.\n\n\nConifer: Improving Complex Constrained Instruction-Following Ability of Large Language Models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2404.02823v1</guid>
      <dc:creator>Haoran Sun, Lixin Liu, Junjie Li, Fengyu Wang, Baohua Dong, Ran Lin, Ruohui Huang</dc:creator>
      <pubDate>Wed, 03 Apr 2024 15:55:39 GMT</pubDate>
    </item>
    <item>
      <title>Towards Better Question Generation in QA-based Event Extraction</title>
      <link>http://arxiv.org/abs/2405.10517v2</link>
      <description>Event Extraction (EE) is an essential information extraction task that aims to extract event-related information from unstructured texts. The paradigm of this task has shifted from conventional classification-based methods to more contemporary question-answering-based (QA-based) approaches. However, in QA-based EE, the quality of the questions dramatically affects the extraction accuracy, and how to generate high-quality questions for QA-based EE remains a challenge. In this work, to tackle this challenge, we suggest four criteria to evaluate the quality of a question and propose a reinforcement learning method, RLQG, for QA-based EE that can generate generalizable, high-quality, and context-dependent questions and provides clear guidance to QA models. The extensive experiments conducted on ACE and RAMS datasets have strongly validated our approach's effectiveness, which also demonstrates its robustness in scenarios with limited training data. The corresponding code of RLQG is released for further research.\n\n\nTowards Better Question Generation in QA-Based Event Extraction</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.10517v2</guid>
      <dc:creator>Zijin Hong, Jian Liu</dc:creator>
      <pubDate>Thu, 06 Jun 2024 13:40:00 GMT</pubDate>
    </item>
    <item>
      <title>ReZero: Boosting MCTS-based Algorithms by Backward-view and Entire-buffer Reanalyze</title>
      <link>http://arxiv.org/abs/2404.16364v3</link>
      <description>Monte Carlo Tree Search (MCTS)-based algorithms, such as MuZero and its derivatives, have achieved widespread success in various decision-making domains. These algorithms employ the reanalyze process to enhance sample efficiency from stale data, albeit at the expense of significant wall-clock time consumption. To address this issue, we propose a general approach named ReZero to boost tree search operations for MCTS-based algorithms. Specifically, drawing inspiration from the one-armed bandit model, we reanalyze training samples through a backward-view reuse technique which obtains the value estimation of a certain child node in advance. To further adapt to this design, we periodically reanalyze the entire buffer instead of frequently reanalyzing the mini-batch. The synergy of these two designs can significantly reduce the search cost and meanwhile guarantee or even improve performance, simplifying both data collecting and reanalyzing. Experiments conducted on Atari environments and board games demonstrate that ReZero substantially improves training speed while maintaining high sample efficiency. The code is available as part of the LightZero benchmark at https://github.com/opendilab/LightZero.\n\n\nReZero: Boosting MCTS-based Algorithms by Just-in-Time and Speedy Reanalyze</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2404.16364v3</guid>
      <dc:creator>Chunyu Xuan, Yazhe Niu, Yuan Pu, Shuai Hu, Yu Liu, Jing Yang</dc:creator>
      <pubDate>Tue, 28 May 2024 05:49:18 GMT</pubDate>
    </item>
    <item>
      <title>Gradient Monitored Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2005.12108v1</link>
      <description>This paper presents a novel neural network training approach for faster convergence and better generalization abilities in deep reinforcement learning. Particularly, we focus on the enhancement of training and evaluation performance in reinforcement learning algorithms by systematically reducing gradient's variance and thereby providing a more targeted learning process. The proposed method which we term as Gradient Monitoring(GM), is an approach to steer the learning in the weight parameters of a neural network based on the dynamic development and feedback from the training process itself. We propose different variants of the GM methodology which have been proven to increase the underlying performance of the model. The one of the proposed variant, Momentum with Gradient Monitoring (M-WGM), allows for a continuous adjustment of the quantum of back-propagated gradients in the network based on certain learning parameters. We further enhance the method with Adaptive Momentum with Gradient Monitoring (AM-WGM) method which allows for automatic adjustment between focused learning of certain weights versus a more dispersed learning depending on the feedback from the rewards collected. As a by-product, it also allows for automatic derivation of the required deep network sizes during training as the algorithm automatically freezes trained weights. The approach is applied to two discrete (Multi-Robot Co-ordination problem and Atari games) and one continuous control task (MuJoCo) using Advantage Actor-Critic (A2C) and Proximal Policy Optimization (PPO) respectively. The results obtained particularly underline the applicability and performance improvements of the methods in terms of generalization capability.\n\n\nReinforcement Learning with Dynamic Multi-Reward Weighting for Multi-Style Controllable Generation</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2005.12108v1</guid>
      <dc:creator>Mohammed Sharafath Abdul Hameed, Gavneet Singh Chadha, Andreas Schwung, Steven X. Ding</dc:creator>
      <pubDate>Mon, 25 May 2020 13:45:47 GMT</pubDate>
    </item>
    <item>
      <title>The Geometry of Multilingual Language Model Representations</title>
      <link>http://arxiv.org/abs/2205.10964v2</link>
      <description>We assess how multilingual language models maintain a shared multilingual representation space while still encoding language-sensitive information in each language. Using XLM-R as a case study, we show that languages occupy similar linear subspaces after mean-centering, evaluated based on causal effects on language modeling performance and direct comparisons between subspaces for 88 languages. The subspace means differ along language-sensitive axes that are relatively stable throughout middle layers, and these axes encode information such as token vocabularies. Shifting representations by language means is sufficient to induce token predictions in different languages. However, we also identify stable language-neutral axes that encode information such as token positions and part-of-speech. We visualize representations projected onto language-sensitive and language-neutral axes, identifying language family and part-of-speech clusters, along with spirals, toruses, and curves representing token position information. These results demonstrate that multilingual language models encode information along orthogonal language-sensitive and language-neutral axes, allowing the models to extract a variety of features for downstream tasks and cross-lingual transfer learning.\n\n\nTransformer-based Causal Language Models Perform Clustering</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2205.10964v2</guid>
      <dc:creator>Tyler A. Chang, Zhuowen Tu, Benjamin K. Bergen</dc:creator>
      <pubDate>Fri, 21 Oct 2022 23:10:27 GMT</pubDate>
    </item>
    <item>
      <title>Hyperparameter-Free Approach for Faster Minimum Bayes Risk Decoding</title>
      <link>http://arxiv.org/abs/2401.02749v2</link>
      <description>Minimum Bayes-Risk (MBR) decoding is shown to be a powerful alternative to beam search decoding for a wide range of text generation tasks. However, MBR requires a huge amount of time for inference to compute the MBR objective, which makes the method infeasible in many situations where response time is critical. Confidence-based pruning (CBP) (Cheng and Vlachos, 2023) has recently been proposed to reduce the inference time in machine translation tasks. Although it is shown to significantly reduce the amount of computation, it requires hyperparameter tuning using a development set to be effective. To this end, we propose Approximate Minimum Bayes-Risk (AMBR) decoding, a hyperparameter-free method to run MBR decoding approximately. AMBR is derived from the observation that the problem of computing the sample-based MBR objective is the medoid identification problem. AMBR uses the Correlated Sequential Halving (CSH) algorithm (Baharav and Tse, 2019), the best approximation algorithm to date for the medoid identification problem, to compute the sample-based MBR objective. We evaluate AMBR on machine translation, text summarization, and image captioning tasks. The results show that AMBR achieves on par with CBP, with CBP selecting hyperparameters through an Oracle for each given computation budget.\n\n\nHyperparameter-Free Approach for Faster Minimum Bayes Risk Decoding</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2401.02749v2</guid>
      <dc:creator>Yuu Jinnai, Kaito Ariu</dc:creator>
      <pubDate>Wed, 12 Jun 2024 01:14:45 GMT</pubDate>
    </item>
    <item>
      <title>EZ-CLIP: Efficient Zeroshot Video Action Recognition</title>
      <link>http://arxiv.org/abs/2312.08010v2</link>
      <description>Recent advancements in large-scale pre-training of visual-language models on paired image-text data have demonstrated impressive generalization capabilities for zero-shot tasks. Building on this success, efforts have been made to adapt these image-based visual-language models, such as CLIP, for videos extending their zero-shot capabilities to the video domain. While these adaptations have shown promising results, they come at a significant computational cost and struggle with effectively modeling the crucial temporal aspects inherent to the video domain. In this study, we present EZ-CLIP, a simple and efficient adaptation of CLIP that addresses these challenges. EZ-CLIP leverages temporal visual prompting for seamless temporal adaptation, requiring no fundamental alterations to the core CLIP architecture while preserving its remarkable generalization abilities. Moreover, we introduce a novel learning objective that guides the temporal visual prompts to focus on capturing motion, thereby enhancing its learning capabilities from video data. We conducted extensive experiments on five different benchmark datasets, thoroughly evaluating EZ-CLIP for zero-shot learning and base-to-novel video action recognition, and also demonstrating its potential for few-shot generalization.Impressively, with a mere 5.2 million learnable parameters (as opposed to the 71.1 million in the prior best model), EZ-CLIP can be efficiently trained on a single GPU, outperforming existing approaches in several evaluations.\n\n\nZero-shot learning to extract assessment criteria and medical services from the preventive healthcare guidelines using large language models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2312.08010v2</guid>
      <dc:creator>Shahzad Ahmad, Sukalpa Chanda, Yogesh S Rawat</dc:creator>
      <pubDate>Fri, 19 Jan 2024 12:19:48 GMT</pubDate>
    </item>
    <item>
      <title>ReNO: Enhancing One-step Text-to-Image Models through Reward-based Noise Optimization</title>
      <link>http://arxiv.org/abs/2406.04312v1</link>
      <description>Text-to-Image (T2I) models have made significant advancements in recent years, but they still struggle to accurately capture intricate details specified in complex compositional prompts. While fine-tuning T2I models with reward objectives has shown promise, it suffers from &quot;reward hacking&quot; and may not generalize well to unseen prompt distributions. In this work, we propose Reward-based Noise Optimization (ReNO), a novel approach that enhances T2I models at inference by optimizing the initial noise based on the signal from one or multiple human preference reward models. Remarkably, solving this optimization problem with gradient ascent for 50 iterations yields impressive results on four different one-step models across two competitive benchmarks, T2I-CompBench and GenEval. Within a computational budget of 20-50 seconds, ReNO-enhanced one-step models consistently surpass the performance of all current open-source Text-to-Image models. Extensive user studies demonstrate that our model is preferred nearly twice as often compared to the popular SDXL model and is on par with the proprietary Stable Diffusion 3 with 8B parameters. Moreover, given the same computational resources, a ReNO-optimized one-step model outperforms widely-used open-source models such as SDXL and PixArt-$\alpha$, highlighting the efficiency and effectiveness of ReNO in enhancing T2I model performance at inference time. Code is available at https://github.com/ExplainableML/ReNO.\n\n\nReNO: Enhancing One-step Text-to-Image Models through Reward-based Noise Optimization</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.04312v1</guid>
      <dc:creator>Luca Eyring, Shyamgopal Karthik, Karsten Roth, Alexey Dosovitskiy, Zeynep Akata</dc:creator>
      <pubDate>Thu, 06 Jun 2024 17:56:40 GMT</pubDate>
    </item>
    <item>
      <title>Open Source Language Models Can Provide Feedback: Evaluating LLMs' Ability to Help Students Using GPT-4-As-A-Judge</title>
      <link>http://arxiv.org/abs/2405.05253v1</link>
      <description>Large language models (LLMs) have shown great potential for the automatic generation of feedback in a wide range of computing contexts. However, concerns have been voiced around the privacy and ethical implications of sending student work to proprietary models. This has sparked considerable interest in the use of open source LLMs in education, but the quality of the feedback that such open models can produce remains understudied. This is a concern as providing flawed or misleading generated feedback could be detrimental to student learning. Inspired by recent work that has utilised very powerful LLMs, such as GPT-4, to evaluate the outputs produced by less powerful models, we conduct an automated analysis of the quality of the feedback produced by several open source models using a dataset from an introductory programming course. First, we investigate the viability of employing GPT-4 as an automated evaluator by comparing its evaluations with those of a human expert. We observe that GPT-4 demonstrates a bias toward positively rating feedback while exhibiting moderate agreement with human raters, showcasing its potential as a feedback evaluator. Second, we explore the quality of feedback generated by several leading open-source LLMs by using GPT-4 to evaluate the feedback. We find that some models offer competitive performance with popular proprietary LLMs, such as ChatGPT, indicating opportunities for their responsible use in educational settings.\n\n\nOpen Source Language Models Can Provide Feedback: Evaluating LLMs' Ability to Help Students Using GPT-4-As-A-Judge</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.05253v1</guid>
      <dc:creator>Charles Koutcheme, Nicola Dainese, Sami Sarsa, Arto Hellas, Juho Leinonen, Paul Denny</dc:creator>
      <pubDate>Wed, 08 May 2024 17:57:39 GMT</pubDate>
    </item>
    <item>
      <title>Step-Controlled DPO: Leveraging Stepwise Error for Enhanced Mathematical Reasoning</title>
      <link>http://arxiv.org/abs/2407.00782v3</link>
      <description>Direct Preference Optimization (DPO) has proven effective at improving the performance of large language models (LLMs) on downstream tasks such as reasoning and alignment. In this work, we propose Step-Controlled DPO (SCDPO), a method for automatically providing stepwise error supervision by creating negative samples of mathematical reasoning rationales that start making errors at a specified step. By applying these samples in DPO training, SCDPO can better align the model to understand reasoning errors and output accurate reasoning steps. We apply SCDPO to both code-integrated and chain-of-thought solutions, empirically showing that it consistently improves the performance compared to naive DPO on three different SFT models, including one existing SFT model and two models we finetuned. Qualitative analysis of the credit assignment of SCDPO and DPO demonstrates the effectiveness of SCDPO at identifying errors in mathematical solutions. We then apply SCDPO to an InternLM2-20B model, resulting in a 20B model that achieves high scores of 88.5% on GSM8K and 58.1% on MATH, rivaling all other open-source LLMs, showing the great potential of our method.\n\n\nStep-DPO: Step-wise Preference Optimization for Long-chain Reasoning of LLMs</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2407.00782v3</guid>
      <dc:creator>Zimu Lu, Aojun Zhou, Ke Wang, Houxing Ren, Weikang Shi, Junting Pan, Mingjie Zhan, Hongsheng Li</dc:creator>
      <pubDate>Mon, 15 Jul 2024 02:03:54 GMT</pubDate>
    </item>
    <item>
      <title>Using Large Language Models to Automate and Expedite Reinforcement Learning with Reward Machine</title>
      <link>http://arxiv.org/abs/2402.07069v1</link>
      <description>We present LARL-RM (Large language model-generated Automaton for Reinforcement Learning with Reward Machine) algorithm in order to encode high-level knowledge into reinforcement learning using automaton to expedite the reinforcement learning. Our method uses Large Language Models (LLM) to obtain high-level domain-specific knowledge using prompt engineering instead of providing the reinforcement learning algorithm directly with the high-level knowledge which requires an expert to encode the automaton. We use chain-of-thought and few-shot methods for prompt engineering and demonstrate that our method works using these approaches. Additionally, LARL-RM allows for fully closed-loop reinforcement learning without the need for an expert to guide and supervise the learning since LARL-RM can use the LLM directly to generate the required high-level knowledge for the task at hand. We also show the theoretical guarantee of our algorithm to converge to an optimal policy. We demonstrate that LARL-RM speeds up the convergence by 30% by implementing our method in two case studies.\n\n\nUsing Large Language Models to Automate and Expedite Reinforcement Learning with Reward Machine</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.07069v1</guid>
      <dc:creator>Shayan Meshkat Alsadat, Jean-Raphael Gaglione, Daniel Neider, Ufuk Topcu, Zhe Xu</dc:creator>
      <pubDate>Sun, 11 Feb 2024 00:00:05 GMT</pubDate>
    </item>
    <item>
      <title>Incentive Compatibility for AI Alignment in Sociotechnical Systems: Positions and Prospects</title>
      <link>http://arxiv.org/abs/2402.12907v2</link>
      <description>The burgeoning integration of artificial intelligence (AI) into human society brings forth significant implications for societal governance and safety. While considerable strides have been made in addressing AI alignment challenges, existing methodologies primarily focus on technical facets, often neglecting the intricate sociotechnical nature of AI systems, which can lead to a misalignment between the development and deployment contexts. To this end, we posit a new problem worth exploring: Incentive Compatibility Sociotechnical Alignment Problem (ICSAP). We hope this can call for more researchers to explore how to leverage the principles of Incentive Compatibility (IC) from game theory to bridge the gap between technical and societal components to maintain AI consensus with human societies in different contexts. We further discuss three classical game problems for achieving IC: mechanism design, contract theory, and Bayesian persuasion, in addressing the perspectives, potentials, and challenges of solving ICSAP, and provide preliminary implementation conceptions.\n\n\nIncentive Compatibility for AI Alignment in Sociotechnical Systems: Positions and Prospects</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.12907v2</guid>
      <dc:creator>Zhaowei Zhang, Fengshuo Bai, Mingzhi Wang, Haoyang Ye, Chengdong Ma, Yaodong Yang</dc:creator>
      <pubDate>Fri, 01 Mar 2024 11:18:44 GMT</pubDate>
    </item>
    <item>
      <title>On Softmax Direct Preference Optimization for Recommendation</title>
      <link>http://arxiv.org/abs/2406.09215v2</link>
      <description>Recommender systems aim to predict personalized rankings based on user preference data. With the rise of Language Models (LMs), LM-based recommenders have been widely explored due to their extensive world knowledge and powerful reasoning abilities. Most of the LM-based recommenders convert historical interactions into language prompts, pairing with a positive item as the target response and fine-tuning LM with a language modeling loss. However, the current objective fails to fully leverage preference data and is not optimized for personalized ranking tasks, which hinders the performance of LM-based recommenders. Inspired by the current advancement of Direct Preference Optimization (DPO) in human preference alignment and the success of softmax loss in recommendations, we propose Softmax-DPO (S-DPO) to instill ranking information into the LM to help LM-based recommenders distinguish preferred items from negatives, rather than solely focusing on positives. Specifically, we incorporate multiple negatives in user preference data and devise an alternative version of DPO loss tailored for LM-based recommenders, connected to softmax sampling strategies. Theoretically, we bridge S-DPO with the softmax loss over negative sampling and find that it has a side effect of mining hard negatives, which assures its exceptional capabilities in recommendation tasks. Empirically, extensive experiments conducted on three real-world datasets demonstrate the superiority of S-DPO to effectively model user preference and further boost recommendation performance while mitigating the data likelihood decline issue of DPO. Our codes are available at https://github.com/chenyuxin1999/S-DPO.\n\n\nOn Softmax Direct Preference Optimization for Recommendation</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.09215v2</guid>
      <dc:creator>Yuxin Chen, Junfei Tan, An Zhang, Zhengyi Yang, Leheng Sheng, Enzhi Zhang, Xiang Wang, Tat-Seng Chua</dc:creator>
      <pubDate>Fri, 14 Jun 2024 15:22:58 GMT</pubDate>
    </item>
    <item>
      <title>Direct Preference Knowledge Distillation for Large Language Models</title>
      <link>http://arxiv.org/abs/2406.19774v1</link>
      <description>In the field of large language models (LLMs), Knowledge Distillation (KD) is a critical technique for transferring capabilities from teacher models to student models. However, existing KD methods face limitations and challenges in distillation of LLMs, including efficiency and insufficient measurement capabilities of traditional KL divergence. It is shown that LLMs can serve as an implicit reward function, which we define as a supplement to KL divergence. In this work, we propose Direct Preference Knowledge Distillation (DPKD) for LLMs. DPKD utilizes distribution divergence to represent the preference loss and implicit reward function. We re-formulate KD of LLMs into two stages: first optimizing and objective consisting of implicit reward and reverse KL divergence and then improving the preference probability of teacher outputs over student outputs. We conducted experiments and analysis on various datasets with LLM parameters ranging from 120M to 13B and demonstrate the broad applicability and effectiveness of our DPKD approach. Meanwhile, we prove the value and effectiveness of the introduced implicit reward and output preference in KD through experiments and theoretical analysis. The DPKD method outperforms the baseline method in both output response precision and exact match percentage. Code and data are available at https://aka.ms/dpkd.\n\n\nDirect Preference Knowledge Distillation for Large Language Models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.19774v1</guid>
      <dc:creator>Yixing Li, Yuxian Gu, Li Dong, Dequan Wang, Yu Cheng, Furu Wei</dc:creator>
      <pubDate>Fri, 28 Jun 2024 09:23:40 GMT</pubDate>
    </item>
    <item>
      <title>InternLM-XComposer-2.5: A Versatile Large Vision Language Model Supporting Long-Contextual Input and Output</title>
      <link>http://arxiv.org/abs/2407.03320v1</link>
      <description>We present InternLM-XComposer-2.5 (IXC-2.5), a versatile large-vision language model that supports long-contextual input and output. IXC-2.5 excels in various text-image comprehension and composition applications, achieving GPT-4V level capabilities with merely 7B LLM backend. Trained with 24K interleaved image-text contexts, it can seamlessly extend to 96K long contexts via RoPE extrapolation. This long-context capability allows IXC-2.5 to excel in tasks requiring extensive input and output contexts. Compared to its previous 2.0 version, InternLM-XComposer-2.5 features three major upgrades in vision-language comprehension: (1) Ultra-High Resolution Understanding, (2) Fine-Grained Video Understanding, and (3) Multi-Turn Multi-Image Dialogue. In addition to comprehension, IXC-2.5 extends to two compelling applications using extra LoRA parameters for text-image composition: (1) Crafting Webpages and (2) Composing High-Quality Text-Image Articles. IXC-2.5 has been evaluated on 28 benchmarks, outperforming existing open-source state-of-the-art models on 16 benchmarks. It also surpasses or competes closely with GPT-4V and Gemini Pro on 16 key tasks. The InternLM-XComposer-2.5 is publicly available at https://github.com/InternLM/InternLM-XComposer.\n\n\nInternLM-XComposer-2.5: A Versatile Large Vision Language Model Supporting Long-Contextual Input and Output</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2407.03320v1</guid>
      <dc:creator>Pan Zhang, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Rui Qian, Lin Chen, Qipeng Guo, Haodong Duan, Bin Wang, Linke Ouyang, Songyang Zhang, Wenwei Zhang, Yining Li, Yang Gao, Peng Sun, Xinyue Zhang, Wei Li, Jingwen Li, Wenhai Wang, Hang Yan, Conghui He, Xingcheng Zhang, Kai Chen, Jifeng Dai, Yu Qiao, Dahua Lin, Jiaqi Wang</dc:creator>
      <pubDate>Wed, 03 Jul 2024 17:59:21 GMT</pubDate>
    </item>
    <item>
      <title>LLMBox: A Comprehensive Library for Large Language Models</title>
      <link>http://arxiv.org/abs/2407.05563v1</link>
      <description>To facilitate the research on large language models (LLMs), this paper presents a comprehensive and unified library, LLMBox, to ease the development, use, and evaluation of LLMs. This library is featured with three main merits: (1) a unified data interface that supports the flexible implementation of various training strategies, (2) a comprehensive evaluation that covers extensive tasks, datasets, and models, and (3) more practical consideration, especially on user-friendliness and efficiency. With our library, users can easily reproduce existing methods, train new models, and conduct comprehensive performance comparisons. To rigorously test LLMBox, we conduct extensive experiments in a diverse coverage of evaluation settings, and experimental results demonstrate the effectiveness and efficiency of our library in supporting various implementations related to LLMs. The detailed introduction and usage guidance can be found at https://github.com/RUCAIBox/LLMBox.\n\n\nLLMBox: A Comprehensive Library for Large Language Models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2407.05563v1</guid>
      <dc:creator>Tianyi Tang, Yiwen Hu, Bingqian Li, Wenyang Luo, Zijing Qin, Haoxiang Sun, Jiapeng Wang, Shiyi Xu, Xiaoxue Cheng, Geyang Guo, Han Peng, Bowen Zheng, Yiru Tang, Yingqian Min, Yushuo Chen, Jie Chen, Yuanqian Zhao, Luran Ding, Yuhao Wang, Zican Dong, Chunxuan Xia, Junyi Li, Kun Zhou, Wayne Xin Zhao, Ji-Rong Wen</dc:creator>
      <pubDate>Mon, 08 Jul 2024 02:39:33 GMT</pubDate>
    </item>
    <item>
      <title>Show, Don't Tell: Aligning Language Models with Demonstrated Feedback</title>
      <link>http://arxiv.org/abs/2406.00888v1</link>
      <description>Language models are aligned to emulate the collective voice of many, resulting in outputs that align with no one in particular. Steering LLMs away from generic output is possible through supervised finetuning or RLHF, but requires prohibitively large datasets for new ad-hoc tasks. We argue that it is instead possible to align an LLM to a specific setting by leveraging a very small number ($&lt;10$) of demonstrations as feedback. Our method, Demonstration ITerated Task Optimization (DITTO), directly aligns language model outputs to a user's demonstrated behaviors. Derived using ideas from online imitation learning, DITTO cheaply generates online comparison data by treating users' demonstrations as preferred over output from the LLM and its intermediate checkpoints. We evaluate DITTO's ability to learn fine-grained style and task alignment across domains such as news articles, emails, and blog posts. Additionally, we conduct a user study soliciting a range of demonstrations from participants ($N=16$). Across our benchmarks and user study, we find that win-rates for DITTO outperform few-shot prompting, supervised fine-tuning, and other self-play methods by an average of 19% points. By using demonstrations as feedback directly, DITTO offers a novel method for effective customization of LLMs.\n\n\nShow, Don't Tell: Aligning Language Models with Demonstrated Feedback</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.00888v1</guid>
      <dc:creator>Omar Shaikh, Michelle Lam, Joey Hejna, Yijia Shao, Michael Bernstein, Diyi Yang</dc:creator>
      <pubDate>Sun, 02 Jun 2024 23:13:56 GMT</pubDate>
    </item>
    <item>
      <title>GRATH: Gradual Self-Truthifying for Large Language Models</title>
      <link>http://arxiv.org/abs/2401.12292v2</link>
      <description>Truthfulness is paramount for large language models (LLMs) as they are increasingly deployed in real-world applications. However, existing LLMs still struggle with generating truthful content, as evidenced by their modest performance on benchmarks like TruthfulQA. To address this issue, we propose GRAdual self-truTHifying (GRATH), a novel post-processing method to enhance truthfulness of LLMs. GRATH utilizes out-of-domain question prompts to generate pairwise truthfulness training data with each pair containing a question and its correct and incorrect answers, and then optimizes the model via direct preference optimization (DPO) to learn from the truthfulness difference between answer pairs. GRATH iteratively refines truthfulness data and updates the model, leading to a gradual improvement in model truthfulness in a self-supervised manner. Empirically, we evaluate GRATH using different 7B-LLMs and compare with LLMs with similar or even larger sizes on benchmark datasets. Our results show that GRATH effectively improves LLMs' truthfulness without compromising other core capabilities. Notably, GRATH achieves state-of-the-art performance on TruthfulQA, with MC1 accuracy of 54.71% and MC2 accuracy of 69.10%, which even surpass those on 70B-LLMs.\n\n\nGRATH: Gradual Self-Truthifying for Large Language Models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2401.12292v2</guid>
      <dc:creator>Weixin Chen, Dawn Song, Bo Li</dc:creator>
      <pubDate>Wed, 31 Jan 2024 06:44:42 GMT</pubDate>
    </item>
    <item>
      <title>NormAd: A Benchmark for Measuring the Cultural Adaptability of Large Language Models</title>
      <link>http://arxiv.org/abs/2404.12464v5</link>
      <description>The integration of large language models (LLMs) into various global cultures fundamentally presents a challenge: LLMs must navigate interactions, respect social norms, and avoid transgressing cultural boundaries. However, it is still unclear if LLMs can adapt their outputs to diverse cultural norms. Our study focuses on this aspect. We introduce NormAd, a novel dataset, which includes 2.6k stories that represent social and cultural norms from 75 countries, to assess the ability of LLMs to adapt to different granular levels of socio-cultural contexts such as the country of origin, its associated cultural values, and prevalent social norms. Our study reveals that LLMs struggle with cultural reasoning across all contextual granularities, showing stronger adaptability to English-centric cultures over those from the Global South. Even with explicit social norms, the top-performing model, Mistral-7b-Instruct, achieves only 81.8% accuracy, lagging behind the 95.6% achieved by humans. Evaluation on NormAd further reveals that LLMs struggle to adapt to stories involving gift-giving across cultures. Due to inherent agreement or sycophancy biases, LLMs find it considerably easier to assess the social acceptability of stories that adhere to norms than those that deviate. Our benchmark measures the cultural adaptability (or lack thereof) of LLMs, emphasizing the potential to make these technologies more equitable and useful for global audiences. We release the NormAd dataset and its associated code on GitHub.\n\n\nNormad: A benchmark for measuring the cultural adaptability of large language models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2404.12464v5</guid>
      <dc:creator>Abhinav Rao, Akhila Yerukola, Vishwa Shah, Katharina Reinecke, Maarten Sap</dc:creator>
      <pubDate>Thu, 11 Jul 2024 14:05:59 GMT</pubDate>
    </item>
    <item>
      <title>Deep Bayesian Active Learning for Preference Modeling in Large Language Models</title>
      <link>http://arxiv.org/abs/2406.10023v1</link>
      <description>Leveraging human preferences for steering the behavior of Large Language Models (LLMs) has demonstrated notable success in recent years. Nonetheless, data selection and labeling are still a bottleneck for these systems, particularly at large scale. Hence, selecting the most informative points for acquiring human feedback may considerably reduce the cost of preference labeling and unleash the further development of LLMs. Bayesian Active Learning provides a principled framework for addressing this challenge and has demonstrated remarkable success in diverse settings. However, previous attempts to employ it for Preference Modeling did not meet such expectations. In this work, we identify that naive epistemic uncertainty estimation leads to the acquisition of redundant samples. We address this by proposing the Bayesian Active Learner for Preference Modeling (BAL-PM), a novel stochastic acquisition policy that not only targets points of high epistemic uncertainty according to the preference model but also seeks to maximize the entropy of the acquired prompt distribution in the feature space spanned by the employed LLM. Notably, our experiments demonstrate that BAL-PM requires 33% to 68% fewer preference labels in two popular human preference datasets and exceeds previous stochastic Bayesian acquisition policies.\n\n\nDeep Bayesian Active Learning for Preference Modeling in Large Language Models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.10023v1</guid>
      <dc:creator>Luckeciano C. Melo, Panagiotis Tigas, Alessandro Abate, Yarin Gal</dc:creator>
      <pubDate>Fri, 14 Jun 2024 13:32:43 GMT</pubDate>
    </item>
    <item>
      <title>Quantitative AI Risk Assessments: Opportunities and Challenges</title>
      <link>http://arxiv.org/abs/2209.06317v2</link>
      <description>Although AI-based systems are increasingly being leveraged to provide value to organizations, individuals, and society, significant attendant risks have been identified. These risks have led to proposed regulations, litigation, and general societal concerns.   As with any promising technology, organizations want to benefit from the positive capabilities of AI technology while reducing the risks. The best way to reduce risks is to implement comprehensive AI lifecycle governance where policies and procedures are described and enforced during the design, development, deployment, and monitoring of an AI system. While support for comprehensive governance is beginning to emerge, organizations often need to identify the risks of deploying an already-built model without knowledge of how it was constructed or access to its original developers.   Such an assessment will quantitatively assess the risks of an existing model in a manner analogous to how a home inspector might assess the energy efficiency of an already-built home or a physician might assess overall patient health based on a battery of tests. This paper explores the concept of a quantitative AI Risk Assessment, exploring the opportunities, challenges, and potential impacts of such an approach, and discussing how it might improve AI regulations.\n\n\nRisks and Opportunities of Open-Source Generative AI</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2209.06317v2</guid>
      <dc:creator>David Piorkowski, Michael Hind, John Richards</dc:creator>
      <pubDate>Wed, 11 Jan 2023 18:20:01 GMT</pubDate>
    </item>
    <item>
      <title>Evaluating Copyright Takedown Methods for Language Models</title>
      <link>http://arxiv.org/abs/2406.18664v3</link>
      <description>Language models (LMs) derive their capabilities from extensive training on diverse data, including potentially copyrighted material. These models can memorize and generate content similar to their training data, posing potential concerns. Therefore, model creators are motivated to develop mitigation methods that prevent generating protected content. We term this procedure as copyright takedowns for LMs, noting the conceptual similarity to (but legal distinction from) the DMCA takedown This paper introduces the first evaluation of the feasibility and side effects of copyright takedowns for LMs. We propose CoTaEval, an evaluation framework to assess the effectiveness of copyright takedown methods, the impact on the model's ability to retain uncopyrightable factual knowledge from the training data whose recitation is embargoed, and how well the model maintains its general utility and efficiency. We examine several strategies, including adding system prompts, decoding-time filtering interventions, and unlearning approaches. Our findings indicate that no tested method excels across all metrics, showing significant room for research in this unique problem setting and indicating potential unresolved challenges for live policy proposals.\n\n\nEvaluating Copyright Takedown Methods for Language Models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.18664v3</guid>
      <dc:creator>Boyi Wei, Weijia Shi, Yangsibo Huang, Noah A. Smith, Chiyuan Zhang, Luke Zettlemoyer, Kai Li, Peter Henderson</dc:creator>
      <pubDate>Thu, 11 Jul 2024 07:45:04 GMT</pubDate>
    </item>
    <item>
      <title>MindAgent: Emergent Gaming Interaction</title>
      <link>http://arxiv.org/abs/2309.09971v2</link>
      <description>Large Language Models (LLMs) have the capacity of performing complex scheduling in a multi-agent system and can coordinate these agents into completing sophisticated tasks that require extensive collaboration. However, despite the introduction of numerous gaming frameworks, the community has insufficient benchmarks towards building general multi-agents collaboration infrastructure that encompass both LLM and human-NPCs collaborations. In this work, we propose a novel infrastructure - MindAgent - to evaluate planning and coordination emergent capabilities for gaming interaction. In particular, our infrastructure leverages existing gaming framework, to i) require understanding of the coordinator for a multi-agent system, ii) collaborate with human players via un-finetuned proper instructions, and iii) establish an in-context learning on few-shot prompt with feedback. Furthermore, we introduce CUISINEWORLD, a new gaming scenario and related benchmark that dispatch a multi-agent collaboration efficiency and supervise multiple agents playing the game simultaneously. We conduct comprehensive evaluations with new auto-metric CoS for calculating the collaboration efficiency. Finally, our infrastructure can be deployed into real-world gaming scenarios in a customized VR version of CUISINEWORLD and adapted in existing broader Minecraft gaming domain. We hope our findings on LLMs and the new infrastructure for general-purpose scheduling and coordination can help shed light on how such skills can be obtained by learning from large language corpora.\n\n\nInteractive Evolution: A Neural-Symbolic Self-Training Framework For Large Language Models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2309.09971v2</guid>
      <dc:creator>Ran Gong, Qiuyuan Huang, Xiaojian Ma, Hoi Vo, Zane Durante, Yusuke Noda, Zilong Zheng, Song-Chun Zhu, Demetri Terzopoulos, Li Fei-Fei, Jianfeng Gao</dc:creator>
      <pubDate>Tue, 19 Sep 2023 14:36:53 GMT</pubDate>
    </item>
    <item>
      <title>COPR: Continual Human Preference Learning via Optimal Policy Regularization</title>
      <link>http://arxiv.org/abs/2402.14228v2</link>
      <description>Reinforcement Learning from Human Feedback (RLHF) is commonly utilized to improve the alignment of Large Language Models (LLMs) with human preferences. Given the evolving nature of human preferences, continual alignment becomes more crucial and practical in comparison to traditional static alignment. Nevertheless, making RLHF compatible with Continual Learning (CL) is challenging due to its complex process. Meanwhile, directly learning new human preferences may lead to Catastrophic Forgetting (CF) of historical preferences, resulting in helpless or harmful outputs. To overcome these challenges, we propose the Continual Optimal Policy Regularization (COPR) method, which draws inspiration from the optimal policy theory. COPR utilizes a sampling distribution as a demonstration and regularization constraints for CL. It adopts the Lagrangian Duality (LD) method to dynamically regularize the current policy based on the historically optimal policy, which prevents CF and avoids over-emphasizing unbalanced objectives. We also provide formal proof for the learnability of COPR. The experimental results show that COPR outperforms strong CL baselines on our proposed benchmark, in terms of reward-based, GPT-4 evaluations and human assessment. Furthermore, we validate the robustness of COPR under various CL settings, including different backbones, replay memory sizes, and learning orders.\n\n\nCOPR: Continual Human Preference Learning via Optimal Policy Regularization</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.14228v2</guid>
      <dc:creator>Han Zhang, Lin Gui, Yu Lei, Yuanzhao Zhai, Yehong Zhang, Yulan He, Hui Wang, Yue Yu, Kam-Fai Wong, Bin Liang, Ruifeng Xu</dc:creator>
      <pubDate>Tue, 27 Feb 2024 08:47:37 GMT</pubDate>
    </item>
    <item>
      <title>Preference Optimization for Molecule Synthesis with Conditional Residual Energy-based Models</title>
      <link>http://arxiv.org/abs/2406.02066v1</link>
      <description>Molecule synthesis through machine learning is one of the fundamental problems in drug discovery. Current data-driven strategies employ one-step retrosynthesis models and search algorithms to predict synthetic routes in a top-bottom manner. Despite their effective performance, these strategies face limitations in the molecule synthetic route generation due to a greedy selection of the next molecule set without any lookahead. Furthermore, existing strategies cannot control the generation of synthetic routes based on possible criteria such as material costs, yields, and step count. In this work, we propose a general and principled framework via conditional residual energy-based models (EBMs), that focus on the quality of the entire synthetic route based on the specific criteria. By incorporating an additional energy-based function into our probabilistic model, our proposed algorithm can enhance the quality of the most probable synthetic routes (with higher probabilities) generated by various strategies in a plug-and-play fashion. Extensive experiments demonstrate that our framework can consistently boost performance across various strategies and outperforms previous state-of-the-art top-1 accuracy by a margin of 2.5%. Code is available at https://github.com/SongtaoLiu0823/CREBM.\n\n\nPreference Optimization for Molecule Synthesis with Conditional Residual Energy-based Models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.02066v1</guid>
      <dc:creator>Songtao Liu, Hanjun Dai, Yue Zhao, Peng Liu</dc:creator>
      <pubDate>Tue, 04 Jun 2024 07:49:30 GMT</pubDate>
    </item>
    <item>
      <title>KwaiYiiMath: Technical Report</title>
      <link>http://arxiv.org/abs/2310.07488v2</link>
      <description>Recent advancements in large language models (LLMs) have demonstrated remarkable abilities in handling a variety of natural language processing (NLP) downstream tasks, even on mathematical tasks requiring multi-step reasoning. In this report, we introduce the KwaiYiiMath which enhances the mathematical reasoning abilities of KwaiYiiBase1, by applying Supervised Fine-Tuning (SFT) and Reinforced Learning from Human Feedback (RLHF), including on both English and Chinese mathematical tasks. Meanwhile, we also constructed a small-scale Chinese primary school mathematics test set (named KMath), consisting of 188 examples to evaluate the correctness of the problem-solving process generated by the models. Empirical studies demonstrate that KwaiYiiMath can achieve state-of-the-art (SOTA) performance on GSM8k, CMath, and KMath compared with the similar size models, respectively.\n\n\nKwaiYiiMath: Technical Report</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2310.07488v2</guid>
      <dc:creator>Jiayi Fu, Lei Lin, Xiaoyang Gao, Pengli Liu, Zhengzong Chen, Zhirui Yang, Shengnan Zhang, Xue Zheng, Yan Li, Yuliang Liu, Xucheng Ye, Yiqiao Liao, Chao Liao, Bin Chen, Chengru Song, Junchen Wan, Zijia Lin, Fuzheng Zhang, Zhongyuan Wang, Di Zhang, Kun Gai</dc:creator>
      <pubDate>Thu, 19 Oct 2023 12:34:32 GMT</pubDate>
    </item>
    <item>
      <title>Cedille: A large autoregressive French language model</title>
      <link>http://arxiv.org/abs/2202.03371v1</link>
      <description>Scaling up the size and training of autoregressive language models has enabled novel ways of solving Natural Language Processing tasks using zero-shot and few-shot learning. While extreme-scale language models such as GPT-3 offer multilingual capabilities, zero-shot learning for languages other than English remain largely unexplored. Here, we introduce Cedille, a large open source auto-regressive language model, specifically trained for the French language. Our results show that Cedille outperforms existing French language models and is competitive with GPT-3 on a range of French zero-shot benchmarks. Furthermore, we provide an in-depth comparison of the toxicity exhibited by these models, showing that Cedille marks an improvement in language model safety thanks to dataset filtering.\n\n\nOR-Bench: An Over-Refusal Benchmark for Large Language Models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2202.03371v1</guid>
      <dc:creator>Martin Müller, Florian Laurent</dc:creator>
      <pubDate>Mon, 07 Feb 2022 17:40:43 GMT</pubDate>
    </item>
    <item>
      <title>Exploring the Intersection of Large Language Models and Agent-Based Modeling via Prompt Engineering</title>
      <link>http://arxiv.org/abs/2308.07411v1</link>
      <description>The final frontier for simulation is the accurate representation of complex, real-world social systems. While agent-based modeling (ABM) seeks to study the behavior and interactions of agents within a larger system, it is unable to faithfully capture the full complexity of human-driven behavior. Large language models (LLMs), like ChatGPT, have emerged as a potential solution to this bottleneck by enabling researchers to explore human-driven interactions in previously unimaginable ways. Our research investigates simulations of human interactions using LLMs. Through prompt engineering, inspired by Park et al. (2023), we present two simulations of believable proxies of human behavior: a two-agent negotiation and a six-agent murder mystery game.\n\n\nSelf-alignment of large language models via multi-agent social simulation</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2308.07411v1</guid>
      <dc:creator>Edward Junprung</dc:creator>
      <pubDate>Mon, 14 Aug 2023 18:58:00 GMT</pubDate>
    </item>
    <item>
      <title>DiffPoGAN: Diffusion Policies with Generative Adversarial Networks for Offline Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2406.09089v1</link>
      <description>Offline reinforcement learning (RL) can learn optimal policies from pre-collected offline datasets without interacting with the environment, but the sampled actions of the agent cannot often cover the action distribution under a given state, resulting in the extrapolation error issue. Recent works address this issue by employing generative adversarial networks (GANs). However, these methods often suffer from insufficient constraints on policy exploration and inaccurate representation of behavior policies. Moreover, the generator in GANs fails in fooling the discriminator while maximizing the expected returns of a policy. Inspired by the diffusion, a generative model with powerful feature expressiveness, we propose a new offline RL method named Diffusion Policies with Generative Adversarial Networks (DiffPoGAN). In this approach, the diffusion serves as the policy generator to generate diverse distributions of actions, and a regularization method based on maximum likelihood estimation (MLE) is developed to generate data that approximate the distribution of behavior policies. Besides, we introduce an additional regularization term based on the discriminator output to effectively constrain policy exploration for policy improvement. Comprehensive experiments are conducted on the datasets for deep data-driven reinforcement learning (D4RL), and experimental results show that DiffPoGAN outperforms state-of-the-art methods in offline RL.\n\n\nPreferred-Action-Optimized Diffusion Policies for Offline Reinforcement Learning</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.09089v1</guid>
      <dc:creator>Xuemin Hu, Shen Li, Yingfen Xu, Bo Tang, Long Chen</dc:creator>
      <pubDate>Thu, 13 Jun 2024 13:15:40 GMT</pubDate>
    </item>
    <item>
      <title>CopyBench: Measuring Literal and Non-Literal Reproduction of Copyright-Protected Text in Language Model Generation</title>
      <link>http://arxiv.org/abs/2407.07087v1</link>
      <description>Evaluating the degree of reproduction of copyright-protected content by language models (LMs) is of significant interest to the AI and legal communities. Although both literal and non-literal similarities are considered by courts when assessing the degree of reproduction, prior research has focused only on literal similarities. To bridge this gap, we introduce CopyBench, a benchmark designed to measure both literal and non-literal copying in LM generations. Using copyrighted fiction books as text sources, we provide automatic evaluation protocols to assess literal and non-literal copying, balanced against the model utility in terms of the ability to recall facts from the copyrighted works and generate fluent completions. We find that, although literal copying is relatively rare, two types of non-literal copying -- event copying and character copying -- occur even in models as small as 7B parameters. Larger models demonstrate significantly more copying, with literal copying rates increasing from 0.2% to 10.5% and non-literal copying from 2.3% to 6.9% when comparing Llama3-8B and 70B models, respectively. We further evaluate the effectiveness of current strategies for mitigating copying and show that (1) training-time alignment can reduce literal copying but may increase non-literal copying, and (2) current inference-time mitigation methods primarily reduce literal but not non-literal copying.\n\n\nCopyBench: Measuring Literal and Non-Literal Reproduction of Copyright-Protected Text in Language Model Generation</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2407.07087v1</guid>
      <dc:creator>Tong Chen, Akari Asai, Niloofar Mireshghallah, Sewon Min, James Grimmelmann, Yejin Choi, Hannaneh Hajishirzi, Luke Zettlemoyer, Pang Wei Koh</dc:creator>
      <pubDate>Tue, 09 Jul 2024 17:58:18 GMT</pubDate>
    </item>
    <item>
      <title>Step-aware Preference Optimization: Aligning Preference with Denoising Performance at Each Step</title>
      <link>http://arxiv.org/abs/2406.04314v1</link>
      <description>Recently, Direct Preference Optimization (DPO) has extended its success from aligning large language models (LLMs) to aligning text-to-image diffusion models with human preferences. Unlike most existing DPO methods that assume all diffusion steps share a consistent preference order with the final generated images, we argue that this assumption neglects step-specific denoising performance and that preference labels should be tailored to each step's contribution. To address this limitation, we propose Step-aware Preference Optimization (SPO), a novel post-training approach that independently evaluates and adjusts the denoising performance at each step, using a step-aware preference model and a step-wise resampler to ensure accurate step-aware supervision. Specifically, at each denoising step, we sample a pool of images, find a suitable win-lose pair, and, most importantly, randomly select a single image from the pool to initialize the next denoising step. This step-wise resampler process ensures the next win-lose image pair comes from the same image, making the win-lose comparison independent of the previous step. To assess the preferences at each step, we train a separate step-aware preference model that can be applied to both noisy and clean images. Our experiments with Stable Diffusion v1.5 and SDXL demonstrate that SPO significantly outperforms the latest Diffusion-DPO in aligning generated images with complex, detailed prompts and enhancing aesthetics, while also achieving more than 20x times faster in training efficiency. Code and model: https://rockeycoss.github.io/spo.github.io/\n\n\nStep-aware Preference Optimization: Aligning Preference with Denoising Performance at Each Step</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.04314v1</guid>
      <dc:creator>Zhanhao Liang, Yuhui Yuan, Shuyang Gu, Bohan Chen, Tiankai Hang, Ji Li, Liang Zheng</dc:creator>
      <pubDate>Thu, 06 Jun 2024 17:57:09 GMT</pubDate>
    </item>
    <item>
      <title>SELMA: Learning and Merging Skill-Specific Text-to-Image Experts with Auto-Generated Data</title>
      <link>http://arxiv.org/abs/2403.06952v1</link>
      <description>Recent text-to-image (T2I) generation models have demonstrated impressive capabilities in creating images from text descriptions. However, these T2I generation models often fall short of generating images that precisely match the details of the text inputs, such as incorrect spatial relationship or missing objects. In this paper, we introduce SELMA: Skill-Specific Expert Learning and Merging with Auto-Generated Data, a novel paradigm to improve the faithfulness of T2I models by fine-tuning models on automatically generated, multi-skill image-text datasets, with skill-specific expert learning and merging. First, SELMA leverages an LLM's in-context learning capability to generate multiple datasets of text prompts that can teach different skills, and then generates the images with a T2I model based on the prompts. Next, SELMA adapts the T2I model to the new skills by learning multiple single-skill LoRA (low-rank adaptation) experts followed by expert merging. Our independent expert fine-tuning specializes multiple models for different skills, and expert merging helps build a joint multi-skill T2I model that can generate faithful images given diverse text prompts, while mitigating the knowledge conflict from different datasets. We empirically demonstrate that SELMA significantly improves the semantic alignment and text faithfulness of state-of-the-art T2I diffusion models on multiple benchmarks (+2.1% on TIFA and +6.9% on DSG), human preference metrics (PickScore, ImageReward, and HPS), as well as human evaluation. Moreover, fine-tuning with image-text pairs auto-collected via SELMA shows comparable performance to fine-tuning with ground truth data. Lastly, we show that fine-tuning with images from a weaker T2I model can help improve the generation quality of a stronger T2I model, suggesting promising weak-to-strong generalization in T2I models.\n\n\nSELMA: Learning and Merging Skill-Specific Text-to-Image Experts with Auto-Generated Data</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2403.06952v1</guid>
      <dc:creator>Jialu Li, Jaemin Cho, Yi-Lin Sung, Jaehong Yoon, Mohit Bansal</dc:creator>
      <pubDate>Mon, 11 Mar 2024 17:35:33 GMT</pubDate>
    </item>
    <item>
      <title>Don't Forget Your Reward Values: Language Model Alignment via Value-based Calibration</title>
      <link>http://arxiv.org/abs/2402.16030v1</link>
      <description>While Reinforcement Learning from Human Feedback (RLHF) significantly enhances the generation quality of Large Language Models (LLMs), recent studies have raised concerns regarding the complexity and instability associated with the Proximal Policy Optimization (PPO) algorithm, proposing a series of order-based calibration methods as viable alternatives. This paper delves further into current order-based methods, examining their inefficiencies in utilizing reward values and addressing misalignment issues. Building upon these findings, we propose a novel \textbf{V}alue-based \textbf{C}ali\textbf{B}ration (VCB) method to better align LLMs with human preferences. Experimental results demonstrate that VCB surpasses existing alignment methods on AI assistant and summarization datasets, providing impressive generalizability, robustness, and stability in diverse settings.\n\n\nValue aligned large language models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.16030v1</guid>
      <dc:creator>Xin Mao, Feng-Lin Li, Huimin Xu, Wei Zhang, Anh Tuan Luu</dc:creator>
      <pubDate>Sun, 25 Feb 2024 08:45:10 GMT</pubDate>
    </item>
    <item>
      <title>A Reliable Knowledge Processing Framework for Combustion Science using Foundation Models</title>
      <link>http://arxiv.org/abs/2401.00544v2</link>
      <description>This research explores the integration of large language models (LLMs) into scientific data assimilation, focusing on combustion science as a case study. Leveraging foundational models integrated with Retrieval-Augmented Generation (RAG) framework, the study introduces an approach to process diverse combustion research data, spanning experimental studies, simulations, and literature. The multifaceted nature of combustion research emphasizes the critical role of knowledge processing in navigating and extracting valuable information from a vast and diverse pool of sources. The developed approach minimizes computational and economic expenses while optimizing data privacy and accuracy. It incorporates prompt engineering and offline open-source LLMs, offering user autonomy in selecting base models. The study provides a thorough examination of text segmentation strategies, conducts comparative studies between LLMs, and explores various optimized prompts to demonstrate the effectiveness of the framework. By incorporating an external database, the framework outperforms a conventional LLM in generating accurate responses and constructing robust arguments. Additionally, the study delves into the investigation of optimized prompt templates for the purpose of efficient extraction of scientific literature. The research addresses concerns related to hallucinations and false research articles by introducing a custom workflow developed with a detection algorithm to filter out inaccuracies. Despite identified areas for improvement, the framework consistently delivers accurate domain-specific responses with minimal human oversight. The prompt-agnostic approach introduced holds promise for future deliberations. The study underscores the significance of integrating LLMs and knowledge processing techniques in scientific research, providing a foundation for advancements in data assimilation and utilization.\n\n\nA reliable knowledge processing framework for combustion science using foundation models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2401.00544v2</guid>
      <dc:creator>Vansh Sharma, Venkat Raman</dc:creator>
      <pubDate>Tue, 02 Jan 2024 03:03:18 GMT</pubDate>
    </item>
    <item>
      <title>PORT: Preference Optimization on Reasoning Traces</title>
      <link>http://arxiv.org/abs/2406.16061v1</link>
      <description>Preference optimization methods have been successfully applied to improve not only the alignment of large language models (LLMs) with human values, but also specific natural language tasks such as summarization and stylistic continuations. This paper proposes using preference optimization methods on Chain-of-Thought steps in order to improve the reasoning performances of language models. While the chosen answers are obtained from datasets that include reasoning traces, we propose two complementary schemes for generating rejected answers: digit corruption, and weak LLM prompting. Our approach leads to increased accuracy on the GSM8K, AQuA-RAT, and ARC benchmarks for Falcon2-11B and Mistral-7B. For example, the approach can lead to up to a relative 8.47% increase in accuracy on the GSM8K benchmark without any extra annotations. This work suggests that spending resources on creating more datasets of reasoning traces would further boost LLM performances on informal reasoning tasks.\n\n\nPORT: Preference Optimization on Reasoning Traces</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.16061v1</guid>
      <dc:creator>Salem Lahlou, Abdalgader Abubaker, Hakim Hacid</dc:creator>
      <pubDate>Sun, 23 Jun 2024 09:51:06 GMT</pubDate>
    </item>
    <item>
      <title>OpenRLHF: An Easy-to-use, Scalable and High-performance RLHF Framework</title>
      <link>http://arxiv.org/abs/2405.11143v3</link>
      <description>As large language models (LLMs) continue to grow by scaling laws, reinforcement learning from human feedback (RLHF) has gained significant attention due to its outstanding performance. However, unlike pretraining or fine-tuning a single model, scaling reinforcement learning from human feedback (RLHF) for training large language models poses coordination challenges across four models. We present OpenRLHF, an open-source framework enabling efficient RLHF scaling. Unlike existing RLHF frameworks that co-locate four models on the same GPUs, OpenRLHF re-designs scheduling for the models beyond 70B parameters using Ray, vLLM, and DeepSpeed, leveraging improved resource utilization and diverse training approaches. Integrating seamlessly with Hugging Face, OpenRLHF provides an out-of-the-box solution with optimized algorithms and launch scripts, which ensures user-friendliness. OpenRLHF implements RLHF, DPO, rejection sampling, and other alignment techniques. Empowering state-of-the-art LLM development, OpenRLHF's code is available at \url{https://github.com/OpenRLHF/OpenRLHF}.\n\n\nOpenRLHF: An Easy-to-use, Scalable and High-performance RLHF Framework</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.11143v3</guid>
      <dc:creator>Jian Hu, Xibin Wu, Weixun Wang, Xianyu, Dehao Zhang, Yu Cao</dc:creator>
      <pubDate>Wed, 17 Jul 2024 09:18:35 GMT</pubDate>
    </item>
    <item>
      <title>VideoPhy: Evaluating Physical Commonsense for Video Generation</title>
      <link>http://arxiv.org/abs/2406.03520v1</link>
      <description>Recent advances in internet-scale video data pretraining have led to the development of text-to-video generative models that can create high-quality videos across a broad range of visual concepts and styles. Due to their ability to synthesize realistic motions and render complex objects, these generative models have the potential to become general-purpose simulators of the physical world. However, it is unclear how far we are from this goal with the existing text-to-video generative models. To this end, we present VideoPhy, a benchmark designed to assess whether the generated videos follow physical commonsense for real-world activities (e.g. marbles will roll down when placed on a slanted surface). Specifically, we curate a list of 688 captions that involve interactions between various material types in the physical world (e.g., solid-solid, solid-fluid, fluid-fluid). We then generate videos conditioned on these captions from diverse state-of-the-art text-to-video generative models, including open models (e.g., VideoCrafter2) and closed models (e.g., Lumiere from Google, Pika). Further, our human evaluation reveals that the existing models severely lack the ability to generate videos adhering to the given text prompts, while also lack physical commonsense. Specifically, the best performing model, Pika, generates videos that adhere to the caption and physical laws for only 19.7% of the instances. VideoPhy thus highlights that the video generative models are far from accurately simulating the physical world. Finally, we also supplement the dataset with an auto-evaluator, VideoCon-Physics, to assess semantic adherence and physical commonsense at scale.\n\n\nVideoPhy: Evaluating Physical Commonsense for Video Generation</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.03520v1</guid>
      <dc:creator>Hritik Bansal, Zongyu Lin, Tianyi Xie, Zeshun Zong, Michal Yarom, Yonatan Bitton, Chenfanfu Jiang, Yizhou Sun, Kai-Wei Chang, Aditya Grover</dc:creator>
      <pubDate>Wed, 05 Jun 2024 17:53:55 GMT</pubDate>
    </item>
    <item>
      <title>Hybrid Alignment Training for Large Language Models</title>
      <link>http://arxiv.org/abs/2406.15178v1</link>
      <description>Alignment training is crucial for enabling large language models (LLMs) to cater to human intentions and preferences. It is typically performed based on two stages with different objectives: instruction-following alignment and human-preference alignment. However, aligning LLMs with these objectives in sequence suffers from an inherent problem: the objectives may conflict, and the LLMs cannot guarantee to simultaneously align with the instructions and human preferences well. To response to these, in this work, we propose a Hybrid Alignment Training (Hbat) approach, based on alternating alignment and modified elastic weight consolidation methods. The basic idea is to alternate between different objectives during alignment training, so that better collaboration can be achieved between the two alignment tasks.We experiment with Hbat on summarization and dialogue tasks. Experimental results show that the proposed \textsc{Hbat} can significantly outperform all baselines. Notably, Hbat yields consistent performance gains over the traditional two-stage alignment training when using both proximal policy optimization and direct preference optimization.\n\n\nLow-Redundant Optimization for Large Language Model Alignment</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.15178v1</guid>
      <dc:creator>Chenglong Wang, Hang Zhou, Kaiyan Chang, Bei Li, Yongyu Mu, Tong Xiao, Tongran Liu, Jingbo Zhu</dc:creator>
      <pubDate>Fri, 21 Jun 2024 14:23:57 GMT</pubDate>
    </item>
    <item>
      <title>PISTOL: Dataset Compilation Pipeline for Structural Unlearning of LLMs</title>
      <link>http://arxiv.org/abs/2406.16810v1</link>
      <description>Recently, machine unlearning, which seeks to erase specific data stored in the pre-trained or fine-tuned models, has emerged as a crucial protective measure for LLMs. However, unlearning approaches for LLMs that have been considered thus far have focused on the removal of independent data points and have not taken into account that the stored facts are logically connected to one another and form an implicit knowledge graph. To facilitate the development of structural unlearning methods, which are essential for the practical application of unlearning, we propose PISTOL, a pipeline for compiling multi-scenario datasets for benchmarking structural LLM unlearning. Additionally, leveraging sample datasets synthesized using PISTOL, we conducted benchmarks with four distinct unlearning methods on both Llama2-7B and Mistral-7B models. This analysis helps to illustrate the prevailing challenges in effectively and robustly removing highly inter-connected data, batched data, or data skewed towards a specific domain. It also highlights the choice of pre-trained model can impact unlearning performance. This work not only advances our understandings on the limitation of current LLMs unlearning methods and proposes future research directions, but also provides a replicable framework for ongoing exploration and validation in the field.\n\n\nPISTOL: Dataset Compilation Pipeline for Structural Unlearning of LLMs</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.16810v1</guid>
      <dc:creator>Xinchi Qiu, William F. Shen, Yihong Chen, Nicola Cancedda, Pontus Stenetorp, Nicholas D. Lane</dc:creator>
      <pubDate>Mon, 24 Jun 2024 17:22:36 GMT</pubDate>
    </item>
    <item>
      <title>Self-Train Before You Transcribe</title>
      <link>http://arxiv.org/abs/2406.12937v1</link>
      <description>When there is a mismatch between the training and test domains, current speech recognition systems show significant performance degradation. Self-training methods, such as noisy student teacher training, can help address this and enable the adaptation of models under such domain shifts. However, self-training typically requires a collection of unlabelled target domain data. For settings where this is not practical, we investigate the benefit of performing noisy student teacher training on recordings in the test set as a test-time adaptation approach. Similarly to the dynamic evaluation approach in language modelling, this enables the transfer of information across utterance boundaries and functions as a method of domain adaptation. A range of in-domain and out-of-domain datasets are used for experiments demonstrating large relative gains of up to 32.2%. Interestingly, our method showed larger gains than the typical self-training setup that utilises separate adaptation data.\n\n\nTranscribe3d: Grounding llms using transcribed information for 3d referential reasoning with self-corrected finetuning</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.12937v1</guid>
      <dc:creator>Robert Flynn, Anton Ragni</dc:creator>
      <pubDate>Mon, 17 Jun 2024 09:21:00 GMT</pubDate>
    </item>
    <item>
      <title>RULE: Reliable Multimodal RAG for Factuality in Medical Vision Language Models</title>
      <link>http://arxiv.org/abs/2407.05131v1</link>
      <description>The recent emergence of Medical Large Vision Language Models (Med-LVLMs) has enhanced medical diagnosis. However, current Med-LVLMs frequently encounter factual issues, often generating responses that do not align with established medical facts. Retrieval-Augmented Generation (RAG), which utilizes external knowledge, can improve the factual accuracy of these models but introduces two major challenges. First, limited retrieved contexts might not cover all necessary information, while excessive retrieval can introduce irrelevant and inaccurate references, interfering with the model's generation. Second, in cases where the model originally responds correctly, applying RAG can lead to an over-reliance on retrieved contexts, resulting in incorrect answers. To address these issues, we propose RULE, which consists of two components. First, we introduce a provably effective strategy for controlling factuality risk through the calibrated selection of the number of retrieved contexts. Second, based on samples where over-reliance on retrieved contexts led to errors, we curate a preference dataset to fine-tune the model, balancing its dependence on inherent knowledge and retrieved contexts for generation. We demonstrate the effectiveness of RULE on three medical VQA datasets, achieving an average improvement of 20.8% in factual accuracy. We publicly release our benchmark and code in https://github.com/richard-peng-xia/RULE.\n\n\nRULE: Reliable Multimodal RAG for Factuality in Medical Vision Language Models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2407.05131v1</guid>
      <dc:creator>Peng Xia, Kangyu Zhu, Haoran Li, Hongtu Zhu, Yun Li, Gang Li, Linjun Zhang, Huaxiu Yao</dc:creator>
      <pubDate>Sat, 06 Jul 2024 16:45:07 GMT</pubDate>
    </item>
    <item>
      <title>Fast Adversarial Attacks on Language Models In One GPU Minute</title>
      <link>http://arxiv.org/abs/2402.15570v1</link>
      <description>In this paper, we introduce a novel class of fast, beam search-based adversarial attack (BEAST) for Language Models (LMs). BEAST employs interpretable parameters, enabling attackers to balance between attack speed, success rate, and the readability of adversarial prompts. The computational efficiency of BEAST facilitates us to investigate its applications on LMs for jailbreaking, eliciting hallucinations, and privacy attacks. Our gradient-free targeted attack can jailbreak aligned LMs with high attack success rates within one minute. For instance, BEAST can jailbreak Vicuna-7B-v1.5 under one minute with a success rate of 89% when compared to a gradient-based baseline that takes over an hour to achieve 70% success rate using a single Nvidia RTX A6000 48GB GPU. Additionally, we discover a unique outcome wherein our untargeted attack induces hallucinations in LM chatbots. Through human evaluations, we find that our untargeted attack causes Vicuna-7B-v1.5 to produce ~15% more incorrect outputs when compared to LM outputs in the absence of our attack. We also learn that 22% of the time, BEAST causes Vicuna to generate outputs that are not relevant to the original prompt. Further, we use BEAST to generate adversarial prompts in a few seconds that can boost the performance of existing membership inference attacks for LMs. We believe that our fast attack, BEAST, has the potential to accelerate research in LM security and privacy. Our codebase is publicly available at https://github.com/vinusankars/BEAST.\n\n\nFast Adversarial Attacks on Language Models In One GPU Minute</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.15570v1</guid>
      <dc:creator>Vinu Sankar Sadasivan, Shoumik Saha, Gaurang Sriramanan, Priyatham Kattakinda, Atoosa Chegini, Soheil Feizi</dc:creator>
      <pubDate>Fri, 23 Feb 2024 19:12:53 GMT</pubDate>
    </item>
    <item>
      <title>MIA-Bench: Towards Better Instruction Following Evaluation of Multimodal LLMs</title>
      <link>http://arxiv.org/abs/2407.01509v2</link>
      <description>We introduce MIA-Bench, a new benchmark designed to evaluate multimodal large language models (MLLMs) on their ability to strictly adhere to complex instructions. Our benchmark comprises a diverse set of 400 image-prompt pairs, each crafted to challenge the models' compliance with layered instructions in generating accurate responses that satisfy specific requested patterns. Evaluation results from a wide array of state-of-the-art MLLMs reveal significant variations in performance, highlighting areas for improvement in instruction fidelity. Additionally, we create extra training data and explore supervised fine-tuning to enhance the models' ability to strictly follow instructions without compromising performance on other tasks. We hope this benchmark not only serves as a tool for measuring MLLM adherence to instructions, but also guides future developments in MLLM training methods.\n\n\nMIA-Bench: Towards Better Instruction Following Evaluation of Multimodal LLMs</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2407.01509v2</guid>
      <dc:creator>Yusu Qian, Hanrong Ye, Jean-Philippe Fauconnier, Peter Grasch, Yinfei Yang, Zhe Gan</dc:creator>
      <pubDate>Wed, 03 Jul 2024 18:11:45 GMT</pubDate>
    </item>
    <item>
      <title>PopAlign: Population-Level Alignment for Fair Text-to-Image Generation</title>
      <link>http://arxiv.org/abs/2406.19668v1</link>
      <description>Text-to-image (T2I) models achieve high-fidelity generation through extensive training on large datasets. However, these models may unintentionally pick up undesirable biases of their training data, such as over-representation of particular identities in gender or ethnicity neutral prompts. Existing alignment methods such as Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO) fail to address this problem effectively because they operate on pairwise preferences consisting of individual samples, while the aforementioned biases can only be measured at a population level. For example, a single sample for the prompt &quot;doctor&quot; could be male or female, but a model generating predominantly male doctors even with repeated sampling reflects a gender bias. To address this limitation, we introduce PopAlign, a novel approach for population-level preference optimization, while standard optimization would prefer entire sets of samples over others. We further derive a stochastic lower bound that directly optimizes for individual samples from preferred populations over others for scalable training. Using human evaluation and standard image quality and bias metrics, we show that PopAlign significantly mitigates the bias of pretrained T2I models while largely preserving the generation quality. Code is available at https://github.com/jacklishufan/PopAlignSDXL.\n\n\nPopAlign: Population-Level Alignment for Fair Text-to-Image Generation</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.19668v1</guid>
      <dc:creator>Shufan Li, Harkanwar Singh, Aditya Grover</dc:creator>
      <pubDate>Fri, 28 Jun 2024 05:38:32 GMT</pubDate>
    </item>
    <item>
      <title>Timo: Towards Better Temporal Reasoning for Language Models</title>
      <link>http://arxiv.org/abs/2406.14192v1</link>
      <description>Reasoning about time is essential for Large Language Models (LLMs) to understand the world. Previous works focus on solving specific tasks, primarily on time-sensitive question answering. While these methods have proven effective, they cannot generalize to a wider spectrum of temporal reasoning tasks. Therefore, we propose a crucial question: Can we build a universal framework to handle a variety of temporal reasoning tasks? To that end, we systematically study 38 temporal reasoning tasks. Based on the observation that 19 tasks are directly related to mathematics, we first leverage the available mathematical dataset to set a solid foundation for temporal reasoning. However, the in-depth study indicates that focusing solely on mathematical enhancement falls short of addressing pure temporal reasoning tasks. To mitigate this limitation, we propose a simple but effective self-critic temporal optimization method to enhance the model's temporal reasoning capabilities without sacrificing general task abilities. Finally, we develop Timo, a model designed to excel in temporal reasoning at the 7B and 13B scales. Notably, Timo outperforms the counterpart LLMs by 10.0 and 7.6 in average accuracy scores and achieves the new state-of-the-art (SOTA) performance of comparable size. Extensive experiments further validate our framework's effectiveness and its generalization across diverse temporal tasks. The code is available at https://github.com/zhaochen0110/Timo.\n\n\nTimo: Towards Better Temporal Reasoning for Language Models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.14192v1</guid>
      <dc:creator>Zhaochen Su, Jun Zhang, Tong Zhu, Xiaoye Qu, Juntao Li, Min Zhang, Yu Cheng</dc:creator>
      <pubDate>Thu, 20 Jun 2024 10:52:14 GMT</pubDate>
    </item>
    <item>
      <title>ChatGLM-RLHF: Practices of Aligning Large Language Models with Human Feedback</title>
      <link>http://arxiv.org/abs/2404.00934v2</link>
      <description>ChatGLM is a free-to-use AI service powered by the ChatGLM family of large language models (LLMs). In this paper, we present the ChatGLM-RLHF pipeline -- a reinforcement learning from human feedback (RLHF) system -- designed to enhance ChatGLM's alignment with human preferences. ChatGLM-RLHF encompasses three major components: the collection of human preference data, the training of the reward model, and the optimization of policies. Throughout the process of integrating ChatGLM-RLHF into production, we encountered and addressed several unprecedented challenges. We introduce the strategies to mitigate reward variance for stabilized large-scale training, implement model parallelism with fused gradient-descent, and design regularization constraints to avoid catastrophic forgetting in LLMs. Experiments show that ChatGLM-RLHF brings significant improvements in alignment tasks compared to the supervised fine-tuned (SFT) version of ChatGLM. For instance, it achieves on average 15\% more wins against ChatGLM-SFT in Chinese alignment tasks. The work presents our practices of aligning LLMs with human preferences, offering insights into the challenges and solutions in RLHF implementations.\n\n\nChatGLM-RLHF: Practices of Aligning Large Language Models with Human Feedback</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2404.00934v2</guid>
      <dc:creator>Zhenyu Hou, Yilin Niu, Zhengxiao Du, Xiaohan Zhang, Xiao Liu, Aohan Zeng, Qinkai Zheng, Minlie Huang, Hongning Wang, Jie Tang, Yuxiao Dong</dc:creator>
      <pubDate>Wed, 03 Apr 2024 17:04:06 GMT</pubDate>
    </item>
    <item>
      <title>Chain of Preference Optimization: Improving Chain-of-Thought Reasoning in LLMs</title>
      <link>http://arxiv.org/abs/2406.09136v1</link>
      <description>The recent development of chain-of-thought (CoT) decoding has enabled large language models (LLMs) to generate explicit logical reasoning paths for complex problem-solving. However, research indicates that these paths are not always deliberate and optimal. The tree-of-thought (ToT) method employs tree-searching to extensively explore the reasoning space and find better reasoning paths that CoT decoding might overlook. This deliberation, however, comes at the cost of significantly increased inference complexity. In this work, we demonstrate that fine-tuning LLMs leveraging the search tree constructed by ToT allows CoT to achieve similar or better performance, thereby avoiding the substantial inference burden. This is achieved through Chain of Preference Optimization (CPO), where LLMs are fine-tuned to align each step of the CoT reasoning paths with those of ToT using the inherent preference information in the tree-search process. Extensive experimental results show that CPO significantly improves LLM performance in solving a variety of complex problems, including question answering, fact verification, and arithmetic reasoning, demonstrating its effectiveness. Our code is available at https://github.com/sail-sg/CPO.\n\n\nChain of Preference Optimization: Improving Chain-of-Thought Reasoning in LLMs</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.09136v1</guid>
      <dc:creator>Xuan Zhang, Chao Du, Tianyu Pang, Qian Liu, Wei Gao, Min Lin</dc:creator>
      <pubDate>Thu, 13 Jun 2024 14:07:02 GMT</pubDate>
    </item>
    <item>
      <title>Are Large Language Models Consistent over Value-laden Questions?</title>
      <link>http://arxiv.org/abs/2407.02996v1</link>
      <description>Large language models (LLMs) appear to bias their survey answers toward certain values. Nonetheless, some argue that LLMs are too inconsistent to simulate particular values. Are they? To answer, we first define value consistency as the similarity of answers across (1) paraphrases of one question, (2) related questions under one topic, (3) multiple-choice and open-ended use-cases of one question, and (4) multilingual translations of a question to English, Chinese, German, and Japanese. We apply these measures to a few large ($&gt;=34b$), open LLMs including llama-3, as well as gpt-4o, using eight thousand questions spanning more than 300 topics. Unlike prior work, we find that models are relatively consistent across paraphrases, use-cases, translations, and within a topic. Still, some inconsistencies remain. Models are more consistent on uncontroversial topics (e.g., in the U.S., &quot;Thanksgiving&quot;) than on controversial ones (&quot;euthanasia&quot;). Base models are both more consistent compared to fine-tuned models and are uniform in their consistency across topics, while fine-tuned models are more inconsistent about some topics (&quot;euthanasia&quot;) than others (&quot;women's rights&quot;) like our human subjects (n=165).\n\n\nAre Large Language Models Consistent over Value-laden Questions?</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2407.02996v1</guid>
      <dc:creator>Jared Moore, Tanvi Deshpande, Diyi Yang</dc:creator>
      <pubDate>Wed, 03 Jul 2024 10:53:54 GMT</pubDate>
    </item>
    <item>
      <title>High-Dimension Human Value Representation in Large Language Models</title>
      <link>http://arxiv.org/abs/2404.07900v2</link>
      <description>The widespread application of Large Language Models (LLMs) across various tasks and fields has necessitated the alignment of these models with human values and preferences. Given various approaches of human value alignment, ranging from Reinforcement Learning with Human Feedback (RLHF), to constitutional learning, etc. there is an urgent need to understand the scope and nature of human values injected into these models before their release. There is also a need for model alignment without a costly large scale human annotation effort. We propose UniVaR, a high-dimensional representation of human value distributions in LLMs, orthogonal to model architecture and training data. Trained from the value-relevant output of eight multilingual LLMs and tested on the output from four multilingual LLMs, namely LlaMA2, ChatGPT, JAIS and Yi, we show that UniVaR is a powerful tool to compare the distribution of human values embedded in different LLMs with different langauge sources. Through UniVaR, we explore how different LLMs prioritize various values in different languages and cultures, shedding light on the complex interplay between human values and language modeling.\n\n\nHigh-Dimension Human Value Representation in Large Language Models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2404.07900v2</guid>
      <dc:creator>Samuel Cahyawijaya, Delong Chen, Yejin Bang, Leila Khalatbari, Bryan Wilie, Ziwei Ji, Etsuko Ishii, Pascale Fung</dc:creator>
      <pubDate>Tue, 25 Jun 2024 12:23:00 GMT</pubDate>
    </item>
    <item>
      <title>Reinforcement Learning for Sequence Design Leveraging Protein Language Models</title>
      <link>http://arxiv.org/abs/2407.03154v1</link>
      <description>Protein sequence design, determined by amino acid sequences, are essential to protein engineering problems in drug discovery. Prior approaches have resorted to evolutionary strategies or Monte-Carlo methods for protein design, but often fail to exploit the structure of the combinatorial search space, to generalize to unseen sequences. In the context of discrete black box optimization over large search spaces, learning a mutation policy to generate novel sequences with reinforcement learning is appealing. Recent advances in protein language models (PLMs) trained on large corpora of protein sequences offer a potential solution to this problem by scoring proteins according to their biological plausibility (such as the TM-score). In this work, we propose to use PLMs as a reward function to generate new sequences. Yet the PLM can be computationally expensive to query due to its large size. To this end, we propose an alternative paradigm where optimization can be performed on scores from a smaller proxy model that is periodically finetuned, jointly while learning the mutation policy. We perform extensive experiments on various sequence lengths to benchmark RL-based approaches, and provide comprehensive evaluations along biological plausibility and diversity of the protein. Our experimental results include favorable evaluations of the proposed sequences, along with high diversity scores, demonstrating that RL is a strong candidate for biological sequence design. Finally, we provide a modular open source implementation can be easily integrated in most RL training loops, with support for replacing the reward model with other PLMs, to spur further research in this domain. The code for all experiments is provided in the supplementary material.\n\n\nReinforcement Learning for Sequence Design Leveraging Protein Language Models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2407.03154v1</guid>
      <dc:creator>Jithendaraa Subramanian, Shivakanth Sujit, Niloy Irtisam, Umong Sain, Derek Nowrouzezahrai, Samira Ebrahimi Kahou, Riashat Islam</dc:creator>
      <pubDate>Wed, 03 Jul 2024 14:31:36 GMT</pubDate>
    </item>
    <item>
      <title>Jailbreak Attacks and Defenses Against Large Language Models: A Survey</title>
      <link>http://arxiv.org/abs/2407.04295v1</link>
      <description>Large Language Models (LLMs) have performed exceptionally in various text-generative tasks, including question answering, translation, code completion, etc. However, the over-assistance of LLMs has raised the challenge of &quot;jailbreaking&quot;, which induces the model to generate malicious responses against the usage policy and society by designing adversarial prompts. With the emergence of jailbreak attack methods exploiting different vulnerabilities in LLMs, the corresponding safety alignment measures are also evolving. In this paper, we propose a comprehensive and detailed taxonomy of jailbreak attack and defense methods. For instance, the attack methods are divided into black-box and white-box attacks based on the transparency of the target model. Meanwhile, we classify defense methods into prompt-level and model-level defenses. Additionally, we further subdivide these attack and defense methods into distinct sub-classes and present a coherent diagram illustrating their relationships. We also conduct an investigation into the current evaluation methods and compare them from different perspectives. Our findings aim to inspire future research and practical implementations in safeguarding LLMs against adversarial attacks. Above all, although jailbreak remains a significant concern within the community, we believe that our work enhances the understanding of this domain and provides a foundation for developing more secure LLMs.\n\n\nJailbreak Attacks and Defenses Against Large Language Models: A Survey</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2407.04295v1</guid>
      <dc:creator>Sibo Yi, Yule Liu, Zhen Sun, Tianshuo Cong, Xinlei He, Jiaxing Song, Ke Xu, Qi Li</dc:creator>
      <pubDate>Fri, 05 Jul 2024 06:57:30 GMT</pubDate>
    </item>
    <item>
      <title>Conversational Fashion Image Retrieval via Multiturn Natural Language Feedback</title>
      <link>http://arxiv.org/abs/2106.04128v1</link>
      <description>We study the task of conversational fashion image retrieval via multiturn natural language feedback. Most previous studies are based on single-turn settings. Existing models on multiturn conversational fashion image retrieval have limitations, such as employing traditional models, and leading to ineffective performance. We propose a novel framework that can effectively handle conversational fashion image retrieval with multiturn natural language feedback texts. One characteristic of the framework is that it searches for candidate images based on exploitation of the encoded reference image and feedback text information together with the conversation history. Furthermore, the image fashion attribute information is leveraged via a mutual attention strategy. Since there is no existing fashion dataset suitable for the multiturn setting of our task, we derive a large-scale multiturn fashion dataset via additional manual annotation efforts on an existing single-turn dataset. The experiments show that our proposed model significantly outperforms existing state-of-the-art methods.\n\n\nMulti-turn Reinforcement Learning from Preference Human Feedback</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2106.04128v1</guid>
      <dc:creator>Yifei Yuan, Wai Lam</dc:creator>
      <pubDate>Tue, 08 Jun 2021 06:34:25 GMT</pubDate>
    </item>
    <item>
      <title>MODRL/D-AM: Multiobjective Deep Reinforcement Learning Algorithm Using Decomposition and Attention Model for Multiobjective Optimization</title>
      <link>http://arxiv.org/abs/2002.05484v1</link>
      <description>Recently, a deep reinforcement learning method is proposed to solve multiobjective optimization problem. In this method, the multiobjective optimization problem is decomposed to a number of single-objective optimization subproblems and all the subproblems are optimized in a collaborative manner. Each subproblem is modeled with a pointer network and the model is trained with reinforcement learning. However, when pointer network extracts the features of an instance, it ignores the underlying structure information of the input nodes. Thus, this paper proposes a multiobjective deep reinforcement learning method using decomposition and attention model to solve multiobjective optimization problem. In our method, each subproblem is solved by an attention model, which can exploit the structure features as well as node features of input nodes. The experiment results on multiobjective travelling salesman problem show the proposed algorithm achieves better performance compared with the previous method.\n\n\nDRUGIMPROVER: Utilizing reinforcement learning for multi-objective alignment in drug optimization</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2002.05484v1</guid>
      <dc:creator>Hong Wu, Jiahai Wang, Zizhen Zhang</dc:creator>
      <pubDate>Thu, 13 Feb 2020 12:59:39 GMT</pubDate>
    </item>
    <item>
      <title>OLLIE: Imitation Learning from Offline Pretraining to Online Finetuning</title>
      <link>http://arxiv.org/abs/2405.17477v3</link>
      <description>In this paper, we study offline-to-online Imitation Learning (IL) that pretrains an imitation policy from static demonstration data, followed by fast finetuning with minimal environmental interaction. We find the na\&quot;ive combination of existing offline IL and online IL methods tends to behave poorly in this context, because the initial discriminator (often used in online IL) operates randomly and discordantly against the policy initialization, leading to misguided policy optimization and $\textit{unlearning}$ of pretraining knowledge. To overcome this challenge, we propose a principled offline-to-online IL method, named $\texttt{OLLIE}$, that simultaneously learns a near-expert policy initialization along with an $\textit{aligned discriminator initialization}$, which can be seamlessly integrated into online IL, achieving smooth and fast finetuning. Empirically, $\texttt{OLLIE}$ consistently and significantly outperforms the baseline methods in $\textbf{20}$ challenging tasks, from continuous control to vision-based domains, in terms of performance, demonstration efficiency, and convergence speed. This work may serve as a foundation for further exploration of pretraining and finetuning in the context of IL.\n\n\nOLLIE: Imitation Learning from Offline Pretraining to Online Finetuning</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.17477v3</guid>
      <dc:creator>Sheng Yue, Xingyuan Hua, Ju Ren, Sen Lin, Junshan Zhang, Yaoxue Zhang</dc:creator>
      <pubDate>Thu, 30 May 2024 17:11:46 GMT</pubDate>
    </item>
    <item>
      <title>mDPO: Conditional Preference Optimization for Multimodal Large Language Models</title>
      <link>http://arxiv.org/abs/2406.11839v1</link>
      <description>Direct preference optimization (DPO) has shown to be an effective method for large language model (LLM) alignment. Recent works have attempted to apply DPO to multimodal scenarios but have found it challenging to achieve consistent improvement. Through a comparative experiment, we identify the unconditional preference problem in multimodal preference optimization, where the model overlooks the image condition. To address this problem, we propose mDPO, a multimodal DPO objective that prevents the over-prioritization of language-only preferences by also optimizing image preference. Moreover, we introduce a reward anchor that forces the reward to be positive for chosen responses, thereby avoiding the decrease in their likelihood -- an intrinsic problem of relative preference optimization. Experiments on two multimodal LLMs of different sizes and three widely used benchmarks demonstrate that mDPO effectively addresses the unconditional preference problem in multimodal preference optimization and significantly improves model performance, particularly in reducing hallucination.\n\n\nmDPO: Conditional Preference Optimization for Multimodal Large Language Models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.11839v1</guid>
      <dc:creator>Fei Wang, Wenxuan Zhou, James Y. Huang, Nan Xu, Sheng Zhang, Hoifung Poon, Muhao Chen</dc:creator>
      <pubDate>Mon, 17 Jun 2024 17:59:58 GMT</pubDate>
    </item>
    <item>
      <title>Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training</title>
      <link>http://arxiv.org/abs/2401.05566v3</link>
      <description>Humans are capable of strategically deceptive behavior: behaving helpfully in most situations, but then behaving very differently in order to pursue alternative objectives when given the opportunity. If an AI system learned such a deceptive strategy, could we detect it and remove it using current state-of-the-art safety training techniques? To study this question, we construct proof-of-concept examples of deceptive behavior in large language models (LLMs). For example, we train models that write secure code when the prompt states that the year is 2023, but insert exploitable code when the stated year is 2024. We find that such backdoor behavior can be made persistent, so that it is not removed by standard safety training techniques, including supervised fine-tuning, reinforcement learning, and adversarial training (eliciting unsafe behavior and then training to remove it). The backdoor behavior is most persistent in the largest models and in models trained to produce chain-of-thought reasoning about deceiving the training process, with the persistence remaining even when the chain-of-thought is distilled away. Furthermore, rather than removing backdoors, we find that adversarial training can teach models to better recognize their backdoor triggers, effectively hiding the unsafe behavior. Our results suggest that, once a model exhibits deceptive behavior, standard techniques could fail to remove such deception and create a false impression of safety.\n\n\nLLMs for Explainable Few-shot Deception Detection</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2401.05566v3</guid>
      <dc:creator>Evan Hubinger, Carson Denison, Jesse Mu, Mike Lambert, Meg Tong, Monte MacDiarmid, Tamera Lanham, Daniel M. Ziegler, Tim Maxwell, Newton Cheng, Adam Jermyn, Amanda Askell, Ansh Radhakrishnan, Cem Anil, David Duvenaud, Deep Ganguli, Fazl Barez, Jack Clark, Kamal Ndousse, Kshitij Sachan, Michael Sellitto, Mrinank Sharma, Nova DasSarma, Roger Grosse, Shauna Kravec, Yuntao Bai, Zachary Witten, Marina Favaro, Jan Brauner, Holden Karnofsky, Paul Christiano, Samuel R. Bowman, Logan Graham, Jared Kaplan, Sören Mindermann, Ryan Greenblatt, Buck Shlegeris, Nicholas Schiefer, Ethan Perez</dc:creator>
      <pubDate>Wed, 17 Jan 2024 20:26:01 GMT</pubDate>
    </item>
    <item>
      <title>Beyond Human Norms: Unveiling Unique Values of Large Language Models through Interdisciplinary Approaches</title>
      <link>http://arxiv.org/abs/2404.12744v2</link>
      <description>Recent advancements in Large Language Models (LLMs) have revolutionized the AI field but also pose potential safety and ethical risks. Deciphering LLMs' embedded values becomes crucial for assessing and mitigating their risks. Despite extensive investigation into LLMs' values, previous studies heavily rely on human-oriented value systems in social sciences. Then, a natural question arises: Do LLMs possess unique values beyond those of humans? Delving into it, this work proposes a novel framework, ValueLex, to reconstruct LLMs' unique value system from scratch, leveraging psychological methodologies from human personality/value research. Based on Lexical Hypothesis, ValueLex introduces a generative approach to elicit diverse values from 30+ LLMs, synthesizing a taxonomy that culminates in a comprehensive value framework via factor analysis and semantic clustering. We identify three core value dimensions, Competence, Character, and Integrity, each with specific subdimensions, revealing that LLMs possess a structured, albeit non-human, value system. Based on this system, we further develop tailored projective tests to evaluate and analyze the value inclinations of LLMs across different model sizes, training methods, and data sources. Our framework fosters an interdisciplinary paradigm of understanding LLMs, paving the way for future AI alignment and regulation.\n\n\nBeyond Human Norms: Unveiling Unique Values of Large Language Models through Interdisciplinary Approaches</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2404.12744v2</guid>
      <dc:creator>Pablo Biedma, Xiaoyuan Yi, Linus Huang, Maosong Sun, Xing Xie</dc:creator>
      <pubDate>Fri, 10 May 2024 06:09:02 GMT</pubDate>
    </item>
    <item>
      <title>Discovering Useful Sentence Representations from Large Pretrained Language Models</title>
      <link>http://arxiv.org/abs/2008.09049v1</link>
      <description>Despite the extensive success of pretrained language models as encoders for building NLP systems, they haven't seen prominence as decoders for sequence generation tasks. We explore the question of whether these models can be adapted to be used as universal decoders. To be considered &quot;universal,&quot; a decoder must have an implicit representation for any target sentence $s$, such that it can recover that sentence exactly when conditioned on its representation. For large transformer-based language models trained on vast amounts of English text, we investigate whether such representations can be easily discovered using standard optimization methods. We present and compare three representation injection techniques for transformer-based models and three accompanying methods which map sentences to and from this representation space. Experiments show that not only do representations exist for sentences from a variety of genres. More importantly, without needing complex optimization algorithms, our methods recover these sentences almost perfectly without fine-tuning the underlying language model at all.\n\n\nFrom Decoding to Meta-Generation: Inference-time Algorithms for Large Language Models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2008.09049v1</guid>
      <dc:creator>Nishant Subramani, Nivedita Suresh</dc:creator>
      <pubDate>Thu, 20 Aug 2020 16:03:51 GMT</pubDate>
    </item>
    <item>
      <title>Seeking Neural Nuggets: Knowledge Transfer in Large Language Models from a Parametric Perspective</title>
      <link>http://arxiv.org/abs/2310.11451v2</link>
      <description>Large Language Models (LLMs) inherently encode a wealth of knowledge within their parameters through pre-training on extensive corpora. While prior research has delved into operations on these parameters to manipulate the underlying implicit knowledge (encompassing detection, editing, and merging), there remains an ambiguous understanding regarding their transferability across models with varying scales. In this paper, we seek to empirically investigate knowledge transfer from larger to smaller models through a parametric perspective. To achieve this, we employ sensitivity-based techniques to extract and align knowledge-specific parameters between different LLMs. Moreover, the LoRA module is used as the intermediary mechanism for injecting the extracted knowledge into smaller models. Evaluations across four benchmarks validate the efficacy of our proposed method. Our findings highlight the critical factors contributing to the process of parametric knowledge transfer, underscoring the transferability of model parameters across LLMs of different scales. Project website: https://maszhongming.github.io/ParaKnowTransfer.\n\n\nSelf-training Large Language Models through Knowledge Detection</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2310.11451v2</guid>
      <dc:creator>Ming Zhong, Chenxin An, Weizhu Chen, Jiawei Han, Pengcheng He</dc:creator>
      <pubDate>Wed, 08 May 2024 12:11:00 GMT</pubDate>
    </item>
    <item>
      <title>Do Explicit Alignments Robustly Improve Multilingual Encoders?</title>
      <link>http://arxiv.org/abs/2010.02537v1</link>
      <description>Multilingual BERT (mBERT), XLM-RoBERTa (XLMR) and other unsupervised multilingual encoders can effectively learn cross-lingual representation. Explicit alignment objectives based on bitexts like Europarl or MultiUN have been shown to further improve these representations. However, word-level alignments are often suboptimal and such bitexts are unavailable for many languages. In this paper, we propose a new contrastive alignment objective that can better utilize such signal, and examine whether these previous alignment methods can be adapted to noisier sources of aligned data: a randomly sampled 1 million pair subset of the OPUS collection. Additionally, rather than report results on a single dataset with a single model run, we report the mean and standard derivation of multiple runs with different seeds, on four datasets and tasks. Our more extensive analysis finds that, while our new objective outperforms previous work, overall these methods do not improve performance with a more robust evaluation framework. Furthermore, the gains from using a better underlying model eclipse any benefits from alignment training. These negative results dictate more care in evaluating these methods and suggest limitations in applying explicit alignment objectives.\n\n\nDecoding-Time Language Model Alignment with Multiple Objectives</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2010.02537v1</guid>
      <dc:creator>Shijie Wu, Mark Dredze</dc:creator>
      <pubDate>Tue, 06 Oct 2020 07:43:17 GMT</pubDate>
    </item>
    <item>
      <title>LLMs Could Autonomously Learn Without External Supervision</title>
      <link>http://arxiv.org/abs/2406.00606v2</link>
      <description>In the quest for super-human performance, Large Language Models (LLMs) have traditionally been tethered to human-annotated datasets and predefined training objectives-a process that is both labor-intensive and inherently limited. This paper presents a transformative approach: Autonomous Learning for LLMs, a self-sufficient learning paradigm that frees models from the constraints of human supervision. This method endows LLMs with the ability to self-educate through direct interaction with text, akin to a human reading and comprehending literature. Our approach eliminates the reliance on annotated data, fostering an Autonomous Learning environment where the model independently identifies and reinforces its knowledge gaps. Empirical results from our comprehensive experiments, which utilized a diverse array of learning materials and were evaluated against standard public quizzes, reveal that Autonomous Learning outstrips the performance of both Pre-training and Supervised Fine-Tuning (SFT), as well as retrieval-augmented methods. These findings underscore the potential of Autonomous Learning to not only enhance the efficiency and effectiveness of LLM training but also to pave the way for the development of more advanced, self-reliant AI systems.\n\n\nLLMs Could Autonomously Learn Without External Supervision</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.00606v2</guid>
      <dc:creator>Ke Ji, Junying Chen, Anningzhe Gao, Wenya Xie, Xiang Wan, Benyou Wang</dc:creator>
      <pubDate>Thu, 06 Jun 2024 22:48:35 GMT</pubDate>
    </item>
    <item>
      <title>Unpacking DPO and PPO: Disentangling Best Practices for Learning from Preference Feedback</title>
      <link>http://arxiv.org/abs/2406.09279v1</link>
      <description>Learning from preference feedback has emerged as an essential step for improving the generation quality and performance of modern language models (LMs). Despite its widespread use, the way preference-based learning is applied varies wildly, with differing data, learning algorithms, and evaluations used, making disentangling the impact of each aspect difficult. In this work, we identify four core aspects of preference-based learning: preference data, learning algorithm, reward model, and policy training prompts, systematically investigate the impact of these components on downstream model performance, and suggest a recipe for strong learning for preference feedback. Our findings indicate that all aspects are important for performance, with better preference data leading to the largest improvements, followed by the choice of learning algorithm, the use of improved reward models, and finally the use of additional unlabeled prompts for policy training. Notably, PPO outperforms DPO by up to 2.5% in math and 1.2% in general domains. High-quality preference data leads to improvements of up to 8% in instruction following and truthfulness. Despite significant gains of up to 5% in mathematical evaluation when scaling up reward models, we surprisingly observe marginal improvements in other categories.   We publicly release the code used for training (https://github.com/hamishivi/EasyLM) and evaluating (https://github.com/allenai/open-instruct) our models, along with the models and datasets themselves (https://huggingface.co/collections/allenai/tulu-v25-suite-66676520fd578080e126f618).\n\n\nUnpacking DPO and PPO: Disentangling Best Practices for Learning from Preference Feedback</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.09279v1</guid>
      <dc:creator>Hamish Ivison, Yizhong Wang, Jiacheng Liu, Zeqiu Wu, Valentina Pyatkin, Nathan Lambert, Noah A. Smith, Yejin Choi, Hannaneh Hajishirzi</dc:creator>
      <pubDate>Thu, 13 Jun 2024 16:17:21 GMT</pubDate>
    </item>
    <item>
      <title>Semantic Gesticulator: Semantics-Aware Co-Speech Gesture Synthesis</title>
      <link>http://arxiv.org/abs/2405.09814v2</link>
      <description>In this work, we present Semantic Gesticulator, a novel framework designed to synthesize realistic gestures accompanying speech with strong semantic correspondence. Semantically meaningful gestures are crucial for effective non-verbal communication, but such gestures often fall within the long tail of the distribution of natural human motion. The sparsity of these movements makes it challenging for deep learning-based systems, trained on moderately sized datasets, to capture the relationship between the movements and the corresponding speech semantics. To address this challenge, we develop a generative retrieval framework based on a large language model. This framework efficiently retrieves suitable semantic gesture candidates from a motion library in response to the input speech. To construct this motion library, we summarize a comprehensive list of commonly used semantic gestures based on findings in linguistics, and we collect a high-quality motion dataset encompassing both body and hand movements. We also design a novel GPT-based model with strong generalization capabilities to audio, capable of generating high-quality gestures that match the rhythm of speech. Furthermore, we propose a semantic alignment mechanism to efficiently align the retrieved semantic gestures with the GPT's output, ensuring the naturalness of the final animation. Our system demonstrates robustness in generating gestures that are rhythmically coherent and semantically explicit, as evidenced by a comprehensive collection of examples. User studies confirm the quality and human-likeness of our results, and show that our system outperforms state-of-the-art systems in terms of semantic appropriateness by a clear margin.\n\n\nSemantic Gesticulator: Semantics-Aware Co-Speech Gesture Synthesis</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.09814v2</guid>
      <dc:creator>Zeyi Zhang, Tenglong Ao, Yuyao Zhang, Qingzhe Gao, Chuan Lin, Baoquan Chen, Libin Liu</dc:creator>
      <pubDate>Fri, 17 May 2024 03:23:12 GMT</pubDate>
    </item>
    <item>
      <title>The Real, the Better: Aligning Large Language Models with Online Human Behaviors</title>
      <link>http://arxiv.org/abs/2405.00578v1</link>
      <description>Large language model alignment is widely used and studied to avoid LLM producing unhelpful and harmful responses. However, the lengthy training process and predefined preference bias hinder adaptation to online diverse human preferences. To this end, this paper proposes an alignment framework, called Reinforcement Learning with Human Behavior (RLHB), to align LLMs by directly leveraging real online human behaviors. By taking the generative adversarial framework, the generator is trained to respond following expected human behavior; while the discriminator tries to verify whether the triplets of query, response, and human behavior come from real online environments. Behavior modeling in natural-language form and the multi-model joint training mechanism enable an active and sustainable online alignment. Experimental results confirm the effectiveness of our proposed methods by both human and automatic evaluations.\n\n\nThe Real, the Better: Aligning Large Language Models with Online Human Behaviors</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.00578v1</guid>
      <dc:creator>Guanying Jiang, Lingyong Yan, Haibo Shi, Dawei Yin</dc:creator>
      <pubDate>Wed, 01 May 2024 15:30:41 GMT</pubDate>
    </item>
    <item>
      <title>KnowTuning: Knowledge-aware Fine-tuning for Large Language Models</title>
      <link>http://arxiv.org/abs/2402.11176v2</link>
      <description>Despite their success at many natural language processing (NLP) tasks, large language models still struggle to effectively leverage knowledge for knowledge-intensive tasks, manifesting limitations such as generating incomplete, non-factual, or illogical answers. These limitations stem from inadequate knowledge awareness of LLMs during vanilla fine-tuning. To address these problems, we propose a knowledge-aware fine-tuning (KnowTuning) method to improve fine-grained and coarse-grained knowledge awareness of LLMs. We devise a fine-grained knowledge augmentation stage to train LLMs to identify difficult fine-grained knowledge in answers. We also propose a coarse-grained knowledge comparison stage to train LLMs to distinguish between reliable and unreliable knowledge, in three aspects: completeness, factuality, and logicality. Extensive experiments on both generic and medical question answering (QA) datasets confirm the effectiveness of KnowTuning, through automatic and human evaluations, across various sizes of LLMs. We further verify that KnowTuning generates more facts with less factual error rate under fine-grained facts evaluation.\n\n\nKnowTuning: Knowledge-aware Fine-tuning for Large Language Models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.11176v2</guid>
      <dc:creator>Yougang Lyu, Lingyong Yan, Shuaiqiang Wang, Haibo Shi, Dawei Yin, Pengjie Ren, Zhumin Chen, Maarten de Rijke, Zhaochun Ren</dc:creator>
      <pubDate>Wed, 17 Apr 2024 11:45:00 GMT</pubDate>
    </item>
    <item>
      <title>A SMART Mnemonic Sounds like &quot;Glue Tonic&quot;: Mixing LLMs with Student Feedback to Make Mnemonic Learning Stick</title>
      <link>http://arxiv.org/abs/2406.15352v1</link>
      <description>Keyword mnemonics are memorable explanations that link new terms to simpler keywords. Prior works generate mnemonics for students, but they do not guide models toward mnemonics students prefer and aid learning. We build SMART, a mnemonic generator trained on feedback from real students learning new terms. To train SMART, we first fine-tune LLaMA-2 on a curated set of user-written mnemonics. We then use LLM alignment to enhance SMART: we deploy mnemonics generated by SMART in a flashcard app to find preferences on mnemonics students favor. We gather 2684 preferences from 45 students across two types: expressed (inferred from ratings) and observed (inferred from student learning), yielding three key findings. First, expressed and observed preferences disagree; what students think is helpful does not fully capture what is truly helpful. Second, Bayesian models can synthesize complementary data from multiple preference types into a single effectiveness signal. SMART is tuned via Direct Preference Optimization on this signal, which we show resolves ties and missing labels in the typical method of pairwise comparisons, augmenting data for LLM output quality gains. Third, mnemonic experts assess SMART as matching GPT-4, at much lower deployment costs, showing the utility of capturing diverse student feedback to align LLMs in education.\n\n\nA SMART Mnemonic Sounds like&quot; Glue Tonic&quot;: Mixing LLMs with Student Feedback to Make Mnemonic Learning Stick</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.15352v1</guid>
      <dc:creator>Nishant Balepur, Matthew Shu, Alexander Hoyle, Alison Robey, Shi Feng, Seraphina Goldfarb-Tarrant, Jordan Boyd-Graber</dc:creator>
      <pubDate>Fri, 21 Jun 2024 17:59:51 GMT</pubDate>
    </item>
    <item>
      <title>Q-Probe: A Lightweight Approach to Reward Maximization for Language Models</title>
      <link>http://arxiv.org/abs/2402.14688v2</link>
      <description>We present an approach called Q-probing to adapt a pre-trained language model to maximize a task-specific reward function. At a high level, Q-probing sits between heavier approaches such as finetuning and lighter approaches such as few shot prompting, but can also be combined with either. The idea is to learn a simple linear function on a model's embedding space that can be used to reweight candidate completions. We theoretically show that this sampling procedure is equivalent to a KL-constrained maximization of the Q-probe as the number of samples increases. To train the Q-probes we consider either reward modeling or a class of novel direct policy learning objectives based on importance weighted policy gradients. With this technique, we see gains in domains with ground-truth rewards (code generation) as well as implicit rewards defined by preference data, even outperforming finetuning in data-limited regimes. Moreover, a Q-probe can be trained on top of an API since it only assumes access to sampling and embeddings. Code: https://github.com/likenneth/q_probe .\n\n\nQ-Probe: A Lightweight Approach to Reward Maximization for Language Models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.14688v2</guid>
      <dc:creator>Kenneth Li, Samy Jelassi, Hugh Zhang, Sham Kakade, Martin Wattenberg, David Brandfonbrener</dc:creator>
      <pubDate>Sun, 02 Jun 2024 15:05:59 GMT</pubDate>
    </item>
    <item>
      <title>Generative AI and Large Language Models for Cyber Security: All Insights You Need</title>
      <link>http://arxiv.org/abs/2405.12750v1</link>
      <description>This paper provides a comprehensive review of the future of cybersecurity through Generative AI and Large Language Models (LLMs). We explore LLM applications across various domains, including hardware design security, intrusion detection, software engineering, design verification, cyber threat intelligence, malware detection, and phishing detection. We present an overview of LLM evolution and its current state, focusing on advancements in models such as GPT-4, GPT-3.5, Mixtral-8x7B, BERT, Falcon2, and LLaMA. Our analysis extends to LLM vulnerabilities, such as prompt injection, insecure output handling, data poisoning, DDoS attacks, and adversarial instructions. We delve into mitigation strategies to protect these models, providing a comprehensive look at potential attack scenarios and prevention techniques. Furthermore, we evaluate the performance of 42 LLM models in cybersecurity knowledge and hardware security, highlighting their strengths and weaknesses. We thoroughly evaluate cybersecurity datasets for LLM training and testing, covering the lifecycle from data creation to usage and identifying gaps for future research. In addition, we review new strategies for leveraging LLMs, including techniques like Half-Quadratic Quantization (HQQ), Reinforcement Learning with Human Feedback (RLHF), Direct Preference Optimization (DPO), Quantized Low-Rank Adapters (QLoRA), and Retrieval-Augmented Generation (RAG). These insights aim to enhance real-time cybersecurity defenses and improve the sophistication of LLM applications in threat detection and response. Our paper provides a foundational understanding and strategic direction for integrating LLMs into future cybersecurity frameworks, emphasizing innovation and robust model deployment to safeguard against evolving cyber threats.\n\n\nGenerative AI and Large Language Models for Cyber Security: All Insights You Need</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.12750v1</guid>
      <dc:creator>Mohamed Amine Ferrag, Fatima Alwahedi, Ammar Battah, Bilel Cherif, Abdechakour Mechri, Norbert Tihanyi</dc:creator>
      <pubDate>Tue, 21 May 2024 13:02:27 GMT</pubDate>
    </item>
    <item>
      <title>NoiseBoost: Alleviating Hallucination with Noise Perturbation for Multimodal Large Language Models</title>
      <link>http://arxiv.org/abs/2405.20081v2</link>
      <description>Multimodal large language models (MLLMs) contribute a powerful mechanism to understanding visual information building on large language models. However, MLLMs are notorious for suffering from hallucinations, especially when generating lengthy, detailed descriptions for images. Our analysis reveals that hallucinations stem from the inherent summarization mechanism of large language models, leading to excessive dependence on linguistic tokens while neglecting vision information. In this paper, we propose NoiseBoost, a broadly applicable and simple method for alleviating hallucinations for MLLMs through the integration of noise feature perturbations. Noise perturbation acts as a regularizer, facilitating a balanced distribution of attention weights among visual and linguistic tokens. Despite its simplicity, NoiseBoost consistently enhances the performance of MLLMs across common training strategies, including supervised fine-tuning and reinforcement learning. Further, NoiseBoost pioneerly enables semi-supervised learning for MLLMs, unleashing the power of unlabeled data. Comprehensive experiments demonstrate that NoiseBoost improves dense caption accuracy by 8.1% with human evaluation and achieves comparable results with 50% of the data by mining unlabeled data. Code and models are available at https://kaiwu5.github.io/noiseboost.\n\n\nNoiseBoost: Alleviating Hallucination with Noise Perturbation for Multimodal Large Language Models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.20081v2</guid>
      <dc:creator>Kai Wu, Boyuan Jiang, Zhengkai Jiang, Qingdong He, Donghao Luo, Shengzhi Wang, Qingwen Liu, Chengjie Wang</dc:creator>
      <pubDate>Fri, 31 May 2024 07:40:04 GMT</pubDate>
    </item>
    <item>
      <title>Not All Preference Pairs Are Created Equal: A Recipe for Annotation-Efficient Iterative Preference Learning</title>
      <link>http://arxiv.org/abs/2406.17312v1</link>
      <description>Iterative preference learning, though yielding superior performances, requires online annotated preference labels. In this work, we study strategies to select worth-annotating response pairs for cost-efficient annotation while achieving competitive or even better performances compared with the random selection baseline for iterative preference learning. Built on assumptions regarding uncertainty and distribution shifts, we propose a comparative view to rank the implicit reward margins as predicted by DPO to select the response pairs that yield more benefits. Through extensive experiments, we show that annotating those response pairs with small margins is generally better than large or random, under both single- and multi-iteration scenarios. Besides, our empirical results suggest allocating more annotation budgets in the earlier iterations rather than later across multiple iterations.\n\n\nNot All Preference Pairs Are Created Equal: A Recipe for Annotation-Efficient Iterative Preference Learning</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.17312v1</guid>
      <dc:creator>Sen Yang, Leyang Cui, Deng Cai, Xinting Huang, Shuming Shi, Wai Lam</dc:creator>
      <pubDate>Tue, 25 Jun 2024 06:49:16 GMT</pubDate>
    </item>
    <item>
      <title>LACIE: Listener-Aware Finetuning for Confidence Calibration in Large Language Models</title>
      <link>http://arxiv.org/abs/2405.21028v2</link>
      <description>When answering questions, LLMs can convey not only an answer, but a level of confidence about the answer being correct. This includes explicit confidence markers (e.g. giving a numeric score) as well as implicit markers, like an authoritative tone or elaborating with additional knowledge. For LLMs to be trustworthy knowledge sources, the confidence they convey should match their actual expertise; however, most current models tend towards overconfidence. To calibrate both implicit and explicit confidence markers, we introduce a pragmatic, listener-aware finetuning method (LACIE) that models the listener, considering not only whether an answer is right, but whether it will be accepted by a listener. We cast calibration as preference optimization, creating data via a two-agent game, where a speaker model's outputs are judged by a simulated listener. We then finetune three LLMs (Mistral-7B, Llama3-8B, Llama3-70B) with LACIE, and show that the resulting models are better calibrated w.r.t. a simulated listener. Crucially, these trends transfer to human listeners, helping them correctly predict model correctness: we conduct a human evaluation where annotators accept or reject an LLM's answers, finding that training with LACIE results in 47% fewer incorrect answers being accepted while maintaining the same level of acceptance for correct answers. Furthermore, LACIE generalizes to another dataset, resulting in a large increase in truthfulness on TruthfulQA when trained on TriviaQA. Our analysis indicates that LACIE leads to a better confidence separation between correct and incorrect examples. Qualitatively, we find that a LACIE-trained model hedges more and implicitly signals certainty when it is correct by using an authoritative tone or including details. Finally, LACIE finetuning leads to an emergent increase in model abstention (e.g. saying &quot;I don't know&quot;) for answers that are likely wrong.\n\n\nLACIE: Listener-Aware Finetuning for Confidence Calibration in Large Language Models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.21028v2</guid>
      <dc:creator>Elias Stengel-Eskin, Peter Hase, Mohit Bansal</dc:creator>
      <pubDate>Wed, 03 Jul 2024 12:49:23 GMT</pubDate>
    </item>
    <item>
      <title>Leftover Lunch: Advantage-based Offline Reinforcement Learning for Language Models</title>
      <link>http://arxiv.org/abs/2305.14718v5</link>
      <description>Reinforcement Learning with Human Feedback (RLHF) is the most prominent method for Language Model (LM) alignment. However, RLHF is an unstable and data-hungry process that continually requires new high-quality LM-generated data for finetuning. We introduce Advantage-Leftover Lunch RL (A-LoL), a new class of offline policy gradient algorithms that enable RL training on any pre-existing data. By assuming the entire LM output sequence as a single action, A-LoL allows incorporating sequence-level classifiers or human-designed scoring functions as rewards. Subsequently, by using LM's value estimate, A-LoL only trains on positive advantage (leftover) data points, making it resilient to noise. Overall, A-LoL is an easy-to-implement, sample-efficient, and stable LM training recipe.   We demonstrate the effectiveness of A-LoL and its variants with a set of four different language generation tasks. We compare against both online RL (PPO) and recent preference-based (DPO, PRO) and reward-based (GOLD) offline RL baselines. On the commonly-used RLHF benchmark, Helpful and Harmless Assistant (HHA), LMs trained with A-LoL methods achieve the highest diversity while also being rated more safe and helpful than the baselines according to humans. Additionally, in the remaining three tasks, A-LoL could optimize multiple distinct reward functions even when using noisy or suboptimal training data.   We also release our experimental code. https://github.com/abaheti95/LoL-RL\n\n\nLeftover Lunch: Advantage-based Offline Reinforcement Learning for Language Models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2305.14718v5</guid>
      <dc:creator>Ashutosh Baheti, Ximing Lu, Faeze Brahman, Ronan Le Bras, Maarten Sap, Mark Riedl</dc:creator>
      <pubDate>Sat, 20 Apr 2024 00:34:59 GMT</pubDate>
    </item>
    <item>
      <title>Suri: Multi-constraint Instruction Following for Long-form Text Generation</title>
      <link>http://arxiv.org/abs/2406.19371v1</link>
      <description>Existing research on instruction following largely focuses on tasks with simple instructions and short responses. In this work, we explore multi-constraint instruction following for generating long-form text. We create Suri, a dataset with 20K human-written long-form texts paired with LLM-generated backtranslated instructions that contain multiple complex constraints. Because of prohibitive challenges associated with collecting human preference judgments on long-form texts, preference-tuning algorithms such as DPO are infeasible in our setting; thus, we propose Instructional ORPO (I-ORPO), an alignment method based on the ORPO algorithm. Instead of receiving negative feedback from dispreferred responses, I-ORPO obtains negative feedback from synthetically corrupted instructions generated by an LLM. Using Suri, we perform supervised and I-ORPO fine-tuning on Mistral-7b-Instruct-v0.2. The resulting models, Suri-SFT and Suri-I-ORPO, generate significantly longer texts (~5K tokens) than base models without significant quality deterioration. Our human evaluation shows that while both SFT and I-ORPO models satisfy most constraints, Suri-I-ORPO generations are generally preferred for their coherent and informative incorporation of the constraints. We release our code at https://github.com/chtmp223/suri.\n\n\nSuri: Multi-constraint Instruction Following for Long-form Text Generation</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.19371v1</guid>
      <dc:creator>Chau Minh Pham, Simeng Sun, Mohit Iyyer</dc:creator>
      <pubDate>Thu, 27 Jun 2024 17:50:35 GMT</pubDate>
    </item>
    <item>
      <title>Preference Learning Algorithms Do Not Learn Preference Rankings</title>
      <link>http://arxiv.org/abs/2405.19534v1</link>
      <description>Preference learning algorithms (e.g., RLHF and DPO) are frequently used to steer LLMs to produce generations that are more preferred by humans, but our understanding of their inner workings is still limited. In this work, we study the conventional wisdom that preference learning trains models to assign higher likelihoods to more preferred outputs than less preferred outputs, measured via $\textit{ranking accuracy}$. Surprisingly, we find that most state-of-the-art preference-tuned models achieve a ranking accuracy of less than 60% on common preference datasets. We furthermore derive the $\textit{idealized ranking accuracy}$ that a preference-tuned LLM would achieve if it optimized the DPO or RLHF objective perfectly. We demonstrate that existing models exhibit a significant $\textit{alignment gap}$ -- $\textit{i.e.}$, a gap between the observed and idealized ranking accuracies. We attribute this discrepancy to the DPO objective, which is empirically and theoretically ill-suited to fix even mild ranking errors in the reference model, and derive a simple and efficient formula for quantifying the difficulty of learning a given preference datapoint. Finally, we demonstrate that ranking accuracy strongly correlates with the empirically popular win rate metric when the model is close to the reference model used in the objective, shedding further light on the differences between on-policy (e.g., RLHF) and off-policy (e.g., DPO) preference learning algorithms.\n\n\nPreference Learning Algorithms Do Not Learn Preference Rankings</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.19534v1</guid>
      <dc:creator>Angelica Chen, Sadhika Malladi, Lily H. Zhang, Xinyi Chen, Qiuyi Zhang, Rajesh Ranganath, Kyunghyun Cho</dc:creator>
      <pubDate>Wed, 29 May 2024 21:29:44 GMT</pubDate>
    </item>
    <item>
      <title>Aligning Large Language Models with Diverse Political Viewpoints</title>
      <link>http://arxiv.org/abs/2406.14155v1</link>
      <description>Large language models such as ChatGPT often exhibit striking political biases. If users query them about political information, they might take a normative stance and reinforce such biases. To overcome this, we align LLMs with diverse political viewpoints from 100,000 comments written by candidates running for national parliament in Switzerland. Such aligned models are able to generate more accurate political viewpoints from Swiss parties compared to commercial models such as ChatGPT. We also propose a procedure to generate balanced overviews from multiple viewpoints using such models.\n\n\nAligning Large Language Models with Diverse Political Viewpoints</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.14155v1</guid>
      <dc:creator>Dominik Stammbach, Philine Widmer, Eunjung Cho, Caglar Gulcehre, Elliott Ash</dc:creator>
      <pubDate>Thu, 20 Jun 2024 09:53:23 GMT</pubDate>
    </item>
    <item>
      <title>A Survey on Human Preference Learning for Large Language Models</title>
      <link>http://arxiv.org/abs/2406.11191v2</link>
      <description>The recent surge of versatile large language models (LLMs) largely depends on aligning increasingly capable foundation models with human intentions by preference learning, enhancing LLMs with excellent applicability and effectiveness in a wide range of contexts. Despite the numerous related studies conducted, a perspective on how human preferences are introduced into LLMs remains limited, which may prevent a deeper comprehension of the relationships between human preferences and LLMs as well as the realization of their limitations. In this survey, we review the progress in exploring human preference learning for LLMs from a preference-centered perspective, covering the sources and formats of preference feedback, the modeling and usage of preference signals, as well as the evaluation of the aligned LLMs. We first categorize the human feedback according to data sources and formats. We then summarize techniques for human preferences modeling and compare the advantages and disadvantages of different schools of models. Moreover, we present various preference usage methods sorted by the objectives to utilize human preference signals. Finally, we summarize some prevailing approaches to evaluate LLMs in terms of alignment with human intentions and discuss our outlooks on the human intention alignment for LLMs.\n\n\nA Survey on Human Preference Learning for Large Language Models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.11191v2</guid>
      <dc:creator>Ruili Jiang, Kehai Chen, Xuefeng Bai, Zhixuan He, Juntao Li, Muyun Yang, Tiejun Zhao, Liqiang Nie, Min Zhang</dc:creator>
      <pubDate>Tue, 18 Jun 2024 08:18:33 GMT</pubDate>
    </item>
    <item>
      <title>Stress-Testing Capability Elicitation With Password-Locked Models</title>
      <link>http://arxiv.org/abs/2405.19550v1</link>
      <description>To determine the safety of large language models (LLMs), AI developers must be able to assess their dangerous capabilities. But simple prompting strategies often fail to elicit an LLM's full capabilities. One way to elicit capabilities more robustly is to fine-tune the LLM to complete the task. In this paper, we investigate the conditions under which fine-tuning-based elicitation suffices to elicit capabilities. To do this, we introduce password-locked models, LLMs fine-tuned such that some of their capabilities are deliberately hidden. Specifically, these LLMs are trained to exhibit these capabilities only when a password is present in the prompt, and to imitate a much weaker LLM otherwise. Password-locked models enable a novel method of evaluating capabilities elicitation methods, by testing whether these password-locked capabilities can be elicited without using the password. We find that a few high-quality demonstrations are often sufficient to fully elicit password-locked capabilities. More surprisingly, fine-tuning can elicit other capabilities that have been locked using the same password, or even different passwords. Furthermore, when only evaluations, and not demonstrations, are available, approaches like reinforcement learning are still often able to elicit capabilities. Overall, our findings suggest that fine-tuning is an effective method of eliciting hidden capabilities of current models, but may be unreliable when high-quality demonstrations are not available, e.g. as may be the case when models' (hidden) capabilities exceed those of human demonstrators.\n\n\nStress-Testing Capability Elicitation With Password-Locked Models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.19550v1</guid>
      <dc:creator>Ryan Greenblatt, Fabien Roger, Dmitrii Krasheninnikov, David Krueger</dc:creator>
      <pubDate>Wed, 29 May 2024 22:26:26 GMT</pubDate>
    </item>
    <item>
      <title>Towards Understanding the Influence of Reward Margin on Preference Model Performance</title>
      <link>http://arxiv.org/abs/2404.04932v1</link>
      <description>Reinforcement Learning from Human Feedback (RLHF) is a widely used framework for the training of language models. However, the process of using RLHF to develop a language model that is well-aligned presents challenges, especially when it comes to optimizing the reward model. Our research has found that existing reward models, when trained using the traditional ranking objective based on human preference data, often struggle to effectively distinguish between responses that are more or less favorable in real-world scenarios. To bridge this gap, our study introduces a novel method to estimate the preference differences without the need for detailed, exhaustive labels from human annotators. Our experimental results provide empirical evidence that incorporating margin values into the training process significantly improves the effectiveness of reward models. This comparative analysis not only demonstrates the superiority of our approach in terms of reward prediction accuracy but also highlights its effectiveness in practical applications.\n\n\nTowards Understanding the Influence of Reward Margin on Preference Model Performance</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2404.04932v1</guid>
      <dc:creator>Bowen Qin, Duanyu Feng, Xi Yang</dc:creator>
      <pubDate>Sun, 07 Apr 2024 12:10:04 GMT</pubDate>
    </item>
    <item>
      <title>APEER: Automatic Prompt Engineering Enhances Large Language Model Reranking</title>
      <link>http://arxiv.org/abs/2406.14449v1</link>
      <description>Large Language Models (LLMs) have significantly enhanced Information Retrieval (IR) across various modules, such as reranking. Despite impressive performance, current zero-shot relevance ranking with LLMs heavily relies on human prompt engineering. Existing automatic prompt engineering algorithms primarily focus on language modeling and classification tasks, leaving the domain of IR, particularly reranking, underexplored. Directly applying current prompt engineering algorithms to relevance ranking is challenging due to the integration of query and long passage pairs in the input, where the ranking complexity surpasses classification tasks. To reduce human effort and unlock the potential of prompt optimization in reranking, we introduce a novel automatic prompt engineering algorithm named APEER. APEER iteratively generates refined prompts through feedback and preference optimization. Extensive experiments with four LLMs and ten datasets demonstrate the substantial performance improvement of APEER over existing state-of-the-art (SoTA) manual prompts. Furthermore, we find that the prompts generated by APEER exhibit better transferability across diverse tasks and LLMs. Code is available at https://github.com/jincan333/APEER.\n\n\nAPEER: Automatic Prompt Engineering Enhances Large Language Model Reranking</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.14449v1</guid>
      <dc:creator>Can Jin, Hongwu Peng, Shiyu Zhao, Zhenting Wang, Wujiang Xu, Ligong Han, Jiahui Zhao, Kai Zhong, Sanguthevar Rajasekaran, Dimitris N. Metaxas</dc:creator>
      <pubDate>Thu, 20 Jun 2024 16:11:45 GMT</pubDate>
    </item>
    <item>
      <title>Interpretable Preferences via Multi-Objective Reward Modeling and Mixture-of-Experts</title>
      <link>http://arxiv.org/abs/2406.12845v1</link>
      <description>Reinforcement learning from human feedback (RLHF) has emerged as the primary method for aligning large language models (LLMs) with human preferences. The RLHF process typically starts by training a reward model (RM) using human preference data. Conventional RMs are trained on pairwise responses to the same user request, with relative ratings indicating which response humans prefer. The trained RM serves as a proxy for human preferences. However, due to the black-box nature of RMs, their outputs lack interpretability, as humans cannot intuitively understand why an RM thinks a response is good or not. As RMs act as human preference proxies, we believe they should be human-interpretable to ensure that their internal decision processes are consistent with human preferences and to prevent reward hacking in LLM alignment. To build RMs with interpretable preferences, we propose a two-stage approach: i) train an Absolute-Rating Multi-Objective Reward Model (ArmoRM) with multi-dimensional absolute-rating data, each dimension corresponding to a human-interpretable objective (e.g., honesty, verbosity, safety); ii) employ a Mixture-of-Experts (MoE) strategy with a gating network that automatically selects the most suitable reward objectives based on the context. We efficiently trained an ArmoRM with Llama-3 8B and a gating network consisting of a shallow MLP on top of the ArmoRM. Our trained model, ArmoRM-Llama3-8B, obtains state-of-the-art performance on RewardBench, a benchmark evaluating RMs for language modeling. Notably, the performance of our model surpasses the LLM-as-a-judge method with GPT-4 judges by a margin, and approaches the performance of the much larger Nemotron-4 340B reward model.\n\n\nInterpretable Preferences via Multi-Objective Reward Modeling and Mixture-of-Experts</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.12845v1</guid>
      <dc:creator>Haoxiang Wang, Wei Xiong, Tengyang Xie, Han Zhao, Tong Zhang</dc:creator>
      <pubDate>Tue, 18 Jun 2024 17:58:28 GMT</pubDate>
    </item>
    <item>
      <title>Predicting the Effectiveness of Self-Training: Application to Sentiment Classification</title>
      <link>http://arxiv.org/abs/1601.03288v1</link>
      <description>The goal of this paper is to investigate the connection between the performance gain that can be obtained by selftraining and the similarity between the corpora used in this approach. Self-training is a semi-supervised technique designed to increase the performance of machine learning algorithms by automatically classifying instances of a task and adding these as additional training material to the same classifier. In the context of language processing tasks, this training material is mostly an (annotated) corpus. Unfortunately self-training does not always lead to a performance increase and whether it will is largely unpredictable. We show that the similarity between corpora can be used to identify those setups for which self-training can be beneficial. We consider this research as a step in the process of developing a classifier that is able to adapt itself to each new test corpus that it is presented with.\n\n\nReflection-Reinforced Self-Training for Language Agents</description>
      <guid isPermaLink="false">http://arxiv.org/abs/1601.03288v1</guid>
      <dc:creator>Vincent Van Asch, Walter Daelemans</dc:creator>
      <pubDate>Wed, 13 Jan 2016 15:55:36 GMT</pubDate>
    </item>
    <item>
      <title>Aligning Agents like Large Language Models</title>
      <link>http://arxiv.org/abs/2406.04208v1</link>
      <description>Training agents to behave as desired in complex 3D environments from high-dimensional sensory information is challenging. Imitation learning from diverse human behavior provides a scalable approach for training an agent with a sensible behavioral prior, but such an agent may not perform the specific behaviors of interest when deployed. To address this issue, we draw an analogy between the undesirable behaviors of imitation learning agents and the unhelpful responses of unaligned large language models (LLMs). We then investigate how the procedure for aligning LLMs can be applied to aligning agents in a 3D environment from pixels. For our analysis, we utilize an academically illustrative part of a modern console game in which the human behavior distribution is multi-modal, but we want our agent to imitate a single mode of this behavior. We demonstrate that we can align our agent to consistently perform the desired mode, while providing insights and advice for successfully applying this approach to training agents. Project webpage at https://adamjelley.github.io/aligning-agents-like-llms .\n\n\nAligning Agents like Large Language Models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.04208v1</guid>
      <dc:creator>Adam Jelley, Yuhan Cao, Dave Bignell, Sam Devlin, Tabish Rashid</dc:creator>
      <pubDate>Thu, 06 Jun 2024 16:05:45 GMT</pubDate>
    </item>
    <item>
      <title>Enhancing Document-level Translation of Large Language Model via Translation Mixed-instructions</title>
      <link>http://arxiv.org/abs/2401.08088v1</link>
      <description>Existing large language models (LLMs) for machine translation are typically fine-tuned on sentence-level translation instructions and achieve satisfactory performance at the sentence level. However, when applied to document-level translation, these models face a significant challenge, particularly when dealing with documents containing over 512 tokens. This challenge arises from the issue of sentence-level coverage, where subsequent sentences in the document remain untranslated. As a result, the document-level translation capability of LLMs fine-tuned on sentence-level translation instructions is significantly limited. We conjecture that the primary cause of LLMs' weak document-level translation performance is the absence of document-to-document mapping ability. To address the issue, we propose an approach that combines sentence-level and document-level translation instructions of varying lengths to fine-tune LLMs. Our proposed translation mixed-instructions enable LLMs (Llama-2~7B and 13B) to maintain consistent translation performance from the sentence level to documents containing as many as 2048 tokens. Extensive experimental results show that the proposed approach significantly enhances the document-level translation capabilities of LLMs on 10 language pairs, effectively mitigating the sentence-level coverage issue in document-level translation. Experimentation on discourse phenomena has demonstrated that our document-level translation approach significantly improves translation quality, both in terms of BLEU score and discourse coherence.\n\n\nFeedback-aligned Mixed LLMs for Machine Language-Molecule Translation</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2401.08088v1</guid>
      <dc:creator>Yachao Li, Junhui Li, Jing Jiang, Min Zhang</dc:creator>
      <pubDate>Tue, 16 Jan 2024 03:28:26 GMT</pubDate>
    </item>
    <item>
      <title>BAPO: Base-Anchored Preference Optimization for Personalized Alignment in Large Language Models</title>
      <link>http://arxiv.org/abs/2407.00693v1</link>
      <description>While learning to align Large Language Models (LLMs) with human preferences has shown remarkable success, aligning these models to meet the diverse user preferences presents further challenges in preserving previous knowledge. This paper examines the impact of personalized preference optimization on LLMs, revealing that the extent of knowledge loss varies significantly with preference heterogeneity. Although previous approaches have utilized the KL constraint between the reference model and the policy model, we observe that they fail to maintain general knowledge and alignment when facing personalized preferences. To this end, we introduce Base-Anchored Preference Optimization (BAPO), a simple yet effective approach that utilizes the initial responses of reference model to mitigate forgetting while accommodating personalized alignment. BAPO effectively adapts to diverse user preferences while minimally affecting global knowledge or general alignment. Our experiments demonstrate the efficacy of BAPO in various setups.\n\n\nBAPO: Base-Anchored Preference Optimization for Personalized Alignment in Large Language Models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2407.00693v1</guid>
      <dc:creator>Gihun Lee, Minchan Jeong, Yujin Kim, Hojung Jung, Jaehoon Oh, Sangmook Kim, Se-Young Yun</dc:creator>
      <pubDate>Sun, 30 Jun 2024 13:30:04 GMT</pubDate>
    </item>
    <item>
      <title>THRONE: An Object-based Hallucination Benchmark for the Free-form Generations of Large Vision-Language Models</title>
      <link>http://arxiv.org/abs/2405.05256v1</link>
      <description>Mitigating hallucinations in large vision-language models (LVLMs) remains an open problem. Recent benchmarks do not address hallucinations in open-ended free-form responses, which we term &quot;Type I hallucinations&quot;. Instead, they focus on hallucinations responding to very specific question formats -- typically a multiple-choice response regarding a particular object or attribute -- which we term &quot;Type II hallucinations&quot;. Additionally, such benchmarks often require external API calls to models which are subject to change. In practice, we observe that a reduction in Type II hallucinations does not lead to a reduction in Type I hallucinations but rather that the two forms of hallucinations are often anti-correlated. To address this, we propose THRONE, a novel object-based automatic framework for quantitatively evaluating Type I hallucinations in LVLM free-form outputs. We use public language models (LMs) to identify hallucinations in LVLM responses and compute informative metrics. By evaluating a large selection of recent LVLMs using public datasets, we show that an improvement in existing metrics do not lead to a reduction in Type I hallucinations, and that established benchmarks for measuring Type I hallucinations are incomplete. Finally, we provide a simple and effective data augmentation method to reduce Type I and Type II hallucinations as a strong baseline.\n\n\nThe Hallucinations Leaderboard--An Open Effort to Measure Hallucinations in Large Language Models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.05256v1</guid>
      <dc:creator>Prannay Kaul, Zhizhong Li, Hao Yang, Yonatan Dukler, Ashwin Swaminathan, C. J. Taylor, Stefano Soatto</dc:creator>
      <pubDate>Wed, 08 May 2024 17:59:11 GMT</pubDate>
    </item>
    <item>
      <title>Optimizing Language Model's Reasoning Abilities with Weak Supervision</title>
      <link>http://arxiv.org/abs/2405.04086v1</link>
      <description>While Large Language Models (LLMs) have demonstrated proficiency in handling complex queries, much of the past work has depended on extensively annotated datasets by human experts. However, this reliance on fully-supervised annotations poses scalability challenges, particularly as models and data requirements grow. To mitigate this, we explore the potential of enhancing LLMs' reasoning abilities with minimal human supervision. In this work, we introduce self-reinforcement, which begins with Supervised Fine-Tuning (SFT) of the model using a small collection of annotated questions. Then it iteratively improves LLMs by learning from the differences in responses from the SFT and unfinetuned models on unlabeled questions. Our approach provides an efficient approach without relying heavily on extensive human-annotated explanations. However, current reasoning benchmarks typically only include golden-reference answers or rationales. Therefore, we present \textsc{PuzzleBen}, a weakly supervised benchmark that comprises 25,147 complex questions, answers, and human-generated rationales across various domains, such as brainteasers, puzzles, riddles, parajumbles, and critical reasoning tasks. A unique aspect of our dataset is the inclusion of 10,000 unannotated questions, enabling us to explore utilizing fewer supersized data to boost LLMs' inference capabilities. Our experiments underscore the significance of \textsc{PuzzleBen}, as well as the effectiveness of our methodology as a promising direction in future endeavors. Our dataset and code will be published soon on \texttt{Anonymity Link}.\n\n\nOptimizing Language Model's Reasoning Abilities with Weak Supervision</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.04086v1</guid>
      <dc:creator>Yongqi Tong, Sizhe Wang, Dawei Li, Yifan Wang, Simeng Han, Zi Lin, Chengsong Huang, Jiaxin Huang, Jingbo Shang</dc:creator>
      <pubDate>Tue, 07 May 2024 07:39:15 GMT</pubDate>
    </item>
    <item>
      <title>Survey for Landing Generative AI in Social and E-commerce Recsys -- the Industry Perspectives</title>
      <link>http://arxiv.org/abs/2406.06475v1</link>
      <description>Recently, generative AI (GAI), with their emerging capabilities, have presented unique opportunities for augmenting and revolutionizing industrial recommender systems (Recsys). Despite growing research efforts at the intersection of these fields, the integration of GAI into industrial Recsys remains in its infancy, largely due to the intricate nature of modern industrial Recsys infrastructure, operations, and product sophistication. Drawing upon our experiences in successfully integrating GAI into several major social and e-commerce platforms, this survey aims to comprehensively examine the underlying system and AI foundations, solution frameworks, connections to key research advancements, as well as summarize the practical insights and challenges encountered in the endeavor to integrate GAI into industrial Recsys. As pioneering work in this domain, we hope outline the representative developments of relevant fields, shed lights on practical GAI adoptions in the industry, and motivate future research.\n\n\nSurvey for Landing Generative AI in Social and E-commerce Recsys--the Industry Perspectives</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.06475v1</guid>
      <dc:creator>Da Xu, Danqing Zhang, Guangyu Yang, Bo Yang, Shuyuan Xu, Lingling Zheng, Cindy Liang</dc:creator>
      <pubDate>Mon, 10 Jun 2024 17:16:59 GMT</pubDate>
    </item>
    <item>
      <title>Model Editing with Canonical Examples</title>
      <link>http://arxiv.org/abs/2402.06155v1</link>
      <description>We introduce model editing with canonical examples, a setting in which (1) a single learning example is provided per desired behavior, (2) evaluation is performed exclusively out-of-distribution, and (3) deviation from an initial model is strictly limited. A canonical example is a simple instance of good behavior, e.g., The capital of Mauritius is Port Louis) or bad behavior, e.g., An aspect of researchers is coldhearted). The evaluation set contains more complex examples of each behavior (like a paragraph in which the capital of Mauritius is called for.) We create three datasets and modify three more for model editing with canonical examples, covering knowledge-intensive improvements, social bias mitigation, and syntactic edge cases. In our experiments on Pythia language models, we find that LoRA outperforms full finetuning and MEMIT. We then turn to the Backpack language model architecture because it is intended to enable targeted improvement. The Backpack defines a large bank of sense vectors--a decomposition of the different uses of each word--which are weighted and summed to form the output logits of the model. We propose sense finetuning, which selects and finetunes a few ($\approx$ 10) sense vectors for each canonical example, and find that it outperforms other finetuning methods, e.g., 4.8% improvement vs 0.3%. Finally, we improve GPT-J-6B by an inference-time ensemble with just the changes from sense finetuning of a 35x smaller Backpack, in one setting outperforming editing GPT-J itself (4.1% vs 1.0%).\n\n\nModel Editing by Pure Fine-Tuning</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.06155v1</guid>
      <dc:creator>John Hewitt, Sarah Chen, Lanruo Lora Xie, Edward Adams, Percy Liang, Christopher D. Manning</dc:creator>
      <pubDate>Fri, 09 Feb 2024 03:08:12 GMT</pubDate>
    </item>
    <item>
      <title>RITFIS: Robust input testing framework for LLMs-based intelligent software</title>
      <link>http://arxiv.org/abs/2402.13518v1</link>
      <description>The dependence of Natural Language Processing (NLP) intelligent software on Large Language Models (LLMs) is increasingly prominent, underscoring the necessity for robustness testing. Current testing methods focus solely on the robustness of LLM-based software to prompts. Given the complexity and diversity of real-world inputs, studying the robustness of LLMbased software in handling comprehensive inputs (including prompts and examples) is crucial for a thorough understanding of its performance.   To this end, this paper introduces RITFIS, a Robust Input Testing Framework for LLM-based Intelligent Software. To our knowledge, RITFIS is the first framework designed to assess the robustness of LLM-based intelligent software against natural language inputs. This framework, based on given threat models and prompts, primarily defines the testing process as a combinatorial optimization problem. Successful test cases are determined by a goal function, creating a transformation space for the original examples through perturbation means, and employing a series of search methods to filter cases that meet both the testing objectives and language constraints. RITFIS, with its modular design, offers a comprehensive method for evaluating the robustness of LLMbased intelligent software.   RITFIS adapts 17 automated testing methods, originally designed for Deep Neural Network (DNN)-based intelligent software, to the LLM-based software testing scenario. It demonstrates the effectiveness of RITFIS in evaluating LLM-based intelligent software through empirical validation. However, existing methods generally have limitations, especially when dealing with lengthy texts and structurally complex threat models. Therefore, we conducted a comprehensive analysis based on five metrics and provided insightful testing method optimization strategies, benefiting both researchers and everyday users.\n\n\nNext-Generation Database Interfaces: A Survey of LLM-based Text-to-SQL</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.13518v1</guid>
      <dc:creator>Mingxuan Xiao, Yan Xiao, Hai Dong, Shunhui Ji, Pengcheng Zhang</dc:creator>
      <pubDate>Wed, 21 Feb 2024 04:00:54 GMT</pubDate>
    </item>
    <item>
      <title>Large Language Models Are Neurosymbolic Reasoners</title>
      <link>http://arxiv.org/abs/2401.09334v1</link>
      <description>A wide range of real-world applications is characterized by their symbolic nature, necessitating a strong capability for symbolic reasoning. This paper investigates the potential application of Large Language Models (LLMs) as symbolic reasoners. We focus on text-based games, significant benchmarks for agents with natural language capabilities, particularly in symbolic tasks like math, map reading, sorting, and applying common sense in text-based worlds. To facilitate these agents, we propose an LLM agent designed to tackle symbolic challenges and achieve in-game objectives. We begin by initializing the LLM agent and informing it of its role. The agent then receives observations and a set of valid actions from the text-based games, along with a specific symbolic module. With these inputs, the LLM agent chooses an action and interacts with the game environments. Our experimental results demonstrate that our method significantly enhances the capability of LLMs as automated agents for symbolic reasoning, and our LLM agent is effective in text-based games involving symbolic tasks, achieving an average performance of 88% across all tasks.\n\n\nLarge Language Models as Agents in Two-Player Games</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2401.09334v1</guid>
      <dc:creator>Meng Fang, Shilong Deng, Yudi Zhang, Zijing Shi, Ling Chen, Mykola Pechenizkiy, Jun Wang</dc:creator>
      <pubDate>Wed, 17 Jan 2024 16:57:19 GMT</pubDate>
    </item>
    <item>
      <title>Do Language Models Exhibit Human-like Structural Priming Effects?</title>
      <link>http://arxiv.org/abs/2406.04847v1</link>
      <description>We explore which linguistic factors -- at the sentence and token level -- play an important role in influencing language model predictions, and investigate whether these are reflective of results found in humans and human corpora (Gries and Kootstra, 2017). We make use of the structural priming paradigm, where recent exposure to a structure facilitates processing of the same structure. We don't only investigate whether, but also where priming effects occur, and what factors predict them. We show that these effects can be explained via the inverse frequency effect, known in human priming, where rarer elements within a prime increase priming effects, as well as lexical dependence between prime and target. Our results provide an important piece in the puzzle of understanding how properties within their context affect structural prediction in language models.\n\n\nDo Language Models Exhibit Human-like Structural Priming Effects?</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.04847v1</guid>
      <dc:creator>Jaap Jumelet, Willem Zuidema, Arabella Sinclair</dc:creator>
      <pubDate>Fri, 07 Jun 2024 11:21:52 GMT</pubDate>
    </item>
    <item>
      <title>ATM: Adversarial Tuning Multi-agent System Makes a Robust Retrieval-Augmented Generator</title>
      <link>http://arxiv.org/abs/2405.18111v2</link>
      <description>Large language models (LLMs) are proven to benefit a lot from retrieval-augmented generation (RAG) in alleviating hallucinations confronted with knowledge-intensive questions. RAG adopts information retrieval techniques to inject external knowledge from semantic-relevant documents as input contexts. However, due to today's Internet being flooded with numerous noisy and fabricating content, it is inevitable that RAG systems are vulnerable to these noises and prone to respond incorrectly. To this end, we propose to optimize the retrieval-augmented Generator with a Adversarial Tuning Multi-agent system (ATM). The ATM steers the Generator to have a robust perspective of useful documents for question answering with the help of an auxiliary Attacker agent. The Generator and the Attacker are tuned adversarially for several iterations. After rounds of multi-agent iterative tuning, the Generator can eventually better discriminate useful documents amongst fabrications. The experimental results verify the effectiveness of ATM and we also observe that the Generator can achieve better performance compared to state-of-the-art baselines.\n\n\nATM: Adversarial Tuning Multi-agent System Makes a Robust Retrieval-Augmented Generator</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.18111v2</guid>
      <dc:creator>Junda Zhu, Lingyong Yan, Haibo Shi, Dawei Yin, Lei Sha</dc:creator>
      <pubDate>Sun, 16 Jun 2024 12:30:32 GMT</pubDate>
    </item>
    <item>
      <title>UICoder: Finetuning Large Language Models to Generate User Interface Code through Automated Feedback</title>
      <link>http://arxiv.org/abs/2406.07739v1</link>
      <description>Large language models (LLMs) struggle to consistently generate UI code that compiles and produces visually relevant designs. Existing approaches to improve generation rely on expensive human feedback or distilling a proprietary model. In this paper, we explore the use of automated feedback (compilers and multi-modal models) to guide LLMs to generate high-quality UI code. Our method starts with an existing LLM and iteratively produces improved models by self-generating a large synthetic dataset using an original model, applying automated tools to aggressively filter, score, and de-duplicate the data into a refined higher quality dataset. The original LLM is improved by finetuning on this refined dataset. We applied our approach to several open-source LLMs and compared the resulting performance to baseline models with both automated metrics and human preferences. Our evaluation shows the resulting models outperform all other downloadable baselines and approach the performance of larger proprietary models.\n\n\nUICoder: Finetuning Large Language Models to Generate User Interface Code through Automated Feedback</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.07739v1</guid>
      <dc:creator>Jason Wu, Eldon Schoop, Alan Leung, Titus Barik, Jeffrey P. Bigham, Jeffrey Nichols</dc:creator>
      <pubDate>Tue, 11 Jun 2024 21:53:46 GMT</pubDate>
    </item>
    <item>
      <title>FedLLM-Bench: Realistic Benchmarks for Federated Learning of Large Language Models</title>
      <link>http://arxiv.org/abs/2406.04845v1</link>
      <description>Federated learning has enabled multiple parties to collaboratively train large language models without directly sharing their data (FedLLM). Following this training paradigm, the community has put massive efforts from diverse aspects including framework, performance, and privacy. However, an unpleasant fact is that there are currently no realistic datasets and benchmarks for FedLLM and previous works all rely on artificially constructed datasets, failing to capture properties in real-world scenarios. Addressing this, we propose FedLLM-Bench, which involves 8 training methods, 4 training datasets, and 6 evaluation metrics, to offer a comprehensive testbed for the FedLLM community. FedLLM-Bench encompasses three datasets (e.g., user-annotated multilingual dataset) for federated instruction tuning and one dataset (e.g., user-annotated preference dataset) for federated preference alignment, whose scale of client number ranges from 38 to 747. Our datasets incorporate several representative diversities: language, quality, quantity, instruction, length, embedding, and preference, capturing properties in real-world scenarios. Based on FedLLM-Bench, we conduct experiments on all datasets to benchmark existing FL methods and provide empirical insights (e.g., multilingual collaboration). We believe that our FedLLM-Bench can benefit the FedLLM community by reducing required efforts, providing a practical testbed, and promoting fair comparisons. Code and datasets are available at https://github.com/rui-ye/FedLLM-Bench.\n\n\nFedLLM-Bench: Realistic Benchmarks for Federated Learning of Large Language Models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.04845v1</guid>
      <dc:creator>Rui Ye, Rui Ge, Xinyu Zhu, Jingyi Chai, Yaxin Du, Yang Liu, Yanfeng Wang, Siheng Chen</dc:creator>
      <pubDate>Fri, 07 Jun 2024 11:19:30 GMT</pubDate>
    </item>
    <item>
      <title>GuardAgent: Safeguard LLM Agents by a Guard Agent via Knowledge-Enabled Reasoning</title>
      <link>http://arxiv.org/abs/2406.09187v1</link>
      <description>The rapid advancement of large language models (LLMs) has catalyzed the deployment of LLM-powered agents across numerous applications, raising new concerns regarding their safety and trustworthiness. Existing methods for enhancing the safety of LLMs are not directly transferable to LLM-powered agents due to their diverse objectives and output modalities. In this paper, we propose GuardAgent, the first LLM agent as a guardrail to other LLM agents. Specifically, GuardAgent oversees a target LLM agent by checking whether its inputs/outputs satisfy a set of given guard requests defined by the users. GuardAgent comprises two steps: 1) creating a task plan by analyzing the provided guard requests, and 2) generating guardrail code based on the task plan and executing the code by calling APIs or using external engines. In both steps, an LLM is utilized as the core reasoning component, supplemented by in-context demonstrations retrieved from a memory module. Such knowledge-enabled reasoning allows GuardAgent to understand various textual guard requests and accurately &quot;translate&quot; them into executable code that provides reliable guardrails. Furthermore, GuardAgent is equipped with an extendable toolbox containing functions and APIs and requires no additional LLM training, which underscores its generalization capabilities and low operational overhead. Additionally, we propose two novel benchmarks: an EICU-AC benchmark for assessing privacy-related access control for healthcare agents and a Mind2Web-SC benchmark for safety evaluation for web agents. We show the effectiveness of GuardAgent on these two benchmarks with 98.7% and 90.0% accuracy in moderating invalid inputs and outputs for the two types of agents, respectively. We also show that GuardAgent is able to define novel functions in adaption to emergent LLM agents and guard requests, which underscores its strong generalization capabilities.\n\n\n-Guard: Robust Reasoning Enabled LLM Guardrail via Knowledge-Enhanced Logical Reasoning</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.09187v1</guid>
      <dc:creator>Zhen Xiang, Linzhi Zheng, Yanjie Li, Junyuan Hong, Qinbin Li, Han Xie, Jiawei Zhang, Zidi Xiong, Chulin Xie, Carl Yang, Dawn Song, Bo Li</dc:creator>
      <pubDate>Thu, 13 Jun 2024 14:49:26 GMT</pubDate>
    </item>
    <item>
      <title>LoFiT: Localized Fine-tuning on LLM Representations</title>
      <link>http://arxiv.org/abs/2406.01563v1</link>
      <description>Recent work in interpretability shows that large language models (LLMs) can be adapted for new tasks in a learning-free way: it is possible to intervene on LLM representations to elicit desired behaviors for alignment. For instance, adding certain bias vectors to the outputs of certain attention heads is reported to boost the truthfulness of models. In this work, we show that localized fine-tuning serves as an effective alternative to such representation intervention methods. We introduce a framework called Localized Fine-Tuning on LLM Representations (LoFiT), which identifies a subset of attention heads that are most important for learning a specific task, then trains offset vectors to add to the model's hidden representations at those selected heads. LoFiT localizes to a sparse set of heads (3%) and learns the offset vectors from limited training data, comparable to the settings used for representation intervention. For truthfulness and reasoning tasks, we find that LoFiT's intervention vectors are more effective for LLM adaptation than vectors from representation intervention methods such as Inference-time Intervention. We also find that the localization step is important: selecting a task-specific set of attention heads can lead to higher performance than intervening on heads selected for a different task. Finally, for the tasks we study, LoFiT achieves comparable performance to other parameter-efficient fine-tuning methods such as LoRA, despite modifying 20x-200x fewer parameters than these methods.\n\n\nLoFiT: Localized Fine-tuning on LLM Representations</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.01563v1</guid>
      <dc:creator>Fangcong Yin, Xi Ye, Greg Durrett</dc:creator>
      <pubDate>Mon, 03 Jun 2024 17:45:41 GMT</pubDate>
    </item>
    <item>
      <title>Joint Demonstration and Preference Learning Improves Policy Alignment with Human Feedback</title>
      <link>http://arxiv.org/abs/2406.06874v2</link>
      <description>Aligning human preference and value is an important requirement for building contemporary foundation models and embodied AI. However, popular approaches such as reinforcement learning with human feedback (RLHF) break down the task into successive stages, such as supervised fine-tuning (SFT), reward modeling (RM), and reinforcement learning (RL), each performing one specific learning task. Such a sequential approach results in serious issues such as significant under-utilization of data and distribution mismatch between the learned reward model and generated policy, which eventually lead to poor alignment performance. We develop a single stage approach named Alignment with Integrated Human Feedback (AIHF), capable of integrating both human preference and demonstration to train reward models and the policy. The proposed approach admits a suite of efficient algorithms, which can easily reduce to, and leverage, popular alignment algorithms such as RLHF and Directly Policy Optimization (DPO), and only requires minor changes to the existing alignment pipelines. We demonstrate the efficiency of the proposed solutions with extensive experiments involving alignment problems in LLMs and robotic control problems in MuJoCo. We observe that the proposed solutions outperform the existing alignment algorithms such as RLHF and DPO by large margins, especially when the amount of high-quality preference data is relatively limited.\n\n\nJoint Demonstration and Preference Learning Improves Policy Alignment with Human Feedback</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.06874v2</guid>
      <dc:creator>Chenliang Li, Siliang Zeng, Zeyi Liao, Jiaxiang Li, Dongyeop Kang, Alfredo Garcia, Mingyi Hong</dc:creator>
      <pubDate>Wed, 19 Jun 2024 15:04:23 GMT</pubDate>
    </item>
    <item>
      <title>A Fundamental Trade-off in Aligned Language Models and its Relation to Sampling Adaptors</title>
      <link>http://arxiv.org/abs/2406.10203v1</link>
      <description>The relationship between the quality of a string and its probability $p(\boldsymbol{y})$ under a language model has been influential in the development of techniques to build good text generation systems. For example, several decoding algorithms have been motivated to manipulate $p(\boldsymbol{y})$ to produce higher-quality text. In this work, we examine the probability--quality relationship in language models explicitly aligned to human preferences, e.g., through Reinforcement Learning through Human Feedback (RLHF). We find that, given a general language model and its aligned version, for corpora sampled from an aligned language model, there exists a trade-off between the average reward and average log-likelihood of the strings under the general language model. We provide a formal treatment of this issue and demonstrate how a choice of sampling adaptor allows for a selection of how much likelihood we exchange for the reward.\n\n\nA Fundamental Trade-off in Aligned Language Models and its Relation to Sampling Adaptors</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.10203v1</guid>
      <dc:creator>Naaman Tan, Josef Valvoda, Anej Svete, Tianyu Liu, Yanxia Qin, Kan Min-Yen, Ryan Cotterell</dc:creator>
      <pubDate>Fri, 14 Jun 2024 17:38:21 GMT</pubDate>
    </item>
    <item>
      <title>Preference Tuning For Toxicity Mitigation Generalizes Across Languages</title>
      <link>http://arxiv.org/abs/2406.16235v1</link>
      <description>Detoxifying multilingual Large Language Models (LLMs) has become crucial due to their increasing global use. In this work, we explore zero-shot cross-lingual generalization of preference tuning in detoxifying LLMs. Unlike previous studies that show limited cross-lingual generalization for other safety tasks, we demonstrate that Direct Preference Optimization (DPO) training with only English data can significantly reduce toxicity in multilingual open-ended generations. For example, the probability of mGPT-1.3B generating toxic continuations drops from 46.8% to 3.9% across 17 different languages after training. Our results also extend to other multilingual LLMs, such as BLOOM, Llama3, and Aya-23. Using mechanistic interpretability tools like causal intervention and activation analysis, we identified the dual multilinguality property of MLP layers in LLMs, which explains the cross-lingual generalization of DPO. Finally, we show that bilingual sentence retrieval can predict the cross-lingual transferability of DPO preference tuning.\n\n\nPreference Tuning For Toxicity Mitigation Generalizes Across Languages</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.16235v1</guid>
      <dc:creator>Xiaochen Li, Zheng-Xin Yong, Stephen H. Bach</dc:creator>
      <pubDate>Sun, 23 Jun 2024 22:53:47 GMT</pubDate>
    </item>
    <item>
      <title>PKU-SafeRLHF: A Safety Alignment Preference Dataset for Llama Family Models</title>
      <link>http://arxiv.org/abs/2406.15513v1</link>
      <description>In this work, we introduce the PKU-SafeRLHF dataset, designed to promote research on safety alignment in large language models (LLMs). As a sibling project to SafeRLHF and BeaverTails, we separate annotations of helpfulness and harmlessness for question-answering pairs, providing distinct perspectives on these coupled attributes. Overall, we provide 44.6k refined prompts and 265k question-answer pairs with safety meta-labels for 19 harm categories and three severity levels ranging from minor to severe, with answers generated by Llama-family models. Based on this, we collected 166.8k preference data, including dual-preference (helpfulness and harmlessness decoupled) and single-preference data (trade-off the helpfulness and harmlessness from scratch), respectively. Using the large-scale annotation data, we further train severity-sensitive moderation for the risk control of LLMs and safety-centric RLHF algorithms for the safety alignment of LLMs. We believe this dataset will be a valuable resource for the community, aiding in the safe deployment of LLMs.\n\n\nPKU-SafeRLHF: A Safety Alignment Preference Dataset for Llama Family Models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.15513v1</guid>
      <dc:creator>Jiaming Ji, Donghai Hong, Borong Zhang, Boyuan Chen, Josef Dai, Boren Zheng, Tianyi Qiu, Boxun Li, Yaodong Yang</dc:creator>
      <pubDate>Thu, 20 Jun 2024 18:37:36 GMT</pubDate>
    </item>
    <item>
      <title>Large Language Models Assume People are More Rational than We Really are</title>
      <link>http://arxiv.org/abs/2406.17055v2</link>
      <description>In order for AI systems to communicate effectively with people, they must understand how we make decisions. However, people's decisions are not always rational, so the implicit internal models of human decision-making in Large Language Models (LLMs) must account for this. Previous empirical evidence seems to suggest that these implicit models are accurate -- LLMs offer believable proxies of human behavior, acting how we expect humans would in everyday interactions. However, by comparing LLM behavior and predictions to a large dataset of human decisions, we find that this is actually not the case: when both simulating and predicting people's choices, a suite of cutting-edge LLMs (GPT-4o &amp; 4-Turbo, Llama-3-8B &amp; 70B, Claude 3 Opus) assume that people are more rational than we really are. Specifically, these models deviate from human behavior and align more closely with a classic model of rational choice -- expected value theory. Interestingly, people also tend to assume that other people are rational when interpreting their behavior. As a consequence, when we compare the inferences that LLMs and people draw from the decisions of others using another psychological dataset, we find that these inferences are highly correlated. Thus, the implicit decision-making models of LLMs appear to be aligned with the human expectation that other people will act rationally, rather than with how people actually act.\n\n\nLarge Language Models Assume People are More Rational than We Really are</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.17055v2</guid>
      <dc:creator>Ryan Liu, Jiayi Geng, Joshua C. Peterson, Ilia Sucholutsky, Thomas L. Griffiths</dc:creator>
      <pubDate>Mon, 01 Jul 2024 17:29:54 GMT</pubDate>
    </item>
    <item>
      <title>Efficacy of Machine-Generated Instructions</title>
      <link>http://arxiv.org/abs/2312.14423v1</link>
      <description>Large &quot;instruction-tuned&quot; language models (i.e., finetuned to respond to instructions) have demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless, they depend heavily on human-written instruction data that is often limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model. We conducted a quantitative study to figure out the efficacy of machine-generated annotations, where we compare the results of a fine-tuned BERT model with human v/s machine-generated annotations. Applying our methods to the vanilla GPT-3 model, we saw that machine generated annotations were 78.54% correct and the fine-tuned model achieved a 96.01% model performance compared to the performance with human-labelled annotations. This result shows that machine-generated annotations are a resource and cost effective way to fine-tune down-stream models.\n\n\nEfficacy of Language Model Self-Play in Non-Zero-Sum Games</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2312.14423v1</guid>
      <dc:creator>Samaksh Gulati, Anshit Verma, Manoj Parmar, Palash Chaudhary</dc:creator>
      <pubDate>Fri, 22 Dec 2023 04:01:30 GMT</pubDate>
    </item>
    <item>
      <title>Learning From Crowdsourced Noisy Labels: A Signal Processing Perspective</title>
      <link>http://arxiv.org/abs/2407.06902v1</link>
      <description>One of the primary catalysts fueling advances in artificial intelligence (AI) and machine learning (ML) is the availability of massive, curated datasets. A commonly used technique to curate such massive datasets is crowdsourcing, where data are dispatched to multiple annotators. The annotator-produced labels are then fused to serve downstream learning and inference tasks. This annotation process often creates noisy labels due to various reasons, such as the limited expertise, or unreliability of annotators, among others. Therefore, a core objective in crowdsourcing is to develop methods that effectively mitigate the negative impact of such label noise on learning tasks. This feature article introduces advances in learning from noisy crowdsourced labels. The focus is on key crowdsourcing models and their methodological treatments, from classical statistical models to recent deep learning-based approaches, emphasizing analytical insights and algorithmic developments. In particular, this article reviews the connections between signal processing (SP) theory and methods, such as identifiability of tensor and nonnegative matrix factorization, and novel, principled solutions of longstanding challenges in crowdsourcing -- showing how SP perspectives drive the advancements of this field. Furthermore, this article touches upon emerging topics that are critical for developing cutting-edge AI/ML systems, such as crowdsourcing in reinforcement learning with human feedback (RLHF) and direct preference optimization (DPO) that are key techniques for fine-tuning large language models (LLMs).\n\n\nLearning From Crowdsourced Noisy Labels: A Signal Processing Perspective</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2407.06902v1</guid>
      <dc:creator>Shahana Ibrahim, Panagiotis A. Traganitis, Xiao Fu, Georgios B. Giannakis</dc:creator>
      <pubDate>Tue, 09 Jul 2024 14:34:40 GMT</pubDate>
    </item>
    <item>
      <title>The Evolution of Multimodal Model Architectures</title>
      <link>http://arxiv.org/abs/2405.17927v1</link>
      <description>This work uniquely identifies and characterizes four prevalent multimodal model architectural patterns in the contemporary multimodal landscape. Systematically categorizing models by architecture type facilitates monitoring of developments in the multimodal domain. Distinct from recent survey papers that present general information on multimodal architectures, this research conducts a comprehensive exploration of architectural details and identifies four specific architectural types. The types are distinguished by their respective methodologies for integrating multimodal inputs into the deep neural network model. The first two types (Type A and B) deeply fuses multimodal inputs within the internal layers of the model, whereas the following two types (Type C and D) facilitate early fusion at the input stage. Type-A employs standard cross-attention, whereas Type-B utilizes custom-designed layers for modality fusion within the internal layers. On the other hand, Type-C utilizes modality-specific encoders, while Type-D leverages tokenizers to process the modalities at the model's input stage. The identified architecture types aid the monitoring of any-to-any multimodal model development. Notably, Type-C and Type-D are currently favored in the construction of any-to-any multimodal models. Type-C, distinguished by its non-tokenizing multimodal model architecture, is emerging as a viable alternative to Type-D, which utilizes input-tokenizing techniques. To assist in model selection, this work highlights the advantages and disadvantages of each architecture type based on data and compute requirements, architecture complexity, scalability, simplification of adding modalities, training objectives, and any-to-any multimodal generation capability.\n\n\nThe Evolution of Multimodal Model Architectures</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.17927v1</guid>
      <dc:creator>Shakti N. Wadekar, Abhishek Chaurasia, Aman Chadha, Eugenio Culurciello</dc:creator>
      <pubDate>Tue, 28 May 2024 07:48:15 GMT</pubDate>
    </item>
    <item>
      <title>Position: Foundation Agents as the Paradigm Shift for Decision Making</title>
      <link>http://arxiv.org/abs/2405.17009v3</link>
      <description>Decision making demands intricate interplay between perception, memory, and reasoning to discern optimal policies. Conventional approaches to decision making face challenges related to low sample efficiency and poor generalization. In contrast, foundation models in language and vision have showcased rapid adaptation to diverse new tasks. Therefore, we advocate for the construction of foundation agents as a transformative shift in the learning paradigm of agents. This proposal is underpinned by the formulation of foundation agents with their fundamental characteristics and challenges motivated by the success of large language models (LLMs). Moreover, we specify the roadmap of foundation agents from large interactive data collection or generation, to self-supervised pretraining and adaptation, and knowledge and value alignment with LLMs. Lastly, we pinpoint critical research questions derived from the formulation and delineate trends for foundation agents supported by real-world use cases, addressing both technical and theoretical aspects to propel the field towards a more comprehensive and impactful future.\n\n\nPosition: Foundation Agents as the Paradigm Shift for Decision Making</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.17009v3</guid>
      <dc:creator>Xiaoqian Liu, Xingzhou Lou, Jianbin Jiao, Junge Zhang</dc:creator>
      <pubDate>Wed, 29 May 2024 14:15:09 GMT</pubDate>
    </item>
    <item>
      <title>Aligning Large Language Models with Representation Editing: A Control Perspective</title>
      <link>http://arxiv.org/abs/2406.05954v2</link>
      <description>Aligning large language models (LLMs) with human objectives is crucial for real-world applications. However, fine-tuning LLMs for alignment often suffers from unstable training and requires substantial computing resources. Test-time alignment techniques, such as prompting and guided decoding, do not modify the underlying model, and their performance remains dependent on the original model's capabilities. To address these challenges, we propose aligning LLMs through representation editing. The core of our method is to view a pre-trained autoregressive LLM as a discrete-time stochastic dynamical system. To achieve alignment for specific objectives, we introduce external control signals into the state space of this language dynamical system. We train a value function directly on the hidden states according to the Bellman equation, enabling gradient-based optimization to obtain the optimal control signals at test time. Our experiments demonstrate that our method outperforms existing test-time alignment techniques while requiring significantly fewer resources compared to fine-tuning methods.\n\n\nAligning Large Language Models with Representation Editing: A Control Perspective</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.05954v2</guid>
      <dc:creator>Lingkai Kong, Haorui Wang, Wenhao Mu, Yuanqi Du, Yuchen Zhuang, Yifei Zhou, Yue Song, Rongzhi Zhang, Kai Wang, Chao Zhang</dc:creator>
      <pubDate>Tue, 11 Jun 2024 21:18:24 GMT</pubDate>
    </item>
    <item>
      <title>Finding Safety Neurons in Large Language Models</title>
      <link>http://arxiv.org/abs/2406.14144v1</link>
      <description>Large language models (LLMs) excel in various capabilities but also pose safety risks such as generating harmful content and misinformation, even after safety alignment. In this paper, we explore the inner mechanisms of safety alignment from the perspective of mechanistic interpretability, focusing on identifying and analyzing safety neurons within LLMs that are responsible for safety behaviors. We propose generation-time activation contrasting to locate these neurons and dynamic activation patching to evaluate their causal effects. Experiments on multiple recent LLMs show that: (1) Safety neurons are sparse and effective. We can restore $90$% safety performance with intervention only on about $5$% of all the neurons. (2) Safety neurons encode transferrable mechanisms. They exhibit consistent effectiveness on different red-teaming datasets. The finding of safety neurons also interprets &quot;alignment tax&quot;. We observe that the identified key neurons for safety and helpfulness significantly overlap, but they require different activation patterns of the shared neurons. Furthermore, we demonstrate an application of safety neurons in detecting unsafe outputs before generation. Our findings may promote further research on understanding LLM alignment. The source codes will be publicly released to facilitate future research.\n\n\nFinding Safety Neurons in Large Language Models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.14144v1</guid>
      <dc:creator>Jianhui Chen, Xiaozhi Wang, Zijun Yao, Yushi Bai, Lei Hou, Juanzi Li</dc:creator>
      <pubDate>Thu, 20 Jun 2024 09:35:22 GMT</pubDate>
    </item>
    <item>
      <title>Towards Robust Alignment of Language Models: Distributionally Robustifying Direct Preference Optimization</title>
      <link>http://arxiv.org/abs/2407.07880v1</link>
      <description>This study addresses the challenge of noise in training datasets for Direct Preference Optimization (DPO), a method for aligning Large Language Models (LLMs) with human preferences. We categorize noise into pointwise noise, which includes low-quality data points, and pairwise noise, which encompasses erroneous data pair associations that affect preference rankings. Utilizing Distributionally Robust Optimization (DRO), we enhance DPO's resilience to these types of noise. Our theoretical insights reveal that DPO inherently embeds DRO principles, conferring robustness to pointwise noise, with the regularization coefficient $\beta$ playing a critical role in its noise resistance. Extending this framework, we introduce Distributionally Robustifying DPO (Dr. DPO), which integrates pairwise robustness by optimizing against worst-case pairwise scenarios. The novel hyperparameter $\beta'$ in Dr. DPO allows for fine-tuned control over data pair reliability, providing a strategic balance between exploration and exploitation in noisy training environments. Empirical evaluations demonstrate that Dr. DPO substantially improves the quality of generated text and response accuracy in preference datasets, showcasing enhanced performance in both noisy and noise-free settings. The code is available at https://github.com/junkangwu/Dr_DPO.\n\n\nTowards Robust Alignment of Language Models: Distributionally Robustifying Direct Preference Optimization</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2407.07880v1</guid>
      <dc:creator>Junkang Wu, Yuexiang Xie, Zhengyi Yang, Jiancan Wu, Jiawei Chen, Jinyang Gao, Bolin Ding, Xiang Wang, Xiangnan He</dc:creator>
      <pubDate>Wed, 10 Jul 2024 17:48:25 GMT</pubDate>
    </item>
    <item>
      <title>AutoWebGLM: Bootstrap And Reinforce A Large Language Model-based Web Navigating Agent</title>
      <link>http://arxiv.org/abs/2404.03648v1</link>
      <description>Large language models (LLMs) have fueled many intelligent agent tasks, such as web navigation -- but most existing agents perform far from satisfying in real-world webpages due to three factors: (1) the versatility of actions on webpages, (2) HTML text exceeding model processing capacity, and (3) the complexity of decision-making due to the open-domain nature of web. In light of the challenge, we develop AutoWebGLM, a GPT-4-outperforming automated web navigation agent built upon ChatGLM3-6B. Inspired by human browsing patterns, we design an HTML simplification algorithm to represent webpages, preserving vital information succinctly. We employ a hybrid human-AI method to build web browsing data for curriculum training. Then, we bootstrap the model by reinforcement learning and rejection sampling to further facilitate webpage comprehension, browser operations, and efficient task decomposition by itself. For testing, we establish a bilingual benchmark -- AutoWebBench -- for real-world web browsing tasks. We evaluate AutoWebGLM across diverse web navigation benchmarks, revealing its improvements but also underlying challenges to tackle real environments. Related code, model, and data will be released at \url{https://github.com/THUDM/AutoWebGLM}.\n\n\nAutoWebGLM: Bootstrap And Reinforce A Large Language Model-based Web Navigating Agent</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2404.03648v1</guid>
      <dc:creator>Hanyu Lai, Xiao Liu, Iat Long Iong, Shuntian Yao, Yuxuan Chen, Pengbo Shen, Hao Yu, Hanchen Zhang, Xiaohan Zhang, Yuxiao Dong, Jie Tang</dc:creator>
      <pubDate>Thu, 04 Apr 2024 17:58:40 GMT</pubDate>
    </item>
    <item>
      <title>From Distributional to Overton Pluralism: Investigating Large Language Model Alignment</title>
      <link>http://arxiv.org/abs/2406.17692v1</link>
      <description>The alignment process changes several properties of a large language model's (LLM's) output distribution. We analyze two aspects of post-alignment distributional shift of LLM responses. First, we re-examine previously reported reductions in response diversity post-alignment. Our analysis suggests that an apparent drop in the diversity of responses is largely explained by quality control and information aggregation. Alignment suppresses irrelevant and unhelpful content while shifting the output distribution toward longer responses that cover information spanning several responses from the base LLM, essentially presenting diverse information in a single response. Finding little evidence that alignment suppresses useful information, it is natural to ask the opposite question: do aligned models surface information that cannot be recovered from base models? Our second investigation shows this is not the case and the behavior of aligned models is recoverable from base models without fine-tuning. A combination of in-context examples and lower-resolution semantic hints about response content can elicit responses from base LLMs that are as similar to alignment-tuned LLM responses as alignment-tuned LLM responses are to each other. Taken together, these results indicate that current alignment techniques capture but do not extend the useful subset of assistant-like base LLM behavior, providing further evidence for the Superficial Alignment Hypothesis. They also show that in-context alignment can go surprisingly far as a strategy for imitating aligned LLMs without fine-tuning. Our code and data is available at https://github.com/thomlake/investigating-alignment.\n\n\nFrom Distributional to Overton Pluralism: Investigating Large Language Model Alignment</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.17692v1</guid>
      <dc:creator>Thom Lake, Eunsol Choi, Greg Durrett</dc:creator>
      <pubDate>Tue, 25 Jun 2024 16:32:33 GMT</pubDate>
    </item>
    <item>
      <title>Style Transfer with Multi-iteration Preference Optimization</title>
      <link>http://arxiv.org/abs/2406.11581v1</link>
      <description>Numerous recent techniques for text style transfer characterize their approaches as variants of reinforcement learning and preference optimization. In this work, we consider the relationship between these approaches and a class of optimization approaches developed primarily for (non-neural) statistical machine translation, formerly known as 'tuning'. Inspired by these techniques from the past, we improve upon established preference optimization approaches, incorporating multiple iterations of exploration and optimization, and choosing contrastive examples by following a 'hope' vs 'fear' sampling strategy. Cognizant of the difference between machine translation and style transfer, however, we further tailor our framework with a new pseudo-parallel generation method and a dynamic weighted reward aggregation method to tackle the lack of parallel data and the need for a multi-objective reward. We evaluate our model on two commonly used text style transfer datasets. Through automatic and human evaluation results we show the effectiveness and the superiority of our model compared to state-of-the-art baselines.\n\n\nStyle Transfer with Multi-iteration Preference Optimization</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.11581v1</guid>
      <dc:creator>Shuai Liu, Jonathan May</dc:creator>
      <pubDate>Mon, 17 Jun 2024 14:20:53 GMT</pubDate>
    </item>
    <item>
      <title>Exploring the landscape of large language models: Foundations, techniques, and challenges</title>
      <link>http://arxiv.org/abs/2404.11973v1</link>
      <description>In this review paper, we delve into the realm of Large Language Models (LLMs), covering their foundational principles, diverse applications, and nuanced training processes. The article sheds light on the mechanics of in-context learning and a spectrum of fine-tuning approaches, with a special focus on methods that optimize efficiency in parameter usage. Additionally, it explores how LLMs can be more closely aligned with human preferences through innovative reinforcement learning frameworks and other novel methods that incorporate human feedback. The article also examines the emerging technique of retrieval augmented generation, integrating external knowledge into LLMs. The ethical dimensions of LLM deployment are discussed, underscoring the need for mindful and responsible application. Concluding with a perspective on future research trajectories, this review offers a succinct yet comprehensive overview of the current state and emerging trends in the evolving landscape of LLMs, serving as an insightful guide for both researchers and practitioners in artificial intelligence.\n\n\nExploring the landscape of large language models: Foundations, techniques, and challenges</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2404.11973v1</guid>
      <dc:creator>Milad Moradi, Ke Yan, David Colwell, Matthias Samwald, Rhona Asgari</dc:creator>
      <pubDate>Thu, 18 Apr 2024 08:01:20 GMT</pubDate>
    </item>
    <item>
      <title>Discovering Preference Optimization Algorithms with and for Large Language Models</title>
      <link>http://arxiv.org/abs/2406.08414v1</link>
      <description>Offline preference optimization is a key method for enhancing and controlling the quality of Large Language Model (LLM) outputs. Typically, preference optimization is approached as an offline supervised learning task using manually-crafted convex loss functions. While these methods are based on theoretical insights, they are inherently constrained by human creativity, so the large search space of possible loss functions remains under explored. We address this by performing LLM-driven objective discovery to automatically discover new state-of-the-art preference optimization algorithms without (expert) human intervention. Specifically, we iteratively prompt an LLM to propose and implement new preference optimization loss functions based on previously-evaluated performance metrics. This process leads to the discovery of previously-unknown and performant preference optimization algorithms. The best performing of these we call Discovered Preference Optimization (DiscoPOP), a novel algorithm that adaptively blends logistic and exponential losses. Experiments demonstrate the state-of-the-art performance of DiscoPOP and its successful transfer to held-out tasks.\n\n\nDiscovering Preference Optimization Algorithms with and for Large Language Models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.08414v1</guid>
      <dc:creator>Chris Lu, Samuel Holt, Claudio Fanconi, Alex J. Chan, Jakob Foerster, Mihaela van der Schaar, Robert Tjarko Lange</dc:creator>
      <pubDate>Wed, 12 Jun 2024 16:58:41 GMT</pubDate>
    </item>
    <item>
      <title>SORRY-Bench: Systematically Evaluating Large Language Model Safety Refusal Behaviors</title>
      <link>http://arxiv.org/abs/2406.14598v1</link>
      <description>Evaluating aligned large language models' (LLMs) ability to recognize and reject unsafe user requests is crucial for safe, policy-compliant deployments. Existing evaluation efforts, however, face three limitations that we address with SORRY-Bench, our proposed benchmark. First, existing methods often use coarse-grained taxonomies of unsafe topics, and are over-representing some fine-grained topics. For example, among the ten existing datasets that we evaluated, tests for refusals of self-harm instructions are over 3x less represented than tests for fraudulent activities. SORRY-Bench improves on this by using a fine-grained taxonomy of 45 potentially unsafe topics, and 450 class-balanced unsafe instructions, compiled through human-in-the-loop methods. Second, linguistic characteristics and formatting of prompts are often overlooked, like different languages, dialects, and more -- which are only implicitly considered in many evaluations. We supplement SORRY-Bench with 20 diverse linguistic augmentations to systematically examine these effects. Third, existing evaluations rely on large LLMs (e.g., GPT-4) for evaluation, which can be computationally expensive. We investigate design choices for creating a fast, accurate automated safety evaluator. By collecting 7K+ human annotations and conducting a meta-evaluation of diverse LLM-as-a-judge designs, we show that fine-tuned 7B LLMs can achieve accuracy comparable to GPT-4 scale LLMs, with lower computational cost. Putting these together, we evaluate over 40 proprietary and open-source LLMs on SORRY-Bench, analyzing their distinctive refusal behaviors. We hope our effort provides a building block for systematic evaluations of LLMs' safety refusal capabilities, in a balanced, granular, and efficient manner.\n\n\nSORRY-Bench: Systematically Evaluating Large Language Model Safety Refusal Behaviors</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.14598v1</guid>
      <dc:creator>Tinghao Xie, Xiangyu Qi, Yi Zeng, Yangsibo Huang, Udari Madhushani Sehwag, Kaixuan Huang, Luxi He, Boyi Wei, Dacheng Li, Ying Sheng, Ruoxi Jia, Bo Li, Kai Li, Danqi Chen, Peter Henderson, Prateek Mittal</dc:creator>
      <pubDate>Thu, 20 Jun 2024 17:56:07 GMT</pubDate>
    </item>
    <item>
      <title>Hindsight Preference Learning for Offline Preference-based Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2407.04451v1</link>
      <description>Offline preference-based reinforcement learning (RL), which focuses on optimizing policies using human preferences between pairs of trajectory segments selected from an offline dataset, has emerged as a practical avenue for RL applications. Existing works rely on extracting step-wise reward signals from trajectory-wise preference annotations, assuming that preferences correlate with the cumulative Markovian rewards. However, such methods fail to capture the holistic perspective of data annotation: Humans often assess the desirability of a sequence of actions by considering the overall outcome rather than the immediate rewards. To address this challenge, we propose to model human preferences using rewards conditioned on future outcomes of the trajectory segments, i.e. the hindsight information. For downstream RL optimization, the reward of each step is calculated by marginalizing over possible future outcomes, the distribution of which is approximated by a variational auto-encoder trained using the offline dataset. Our proposed method, Hindsight Preference Learning (HPL), can facilitate credit assignment by taking full advantage of vast trajectory data available in massive unlabeled datasets. Comprehensive empirical studies demonstrate the benefits of HPL in delivering robust and advantageous rewards across various domains. Our code is publicly released at https://github.com/typoverflow/WiseRL.\n\n\nHindsight Preference Learning for Offline Preference-based Reinforcement Learning</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2407.04451v1</guid>
      <dc:creator>Chen-Xiao Gao, Shengjun Fang, Chenjun Xiao, Yang Yu, Zongzhang Zhang</dc:creator>
      <pubDate>Fri, 05 Jul 2024 12:05:37 GMT</pubDate>
    </item>
    <item>
      <title>Procedural Dilemma Generation for Evaluating Moral Reasoning in Humans and Language Models</title>
      <link>http://arxiv.org/abs/2404.10975v1</link>
      <description>As AI systems like language models are increasingly integrated into decision-making processes affecting people's lives, it's critical to ensure that these systems have sound moral reasoning. To test whether they do, we need to develop systematic evaluations. We provide a framework that uses a language model to translate causal graphs that capture key aspects of moral dilemmas into prompt templates. With this framework, we procedurally generated a large and diverse set of moral dilemmas -- the OffTheRails benchmark -- consisting of 50 scenarios and 400 unique test items. We collected moral permissibility and intention judgments from human participants for a subset of our items and compared these judgments to those from two language models (GPT-4 and Claude-2) across eight conditions. We find that moral dilemmas in which the harm is a necessary means (as compared to a side effect) resulted in lower permissibility and higher intention ratings for both participants and language models. The same pattern was observed for evitable versus inevitable harmful outcomes. However, there was no clear effect of whether the harm resulted from an agent's action versus from having omitted to act. We discuss limitations of our prompt generation pipeline and opportunities for improving scenarios to increase the strength of experimental effects.\n\n\nProcedural Dilemma Generation for Evaluating Moral Reasoning in Humans and Language Models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2404.10975v1</guid>
      <dc:creator>Jan-Philipp Fränken, Kanishk Gandhi, Tori Qiu, Ayesha Khawaja, Noah D. Goodman, Tobias Gerstenberg</dc:creator>
      <pubDate>Wed, 17 Apr 2024 01:13:04 GMT</pubDate>
    </item>
    <item>
      <title>ICLGuard: Controlling In-Context Learning Behavior for Applicability Authorization</title>
      <link>http://arxiv.org/abs/2407.06955v1</link>
      <description>In-context learning (ICL) is a recent advancement in the capabilities of large language models (LLMs). This feature allows users to perform a new task without updating the model. Concretely, users can address tasks during the inference time by conditioning on a few input-label pair demonstrations along with the test input. It is different than the conventional fine-tuning paradigm and offers more flexibility. However, this capability also introduces potential issues. For example, users may use the model on any data without restriction, such as performing tasks with improper or sensitive content, which might violate the model policy or conflict with the model owner's interests. As a model owner, it is crucial to establish a mechanism to control the model's behavior under ICL, depending on the model owner's requirements for various content. To this end, we introduce the concept of &quot;applicability authorization&quot; tailored for LLMs, particularly for ICL behavior, and propose a simple approach, ICLGuard. It is a fine-tuning framework designed to allow the model owner to regulate ICL behavior on different data. ICLGuard preserves the original LLM and fine-tunes only a minimal set of additional trainable parameters to &quot;guard&quot; the LLM. Empirical results show that the guarded LLM can deactivate its ICL ability on target data without affecting its ICL ability on other data and its general functionality across all data.\n\n\nICLGuard: Controlling In-Context Learning Behavior for Applicability Authorization</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2407.06955v1</guid>
      <dc:creator>Wai Man Si, Michael Backes, Yang Zhang</dc:creator>
      <pubDate>Tue, 09 Jul 2024 15:35:06 GMT</pubDate>
    </item>
    <item>
      <title>RL on Incorrect Synthetic Data Scales the Efficiency of LLM Math Reasoning by Eight-Fold</title>
      <link>http://arxiv.org/abs/2406.14532v1</link>
      <description>Training on model-generated synthetic data is a promising approach for finetuning LLMs, but it remains unclear when it helps or hurts. In this paper, we investigate this question for math reasoning via an empirical study, followed by building a conceptual understanding of our observations. First, we find that while the typical approach of finetuning a model on synthetic correct or positive problem-solution pairs generated by capable models offers modest performance gains, sampling more correct solutions from the finetuned learner itself followed by subsequent fine-tuning on this self-generated data $\textbf{doubles}$ the efficiency of the same synthetic problems. At the same time, training on model-generated positives can amplify various spurious correlations, resulting in flat or even inverse scaling trends as the amount of data increases. Surprisingly, we find that several of these issues can be addressed if we also utilize negative responses, i.e., model-generated responses that are deemed incorrect by a final answer verifier. Crucially, these negatives must be constructed such that the training can appropriately recover the utility or advantage of each intermediate step in the negative response. With this per-step scheme, we are able to attain consistent gains over only positive data, attaining performance similar to amplifying the amount of synthetic data by $\mathbf{8 \times}$. We show that training on per-step negatives can help to unlearn spurious correlations in the positive data, and is equivalent to advantage-weighted reinforcement learning (RL), implying that it inherits robustness benefits of RL over imitating positive data alone.\n\n\nRL on Incorrect Synthetic Data Scales the Efficiency of LLM Math Reasoning by Eight-Fold</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.14532v1</guid>
      <dc:creator>Amrith Setlur, Saurabh Garg, Xinyang Geng, Naman Garg, Virginia Smith, Aviral Kumar</dc:creator>
      <pubDate>Thu, 20 Jun 2024 17:45:54 GMT</pubDate>
    </item>
    <item>
      <title>Corruption Robust Offline Reinforcement Learning with Human Feedback</title>
      <link>http://arxiv.org/abs/2402.06734v1</link>
      <description>We study data corruption robustness for reinforcement learning with human feedback (RLHF) in an offline setting. Given an offline dataset of pairs of trajectories along with feedback about human preferences, an $\varepsilon$-fraction of the pairs is corrupted (e.g., feedback flipped or trajectory features manipulated), capturing an adversarial attack or noisy human preferences. We aim to design algorithms that identify a near-optimal policy from the corrupted data, with provable guarantees. Existing theoretical works have separately studied the settings of corruption robust RL (learning from scalar rewards directly under corruption) and offline RLHF (learning from human feedback without corruption); however, they are inapplicable to our problem of dealing with corrupted data in offline RLHF setting. To this end, we design novel corruption robust offline RLHF methods under various assumptions on the coverage of the data-generating distributions. At a high level, our methodology robustifies an offline RLHF framework by first learning a reward model along with confidence sets and then learning a pessimistic optimal policy over the confidence set. Our key insight is that learning optimal policy can be done by leveraging an offline corruption-robust RL oracle in different ways (e.g., zero-order oracle or first-order oracle), depending on the data coverage assumptions. To our knowledge, ours is the first work that provides provable corruption robust offline RLHF methods.\n\n\nRobust Reinforcement Learning from Corrupted Human Feedback</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.06734v1</guid>
      <dc:creator>Debmalya Mandal, Andi Nika, Parameswaran Kamalaruban, Adish Singla, Goran Radanović</dc:creator>
      <pubDate>Fri, 09 Feb 2024 19:09:48 GMT</pubDate>
    </item>
    <item>
      <title>A Survey on Large Language Models from General Purpose to Medical Applications: Datasets, Methodologies, and Evaluations</title>
      <link>http://arxiv.org/abs/2406.10303v1</link>
      <description>Large Language Models (LLMs) have demonstrated surprising performance across various natural language processing tasks. Recently, medical LLMs enhanced with domain-specific knowledge have exhibited excellent capabilities in medical consultation and diagnosis. These models can smoothly simulate doctor-patient dialogues and provide professional medical advice. Most medical LLMs are developed through continued training of open-source general LLMs, which require significantly fewer computational resources than training LLMs from scratch. Additionally, this approach offers better protection of patient privacy compared to API-based solutions. This survey systematically explores how to train medical LLMs based on general LLMs. It covers: (a) how to acquire training corpus and construct customized medical training sets, (b) how to choose a appropriate training paradigm, (c) how to choose a suitable evaluation benchmark, and (d) existing challenges and promising future research directions are discussed. This survey can provide guidance for the development of LLMs focused on various medical applications, such as medical education, diagnostic planning, and clinical assistants.\n\n\nA Survey on Large Language Models from General Purpose to Medical Applications: Datasets, Methodologies, and Evaluations</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.10303v1</guid>
      <dc:creator>Jinqiang Wang, Huansheng Ning, Yi Peng, Qikai Wei, Daniel Tesfai, Wenwei Mao, Tao Zhu, Runhe Huang</dc:creator>
      <pubDate>Fri, 14 Jun 2024 02:42:20 GMT</pubDate>
    </item>
    <item>
      <title>Using ChatGPT for Thematic Analysis</title>
      <link>http://arxiv.org/abs/2405.08828v1</link>
      <description>The utilisation of AI-driven tools, notably ChatGPT, within academic research is increasingly debated from several perspectives including ease of implementation, and potential enhancements in research efficiency, as against ethical concerns and risks such as biases and unexplained AI operations. This paper explores the use of the GPT model for initial coding in qualitative thematic analysis using a sample of UN policy documents. The primary aim of this study is to contribute to the methodological discussion regarding the integration of AI tools, offering a practical guide to validation for using GPT as a collaborative research assistant. The paper outlines the advantages and limitations of this methodology and suggests strategies to mitigate risks. Emphasising the importance of transparency and reliability in employing GPT within research methodologies, this paper argues for a balanced use of AI in supported thematic analysis, highlighting its potential to elevate research efficacy and outcomes.\n\n\nChatGPT: perspectives from human–computer interaction and psychology</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.08828v1</guid>
      <dc:creator>Aleksei Turobov, Diane Coyle, Verity Harding</dc:creator>
      <pubDate>Mon, 13 May 2024 15:30:25 GMT</pubDate>
    </item>
    <item>
      <title>Preference as Reward, Maximum Preference Optimization with Importance Sampling</title>
      <link>http://arxiv.org/abs/2312.16430v5</link>
      <description>Preference learning is a key technology for aligning language models with human values. Reinforcement Learning from Human Feedback (RLHF) is a model-based algorithm to optimize preference learning, which first fits a reward model for preference scores and then optimizes the generating policy with an on-policy PPO algorithm to maximize the reward. The processing of RLHF is complex, time-consuming, and unstable. The Direct Preference Optimization (DPO) algorithm uses an off-policy algorithm to directly optimize the generating policy and eliminates the need for a reward model. DPO is more data-efficient and stable. However, DPO has a drawback of overfitting to the preference data and ignoring the KL-regularization term when the preference is deterministic. Identity mapping Preference Optimization(IPO) uses a root-finding MSE loss to incorporate KL-regularization. However, both DPO and IPO fail to properly address the KL-regularization term because the support of the preference distribution is not equal to the reference distribution. In this paper, we propose a simple and intuitive off-policy preference optimization algorithm from an importance sampling view, which we call Maximum Preference Optimization (MPO). MPO incorporates the off-policy KL-regularization term, making regularization truly effective. MPO achieves the best of both worlds by combining the objectives of RLHF and IPO while being an off-policy algorithm. Furthermore, MPO eliminates the need for a reward model and reference policy, simplifying the learning process and reducing memory usage.\n\n\nPreference as Reward, Maximum Preference Optimization with Importance Sampling</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2312.16430v5</guid>
      <dc:creator>Zaifan Jiang, Xing Huang, Chao Wei</dc:creator>
      <pubDate>Mon, 25 Mar 2024 06:32:49 GMT</pubDate>
    </item>
    <item>
      <title>Adaptive Preference Scaling for Reinforcement Learning with Human Feedback</title>
      <link>http://arxiv.org/abs/2406.02764v1</link>
      <description>Reinforcement learning from human feedback (RLHF) is a prevalent approach to align AI systems with human values by learning rewards from human preference data. Due to various reasons, however, such data typically takes the form of rankings over pairs of trajectory segments, which fails to capture the varying strengths of preferences across different pairs. In this paper, we propose a novel adaptive preference loss, underpinned by distributionally robust optimization (DRO), designed to address this uncertainty in preference strength. By incorporating an adaptive scaling parameter into the loss for each pair, our method increases the flexibility of the reward function. Specifically, it assigns small scaling parameters to pairs with ambiguous preferences, leading to more comparable rewards, and large scaling parameters to those with clear preferences for more distinct rewards. Computationally, our proposed loss function is strictly convex and univariate with respect to each scaling parameter, enabling its efficient optimization through a simple second-order algorithm. Our method is versatile and can be readily adapted to various preference optimization frameworks, including direct preference optimization (DPO). Our experiments with robotic control and natural language generation with large language models (LLMs) show that our method not only improves policy performance but also aligns reward function selection more closely with policy optimization, simplifying the hyperparameter tuning process.\n\n\nAdaptive Preference Scaling for Reinforcement Learning with Human Feedback</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.02764v1</guid>
      <dc:creator>Ilgee Hong, Zichong Li, Alexander Bukharin, Yixiao Li, Haoming Jiang, Tianbao Yang, Tuo Zhao</dc:creator>
      <pubDate>Tue, 04 Jun 2024 20:33:22 GMT</pubDate>
    </item>
    <item>
      <title>AdvisorQA: Towards Helpful and Harmless Advice-seeking Question Answering with Collective Intelligence</title>
      <link>http://arxiv.org/abs/2404.11826v1</link>
      <description>As the integration of large language models into daily life is on the rise, there is a clear gap in benchmarks for advising on subjective and personal dilemmas. To address this, we introduce AdvisorQA, the first benchmark developed to assess LLMs' capability in offering advice for deeply personalized concerns, utilizing the LifeProTips subreddit forum. This forum features a dynamic interaction where users post advice-seeking questions, receiving an average of 8.9 advice per query, with 164.2 upvotes from hundreds of users, embodying a collective intelligence framework. Therefore, we've completed a benchmark encompassing daily life questions, diverse corresponding responses, and majority vote ranking to train our helpfulness metric. Baseline experiments validate the efficacy of AdvisorQA through our helpfulness metric, GPT-4, and human evaluation, analyzing phenomena beyond the trade-off between helpfulness and harmlessness. AdvisorQA marks a significant leap in enhancing QA systems for providing personalized, empathetic advice, showcasing LLMs' improved understanding of human subjectivity.\n\n\nAdvisorQA: Towards Helpful and Harmless Advice-seeking Question Answering with Collective Intelligence</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2404.11826v1</guid>
      <dc:creator>Minbeom Kim, Hwanhee Lee, Joonsuk Park, Hwaran Lee, Kyomin Jung</dc:creator>
      <pubDate>Thu, 18 Apr 2024 01:15:41 GMT</pubDate>
    </item>
    <item>
      <title>&quot;In Dialogues We Learn&quot;: Towards Personalized Dialogue Without Pre-defined Profiles through In-Dialogue Learning</title>
      <link>http://arxiv.org/abs/2403.03102v3</link>
      <description>Personalized dialogue systems have gained significant attention in recent years for their ability to generate responses in alignment with different personas. However, most existing approaches rely on pre-defined personal profiles, which are not only time-consuming and labor-intensive to create but also lack flexibility. We propose In-Dialogue Learning (IDL), a fine-tuning framework that enhances the ability of pre-trained large language models to leverage dialogue history to characterize persona for completing personalized dialogue generation tasks without pre-defined profiles. Our experiments on three datasets demonstrate that IDL brings substantial improvements, with BLEU and ROUGE scores increasing by up to 200% and 247%, respectively. Additionally, the results of human evaluations further validate the efficacy of our proposed method.\n\n\n&quot; In Dialogues We Learn&quot;: Towards Personalized Dialogue Without Pre-defined Profiles through In-Dialogue Learning</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2403.03102v3</guid>
      <dc:creator>Chuanqi Cheng, Quan Tu, Wei Wu, Shuo Shang, Cunli Mao, Zhengtao Yu, Rui Yan</dc:creator>
      <pubDate>Tue, 12 Mar 2024 05:33:16 GMT</pubDate>
    </item>
    <item>
      <title>Program-Aided Reasoners (better) Know What They Know</title>
      <link>http://arxiv.org/abs/2311.09553v1</link>
      <description>Prior work shows that program-aided reasoning, in which large language models (LLMs) are combined with programs written in programming languages such as Python, can significantly improve accuracy on various reasoning tasks. However, while accuracy is essential, it is also important for such reasoners to &quot;know what they know&quot;, which can be quantified through the calibration of the model. In this paper, we compare the calibration of Program Aided Language Models (PAL) and text-based Chain-of-thought (COT) prompting techniques over 5 datasets and 2 model types: LLaMA models and OpenAI models. Our results indicate that PAL leads to improved calibration in 75% of the instances. Our analysis uncovers that prompting styles that produce lesser diversity in generations also have more calibrated results, and thus we also experiment with inducing lower generation diversity using temperature scaling and find that for certain temperatures, PAL is not only more accurate but is also more calibrated than COT. Overall, we demonstrate that, in the majority of cases, program-aided reasoners better know what they know than text-based counterparts.\n\n\nCalibration-Tuning: Teaching Large Language Models to Know What They Don't Know</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2311.09553v1</guid>
      <dc:creator>Anubha Kabra, Sanketh Rangreji, Yash Mathur, Aman Madaan, Emmy Liu, Graham Neubig</dc:creator>
      <pubDate>Thu, 16 Nov 2023 04:17:49 GMT</pubDate>
    </item>
    <item>
      <title>Quantum Lazy Training</title>
      <link>http://arxiv.org/abs/2202.08232v7</link>
      <description>In the training of over-parameterized model functions via gradient descent, sometimes the parameters do not change significantly and remain close to their initial values. This phenomenon is called lazy training, and motivates consideration of the linear approximation of the model function around the initial parameters. In the lazy regime, this linear approximation imitates the behavior of the parameterized function whose associated kernel, called the tangent kernel, specifies the training performance of the model. Lazy training is known to occur in the case of (classical) neural networks with large widths. In this paper, we show that the training of geometrically local parameterized quantum circuits enters the lazy regime for large numbers of qubits. More precisely, we prove bounds on the rate of changes of the parameters of such a geometrically local parameterized quantum circuit in the training process, and on the precision of the linear approximation of the associated quantum model function; both of these bounds tend to zero as the number of qubits grows. We support our analytic results with numerical simulations.\n\n\nTraining of Physical Neural Networks</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2202.08232v7</guid>
      <dc:creator>Erfan Abedi, Salman Beigi, Leila Taghavi</dc:creator>
      <pubDate>Fri, 21 Apr 2023 02:43:33 GMT</pubDate>
    </item>
    <item>
      <title>Direct Preference Optimization of Video Large Multimodal Models from Language Model Reward</title>
      <link>http://arxiv.org/abs/2404.01258v2</link>
      <description>Preference modeling techniques, such as direct preference optimization (DPO), has shown effective in enhancing the generalization abilities of large language model (LLM). However, in tasks involving video instruction-following, providing informative feedback, especially for detecting hallucinations in generated responses, remains a significant challenge. Previous studies have explored using large large multimodal models (LMMs) as reward models to guide preference modeling, but their ability to accurately assess the factuality of generated responses compared to corresponding videos has not been conclusively established. This paper introduces a novel framework that utilizes detailed video captions as a proxy of video content, enabling language models to incorporate this information as supporting evidence for scoring video Question Answering (QA) predictions. Our approach demonstrates robust alignment with OpenAI GPT-4V model's reward mechanism, which directly takes video frames as input. Furthermore, we show that applying this tailored reward through DPO significantly improves the performance of video LMMs on video QA tasks.\n\n\nDirect Preference Optimization of Video Large Multimodal Models from Language Model Reward</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2404.01258v2</guid>
      <dc:creator>Ruohong Zhang, Liangke Gui, Zhiqing Sun, Yihao Feng, Keyang Xu, Yuanhan Zhang, Di Fu, Chunyuan Li, Alexander Hauptmann, Yonatan Bisk, Yiming Yang</dc:creator>
      <pubDate>Tue, 02 Apr 2024 12:47:49 GMT</pubDate>
    </item>
    <item>
      <title>Dishonesty in Helpful and Harmless Alignment</title>
      <link>http://arxiv.org/abs/2406.01931v2</link>
      <description>People tell lies when seeking rewards. Large language models (LLMs) are aligned to human values with reinforcement learning where they get rewards if they satisfy human preference. We find that this also induces dishonesty in helpful and harmless alignment where LLMs tell lies in generating harmless responses. Using the latest interpreting tools, we detect dishonesty, show how LLMs can be harmful if their honesty is increased, and analyze such conflicts at the parameter-level. Given these preliminaries and the hypothesis that reward-seeking stimulates dishonesty, we theoretically show that the dishonesty can in-turn decrease the alignment performances and augment reward-seeking alignment with representation regularization. Extensive results, including GPT-4 annotated win-rates, perplexities, and cases studies demonstrate that we can train more honest, helpful, and harmless LLMs. We will make all our codes and results be open-sourced upon this paper's acceptance.\n\n\nDishonesty in Helpful and Harmless Alignment</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.01931v2</guid>
      <dc:creator>Youcheng Huang, Jingkun Tang, Duanyu Feng, Zheng Zhang, Wenqiang Lei, Jiancheng Lv, Anthony G. Cohn</dc:creator>
      <pubDate>Wed, 05 Jun 2024 07:21:19 GMT</pubDate>
    </item>
    <item>
      <title>MSAGPT: Neural Prompting Protein Structure Prediction via MSA Generative Pre-Training</title>
      <link>http://arxiv.org/abs/2406.05347v2</link>
      <description>Multiple Sequence Alignment (MSA) plays a pivotal role in unveiling the evolutionary trajectories of protein families. The accuracy of protein structure predictions is often compromised for protein sequences that lack sufficient homologous information to construct high quality MSA. Although various methods have been proposed to generate virtual MSA under these conditions, they fall short in comprehensively capturing the intricate coevolutionary patterns within MSA or require guidance from external oracle models. Here we introduce MSAGPT, a novel approach to prompt protein structure predictions via MSA generative pretraining in the low MSA regime. MSAGPT employs a simple yet effective 2D evolutionary positional encoding scheme to model complex evolutionary patterns. Endowed by this, its flexible 1D MSA decoding framework facilitates zero or few shot learning. Moreover, we demonstrate that leveraging the feedback from AlphaFold2 can further enhance the model capacity via Rejective Fine tuning (RFT) and Reinforcement Learning from AF2 Feedback (RLAF). Extensive experiments confirm the efficacy of MSAGPT in generating faithful virtual MSA to enhance the structure prediction accuracy. The transfer learning capabilities also highlight its great potential for facilitating other protein tasks.\n\n\nMSAGPT: Neural Prompting Protein Structure Prediction via MSA Generative Pre-Training</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.05347v2</guid>
      <dc:creator>Bo Chen, Zhilei Bei, Xingyi Cheng, Pan Li, Jie Tang, Le Song</dc:creator>
      <pubDate>Tue, 11 Jun 2024 02:42:17 GMT</pubDate>
    </item>
    <item>
      <title>Distributional Preference Learning: Understanding and Accounting for Hidden Context in RLHF</title>
      <link>http://arxiv.org/abs/2312.08358v2</link>
      <description>In practice, preference learning from human feedback depends on incomplete data with hidden context. Hidden context refers to data that affects the feedback received, but which is not represented in the data used to train a preference model. This captures common issues of data collection, such as having human annotators with varied preferences, cognitive processes that result in seemingly irrational behavior, and combining data labeled according to different criteria. We prove that standard applications of preference learning, including reinforcement learning from human feedback (RLHF), implicitly aggregate over hidden contexts according to a well-known voting rule called Borda count. We show this can produce counter-intuitive results that are very different from other methods which implicitly aggregate via expected utility. Furthermore, our analysis formalizes the way that preference learning from users with diverse values tacitly implements a social choice function. A key implication of this result is that annotators have an incentive to misreport their preferences in order to influence the learned model, leading to vulnerabilities in the deployment of RLHF. As a step towards mitigating these problems, we introduce a class of methods called distributional preference learning (DPL). DPL methods estimate a distribution of possible score values for each alternative in order to better account for hidden context. Experimental results indicate that applying DPL to RLHF for LLM chatbots identifies hidden context in the data and significantly reduces subsequent jailbreak vulnerability. Our code and data are available at https://github.com/cassidylaidlaw/hidden-context\n\n\nPareto-Optimal Learning from Preferences with Hidden Context</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2312.08358v2</guid>
      <dc:creator>Anand Siththaranjan, Cassidy Laidlaw, Dylan Hadfield-Menell</dc:creator>
      <pubDate>Wed, 17 Apr 2024 01:58:09 GMT</pubDate>
    </item>
    <item>
      <title>Language Model Decoding as Direct Metrics Optimization</title>
      <link>http://arxiv.org/abs/2310.01041v2</link>
      <description>Despite the remarkable advances in language modeling, current mainstream decoding methods still struggle to generate texts that align with human texts across different aspects. In particular, sampling-based methods produce less-repetitive texts which are often disjunctive in discourse, while search-based methods maintain topic coherence at the cost of increased repetition. Overall, these methods fall short in achieving holistic alignment across a broad range of aspects. In this work, we frame decoding from a language model as an optimization problem with the goal of strictly matching the expected performance with human texts measured by multiple metrics of desired aspects simultaneously. The resulting decoding distribution enjoys an analytical solution that scales the input language model distribution via a sequence-level energy function defined by these metrics. And most importantly, we prove that this induced distribution is guaranteed to improve the perplexity on human texts, which suggests a better approximation to the underlying distribution of human texts. To facilitate tractable sampling from this globally normalized distribution, we adopt the Sampling-Importance-Resampling technique. Experiments on various domains and model scales demonstrate the superiority of our method in metrics alignment with human texts and human evaluation over strong baselines.\n\n\nDirect Alignment of Language Models via Quality-Aware Self-Refinement</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2310.01041v2</guid>
      <dc:creator>Haozhe Ji, Pei Ke, Hongning Wang, Minlie Huang</dc:creator>
      <pubDate>Wed, 05 Jun 2024 08:23:11 GMT</pubDate>
    </item>
    <item>
      <title>Enhancing Zero-shot Text-to-Speech Synthesis with Human Feedback</title>
      <link>http://arxiv.org/abs/2406.00654v1</link>
      <description>In recent years, text-to-speech (TTS) technology has witnessed impressive advancements, particularly with large-scale training datasets, showcasing human-level speech quality and impressive zero-shot capabilities on unseen speakers. However, despite human subjective evaluations, such as the mean opinion score (MOS), remaining the gold standard for assessing the quality of synthetic speech, even state-of-the-art TTS approaches have kept human feedback isolated from training that resulted in mismatched training objectives and evaluation metrics. In this work, we investigate a novel topic of integrating subjective human evaluation into the TTS training loop. Inspired by the recent success of reinforcement learning from human feedback, we propose a comprehensive sampling-annotating-learning framework tailored to TTS optimization, namely uncertainty-aware optimization (UNO). Specifically, UNO eliminates the need for a reward model or preference data by directly maximizing the utility of speech generations while considering the uncertainty that lies in the inherent variability in subjective human speech perception and evaluations. Experimental results of both subjective and objective evaluations demonstrate that UNO considerably improves the zero-shot performance of TTS models in terms of MOS, word error rate, and speaker similarity. Additionally, we present a remarkable ability of UNO that it can adapt to the desired speaking style in emotional TTS seamlessly and flexibly.\n\n\nEnhancing Zero-shot Text-to-Speech Synthesis with Human Feedback</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.00654v1</guid>
      <dc:creator>Chen Chen, Yuchen Hu, Wen Wu, Helin Wang, Eng Siong Chng, Chao Zhang</dc:creator>
      <pubDate>Sun, 02 Jun 2024 07:54:33 GMT</pubDate>
    </item>
    <item>
      <title>The Role of Learning Algorithms in Collective Action</title>
      <link>http://arxiv.org/abs/2405.06582v3</link>
      <description>Collective action in machine learning is the study of the control that a coordinated group can have over machine learning algorithms. While previous research has concentrated on assessing the impact of collectives against Bayes (sub-)optimal classifiers, this perspective is limited in that it does not account for the choice of learning algorithm. Since classifiers seldom behave like Bayes classifiers and are influenced by the choice of learning algorithms along with their inherent biases, in this work we initiate the study of how the choice of the learning algorithm plays a role in the success of a collective in practical settings. Specifically, we focus on distributionally robust optimization (DRO), popular for improving a worst group error, and on the ubiquitous stochastic gradient descent (SGD), due to its inductive bias for &quot;simpler&quot; functions. Our empirical results, supported by a theoretical foundation, show that the effective size and success of the collective are highly dependent on properties of the learning algorithm. This highlights the necessity of taking the learning algorithm into account when studying the impact of collective action in machine learning.\n\n\nThe Role of Learning Algorithms in Collective Action</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.06582v3</guid>
      <dc:creator>Omri Ben-Dov, Jake Fawkes, Samira Samadi, Amartya Sanyal</dc:creator>
      <pubDate>Tue, 04 Jun 2024 07:34:01 GMT</pubDate>
    </item>
    <item>
      <title>Robust Zero-Shot Text-to-Speech Synthesis with Reverse Inference Optimization</title>
      <link>http://arxiv.org/abs/2407.02243v1</link>
      <description>In this paper, we propose reverse inference optimization (RIO), a simple and effective method designed to enhance the robustness of autoregressive-model-based zero-shot text-to-speech (TTS) systems using reinforcement learning from human feedback (RLHF). To assess the quality of speech produced by the TTS system without human annotations, RIO introduces a novel concept termed as reverse inference based on the Bayesian principle, which suggests that a high-quality generated speech should be able to be used as a prompt for subsequent generation using the same TTS model. By leveraging reverse inference as the standard to select exemplars used in RLHF from the speech samples generated by the TTS system itself, RIO steers the subsequent optimization towards a direction of enhancing the TTS robustness. The RIO framework, comprising sampling, automatic annotating, and learning, obviates the need for a reward model or pairwise preference data, and significantly improves the stability of zero-shot TTS performance by reducing the discrepancies between training and inference conditions. Our experimental results verify that RIO can effectively improve both subjective and objective metrics, including mean opinion scores, word error rates, and speaker similarity. Remarkably, RIO can also diminish the incidence of bad outputs to nearly zero percent, rivalling the robustness when using ground-truth speech as the prompt.\n\n\nRobust Zero-Shot Text-to-Speech Synthesis with Reverse Inference Optimization</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2407.02243v1</guid>
      <dc:creator>Yuchen Hu, Chen Chen, Siyin Wang, Eng Siong Chng, Chao Zhang</dc:creator>
      <pubDate>Tue, 02 Jul 2024 13:04:04 GMT</pubDate>
    </item>
    <item>
      <title>Projective Preferential Bayesian Optimization</title>
      <link>http://arxiv.org/abs/2002.03113v4</link>
      <description>Bayesian optimization is an effective method for finding extrema of a black-box function. We propose a new type of Bayesian optimization for learning user preferences in high-dimensional spaces. The central assumption is that the underlying objective function cannot be evaluated directly, but instead a minimizer along a projection can be queried, which we call a projective preferential query. The form of the query allows for feedback that is natural for a human to give, and which enables interaction. This is demonstrated in a user experiment in which the user feedback comes in the form of optimal position and orientation of a molecule adsorbing to a surface. We demonstrate that our framework is able to find a global minimum of a high-dimensional black-box function, which is an infeasible task for existing preferential Bayesian optimization frameworks that are based on pairwise comparisons.\n\n\nPreferential Multi-Objective Bayesian Optimization</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2002.03113v4</guid>
      <dc:creator>Petrus Mikkola, Milica Todorović, Jari Järvi, Patrick Rinke, Samuel Kaski</dc:creator>
      <pubDate>Fri, 14 Aug 2020 12:10:55 GMT</pubDate>
    </item>
    <item>
      <title>Adversarial Contrastive Decoding: Boosting Safety Alignment of Large Language Models via Opposite Prompt Optimization</title>
      <link>http://arxiv.org/abs/2406.16743v1</link>
      <description>With the widespread application of Large Language Models (LLMs), it has become a significant concern to ensure their safety and prevent harmful responses. While current safe-alignment methods based on instruction fine-tuning and Reinforcement Learning from Human Feedback (RLHF) can effectively reduce harmful responses from LLMs, they often require high-quality datasets and heavy computational overhead during model training. Another way to align language models is to modify the logit of tokens in model outputs without heavy training. Recent studies have shown that contrastive decoding can enhance the performance of language models by reducing the likelihood of confused tokens. However, these methods require the manual selection of contrastive models or instruction templates. To this end, we propose Adversarial Contrastive Decoding (ACD), an optimization-based framework to generate two opposite system prompts for prompt-based contrastive decoding. ACD only needs to apply a lightweight prompt tuning on a rather small anchor dataset (&lt; 3 min for each model) without training the target model. Experiments conducted on extensive models and benchmarks demonstrate that the proposed method achieves much better safety performance than previous model training-free decoding methods without sacrificing its original generation ability.\n\n\nAdversarial Contrastive Decoding: Boosting Safety Alignment of Large Language Models via Opposite Prompt Optimization</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.16743v1</guid>
      <dc:creator>Zhengyue Zhao, Xiaoyun Zhang, Kaidi Xu, Xing Hu, Rui Zhang, Zidong Du, Qi Guo, Yunji Chen</dc:creator>
      <pubDate>Mon, 24 Jun 2024 15:51:30 GMT</pubDate>
    </item>
    <item>
      <title>Modular Pluralism: Pluralistic Alignment via Multi-LLM Collaboration</title>
      <link>http://arxiv.org/abs/2406.15951v1</link>
      <description>While existing alignment paradigms have been integral in developing large language models (LLMs), LLMs often learn an averaged human preference and struggle to model diverse preferences across cultures, demographics, and communities. We propose Modular Pluralism, a modular framework based on multi-LLM collaboration for pluralistic alignment: it &quot;plugs into&quot; a base LLM a pool of smaller but specialized community LMs, where models collaborate in distinct modes to flexibility support three modes of pluralism: Overton, steerable, and distributional. Modular Pluralism is uniquely compatible with black-box LLMs and offers the modular control of adding new community LMs for previously underrepresented communities. We evaluate Modular Pluralism with six tasks and four datasets featuring questions/instructions with value-laden and perspective-informed responses. Extensive experiments demonstrate that Modular Pluralism advances the three pluralism objectives across six black-box and open-source LLMs. Further analysis reveals that LLMs are generally faithful to the inputs from smaller community LLMs, allowing seamless patching by adding a new community LM to better cover previously underrepresented communities.\n\n\nModular Pluralism: Pluralistic Alignment via Multi-LLM Collaboration</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.15951v1</guid>
      <dc:creator>Shangbin Feng, Taylor Sorensen, Yuhan Liu, Jillian Fisher, Chan Young Park, Yejin Choi, Yulia Tsvetkov</dc:creator>
      <pubDate>Sat, 22 Jun 2024 22:07:40 GMT</pubDate>
    </item>
    <item>
      <title>Getting More Juice Out of the SFT Data: Reward Learning from Human Demonstration Improves SFT for LLM Alignment</title>
      <link>http://arxiv.org/abs/2405.17888v2</link>
      <description>Aligning human preference and value is an important requirement for contemporary foundation models. State-of-the-art techniques such as Reinforcement Learning from Human Feedback (RLHF) often consist of two stages: 1) supervised fine-tuning (SFT), where the model is fine-tuned by learning from human demonstration data; 2) Preference learning, where preference data is used to learn a reward model, which is in turn used by a reinforcement learning (RL) step to fine-tune the model. Such reward model serves as a proxy to human preference, and it is critical to guide the RL step towards improving the model quality. In this work, we argue that the SFT stage significantly benefits from learning a reward model as well. Instead of using the human demonstration data directly via supervised learning, we propose to leverage an Inverse Reinforcement Learning (IRL) technique to (explicitly or implicitly) build an reward model, while learning the policy model. This approach leads to new SFT algorithms that are not only efficient to implement, but also promote the ability to distinguish between the preferred and non-preferred continuations. Moreover, we identify a connection between the proposed IRL based approach, and certain self-play approach proposed recently, and showed that self-play is a special case of modeling a reward-learning agent. Theoretically, we show that the proposed algorithms converge to the stationary solutions of the IRL problem. Empirically, we align 1B and 7B models using proposed methods and evaluate them on a reward benchmark model and the HuggingFace Open LLM Leaderboard. The proposed methods show significant performance improvement over existing SFT approaches. Our results indicate that it is beneficial to explicitly or implicitly leverage reward learning throughout the entire alignment process.\n\n\nGetting More Juice Out of the SFT Data: Reward Learning from Human Demonstration Improves SFT for LLM Alignment</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.17888v2</guid>
      <dc:creator>Jiaxiang Li, Siliang Zeng, Hoi-To Wai, Chenliang Li, Alfredo Garcia, Mingyi Hong</dc:creator>
      <pubDate>Wed, 29 May 2024 13:33:33 GMT</pubDate>
    </item>
    <item>
      <title>Word Alignment as Preference for Machine Translation</title>
      <link>http://arxiv.org/abs/2405.09223v1</link>
      <description>The problem of hallucination and omission, a long-standing problem in machine translation (MT), is more pronounced when a large language model (LLM) is used in MT because an LLM itself is susceptible to these phenomena. In this work, we mitigate the problem in an LLM-based MT model by guiding it to better word alignment. We first study the correlation between word alignment and the phenomena of hallucination and omission in MT. Then we propose to utilize word alignment as preference to optimize the LLM-based MT model. The preference data are constructed by selecting chosen and rejected translations from multiple MT tools. Subsequently, direct preference optimization is used to optimize the LLM-based model towards the preference signal. Given the absence of evaluators specifically designed for hallucination and omission in MT, we further propose selecting hard instances and utilizing GPT-4 to directly evaluate the performance of the models in mitigating these issues. We verify the rationality of these designed evaluation methods by experiments, followed by extensive results demonstrating the effectiveness of word alignment-based preference optimization to mitigate hallucination and omission.\n\n\nWord Alignment as Preference for Machine Translation</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.09223v1</guid>
      <dc:creator>Qiyu Wu, Masaaki Nagata, Zhongtao Miao, Yoshimasa Tsuruoka</dc:creator>
      <pubDate>Wed, 15 May 2024 10:04:19 GMT</pubDate>
    </item>
    <item>
      <title>Reading Off Kurosh Decompositions</title>
      <link>http://arxiv.org/abs/0706.0101v2</link>
      <description>Geometric methods proposed by Stallings for treating finitely generated subgroups of free groups were successfully used by many authors to solve a wide collection of decision problems for free groups and their subgroups.   In the present paper we employ our generalized Stallings' folding method to introduce a procedure, which given a subgroup H of a free product of finite groups reads off its Kurosh decomposition from the subgroup graph of H.\n\n\nOff the rails: Procedural dilemma generation for moral reasoning</description>
      <guid isPermaLink="false">http://arxiv.org/abs/0706.0101v2</guid>
      <dc:creator>L. Markus-Epstein</dc:creator>
      <pubDate>Wed, 04 Jul 2007 10:47:37 GMT</pubDate>
    </item>
    <item>
      <title>Are Large Language Models Really Bias-Free? Jailbreak Prompts for Assessing Adversarial Robustness to Bias Elicitation</title>
      <link>http://arxiv.org/abs/2407.08441v1</link>
      <description>Large Language Models (LLMs) have revolutionized artificial intelligence, demonstrating remarkable computational power and linguistic capabilities. However, these models are inherently prone to various biases stemming from their training data. These include selection, linguistic, and confirmation biases, along with common stereotypes related to gender, ethnicity, sexual orientation, religion, socioeconomic status, disability, and age. This study explores the presence of these biases within the responses given by the most recent LLMs, analyzing the impact on their fairness and reliability. We also investigate how known prompt engineering techniques can be exploited to effectively reveal hidden biases of LLMs, testing their adversarial robustness against jailbreak prompts specially crafted for bias elicitation. Extensive experiments are conducted using the most widespread LLMs at different scales, confirming that LLMs can still be manipulated to produce biased or inappropriate responses, despite their advanced capabilities and sophisticated alignment processes. Our findings underscore the importance of enhancing mitigation techniques to address these safety issues, toward a more sustainable and inclusive artificial intelligence.\n\n\nAre Large Language Models Really Bias-Free? Jailbreak Prompts for Assessing Adversarial Robustness to Bias Elicitation</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2407.08441v1</guid>
      <dc:creator>Riccardo Cantini, Giada Cosenza, Alessio Orsino, Domenico Talia</dc:creator>
      <pubDate>Thu, 11 Jul 2024 12:30:19 GMT</pubDate>
    </item>
    <item>
      <title>Direct Preference Optimization With Unobserved Preference Heterogeneity</title>
      <link>http://arxiv.org/abs/2405.15065v1</link>
      <description>RLHF has emerged as a pivotal step in aligning language models with human objectives and values. It typically involves learning a reward model from human preference data and then using reinforcement learning to update the generative model accordingly. Conversely, Direct Preference Optimization (DPO) directly optimizes the generative model with preference data, skipping reinforcement learning. However, both RLHF and DPO assume uniform preferences, overlooking the reality of diverse human annotators. This paper presents a new method to align generative models with varied human preferences. We propose an Expectation-Maximization adaptation to DPO, generating a mixture of models based on latent preference types of the annotators. We then introduce a min-max regret ensemble learning model to produce a single generative method to minimize worst-case regret among annotator subgroups with similar latent factors. Our algorithms leverage the simplicity of DPO while accommodating diverse preferences. Experimental results validate the effectiveness of our approach in producing equitable generative policies.\n\n\nDirect Preference Optimization With Unobserved Preference Heterogeneity</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.15065v1</guid>
      <dc:creator>Keertana Chidambaram, Karthik Vinay Seetharaman, Vasilis Syrgkanis</dc:creator>
      <pubDate>Thu, 23 May 2024 21:25:20 GMT</pubDate>
    </item>
    <item>
      <title>Improving Conversational Abilities of Quantized Large Language Models via Direct Preference Alignment</title>
      <link>http://arxiv.org/abs/2407.03051v1</link>
      <description>The rapid advancement of large language models (LLMs) has facilitated their transformation into conversational chatbots that can grasp contextual nuances and generate pertinent sentences, closely mirroring human values through advanced techniques such as instruction tuning and reinforcement learning from human feedback (RLHF). However, the computational efficiency required for LLMs, achieved through techniques like post-training quantization (PTQ), presents challenges such as token-flipping that can impair chatbot performance. In response, we propose a novel preference alignment approach, quantization-aware direct preference optimization (QDPO), that aligns quantized LLMs with their full-precision counterparts, improving conversational abilities. Evaluated on two instruction-tuned LLMs in various languages, QDPO demonstrated superior performance in improving conversational abilities compared to established PTQ and knowledge-distillation fine-tuning techniques, marking a significant step forward in the development of efficient and effective conversational LLMs.\n\n\nImproving Conversational Abilities of Quantized Large Language Models via Direct Preference Alignment</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2407.03051v1</guid>
      <dc:creator>Janghwan Lee, Seongmin Park, Sukjin Hong, Minsoo Kim, Du-Seong Chang, Jungwook Choi</dc:creator>
      <pubDate>Wed, 03 Jul 2024 12:19:06 GMT</pubDate>
    </item>
    <item>
      <title>The Impossibility of Fair LLMs</title>
      <link>http://arxiv.org/abs/2406.03198v1</link>
      <description>The need for fair AI is increasingly clear in the era of general-purpose systems such as ChatGPT, Gemini, and other large language models (LLMs). However, the increasing complexity of human-AI interaction and its social impacts have raised questions of how fairness standards could be applied. Here, we review the technical frameworks that machine learning researchers have used to evaluate fairness, such as group fairness and fair representations, and find that their application to LLMs faces inherent limitations. We show that each framework either does not logically extend to LLMs or presents a notion of fairness that is intractable for LLMs, primarily due to the multitudes of populations affected, sensitive attributes, and use cases. To address these challenges, we develop guidelines for the more realistic goal of achieving fairness in particular use cases: the criticality of context, the responsibility of LLM developers, and the need for stakeholder participation in an iterative process of design and evaluation. Moreover, it may eventually be possible and even necessary to use the general-purpose capabilities of AI systems to address fairness challenges as a form of scalable AI-assisted alignment.\n\n\nThe Impossibility of Fair LLMs</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.03198v1</guid>
      <dc:creator>Jacy Anthis, Kristian Lum, Michael Ekstrand, Avi Feller, Alexander D'Amour, Chenhao Tan</dc:creator>
      <pubDate>Tue, 28 May 2024 04:36:15 GMT</pubDate>
    </item>
    <item>
      <title>Large Language Model Unlearning via Embedding-Corrupted Prompts</title>
      <link>http://arxiv.org/abs/2406.07933v1</link>
      <description>Large language models (LLMs) have advanced to encompass extensive knowledge across diverse domains. Yet controlling what a large language model should not know is important for ensuring alignment and thus safe use. However, accurately and efficiently unlearning knowledge from an LLM remains challenging due to the potential collateral damage caused by the fuzzy boundary between retention and forgetting, and the large computational requirements for optimization across state-of-the-art models with hundreds of billions of parameters. In this work, we present Embedding-COrrupted (ECO) Prompts, a lightweight unlearning framework for large language models to address both the challenges of knowledge entanglement and unlearning efficiency. Instead of relying on the LLM itself to unlearn, we enforce an unlearned state during inference by employing a prompt classifier to identify and safeguard prompts to forget. We learn corruptions added to prompt embeddings via zeroth order optimization toward the unlearning objective offline and corrupt prompts flagged by the classifier during inference. We find that these embedding-corrupted prompts not only lead to desirable outputs that satisfy the unlearning objective but also closely approximate the output from a model that has never been trained on the data intended for forgetting. Through extensive experiments on unlearning, we demonstrate the superiority of our method in achieving promising unlearning at nearly zero side effects in general domains and domains closely related to the unlearned ones. Additionally, we highlight the scalability of our method to 100 LLMs, ranging from 0.5B to 236B parameters, incurring no additional cost as the number of parameters increases.\n\n\nLarge Language Model Unlearning via Embedding-Corrupted Prompts</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.07933v1</guid>
      <dc:creator>Chris Yuhao Liu, Yaxuan Wang, Jeffrey Flanigan, Yang Liu</dc:creator>
      <pubDate>Wed, 12 Jun 2024 06:56:20 GMT</pubDate>
    </item>
    <item>
      <title>ProgressGym: Alignment with a Millennium of Moral Progress</title>
      <link>http://arxiv.org/abs/2406.20087v1</link>
      <description>Frontier AI systems, including large language models (LLMs), hold increasing influence over the epistemology of human users. Such influence can reinforce prevailing societal values, potentially contributing to the lock-in of misguided moral beliefs and, consequently, the perpetuation of problematic moral practices on a broad scale. We introduce progress alignment as a technical solution to mitigate this imminent risk. Progress alignment algorithms learn to emulate the mechanics of human moral progress, thereby addressing the susceptibility of existing alignment methods to contemporary moral blindspots. To empower research in progress alignment, we introduce ProgressGym, an experimental framework allowing the learning of moral progress mechanics from history, in order to facilitate future progress in real-world moral decisions. Leveraging 9 centuries of historical text and 18 historical LLMs, ProgressGym enables codification of real-world progress alignment challenges into concrete benchmarks. Specifically, we introduce three core challenges: tracking evolving values (PG-Follow), preemptively anticipating moral progress (PG-Predict), and regulating the feedback loop between human and AI value shifts (PG-Coevolve). Alignment methods without a temporal dimension are inapplicable to these tasks. In response, we present lifelong and extrapolative algorithms as baseline methods of progress alignment, and build an open leaderboard soliciting novel algorithms and challenges. The framework and the leaderboard are available at https://github.com/PKU-Alignment/ProgressGym and https://huggingface.co/spaces/PKU-Alignment/ProgressGym-LeaderBoard respectively.\n\n\nProgressGym: Alignment with a Millennium of Moral Progress</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.20087v1</guid>
      <dc:creator>Tianyi Qiu, Yang Zhang, Xuchuan Huang, Jasmine Xinze Li, Jiaming Ji, Yaodong Yang</dc:creator>
      <pubDate>Fri, 28 Jun 2024 17:55:24 GMT</pubDate>
    </item>
    <item>
      <title>Eliminating Biased Length Reliance of Direct Preference Optimization via Down-Sampled KL Divergence</title>
      <link>http://arxiv.org/abs/2406.10957v1</link>
      <description>Direct Preference Optimization (DPO) has emerged as a prominent algorithm for the direct and robust alignment of Large Language Models (LLMs) with human preferences, offering a more straightforward alternative to the complex Reinforcement Learning from Human Feedback (RLHF). Despite its promising efficacy, DPO faces a notable drawback: &quot;verbosity&quot;, a common over-optimization phenomenon also observed in RLHF. While previous studies mainly attributed verbosity to biased labels within the data, we propose that the issue also stems from an inherent algorithmic length reliance in DPO. Specifically, we suggest that the discrepancy between sequence-level Kullback-Leibler (KL) divergences between chosen and rejected sequences, used in DPO, results in overestimated or underestimated rewards due to varying token lengths. Empirically, we utilize datasets with different label lengths to demonstrate the presence of biased rewards. We then introduce an effective downsampling approach, named SamPO, to eliminate potential length reliance. Our experimental evaluations, conducted across three LLMs of varying scales and a diverse array of conditional and open-ended benchmarks, highlight the efficacy of SamPO in mitigating verbosity, achieving improvements of 5% to 12% over DPO through debaised rewards. Our codes can be accessed at: https://github.com/LuJunru/SamPO/.\n\n\nEliminating Biased Length Reliance of Direct Preference Optimization via Down-Sampled KL Divergence</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.10957v1</guid>
      <dc:creator>Junru Lu, Jiazheng Li, Siyu An, Meng Zhao, Yulan He, Di Yin, Xing Sun</dc:creator>
      <pubDate>Sun, 16 Jun 2024 14:24:30 GMT</pubDate>
    </item>
    <item>
      <title>MEEL: Multi-Modal Event Evolution Learning</title>
      <link>http://arxiv.org/abs/2404.10429v1</link>
      <description>Multi-modal Event Reasoning (MMER) endeavors to endow machines with the ability to comprehend intricate event relations across diverse data modalities. MMER is fundamental and underlies a wide broad of applications. Despite extensive instruction fine-tuning, current multi-modal large language models still fall short in such ability. The disparity stems from that existing models are insufficient to capture underlying principles governing event evolution in various scenarios. In this paper, we introduce Multi-Modal Event Evolution Learning (MEEL) to enable the model to grasp the event evolution mechanism, yielding advanced MMER ability. Specifically, we commence with the design of event diversification to gather seed events from a rich spectrum of scenarios. Subsequently, we employ ChatGPT to generate evolving graphs for these seed events. We propose an instruction encapsulation process that formulates the evolving graphs into instruction-tuning data, aligning the comprehension of event reasoning to humans. Finally, we observe that models trained in this way are still struggling to fully comprehend event evolution. In such a case, we propose the guiding discrimination strategy, in which models are trained to discriminate the improper evolution direction. We collect and curate a benchmark M-EV2 for MMER. Extensive experiments on M-EV2 validate the effectiveness of our approach, showcasing competitive performance in open-source multi-modal LLMs.\n\n\nMEEL: Multi-Modal Event Evolution Learning</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2404.10429v1</guid>
      <dc:creator>Zhengwei Tao, Zhi Jin, Junqiang Huang, Xiancai Chen, Xiaoying Bai, Haiyan Zhao, Yifan Zhang, Chongyang Tao</dc:creator>
      <pubDate>Tue, 16 Apr 2024 09:46:37 GMT</pubDate>
    </item>
    <item>
      <title>How Useful is Intermittent, Asynchronous Expert Feedback for Bayesian Optimization?</title>
      <link>http://arxiv.org/abs/2406.06459v1</link>
      <description>Bayesian optimization (BO) is an integral part of automated scientific discovery -- the so-called self-driving lab -- where human inputs are ideally minimal or at least non-blocking. However, scientists often have strong intuition, and thus human feedback is still useful. Nevertheless, prior works in enhancing BO with expert feedback, such as by incorporating it in an offline or online but blocking (arrives at each BO iteration) manner, are incompatible with the spirit of self-driving labs. In this work, we study whether a small amount of randomly arriving expert feedback that is being incorporated in a non-blocking manner can improve a BO campaign. To this end, we run an additional, independent computing thread on top of the BO loop to handle the feedback-gathering process. The gathered feedback is used to learn a Bayesian preference model that can readily be incorporated into the BO thread, to steer its exploration-exploitation process. Experiments on toy and chemistry datasets suggest that even just a few intermittent, asynchronous expert feedback can be useful for improving or constraining BO. This can especially be useful for its implication in improving self-driving labs, e.g. making them more data-efficient and less costly.\n\n\nHow Useful is Intermittent, Asynchronous Expert Feedback for Bayesian Optimization?</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.06459v1</guid>
      <dc:creator>Agustinus Kristiadi, Felix Strieth-Kalthoff, Sriram Ganapathi Subramanian, Vincent Fortuin, Pascal Poupart, Geoff Pleiss</dc:creator>
      <pubDate>Mon, 10 Jun 2024 16:53:58 GMT</pubDate>
    </item>
    <item>
      <title>A Critical Look At Tokenwise Reward-Guided Text Generation</title>
      <link>http://arxiv.org/abs/2406.07780v1</link>
      <description>Large language models (LLMs) can significantly be improved by aligning to human preferences -- the so-called reinforcement learning from human feedback (RLHF). However, the cost of fine-tuning an LLM is prohibitive for many users. Due to their ability to bypass LLM finetuning, tokenwise reward-guided text generation (RGTG) methods have recently been proposed. They use a reward model trained on full sequences to score partial sequences during a tokenwise decoding, in a bid to steer the generation towards sequences with high rewards. However, these methods have so far been only heuristically motivated and poorly analyzed. In this work, we show that reward models trained on full sequences are not compatible with scoring partial sequences. To alleviate this issue, we propose to explicitly train a Bradley-Terry reward model on partial sequences, and autoregressively sample from the implied tokenwise policy during decoding time. We study the property of this reward model and the implied policy. In particular, we show that this policy is proportional to the ratio of two distinct RLHF policies. We show that our simple approach outperforms previous RGTG methods and achieves similar performance as strong offline baselines but without large-scale LLM finetuning.\n\n\nA Critical Look At Tokenwise Reward-Guided Text Generation</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.07780v1</guid>
      <dc:creator>Ahmad Rashid, Ruotian Wu, Julia Grosse, Agustinus Kristiadi, Pascal Poupart</dc:creator>
      <pubDate>Wed, 12 Jun 2024 00:19:40 GMT</pubDate>
    </item>
    <item>
      <title>Large Language Models are Contrastive Reasoners</title>
      <link>http://arxiv.org/abs/2403.08211v2</link>
      <description>Prompting methods play a crucial role in enhancing the capabilities of pre-trained large language models (LLMs). We explore how contrastive prompting (CP) significantly improves the ability of large language models to perform complex reasoning. We demonstrate that LLMs are decent contrastive reasoners by simply adding &quot;Let's give a correct and a wrong answer.&quot; before LLMs provide answers. Experiments on various large language models show that zero-shot contrastive prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks without any hand-crafted few-shot examples, such as increasing the accuracy on GSM8K from 35.9% to 88.8% and AQUA-RAT from 41.3% to 62.2% with the state-of-the-art GPT-4 model. Our method not only surpasses zero-shot CoT and few-shot CoT in most arithmetic and commonsense reasoning tasks but also can seamlessly integrate with existing prompting methods, resulting in improved or comparable results when compared to state-of-the-art methods. Our code is available at https://github.com/yao8839836/cp\n\n\nLarge Language Models are Contrastive Reasoners</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2403.08211v2</guid>
      <dc:creator>Liang Yao</dc:creator>
      <pubDate>Wed, 22 May 2024 21:06:37 GMT</pubDate>
    </item>
    <item>
      <title>Calibrating LLMs with Preference Optimization on Thought Trees for Generating Rationale in Science Question Scoring</title>
      <link>http://arxiv.org/abs/2406.19949v1</link>
      <description>Generating rationales that justify scoring decisions has been a promising way to facilitate explainability in automated scoring systems. However, existing methods do not match the accuracy of classifier-based methods. Plus, the generated rationales often contain hallucinated information. To address these issues, we propose a novel framework capable of generating more faithful rationales and, more importantly, matching performance with classifier-based black-box scoring systems. We first mimic the human assessment process by querying Large Language Models (LLMs) to generate a thought tree. We then summarise intermediate assessment decisions from each thought tree path for creating synthetic rationale data and rationale preference data. Finally, we utilise the generated synthetic data to calibrate LLMs through a two-step training process: supervised fine-tuning and preference optimization. Extensive experimental results demonstrate that our framework achieves a 38% assessment performance improvement in the QWK score compared to prior work while producing higher-quality rationales, as recognised by human evaluators and LLMs. Our work sheds light on the effectiveness of performing preference optimization using synthetic preference data obtained from thought tree paths.\n\n\nCalibrating LLMs with Preference Optimization on Thought Trees for Generating Rationale in Science Question Scoring</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.19949v1</guid>
      <dc:creator>Jiazheng Li, Hainiu Xu, Zhaoyue Sun, Yuxiang Zhou, David West, Cesare Aloisi, Yulan He</dc:creator>
      <pubDate>Fri, 28 Jun 2024 14:33:05 GMT</pubDate>
    </item>
    <item>
      <title>Exploring Multi-Lingual Bias of Large Code Models in Code Generation</title>
      <link>http://arxiv.org/abs/2404.19368v1</link>
      <description>Code generation aims to synthesize code and fulfill functional requirements based on natural language (NL) specifications, which can greatly improve development efficiency. In the era of large language models (LLMs), large code models (LCMs) have been recently proposed to generate source code. LCMs can generate highly feasible solutions for programming problems described in natural language. Despite the effectiveness, we observe a noticeable multilingual bias in the generation performance of LCMs. Specifically, LCMs demonstrate proficiency in generating solutions when provided with instructions in English, yet may falter when faced with semantically equivalent instructions in other NLs such as Chinese. Moreover, the ability of LCMs to generate code exhibits variety across different programming languages (PLs), such as Python and C++. The observed phenomenon indicates the presence of multi-lingual bias within the generative capabilities of LCMs, which has remained unexplored.   In this paper, we aim to investigate the multi-lingual bias that exists in current LCMs. First, we initiate our investigation by constructing the first multi-lingual evaluation benchmark X-HumanEval-X, enabling us to systematically evaluate the extent of multi-lingual bias that exists in current LCMs. In our large-scale experiments on nine popular LCMs, we observe a pronounced multi-lingual bias of LCMs in code generation, including multi-NL and multi-PL bias. Specifically, when using Chinese instructions, the code generation capabilities of LCMs decrease by at least 13% in terms of the Pass@1 metric. Furthermore, LCMs perform variously across different programming languages, e.g., the performance gap between Python and C++ reaches as high as 20.9%. ...\n\n\nExploring Multi-Lingual Bias of Large Code Models in Code Generation</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2404.19368v1</guid>
      <dc:creator>Chaozheng Wang, Zongjie Li, Cuiyun Gao, Wenxuan Wang, Ting Peng, Hailiang Huang, Yuetang Deng, Shuai Wang, Michael R. Lyu</dc:creator>
      <pubDate>Tue, 30 Apr 2024 08:51:49 GMT</pubDate>
    </item>
    <item>
      <title>Super(ficial)-alignment: Strong Models May Deceive Weak Models in Weak-to-Strong Generalization</title>
      <link>http://arxiv.org/abs/2406.11431v1</link>
      <description>Superalignment, where humans are weak supervisors of superhuman models, has become an important and widely discussed issue in the current era of rapid development of Large Language Models (LLMs). The recent work preliminarily studies this problem by using weak models to supervise strong models. It discovers that weakly supervised strong students can consistently outperform weak teachers towards the alignment target, leading to a weak-to-strong generalization phenomenon. However, we are concerned that behind such a promising phenomenon, whether there exists an issue of weak-to-strong deception, where strong models may deceive weak models by exhibiting well-aligned in areas known to weak models but producing misaligned behaviors in cases weak models do not know. We then take an initial step towards exploring this security issue in a specific but realistic multi-objective alignment case, where there may be some alignment targets conflicting with each other (e.g., helpfulness v.s. harmlessness). Such a conflict is likely to cause strong models to deceive weak models in one alignment dimension to gain high reward in other alignment dimension. Our experiments on both the reward modeling task and the preference optimization scenario indicate: (1) the weak-to-strong deception exists; (2) the deception phenomenon may intensify as the capability gap between weak and strong models increases. We also discuss potential solutions and find bootstrapping with an intermediate model can mitigate the deception to some extent. Our work highlights the urgent need to pay more attention to the true reliability of superalignment.\n\n\nSuper (ficial)-alignment: Strong Models May Deceive Weak Models in Weak-to-Strong Generalization</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.11431v1</guid>
      <dc:creator>Wenkai Yang, Shiqi Shen, Guangyao Shen, Zhi Gong, Yankai Lin</dc:creator>
      <pubDate>Mon, 17 Jun 2024 11:36:39 GMT</pubDate>
    </item>
    <item>
      <title>Soft Preference Optimization: Aligning Language Models to Expert Distributions</title>
      <link>http://arxiv.org/abs/2405.00747v3</link>
      <description>We propose Soft Preference Optimization (SPO), a method for aligning generative models, such as Large Language Models (LLMs), with human preferences, without the need for a reward model. SPO optimizes model outputs directly over a preference dataset through a natural loss function that integrates preference loss with a regularization term across the model's entire output distribution rather than limiting it to the preference dataset. Although SPO does not require the assumption of an existing underlying reward model, we demonstrate that, under the Bradley-Terry (BT) model assumption, it converges to a softmax of scaled rewards, with the distribution's &quot;softness&quot; adjustable via the softmax exponent, an algorithm parameter. We showcase SPO's methodology, its theoretical foundation, and its comparative advantages in simplicity, computational efficiency, and alignment precision.\n\n\nSoft Preference Optimization: Aligning Language Models to Expert Distributions</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.00747v3</guid>
      <dc:creator>Arsalan Sharifnassab, Sina Ghiassian, Saber Salehkaleybar, Surya Kanoria, Dale Schuurmans</dc:creator>
      <pubDate>Mon, 27 May 2024 19:59:00 GMT</pubDate>
    </item>
    <item>
      <title>Virtual Personas for Language Models via an Anthology of Backstories</title>
      <link>http://arxiv.org/abs/2407.06576v1</link>
      <description>Large language models (LLMs) are trained from vast repositories of text authored by millions of distinct authors, reflecting an enormous diversity of human traits. While these models bear the potential to be used as approximations of human subjects in behavioral studies, prior efforts have been limited in steering model responses to match individual human users. In this work, we introduce &quot;Anthology&quot;, a method for conditioning LLMs to particular virtual personas by harnessing open-ended life narratives, which we refer to as &quot;backstories.&quot; We show that our methodology enhances the consistency and reliability of experimental outcomes while ensuring better representation of diverse sub-populations. Across three nationally representative human surveys conducted as part of Pew Research Center's American Trends Panel (ATP), we demonstrate that Anthology achieves up to 18% improvement in matching the response distributions of human respondents and 27% improvement in consistency metrics. Our code and generated backstories are available at https://github.com/CannyLab/anthology.\n\n\nVirtual Personas for Language Models via an Anthology of Backstories</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2407.06576v1</guid>
      <dc:creator>Suhong Moon, Marwa Abdulhai, Minwoo Kang, Joseph Suh, Widyadewi Soedarmadji, Eran Kohen Behar, David M. Chan</dc:creator>
      <pubDate>Tue, 09 Jul 2024 06:11:18 GMT</pubDate>
    </item>
    <item>
      <title>Towards Building Specialized Generalist AI with System 1 and System 2 Fusion</title>
      <link>http://arxiv.org/abs/2407.08642v1</link>
      <description>In this perspective paper, we introduce the concept of Specialized Generalist Artificial Intelligence (SGAI or simply SGI) as a crucial milestone toward Artificial General Intelligence (AGI). Compared to directly scaling general abilities, SGI is defined as AI that specializes in at least one task, surpassing human experts, while also retaining general abilities. This fusion path enables SGI to rapidly achieve high-value areas. We categorize SGI into three stages based on the level of mastery over professional skills and generality performance. Additionally, we discuss the necessity of SGI in addressing issues associated with large language models, such as their insufficient generality, specialized capabilities, uncertainty in innovation, and practical applications. Furthermore, we propose a conceptual framework for developing SGI that integrates the strengths of Systems 1 and 2 cognitive processing. This framework comprises three layers and four key components, which focus on enhancing individual abilities and facilitating collaborative evolution. We conclude by summarizing the potential challenges and suggesting future directions. We hope that the proposed SGI will provide insights into further research and applications towards achieving AGI.\n\n\nTowards Building Specialized Generalist AI with System 1 and System 2 Fusion</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2407.08642v1</guid>
      <dc:creator>Kaiyan Zhang, Biqing Qi, Bowen Zhou</dc:creator>
      <pubDate>Thu, 11 Jul 2024 16:23:16 GMT</pubDate>
    </item>
    <item>
      <title>Improving Reward Models with Synthetic Critiques</title>
      <link>http://arxiv.org/abs/2405.20850v1</link>
      <description>Reward models (RM) play a critical role in aligning language models through the process of reinforcement learning from human feedback. RMs are trained to predict a score reflecting human preference, which requires significant time and cost for human annotation. Additionally, RMs tend to quickly overfit on superficial features in the training set, hindering their generalization performance on unseen distributions. We propose a novel approach using synthetic natural language critiques generated by large language models to provide additional feedback, evaluating aspects such as instruction following, correctness, and style. This offers richer signals and more robust features for RMs to assess and score on. We demonstrate that high-quality critiques improve the performance and data efficiency of RMs initialized from different pretrained models. Conversely, we also show that low-quality critiques negatively impact performance. Furthermore, incorporating critiques enhances the interpretability and robustness of RM training.\n\n\nImproving Reward Models with Synthetic Critiques</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.20850v1</guid>
      <dc:creator>Zihuiwen Ye, Fraser Greenlee-Scott, Max Bartolo, Phil Blunsom, Jon Ander Campos, Matthias Gallé</dc:creator>
      <pubDate>Fri, 31 May 2024 14:33:07 GMT</pubDate>
    </item>
    <item>
      <title>A Review of Large Language Models and Autonomous Agents in Chemistry</title>
      <link>http://arxiv.org/abs/2407.01603v1</link>
      <description>Large language models (LLMs) are emerging as a powerful tool in chemistry across multiple domains. In chemistry, LLMs are able to accurately predict properties, design new molecules, optimize synthesis pathways, and accelerate drug and material discovery. A core emerging idea is combining LLMs with chemistry-specific tools like synthesis planners and databases, leading to so-called &quot;agents.&quot; This review covers LLMs' recent history, current capabilities, design, challenges specific to chemistry, and future directions. Particular attention is given to agents and their emergence as a cross-chemistry paradigm. Agents have proven effective in diverse domains of chemistry, but challenges remain. It is unclear if creating domain-specific versus generalist agents and developing autonomous pipelines versus &quot;co-pilot&quot; systems will accelerate chemistry. An emerging direction is the development of multi-agent systems using a human-in-the-loop approach. Due to the incredibly fast development of this field, a repository has been built to keep track of the latest studies: https://github.com/ur-whitelab/LLMs-in-science.\n\n\nA Review of Large Language Models and Autonomous Agents in Chemistry</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2407.01603v1</guid>
      <dc:creator>Mayk Caldas Ramos, Christopher J. Collison, Andrew D. White</dc:creator>
      <pubDate>Wed, 26 Jun 2024 17:33:21 GMT</pubDate>
    </item>
    <item>
      <title>Personalized Steering of Large Language Models: Versatile Steering Vectors Through Bi-directional Preference Optimization</title>
      <link>http://arxiv.org/abs/2406.00045v1</link>
      <description>Researchers have been studying approaches to steer the behavior of Large Language Models (LLMs) and build personalized LLMs tailored for various applications. While fine-tuning seems to be a direct solution, it requires substantial computational resources and may significantly affect the utility of the original LLM. Recent endeavors have introduced more lightweight strategies, focusing on extracting &quot;steering vectors&quot; to guide the model's output toward desired behaviors by adjusting activations within specific layers of the LLM's transformer architecture. However, such steering vectors are directly extracted from the activations of human preference data and thus often lead to suboptimal results and occasional failures, especially in alignment-related scenarios. This work proposes an innovative approach that could produce more effective steering vectors through bi-directional preference optimization. Our method is designed to allow steering vectors to directly influence the generation probability of contrastive human preference data pairs, thereby offering a more precise representation of the target behavior. By carefully adjusting the direction and magnitude of the steering vector, we enabled personalized control over the desired behavior across a spectrum of intensities. Extensive experimentation across various open-ended generation tasks, particularly focusing on steering AI personas, has validated the efficacy of our approach. Moreover, we comprehensively investigate critical alignment-concerning scenarios, such as managing truthfulness, mitigating hallucination, and addressing jailbreaking attacks. Remarkably, our method can still demonstrate outstanding steering effectiveness across these scenarios. Furthermore, we showcase the transferability of our steering vectors across different models/LoRAs and highlight the synergistic benefits of applying multiple vectors simultaneously.\n\n\nPersonalized Steering of Large Language Models: Versatile Steering Vectors Through Bi-directional Preference Optimization</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.00045v1</guid>
      <dc:creator>Yuanpu Cao, Tianrong Zhang, Bochuan Cao, Ziyi Yin, Lu Lin, Fenglong Ma, Jinghui Chen</dc:creator>
      <pubDate>Tue, 28 May 2024 05:10:40 GMT</pubDate>
    </item>
    <item>
      <title>BoNBoN Alignment for Large Language Models and the Sweetness of Best-of-n Sampling</title>
      <link>http://arxiv.org/abs/2406.00832v2</link>
      <description>This paper concerns the problem of aligning samples from large language models to human preferences using best-of-$n$ sampling, where we draw $n$ samples, rank them, and return the best one. We consider two fundamental problems. First: what is the relationship between best-of-$n$ and approaches to alignment that train LLMs to output samples with a high expected reward (e.g., RLHF or DPO)? To answer this, we embed both the best-of-$n$ distribution and the sampling distributions learned by alignment procedures in a common class of tiltings of the base LLM distribution. We then show that, within this class, best-of-$n$ is essentially optimal in terms of the trade-off between win-rate against the base model vs KL distance from the base model. That is, best-of-$n$ is the best choice of alignment distribution if the goal is to maximize win rate. However, best-of-$n$ requires drawing $n$ samples for each inference, a substantial cost. To avoid this, the second problem we consider is how to fine-tune a LLM to mimic the best-of-$n$ sampling distribution. We derive BoNBoN Alignment to achieve this by exploiting the special structure of the best-of-$n$ distribution. Experiments show that BoNBoN alignment yields substantial improvements in producing a model that is preferred to the base policy while minimally affecting off-target aspects.\n\n\nBoNBoN Alignment for Large Language Models and the Sweetness of Best-of-n Sampling</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.00832v2</guid>
      <dc:creator>Lin Gui, Cristina Gârbacea, Victor Veitch</dc:creator>
      <pubDate>Wed, 05 Jun 2024 05:23:40 GMT</pubDate>
    </item>
    <item>
      <title>MetaAligner: Towards Generalizable Multi-Objective Alignment of Language Models</title>
      <link>http://arxiv.org/abs/2403.17141v2</link>
      <description>Recent advancements in large language models (LLMs) aim to tackle heterogeneous human expectations and values via multi-objective preference alignment. However, existing methods are parameter-adherent to the policy model, leading to two key limitations: (1) the high-cost repetition of their alignment algorithms for each new target model; (2) they cannot expand to unseen objectives due to their static alignment objectives. In this work, we propose Meta-Objective Aligner (MetaAligner), a model that performs conditional weak-to-strong correction for weak responses to approach strong responses. MetaAligner is the first policy-agnostic and generalizable method for multi-objective preference alignment, which enables plug-and-play alignment by decoupling parameter updates from the policy models and facilitates zero-shot preference alignment for unseen objectives via in-context learning. Experimental results show that MetaAligner achieves significant and balanced improvements in multi-objective alignments on 10 state-of-the-art policy models, and outperforms previous alignment methods with down to 15.71x less GPU training hours. The model also effectively aligns unseen objectives, marking the first step towards generalizable multi-objective preference alignment.\n\n\nMetaAligner: Conditional Weak-to-Strong Correction for Generalizable Multi-Objective Alignment of Language Models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2403.17141v2</guid>
      <dc:creator>Kailai Yang, Zhiwei Liu, Qianqian Xie, Jimin Huang, Tianlin Zhang, Sophia Ananiadou</dc:creator>
      <pubDate>Mon, 06 May 2024 14:17:41 GMT</pubDate>
    </item>
    <item>
      <title>MEReQ: Max-Ent Residual-Q Inverse RL for Sample-Efficient Alignment from Intervention</title>
      <link>http://arxiv.org/abs/2406.16258v1</link>
      <description>Aligning robot behavior with human preferences is crucial for deploying embodied AI agents in human-centered environments. A promising solution is interactive imitation learning from human intervention, where a human expert observes the policy's execution and provides interventions as feedback. However, existing methods often fail to utilize the prior policy efficiently to facilitate learning, thus hindering sample efficiency. In this work, we introduce MEReQ (Maximum-Entropy Residual-Q Inverse Reinforcement Learning), designed for sample-efficient alignment from human intervention. Instead of inferring the complete human behavior characteristics, MEReQ infers a residual reward function that captures the discrepancy between the human expert's and the prior policy's underlying reward functions. It then employs Residual Q-Learning (RQL) to align the policy with human preferences using this residual reward function. Extensive evaluations on simulated and real-world tasks demonstrate that MEReQ achieves sample-efficient policy alignment from human intervention.\n\n\nMEReQ: Max-Ent Residual-Q Inverse RL for Sample-Efficient Alignment from Intervention</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.16258v1</guid>
      <dc:creator>Yuxin Chen, Chen Tang, Chenran Li, Ran Tian, Peter Stone, Masayoshi Tomizuka, Wei Zhan</dc:creator>
      <pubDate>Mon, 24 Jun 2024 01:51:09 GMT</pubDate>
    </item>
    <item>
      <title>ContraSolver: Self-Alignment of Language Models by Resolving Internal Preference Contradictions</title>
      <link>http://arxiv.org/abs/2406.08842v1</link>
      <description>While substantial advancements have been made in developing large language models (LLMs), achieving control over their behavior can be difficult. Direct preference optimization (DPO) assumes the existence of a latent reward function to evaluate the responses of LLMs. This assumption indicates a strict preference ordering of different responses to the same input. However, there always exist contradictions of preference in LLMs according to our experimental observations. In this paper, we construct a graph structure of the preference relationship among different responses with self-annotation to find contradictions in the preference order. We propose ContraSolver, an algorithm that traverses all edges on the preference graph to identify those that might cause contradictions. ContraSolver initializes the graph with a maximum spanning tree and identifies contradictory edges, prioritizing the resolution of low-confidence preferences while preserving high-confidence ones. Experimental results on four different generation tasks show that the performance of different LLMs can be largely improved through our completely unsupervised self-alignment. Furthermore, by analyzing the preference graphs of LLMs with and without self-alignment by ContraSolver, we quantify the reduction in contradictions, suggesting that resolving preference contradictions is crucial for achieving better alignment performance.\n\n\nContraSolver: Self-Alignment of Language Models by Resolving Internal Preference Contradictions</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.08842v1</guid>
      <dc:creator>Xu Zhang, Xunjian Yin, Xiaojun Wan</dc:creator>
      <pubDate>Thu, 13 Jun 2024 06:08:04 GMT</pubDate>
    </item>
    <item>
      <title>The Perils of Optimizing Learned Reward Functions: Low Training Error Does Not Guarantee Low Regret</title>
      <link>http://arxiv.org/abs/2406.15753v1</link>
      <description>In reinforcement learning, specifying reward functions that capture the intended task can be very challenging. Reward learning aims to address this issue by learning the reward function. However, a learned reward model may have a low error on the training distribution, and yet subsequently produce a policy with large regret. We say that such a reward model has an error-regret mismatch. The main source of an error-regret mismatch is the distributional shift that commonly occurs during policy optimization. In this paper, we mathematically show that a sufficiently low expected test error of the reward model guarantees low worst-case regret, but that for any fixed expected test error, there exist realistic data distributions that allow for error-regret mismatch to occur. We then show that similar problems persist even when using policy regularization techniques, commonly employed in methods such as RLHF. Our theoretical results highlight the importance of developing new ways to measure the quality of learned reward models.\n\n\nThe Perils of Optimizing Learned Reward Functions: Low Training Error Does Not Guarantee Low Regret</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.15753v1</guid>
      <dc:creator>Lukas Fluri, Leon Lang, Alessandro Abate, Patrick Forré, David Krueger, Joar Skalse</dc:creator>
      <pubDate>Sat, 22 Jun 2024 06:43:51 GMT</pubDate>
    </item>
    <item>
      <title>An Information Bottleneck Perspective for Effective Noise Filtering on Retrieval-Augmented Generation</title>
      <link>http://arxiv.org/abs/2406.01549v2</link>
      <description>Retrieval-augmented generation integrates the capabilities of large language models with relevant information retrieved from an extensive corpus, yet encounters challenges when confronted with real-world noisy data. One recent solution is to train a filter module to find relevant content but only achieve suboptimal noise compression. In this paper, we propose to introduce the information bottleneck theory into retrieval-augmented generation. Our approach involves the filtration of noise by simultaneously maximizing the mutual information between compression and ground output, while minimizing the mutual information between compression and retrieved passage. In addition, we derive the formula of information bottleneck to facilitate its application in novel comprehensive evaluations, the selection of supervised fine-tuning data, and the construction of reinforcement learning rewards. Experimental results demonstrate that our approach achieves significant improvements across various question answering datasets, not only in terms of the correctness of answer generation but also in the conciseness with $2.5\%$ compression rate.\n\n\nAn Information Bottleneck Perspective for Effective Noise Filtering on Retrieval-Augmented Generation</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.01549v2</guid>
      <dc:creator>Kun Zhu, Xiaocheng Feng, Xiyuan Du, Yuxuan Gu, Weijiang Yu, Haotian Wang, Qianglong Chen, Zheng Chu, Jingchang Chen, Bing Qin</dc:creator>
      <pubDate>Thu, 04 Jul 2024 14:21:39 GMT</pubDate>
    </item>
    <item>
      <title>Oneshot Differentially Private Top-k Selection</title>
      <link>http://arxiv.org/abs/2105.08233v2</link>
      <description>Being able to efficiently and accurately select the top-$k$ elements with differential privacy is an integral component of various private data analysis tasks. In this paper, we present the oneshot Laplace mechanism, which generalizes the well-known Report Noisy Max mechanism to reporting noisy top-$k$ elements. We show that the oneshot Laplace mechanism with a noise level of $\widetilde{O}(\sqrt{k}/\eps)$ is approximately differentially private. Compared to the previous peeling approach of running Report Noisy Max $k$ times, the oneshot Laplace mechanism only adds noises and computes the top $k$ elements once, hence much more efficient for large $k$. In addition, our proof of privacy relies on a novel coupling technique that bypasses the use of composition theorems. Finally, we present a novel application of efficient top-$k$ selection in the classical problem of ranking from pairwise comparisons.\n\n\nOne-Shot Safety Alignment for Large Language Models via Optimal Dualization</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2105.08233v2</guid>
      <dc:creator>Gang Qiao, Weijie J. Su, Li Zhang</dc:creator>
      <pubDate>Wed, 23 Jun 2021 17:32:13 GMT</pubDate>
    </item>
    <item>
      <title>Culturally Aware and Adapted NLP: A Taxonomy and a Survey of the State of the Art</title>
      <link>http://arxiv.org/abs/2406.03930v1</link>
      <description>The surge of interest in culturally aware and adapted Natural Language Processing (NLP) has inspired much recent research. However, the lack of common understanding of the concept of &quot;culture&quot; has made it difficult to evaluate progress in this emerging area. Drawing on prior research in NLP and related fields, we propose an extensive taxonomy of elements of culture that can provide a systematic framework for analyzing and understanding research progress. Using the taxonomy, we survey existing resources and models for culturally aware and adapted NLP, providing an overview of the state of the art and the research gaps that still need to be filled.\n\n\nCulturally Aware and Adapted NLP: A Taxonomy and a Survey of the State of the Art</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.03930v1</guid>
      <dc:creator>Chen Cecilia Liu, Iryna Gurevych, Anna Korhonen</dc:creator>
      <pubDate>Thu, 06 Jun 2024 10:16:43 GMT</pubDate>
    </item>
    <item>
      <title>TS-Align: A Teacher-Student Collaborative Framework for Scalable Iterative Finetuning of Large Language Models</title>
      <link>http://arxiv.org/abs/2405.20215v3</link>
      <description>Mainstream approaches to aligning large language models (LLMs) heavily rely on human preference data, particularly when models require periodic updates. The standard process for iterative alignment of LLMs involves collecting new human feedback for each update. However, the data collection process is costly and challenging to scale. To address this issue, we introduce the &quot;TS-Align&quot; framework, which fine-tunes a policy model using pairwise feedback data automatically mined from its outputs. This automatic mining process is efficiently accomplished through the collaboration between a large-scale teacher model and a small-scale student model. The policy fine-tuning process can be iteratively repeated using on-policy generations within our proposed teacher-student collaborative framework. Through extensive experiments, we demonstrate that our final aligned policy outperforms the base policy model with an average win rate of 69.7% across seven conversational or instruction-following datasets. Furthermore, we show that the ranking capability of the teacher is effectively distilled into the student through our pipeline, resulting in a small-scale yet effective reward model for policy model alignment.\n\n\nTS-Align: A Teacher-Student Collaborative Framework for Scalable Iterative Finetuning of Large Language Models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.20215v3</guid>
      <dc:creator>Chen Zhang, Chengguang Tang, Dading Chong, Ke Shi, Guohua Tang, Feng Jiang, Haizhou Li</dc:creator>
      <pubDate>Fri, 14 Jun 2024 08:23:33 GMT</pubDate>
    </item>
    <item>
      <title>The Expressibility of Polynomial based Attention Scheme</title>
      <link>http://arxiv.org/abs/2310.20051v1</link>
      <description>Large language models (LLMs) have significantly improved various aspects of our daily lives. These models have impacted numerous domains, from healthcare to education, enhancing productivity, decision-making processes, and accessibility. As a result, they have influenced and, to some extent, reshaped people's lifestyles. However, the quadratic complexity of attention in transformer architectures poses a challenge when scaling up these models for processing long textual contexts. This issue makes it impractical to train very large models on lengthy texts or use them efficiently during inference. While a recent study by [KMZ23] introduced a technique that replaces the softmax with a polynomial function and polynomial sketching to speed up attention mechanisms, the theoretical understandings of this new approach are not yet well understood.   In this paper, we offer a theoretical analysis of the expressive capabilities of polynomial attention. Our study reveals a disparity in the ability of high-degree and low-degree polynomial attention. Specifically, we construct two carefully designed datasets, namely $\mathcal{D}_0$ and $\mathcal{D}_1$, where $\mathcal{D}_1$ includes a feature with a significantly larger value compared to $\mathcal{D}_0$. We demonstrate that with a sufficiently high degree $\beta$, a single-layer polynomial attention network can distinguish between $\mathcal{D}_0$ and $\mathcal{D}_1$. However, with a low degree $\beta$, the network cannot effectively separate the two datasets. This analysis underscores the greater effectiveness of high-degree polynomials in amplifying large values and distinguishing between datasets. Our analysis offers insight into the representational capacity of polynomial attention and provides a rationale for incorporating higher-degree polynomials in attention mechanisms to capture intricate linguistic correlations.\n\n\nThe Expressibility of Polynomial based Attention Scheme</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2310.20051v1</guid>
      <dc:creator>Zhao Song, Guangyi Xu, Junze Yin</dc:creator>
      <pubDate>Mon, 30 Oct 2023 22:16:18 GMT</pubDate>
    </item>
    <item>
      <title>Regularizing Hidden States Enables Learning Generalizable Reward Model for LLMs</title>
      <link>http://arxiv.org/abs/2406.10216v1</link>
      <description>Reward models trained on human preference data have been proven to be effective for aligning Large Language Models (LLMs) with human intent within the reinforcement learning from human feedback (RLHF) framework. However, the generalization capabilities of current reward models to unseen prompts and responses are limited. This limitation can lead to an unexpected phenomenon known as reward over-optimization, where excessive optimization of rewards results in a decline in actual performance. While previous research has advocated for constraining policy optimization, our study proposes a novel approach to enhance the reward model's generalization ability against distribution shifts by regularizing the hidden states. Specifically, we retain the base model's language model head and incorporate a suite of text-generation losses to preserve the hidden states' text generation capabilities, while concurrently learning a reward head behind the same hidden states. Our experimental results demonstrate that the introduced regularization technique markedly improves the accuracy of learned reward models across a variety of out-of-distribution (OOD) tasks and effectively alleviate the over-optimization issue in RLHF, offering a more reliable and robust preference learning paradigm.\n\n\nRegularizing Hidden States Enables Learning Generalizable Reward Model for LLMs</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.10216v1</guid>
      <dc:creator>Rui Yang, Ruomeng Ding, Yong Lin, Huan Zhang, Tong Zhang</dc:creator>
      <pubDate>Fri, 14 Jun 2024 17:49:59 GMT</pubDate>
    </item>
    <item>
      <title>Spontaneous Reward Hacking in Iterative Self-Refinement</title>
      <link>http://arxiv.org/abs/2407.04549v1</link>
      <description>Language models are capable of iteratively improving their outputs based on natural language feedback, thus enabling in-context optimization of user preference. In place of human users, a second language model can be used as an evaluator, providing feedback along with numerical ratings which the generator attempts to optimize. However, because the evaluator is an imperfect proxy of user preference, this optimization can lead to reward hacking, where the evaluator's ratings improve while the generation quality remains stagnant or even decreases as judged by actual user preference. The concern of reward hacking is heightened in iterative self-refinement where the generator and the evaluator use the same underlying language model, in which case the optimization pressure can drive them to exploit shared vulnerabilities. Using an essay editing task, we show that iterative self-refinement leads to deviation between the language model evaluator and human judgment, demonstrating that reward hacking can occur spontaneously in-context with the use of iterative self-refinement. In addition, we study conditions under which reward hacking occurs and observe two factors that affect reward hacking severity: model size and context sharing between the generator and the evaluator.\n\n\nIterative Length-Regularized Direct Preference Optimization: A Case Study on Improving 7B Language Models to GPT-4 Level</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2407.04549v1</guid>
      <dc:creator>Jane Pan, He He, Samuel R. Bowman, Shi Feng</dc:creator>
      <pubDate>Fri, 05 Jul 2024 14:34:50 GMT</pubDate>
    </item>
    <item>
      <title>Toward Self-Improvement of LLMs via Imagination, Searching, and Criticizing</title>
      <link>http://arxiv.org/abs/2404.12253v1</link>
      <description>Despite the impressive capabilities of Large Language Models (LLMs) on various tasks, they still struggle with scenarios that involves complex reasoning and planning. Recent work proposed advanced prompting techniques and the necessity of fine-tuning with high-quality data to augment LLMs' reasoning abilities. However, these approaches are inherently constrained by data availability and quality. In light of this, self-correction and self-learning emerge as viable solutions, employing strategies that allow LLMs to refine their outputs and learn from self-assessed rewards. Yet, the efficacy of LLMs in self-refining its response, particularly in complex reasoning and planning task, remains dubious. In this paper, we introduce AlphaLLM for the self-improvements of LLMs, which integrates Monte Carlo Tree Search (MCTS) with LLMs to establish a self-improving loop, thereby enhancing the capabilities of LLMs without additional annotations. Drawing inspiration from the success of AlphaGo, AlphaLLM addresses the unique challenges of combining MCTS with LLM for self-improvement, including data scarcity, the vastness search spaces of language tasks, and the subjective nature of feedback in language tasks. AlphaLLM is comprised of prompt synthesis component, an efficient MCTS approach tailored for language tasks, and a trio of critic models for precise feedback. Our experimental results in mathematical reasoning tasks demonstrate that AlphaLLM significantly enhances the performance of LLMs without additional annotations, showing the potential for self-improvement in LLMs.\n\n\nToward Self-Improvement of LLMs via Imagination, Searching, and Criticizing</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2404.12253v1</guid>
      <dc:creator>Ye Tian, Baolin Peng, Linfeng Song, Lifeng Jin, Dian Yu, Haitao Mi, Dong Yu</dc:creator>
      <pubDate>Thu, 18 Apr 2024 15:21:34 GMT</pubDate>
    </item>
    <item>
      <title>Efficient Knowledge Infusion via KG-LLM Alignment</title>
      <link>http://arxiv.org/abs/2406.03746v1</link>
      <description>To tackle the problem of domain-specific knowledge scarcity within large language models (LLMs), knowledge graph-retrievalaugmented method has been proven to be an effective and efficient technique for knowledge infusion. However, existing approaches face two primary challenges: knowledge mismatch between public available knowledge graphs and the specific domain of the task at hand, and poor information compliance of LLMs with knowledge graphs. In this paper, we leverage a small set of labeled samples and a large-scale corpus to efficiently construct domain-specific knowledge graphs by an LLM, addressing the issue of knowledge mismatch. Additionally, we propose a three-stage KG-LLM alignment strategyto enhance the LLM's capability to utilize information from knowledge graphs. We conduct experiments with a limited-sample setting on two biomedical question-answering datasets, and the results demonstrate that our approach outperforms existing baselines.\n\n\nSupportiveness-based Knowledge Rewriting for Retrieval-augmented Language Modeling</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.03746v1</guid>
      <dc:creator>Zhouyu Jiang, Ling Zhong, Mengshu Sun, Jun Xu, Rui Sun, Hui Cai, Shuhan Luo, Zhiqiang Zhang</dc:creator>
      <pubDate>Thu, 06 Jun 2024 04:55:55 GMT</pubDate>
    </item>
    <item>
      <title>Self-Control of LLM Behaviors by Compressing Suffix Gradient into Prefix Controller</title>
      <link>http://arxiv.org/abs/2406.02721v2</link>
      <description>We propose Self-Control, a novel method utilizing suffix gradients to control the behavior of large language models (LLMs) without explicit human annotations. Given a guideline expressed in suffix string and the model's self-assessment of adherence, Self-Control computes the gradient of this self-judgment concerning the model's hidden states, directly influencing the auto-regressive generation process towards desired behaviors. To enhance efficiency, we introduce Self-Control_{prefix}, a compact module that encapsulates the learned representations from suffix gradients into a Prefix Controller, facilitating inference-time control for various LLM behaviors. Our experiments demonstrate Self-Control's efficacy across multiple domains, including emotional modulation, ensuring harmlessness, and enhancing complex reasoning. Especially, Self-Control_{prefix} enables a plug-and-play control and jointly controls multiple attributes, improving model outputs without altering model parameters or increasing inference-time costs.\n\n\nSelf-Control of LLM Behaviors by Compressing Suffix Gradient into Prefix Controller</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.02721v2</guid>
      <dc:creator>Min Cai, Yuchen Zhang, Shichang Zhang, Fan Yin, Difan Zou, Yisong Yue, Ziniu Hu</dc:creator>
      <pubDate>Tue, 18 Jun 2024 15:58:38 GMT</pubDate>
    </item>
    <item>
      <title>RKLD: Reverse KL-Divergence-based Knowledge Distillation for Unlearning Personal Information in Large Language Models</title>
      <link>http://arxiv.org/abs/2406.01983v1</link>
      <description>With the passage of the Right to Be Forgotten (RTBF) regulations and the scaling up of language model training datasets, research on model unlearning in large language models (LLMs) has become more crucial. Before the era of LLMs, machine unlearning research focused mainly on classification tasks in models with small parameters. In these tasks, the content to be forgotten or retained is clear and straightforward. However, as parameter sizes have grown and tasks have become more complex, balancing forget quality and model utility has become more challenging, especially in scenarios involving personal data instead of classification results. Existing methods based on gradient ascent and its variants often struggle with this balance, leading to unintended information loss or partial forgetting. To address this challenge, we propose RKLD, a novel \textbf{R}everse \textbf{KL}-Divergence-based Knowledge \textbf{D}istillation unlearning algorithm for LLMs targeting the unlearning of personal information. Through RKLD, we achieve significant forget quality and effectively maintain the model utility in our experiments.\n\n\nRKLD: Reverse KL-Divergence-based Knowledge Distillation for Unlearning Personal Information in Large Language Models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.01983v1</guid>
      <dc:creator>Bichen Wang, Yuzhe Zi, Yixin Sun, Yanyan Zhao, Bing Qin</dc:creator>
      <pubDate>Tue, 04 Jun 2024 05:51:43 GMT</pubDate>
    </item>
    <item>
      <title>Effectiveness Assessment of Recent Large Vision-Language Models</title>
      <link>http://arxiv.org/abs/2403.04306v4</link>
      <description>The advent of large vision-language models (LVLMs) represents a remarkable advance in the quest for artificial general intelligence. However, the model's effectiveness in both specialized and general tasks warrants further investigation. This paper endeavors to evaluate the competency of popular LVLMs in specialized and general tasks, respectively, aiming to offer a comprehensive understanding of these novel models. To gauge their effectiveness in specialized tasks, we employ six challenging tasks in three different application scenarios: natural, healthcare, and industrial. These six tasks include salient/camouflaged/transparent object detection, as well as polyp detection, skin lesion detection, and industrial anomaly detection. We examine the performance of three recent open-source LVLMs, including MiniGPT-v2, LLaVA-1.5, and Shikra, on both visual recognition and localization in these tasks. Moreover, we conduct empirical investigations utilizing the aforementioned LVLMs together with GPT-4V, assessing their multi-modal understanding capabilities in general tasks including object counting, absurd question answering, affordance reasoning, attribute recognition, and spatial relation reasoning. Our investigations reveal that these LVLMs demonstrate limited proficiency not only in specialized tasks but also in general tasks. We delve deep into this inadequacy and uncover several potential factors, including limited cognition in specialized tasks, object hallucination, text-to-image interference, and decreased robustness in complex problems. We hope that this study can provide useful insights for the future development of LVLMs, helping researchers improve LVLMs for both general and specialized applications.\n\n\nRecent Advances in Large Language Models for Healthcare</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2403.04306v4</guid>
      <dc:creator>Yao Jiang, Xinyu Yan, Ge-Peng Ji, Keren Fu, Meijun Sun, Huan Xiong, Deng-Ping Fan, Fahad Shahbaz Khan</dc:creator>
      <pubDate>Tue, 11 Jun 2024 07:42:51 GMT</pubDate>
    </item>
    <item>
      <title>AI Risk Management Should Incorporate Both Safety and Security</title>
      <link>http://arxiv.org/abs/2405.19524v1</link>
      <description>The exposure of security vulnerabilities in safety-aligned language models, e.g., susceptibility to adversarial attacks, has shed light on the intricate interplay between AI safety and AI security. Although the two disciplines now come together under the overarching goal of AI risk management, they have historically evolved separately, giving rise to differing perspectives. Therefore, in this paper, we advocate that stakeholders in AI risk management should be aware of the nuances, synergies, and interplay between safety and security, and unambiguously take into account the perspectives of both disciplines in order to devise mostly effective and holistic risk mitigation approaches. Unfortunately, this vision is often obfuscated, as the definitions of the basic concepts of &quot;safety&quot; and &quot;security&quot; themselves are often inconsistent and lack consensus across communities. With AI risk management being increasingly cross-disciplinary, this issue is particularly salient. In light of this conceptual challenge, we introduce a unified reference framework to clarify the differences and interplay between AI safety and AI security, aiming to facilitate a shared understanding and effective collaboration across communities.\n\n\nAI Risk Management Should Incorporate Both Safety and Security</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.19524v1</guid>
      <dc:creator>Xiangyu Qi, Yangsibo Huang, Yi Zeng, Edoardo Debenedetti, Jonas Geiping, Luxi He, Kaixuan Huang, Udari Madhushani, Vikash Sehwag, Weijia Shi, Boyi Wei, Tinghao Xie, Danqi Chen, Pin-Yu Chen, Jeffrey Ding, Ruoxi Jia, Jiaqi Ma, Arvind Narayanan, Weijie J Su, Mengdi Wang, Chaowei Xiao, Bo Li, Dawn Song, Peter Henderson, Prateek Mittal</dc:creator>
      <pubDate>Wed, 29 May 2024 21:00:47 GMT</pubDate>
    </item>
    <item>
      <title>A tutorial on learning from preferences and choices with Gaussian Processes</title>
      <link>http://arxiv.org/abs/2403.11782v4</link>
      <description>Preference modelling lies at the intersection of economics, decision theory, machine learning and statistics. By understanding individuals' preferences and how they make choices, we can build products that closely match their expectations, paving the way for more efficient and personalised applications across a wide range of domains. The objective of this tutorial is to present a cohesive and comprehensive framework for preference learning with Gaussian Processes (GPs), demonstrating how to seamlessly incorporate rationality principles (from economics and decision theory) into the learning process. By suitably tailoring the likelihood function, this framework enables the construction of preference learning models that encompass random utility models, limits of discernment, and scenarios with multiple conflicting utilities for both object- and label-preference. This tutorial builds upon established research while simultaneously introducing some novel GP-based models to address specific gaps in the existing literature.\n\n\nA tutorial on learning from preferences and choices with Gaussian Processes</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2403.11782v4</guid>
      <dc:creator>Alessio Benavoli, Dario Azzimonti</dc:creator>
      <pubDate>Sat, 01 Jun 2024 20:50:23 GMT</pubDate>
    </item>
    <item>
      <title>Side Effects in Steering Fragments</title>
      <link>http://arxiv.org/abs/1109.2222v1</link>
      <description>In this thesis I will give a formal definition of side effects. I will do so by modifying a system for modelling program instructions and program states, Quantified Dynamic Logic, to a system called DLAf (for Dynamic Logic with Assignments as Formulas), which in contrast to QDL allows assignments in formulas and makes use of short-circuit evaluation. I will show the underlying logic in those formulas to be a variant of short-circuit logic called repetition-proof short-circuit logic.   Using DLAf I will define the actual and the expected evaluation of a single instruction. The side effects are then defined to be the difference between the two. I will give rules for composing those side effects in single instructions, thus scaling up our definition of side effects to a definition of side effects in deterministic \dlaf-programs. Using this definition I will give a classification of side effects, introducing as most important class that of marginal side effects. Finally, I will show how to use our system for calculating the side effects in a real system such as Program Algebra (PGA).\n\n\nSteering Without Side Effects: Improving Post-Deployment Control of Language Models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/1109.2222v1</guid>
      <dc:creator>Lars Wortel</dc:creator>
      <pubDate>Sat, 10 Sep 2011 13:24:08 GMT</pubDate>
    </item>
    <item>
      <title>Navigating the Safety Landscape: Measuring Risks in Finetuning Large Language Models</title>
      <link>http://arxiv.org/abs/2405.17374v2</link>
      <description>Safety alignment is the key to guiding the behaviors of large language models (LLMs) that are in line with human preferences and restrict harmful behaviors at inference time, but recent studies show that it can be easily compromised by finetuning with only a few adversarially designed training examples. We aim to measure the risks in finetuning LLMs through navigating the LLM safety landscape. We discover a new phenomenon observed universally in the model parameter space of popular open-source LLMs, termed as &quot;safety basin&quot;: randomly perturbing model weights maintains the safety level of the original aligned model in its local neighborhood. Our discovery inspires us to propose the new VISAGE safety metric that measures the safety in LLM finetuning by probing its safety landscape. Visualizing the safety landscape of the aligned model enables us to understand how finetuning compromises safety by dragging the model away from the safety basin. LLM safety landscape also highlights the system prompt's critical role in protecting a model, and that such protection transfers to its perturbed variants within the safety basin. These observations from our safety landscape research provide new insights for future work on LLM safety community.\n\n\nNavigating the Safety Landscape: Measuring Risks in Finetuning Large Language Models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.17374v2</guid>
      <dc:creator>ShengYun Peng, Pin-Yu Chen, Matthew Hull, Duen Horng Chau</dc:creator>
      <pubDate>Tue, 28 May 2024 04:58:52 GMT</pubDate>
    </item>
    <item>
      <title>The Importance of Online Data: Understanding Preference Fine-tuning via Coverage</title>
      <link>http://arxiv.org/abs/2406.01462v2</link>
      <description>Learning from human preference data has emerged as the dominant paradigm for fine-tuning large language models (LLMs). The two most common families of techniques -- online reinforcement learning (RL) such as Proximal Policy Optimization (PPO) and offline contrastive methods such as Direct Preference Optimization (DPO) -- were positioned as equivalent in prior work due to the fact that both have to start from the same offline preference dataset. To further expand our theoretical understanding of the similarities and differences between online and offline techniques for preference fine-tuning, we conduct a rigorous analysis through the lens of dataset coverage, a concept that captures how the training data covers the test distribution and is widely used in RL. We prove that a global coverage condition is both necessary and sufficient for offline contrastive methods to converge to the optimal policy, but a weaker partial coverage condition suffices for online RL methods. This separation provides one explanation of why online RL methods can perform better than offline methods, especially when the offline preference data is not diverse enough. Finally, motivated by our preceding theoretical observations, we derive a hybrid preference optimization (HyPO) algorithm that uses offline data for contrastive-based preference optimization and online data for KL regularization. Theoretically and empirically, we demonstrate that HyPO is more performant than its pure offline counterpart DPO, while still preserving its computation and memory efficiency.\n\n\nUnderstanding Preference Fine-Tuning Through the Lens of Coverage</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.01462v2</guid>
      <dc:creator>Yuda Song, Gokul Swamy, Aarti Singh, J. Andrew Bagnell, Wen Sun</dc:creator>
      <pubDate>Tue, 16 Jul 2024 16:51:38 GMT</pubDate>
    </item>
    <item>
      <title>Bridging the Bosphorus: Advancing Turkish Large Language Models through Strategies for Low-Resource Language Adaptation and Benchmarking</title>
      <link>http://arxiv.org/abs/2405.04685v1</link>
      <description>Large Language Models (LLMs) are becoming crucial across various fields, emphasizing the urgency for high-quality models in underrepresented languages. This study explores the unique challenges faced by low-resource languages, such as data scarcity, model selection, evaluation, and computational limitations, with a special focus on Turkish. We conduct an in-depth analysis to evaluate the impact of training strategies, model choices, and data availability on the performance of LLMs designed for underrepresented languages. Our approach includes two methodologies: (i) adapting existing LLMs originally pretrained in English to understand Turkish, and (ii) developing a model from the ground up using Turkish pretraining data, both supplemented with supervised fine-tuning on a novel Turkish instruction-tuning dataset aimed at enhancing reasoning capabilities. The relative performance of these methods is evaluated through the creation of a new leaderboard for Turkish LLMs, featuring benchmarks that assess different reasoning and knowledge skills. Furthermore, we conducted experiments on data and model scaling, both during pretraining and fine-tuning, simultaneously emphasizing the capacity for knowledge transfer across languages and addressing the challenges of catastrophic forgetting encountered during fine-tuning on a different language. Our goal is to offer a detailed guide for advancing the LLM framework in low-resource linguistic contexts, thereby making natural language processing (NLP) benefits more globally accessible.\n\n\nBridging the Bosphorus: Advancing Turkish Large Language Models through Strategies for Low-Resource Language Adaptation and Benchmarking</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.04685v1</guid>
      <dc:creator>Emre Can Acikgoz, Mete Erdogan, Deniz Yuret</dc:creator>
      <pubDate>Tue, 07 May 2024 21:58:45 GMT</pubDate>
    </item>
    <item>
      <title>ReMoDetect: Reward Models Recognize Aligned LLM's Generations</title>
      <link>http://arxiv.org/abs/2405.17382v1</link>
      <description>The remarkable capabilities and easy accessibility of large language models (LLMs) have significantly increased societal risks (e.g., fake news generation), necessitating the development of LLM-generated text (LGT) detection methods for safe usage. However, detecting LGTs is challenging due to the vast number of LLMs, making it impractical to account for each LLM individually; hence, it is crucial to identify the common characteristics shared by these models. In this paper, we draw attention to a common feature of recent powerful LLMs, namely the alignment training, i.e., training LLMs to generate human-preferable texts. Our key finding is that as these aligned LLMs are trained to maximize the human preferences, they generate texts with higher estimated preferences even than human-written texts; thus, such texts are easily detected by using the reward model (i.e., an LLM trained to model human preference distribution). Based on this finding, we propose two training schemes to further improve the detection ability of the reward model, namely (i) continual preference fine-tuning to make the reward model prefer aligned LGTs even further and (ii) reward modeling of Human/LLM mixed texts (a rephrased texts from human-written texts using aligned LLMs), which serves as a median preference text corpus between LGTs and human-written texts to learn the decision boundary better. We provide an extensive evaluation by considering six text domains across twelve aligned LLMs, where our method demonstrates state-of-the-art results. Code is available at https://github.com/hyunseoklee-ai/reward_llm_detect.\n\n\nReMoDetect: Reward Models Recognize Aligned LLM's Generations</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.17382v1</guid>
      <dc:creator>Hyunseok Lee, Jihoon Tack, Jinwoo Shin</dc:creator>
      <pubDate>Mon, 27 May 2024 17:38:33 GMT</pubDate>
    </item>
    <item>
      <title>Aligning LLMs through Multi-perspective User Preference Ranking-based Feedback for Programming Question Answering</title>
      <link>http://arxiv.org/abs/2406.00037v1</link>
      <description>Code Community Question Answering (CCQA) seeks to tackle programming-related issues, thereby boosting productivity in both software engineering and academic research. Recent advancements in Reinforcement Learning from Human Feedback (RLHF) have transformed the fine-tuning process of Large Language Models (LLMs) to produce responses that closely mimic human behavior. Leveraging LLMs with RLHF for practical CCQA applications has thus emerged as a promising area of study. Unlike standard code question-answering tasks, CCQA involves multiple possible answers, with varying user preferences for each response. Additionally, code communities often show a preference for new APIs. These challenges prevent LLMs from generating responses that cater to the diverse preferences of users in CCQA tasks. To address these issues, we propose a novel framework called Aligning LLMs through Multi-perspective User Preference Ranking-based Feedback for Programming Question Answering (ALMupQA) to create user-focused responses. Our approach starts with Multi-perspective Preference Ranking Alignment (MPRA), which synthesizes varied user preferences based on the characteristics of answers from code communities. We then introduce a Retrieval-augmented In-context Learning (RIL) module to mitigate the problem of outdated answers by retrieving responses to similar questions from a question bank. Due to the limited availability of high-quality, multi-answer CCQA datasets, we also developed a dataset named StaCCQA from real code communities. Extensive experiments demonstrated the effectiveness of the ALMupQA framework in terms of accuracy and user preference. Compared to the base model, ALMupQA showed nearly an 11% improvement in BLEU, with increases of 20% and 17.5% in BERTScore and CodeBERTScore, respectively.\n\n\nAligning LLMs through Multi-perspective User Preference Ranking-based Feedback for Programming Question Answering</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.00037v1</guid>
      <dc:creator>Hongyu Yang, Liyang He, Min Hou, Shuanghong Shen, Rui Li, Jiahui Hou, Jianhui Ma, Junda Zhao</dc:creator>
      <pubDate>Mon, 27 May 2024 14:21:31 GMT</pubDate>
    </item>
    <item>
      <title>GREEN: Generative Radiology Report Evaluation and Error Notation</title>
      <link>http://arxiv.org/abs/2405.03595v1</link>
      <description>Evaluating radiology reports is a challenging problem as factual correctness is extremely important due to the need for accurate medical communication about medical images. Existing automatic evaluation metrics either suffer from failing to consider factual correctness (e.g., BLEU and ROUGE) or are limited in their interpretability (e.g., F1CheXpert and F1RadGraph). In this paper, we introduce GREEN (Generative Radiology Report Evaluation and Error Notation), a radiology report generation metric that leverages the natural language understanding of language models to identify and explain clinically significant errors in candidate reports, both quantitatively and qualitatively. Compared to current metrics, GREEN offers: 1) a score aligned with expert preferences, 2) human interpretable explanations of clinically significant errors, enabling feedback loops with end-users, and 3) a lightweight open-source method that reaches the performance of commercial counterparts. We validate our GREEN metric by comparing it to GPT-4, as well as to error counts of 6 experts and preferences of 2 experts. Our method demonstrates not only higher correlation with expert error counts, but simultaneously higher alignment with expert preferences when compared to previous approaches.&quot;\n\n\nGREEN: Generative Radiology Report Evaluation and Error Notation</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.03595v1</guid>
      <dc:creator>Sophie Ostmeier, Justin Xu, Zhihong Chen, Maya Varma, Louis Blankemeier, Christian Bluethgen, Arne Edward Michalson, Michael Moseley, Curtis Langlotz, Akshay S Chaudhari, Jean-Benoit Delbrouck</dc:creator>
      <pubDate>Mon, 06 May 2024 16:04:03 GMT</pubDate>
    </item>
    <item>
      <title>Disperse-Then-Merge: Pushing the Limits of Instruction Tuning via Alignment Tax Reduction</title>
      <link>http://arxiv.org/abs/2405.13432v1</link>
      <description>Supervised fine-tuning (SFT) on instruction-following corpus is a crucial approach toward the alignment of large language models (LLMs). However, the performance of LLMs on standard knowledge and reasoning benchmarks tends to suffer from deterioration at the latter stage of the SFT process, echoing the phenomenon of alignment tax. Through our pilot study, we put a hypothesis that the data biases are probably one cause behind the phenomenon. To address the issue, we introduce a simple disperse-then-merge framework. To be concrete, we disperse the instruction-following data into portions and train multiple sub-models using different data portions. Then we merge multiple models into a single one via model merging techniques. Despite its simplicity, our framework outperforms various sophisticated methods such as data curation and training regularization on a series of standard knowledge and reasoning benchmarks.\n\n\nDisperse-Then-Merge: Pushing the Limits of Instruction Tuning via Alignment Tax Reduction</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.13432v1</guid>
      <dc:creator>Tingchen Fu, Deng Cai, Lemao Liu, Shuming Shi, Rui Yan</dc:creator>
      <pubDate>Wed, 22 May 2024 08:18:19 GMT</pubDate>
    </item>
    <item>
      <title>Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions</title>
      <link>http://arxiv.org/abs/2406.09264v2</link>
      <description>Recent advancements in general-purpose AI have highlighted the importance of guiding AI systems towards the intended goals, ethical principles, and values of individuals and groups, a concept broadly recognized as alignment. However, the lack of clarified definitions and scopes of human-AI alignment poses a significant obstacle, hampering collaborative efforts across research domains to achieve this alignment. In particular, ML- and philosophy-oriented alignment research often views AI alignment as a static, unidirectional process (i.e., aiming to ensure that AI systems' objectives match humans) rather than an ongoing, mutual alignment problem [429]. This perspective largely neglects the long-term interaction and dynamic changes of alignment. To understand these gaps, we introduce a systematic review of over 400 papers published between 2019 and January 2024, spanning multiple domains such as Human-Computer Interaction (HCI), Natural Language Processing (NLP), Machine Learning (ML), and others. We characterize, define and scope human-AI alignment. From this, we present a conceptual framework of &quot;Bidirectional Human-AI Alignment&quot; to organize the literature from a human-centered perspective. This framework encompasses both 1) conventional studies of aligning AI to humans that ensures AI produces the intended outcomes determined by humans, and 2) a proposed concept of aligning humans to AI, which aims to help individuals and society adjust to AI advancements both cognitively and behaviorally. Additionally, we articulate the key findings derived from literature analysis, including discussions about human values, interaction techniques, and evaluations. To pave the way for future studies, we envision three key challenges for future directions and propose examples of potential future solutions.\n\n\nTowards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.09264v2</guid>
      <dc:creator>Hua Shen, Tiffany Knearem, Reshmi Ghosh, Kenan Alkiek, Kundan Krishna, Yachuan Liu, Ziqiao Ma, Savvas Petridis, Yi-Hao Peng, Li Qiwei, Sushrita Rakshit, Chenglei Si, Yutong Xie, Jeffrey P. Bigham, Frank Bentley, Joyce Chai, Zachary Lipton, Qiaozhu Mei, Rada Mihalcea, Michael Terry, Diyi Yang, Meredith Ringel Morris, Paul Resnick, David Jurgens</dc:creator>
      <pubDate>Mon, 17 Jun 2024 16:58:35 GMT</pubDate>
    </item>
    <item>
      <title>Themis: Towards Flexible and Interpretable NLG Evaluation</title>
      <link>http://arxiv.org/abs/2406.18365v1</link>
      <description>The evaluation of natural language generation (NLG) tasks is a significant and longstanding research issue. With the recent emergence of powerful large language models (LLMs), some studies have turned to LLM-based automatic evaluation methods, which demonstrate great potential to become a new evaluation paradigm following traditional string-based and model-based metrics. However, despite the improved performance of existing methods, they still possess some deficiencies, such as dependency on references and limited evaluation flexibility. Therefore, in this paper, we meticulously construct a large-scale NLG evaluation corpus NLG-Eval with human and GPT-4 annotations to alleviate the lack of relevant data in this field. Furthermore, we propose Themis, an LLM dedicated to NLG evaluation, which has been trained with our designed multi-perspective consistency and rating-oriented preference alignment methods. Themis can conduct flexible and interpretable evaluations without references, and it exhibits superior evaluation performance on various NLG tasks, simultaneously generalizing well to unseen tasks and surpassing other evaluation models, including GPT-4.\n\n\nThemis: Towards Flexible and Interpretable NLG Evaluation</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.18365v1</guid>
      <dc:creator>Xinyu Hu, Li Lin, Mingqi Gao, Xunjian Yin, Xiaojun Wan</dc:creator>
      <pubDate>Wed, 26 Jun 2024 14:04:29 GMT</pubDate>
    </item>
    <item>
      <title>Can LLM Graph Reasoning Generalize beyond Pattern Memorization?</title>
      <link>http://arxiv.org/abs/2406.15992v1</link>
      <description>Large language models (LLMs) demonstrate great potential for problems with implicit graphical structures, while recent works seek to enhance the graph reasoning capabilities of LLMs through specialized instruction tuning. The resulting 'graph LLMs' are evaluated with in-distribution settings only, thus it remains underexplored whether LLMs are learning generalizable graph reasoning skills or merely memorizing patterns in the synthetic training data. To this end, we propose the NLGift benchmark, an evaluation suite of LLM graph reasoning generalization: whether LLMs could go beyond semantic, numeric, structural, reasoning patterns in the synthetic training data and improve utility on real-world graph-based tasks. Extensive experiments with two LLMs across four graph reasoning tasks demonstrate that while generalization on simple patterns (semantic, numeric) is somewhat satisfactory, LLMs struggle to generalize across reasoning and real-world patterns, casting doubt on the benefit of synthetic graph tuning for real-world tasks with underlying network structures. We explore three strategies to improve LLM graph reasoning generalization, and we find that while post-training alignment is most promising for real-world tasks, empowering LLM graph reasoning to go beyond pattern memorization remains an open research question.\n\n\nCan LLM Graph Reasoning Generalize beyond Pattern Memorization?</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.15992v1</guid>
      <dc:creator>Yizhuo Zhang, Heng Wang, Shangbin Feng, Zhaoxuan Tan, Xiaochuang Han, Tianxing He, Yulia Tsvetkov</dc:creator>
      <pubDate>Sun, 23 Jun 2024 02:59:15 GMT</pubDate>
    </item>
    <item>
      <title>User Modeling in the Era of Large Language Models: Current Research and Future Directions</title>
      <link>http://arxiv.org/abs/2312.11518v2</link>
      <description>User modeling (UM) aims to discover patterns or learn representations from user data about the characteristics of a specific user, such as profile, preference, and personality. The user models enable personalization and suspiciousness detection in many online applications such as recommendation, education, and healthcare. Two common types of user data are text and graph, as the data usually contain a large amount of user-generated content (UGC) and online interactions. The research of text and graph mining is developing rapidly, contributing many notable solutions in the past two decades. Recently, large language models (LLMs) have shown superior performance on generating, understanding, and even reasoning over text data. The approaches of user modeling have been equipped with LLMs and soon become outstanding. This article summarizes existing research about how and why LLMs are great tools of modeling and understanding UGC. Then it reviews a few categories of large language models for user modeling (LLM-UM) approaches that integrate the LLMs with text and graph-based methods in different ways. Then it introduces specific LLM-UM techniques for a variety of UM applications. Finally, it presents remaining challenges and future directions in the LLM-UM research. We maintain the reading list at: https://github.com/TamSiuhin/LLM-UM-Reading\n\n\nResearch on Large Language Model for Coal Mine Equipment Maintenance Based on Multi-Source Text</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2312.11518v2</guid>
      <dc:creator>Zhaoxuan Tan, Meng Jiang</dc:creator>
      <pubDate>Sat, 23 Dec 2023 21:39:52 GMT</pubDate>
    </item>
    <item>
      <title>DIPPER: Direct Preference Optimization to Accelerate Primitive-Enabled Hierarchical Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2406.10892v1</link>
      <description>Learning control policies to perform complex robotics tasks from human preference data presents significant challenges. On the one hand, the complexity of such tasks typically requires learning policies to perform a variety of subtasks, then combining them to achieve the overall goal. At the same time, comprehensive, well-engineered reward functions are typically unavailable in such problems, while limited human preference data often is; making efficient use of such data to guide learning is therefore essential. Methods for learning to perform complex robotics tasks from human preference data must overcome both these challenges simultaneously. In this work, we introduce DIPPER: Direct Preference Optimization to Accelerate Primitive-Enabled Hierarchical Reinforcement Learning, an efficient hierarchical approach that leverages direct preference optimization to learn a higher-level policy and reinforcement learning to learn a lower-level policy. DIPPER enjoys improved computational efficiency due to its use of direct preference optimization instead of standard preference-based approaches such as reinforcement learning from human feedback, while it also mitigates the well-known hierarchical reinforcement learning issues of non-stationarity and infeasible subgoal generation due to our use of primitive-informed regularization inspired by a novel bi-level optimization formulation of the hierarchical reinforcement learning problem. To validate our approach, we perform extensive experimental analysis on a variety of challenging robotics tasks, demonstrating that DIPPER outperforms hierarchical and non-hierarchical baselines, while ameliorating the non-stationarity and infeasible subgoal generation issues of hierarchical reinforcement learning.\n\n\nDIPPER: Direct Preference Optimization to Accelerate Primitive-Enabled Hierarchical Reinforcement Learning</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.10892v1</guid>
      <dc:creator>Utsav Singh, Souradip Chakraborty, Wesley A. Suttle, Brian M. Sadler, Vinay P Namboodiri, Amrit Singh Bedi</dc:creator>
      <pubDate>Sun, 16 Jun 2024 10:49:41 GMT</pubDate>
    </item>
    <item>
      <title>Lazy Safety Alignment for Large Language Models against Harmful Fine-tuning</title>
      <link>http://arxiv.org/abs/2405.18641v4</link>
      <description>Recent studies show that Large Language Models (LLMs) with safety alignment can be jail-broken by fine-tuning on a dataset mixed with harmful data. First time in the literature, we show that the jail-broken effect can be mitigated by separating states in the finetuning stage to optimize the alignment and user datasets. Unfortunately, our subsequent study shows that this simple Bi-State Optimization (BSO) solution experiences convergence instability when steps invested in its alignment state is too small, leading to downgraded alignment performance. By statistical analysis, we show that the \textit{excess drift} towards consensus could be a probable reason for the instability. To remedy this issue, we propose \textbf{L}azy(\textbf{i}) \textbf{s}afety \textbf{a}lignment (\textbf{Lisa}), which introduces a proximal term to constraint the drift of each state. Theoretically, the benefit of the proximal term is supported by the convergence analysis, wherein we show that a sufficient large proximal factor is necessary to guarantee Lisa's convergence. Empirically, our results on four downstream finetuning tasks show that Lisa with a proximal term can significantly increase alignment performance while maintaining the LLM's accuracy on the user tasks. Code is available at \url{https://github.com/git-disl/Lisa}.\n\n\nLazy Safety Alignment for Large Language Models against Harmful Fine-tuning</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.18641v4</guid>
      <dc:creator>Tiansheng Huang, Sihao Hu, Fatih Ilhan, Selim Furkan Tekin, Ling Liu</dc:creator>
      <pubDate>Wed, 26 Jun 2024 18:54:59 GMT</pubDate>
    </item>
    <item>
      <title>Legend: Leveraging Representation Engineering to Annotate Safety Margin for Preference Datasets</title>
      <link>http://arxiv.org/abs/2406.08124v1</link>
      <description>The success of the reward model in distinguishing between responses with subtle safety differences depends critically on the high-quality preference dataset, which should capture the fine-grained nuances of harmful and harmless responses. This motivates the need to develop a dataset involving preference margins, which accurately quantify how harmless one response is compared to another. In this paper, we take the first step to propose an effective and cost-efficient framework to promote the margin-enhanced preference dataset development. Our framework, Legend, Leverages representation engineering to annotate preference datasets. It constructs the specific direction within the LLM's embedding space that represents safety. By leveraging this safety direction, Legend can then leverage the semantic distances of paired responses along this direction to annotate margins automatically. We experimentally demonstrate our effectiveness in both reward modeling and harmless alignment for LLMs. Legend also stands out for its efficiency, requiring only the inference time rather than additional training. This efficiency allows for easier implementation and scalability, making Legend particularly valuable for practical applications in aligning LLMs with safe conversations.\n\n\nLegend: Leveraging Representation Engineering to Annotate Safety Margin for Preference Datasets</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.08124v1</guid>
      <dc:creator>Duanyu Feng, Bowen Qin, Chen Huang, Youcheng Huang, Zheng Zhang, Wenqiang Lei</dc:creator>
      <pubDate>Wed, 12 Jun 2024 12:06:32 GMT</pubDate>
    </item>
    <item>
      <title>MACAROON: Training Vision-Language Models To Be Your Engaged Partners</title>
      <link>http://arxiv.org/abs/2406.14137v1</link>
      <description>Large vision-language models (LVLMs), while proficient in following instructions and responding to diverse questions, invariably generate detailed responses even when questions are ambiguous or unanswerable, leading to hallucinations and bias issues. Thus, it is essential for LVLMs to proactively engage with humans to ask for clarifications or additional information for better responses. In this study, we aim to shift LVLMs from passive answer providers to proactive engaged partners. We begin by establishing a three-tiered hierarchy for questions of invalid, ambiguous, and personalizable nature to measure the proactive engagement capabilities of LVLMs. Utilizing this hierarchy, we create PIE, (ProactIve Engagement Evaluation) through GPT-4o and human annotators, consisting of 853 questions across six distinct, fine-grained question types that are verified by human annotators and accompanied with well-defined metrics. Our evaluations on \benchmark indicate poor performance of existing LVLMs, with the best-performing open-weights model only achieving an Aggregate Align Rate (AAR) of 0.28. In response, we introduce MACAROON, self-iMaginAtion for ContrAstive pReference OptimizatiON, which instructs LVLMs to autonomously generate contrastive response pairs for unlabeled questions given the task description and human-crafted criteria. Then, the self-imagined data is formatted for conditional reinforcement learning. Experimental results show MACAROON effectively improves LVLMs' capabilities to be proactively engaged (0.84 AAR) while maintaining comparable performance on general tasks.\n\n\nMACAROON: Training Vision-Language Models To Be Your Engaged Partners</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.14137v1</guid>
      <dc:creator>Shujin Wu, Yi R. Fung, Sha Li, Yixin Wan, Kai-Wei Chang, Heng Ji</dc:creator>
      <pubDate>Thu, 20 Jun 2024 09:27:33 GMT</pubDate>
    </item>
    <item>
      <title>Intrinsic Evaluation of Unlearning Using Parametric Knowledge Traces</title>
      <link>http://arxiv.org/abs/2406.11614v1</link>
      <description>The task of &quot;unlearning&quot; certain concepts in large language models (LLMs) has attracted immense attention recently, due to its importance for mitigating undesirable model behaviours, such as the generation of harmful, private, or incorrect information. Current protocols to evaluate unlearning methods largely rely on behavioral tests, without monitoring the presence of unlearned knowledge within the model's parameters. This residual knowledge can be adversarially exploited to recover the erased information post-unlearning. We argue that unlearning should also be evaluated internally, by considering changes in the parametric knowledge traces of the unlearned concepts. To this end, we propose a general methodology for eliciting directions in the parameter space (termed &quot;concept vectors&quot;) that encode concrete concepts, and construct ConceptVectors, a benchmark dataset containing hundreds of common concepts and their parametric knowledge traces within two open-source LLMs. Evaluation on ConceptVectors shows that existing unlearning methods minimally impact concept vectors, while directly ablating these vectors demonstrably removes the associated knowledge from the LLMs and significantly reduces their susceptibility to adversarial manipulation. Our results highlight limitations in behavioral-based unlearning evaluations and call for future work to include parametric-based evaluations. To support this, we release our code and benchmark at https://github.com/yihuaihong/ConceptVectors.\n\n\nIntrinsic Evaluation of Unlearning Using Parametric Knowledge Traces</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.11614v1</guid>
      <dc:creator>Yihuai Hong, Lei Yu, Shauli Ravfogel, Haiqin Yang, Mor Geva</dc:creator>
      <pubDate>Mon, 17 Jun 2024 15:00:35 GMT</pubDate>
    </item>
    <item>
      <title>Contrastive Policy Gradient: Aligning LLMs on sequence-level scores in a supervised-friendly fashion</title>
      <link>http://arxiv.org/abs/2406.19185v1</link>
      <description>Reinforcement Learning (RL) has been used to finetune Large Language Models (LLMs) using a reward model trained from preference data, to better align with human judgment. The recently introduced direct alignment methods, which are often simpler, more stable, and computationally lighter, can more directly achieve this. However, these approaches cannot optimize arbitrary rewards, and the preference-based ones are not the only rewards of interest for LLMs (eg., unit tests for code generation or textual entailment for summarization, among others). RL-finetuning is usually done with a variation of policy gradient, which calls for on-policy or near-on-policy samples, requiring costly generations. We introduce Contrastive Policy Gradient, or CoPG, a simple and mathematically principled new RL algorithm that can estimate the optimal policy even from off-policy data. It can be seen as an off-policy policy gradient approach that does not rely on important sampling techniques and highlights the importance of using (the right) state baseline. We show this approach to generalize the direct alignment method IPO (identity preference optimization) and classic policy gradient. We experiment with the proposed CoPG on a toy bandit problem to illustrate its properties, as well as for finetuning LLMs on a summarization task, using a learned reward function considered as ground truth for the purpose of the experiments.\n\n\nContrastive Policy Gradient: Aligning LLMs on sequence-level scores in a supervised-friendly fashion</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.19185v1</guid>
      <dc:creator>Yannis Flet-Berliac, Nathan Grinsztajn, Florian Strub, Eugene Choi, Chris Cremer, Arash Ahmadian, Yash Chandak, Mohammad Gheshlaghi Azar, Olivier Pietquin, Matthieu Geist</dc:creator>
      <pubDate>Thu, 27 Jun 2024 14:03:49 GMT</pubDate>
    </item>
    <item>
      <title>The Responsible Foundation Model Development Cheatsheet: A Review of Tools &amp; Resources</title>
      <link>http://arxiv.org/abs/2406.16746v2</link>
      <description>Foundation model development attracts a rapidly expanding body of contributors, scientists, and applications. To help shape responsible development practices, we introduce the Foundation Model Development Cheatsheet: a growing collection of 250+ tools and resources spanning text, vision, and speech modalities. We draw on a large body of prior work to survey resources (e.g. software, documentation, frameworks, guides, and practical tools) that support informed data selection, processing, and understanding, precise and limitation-aware artifact documentation, efficient model training, advance awareness of the environmental impact from training, careful model evaluation of capabilities, risks, and claims, as well as responsible model release, licensing and deployment practices. We hope this curated collection of resources helps guide more responsible development. The process of curating this list, enabled us to review the AI development ecosystem, revealing what tools are critically missing, misused, or over-used in existing practices. We find that (i) tools for data sourcing, model evaluation, and monitoring are critically under-serving ethical and real-world needs, (ii) evaluations for model safety, capabilities, and environmental impact all lack reproducibility and transparency, (iii) text and particularly English-centric analyses continue to dominate over multilingual and multi-modal analyses, and (iv) evaluation of systems, rather than just models, is needed so that capabilities and impact are assessed in context.\n\n\nThe Responsible Foundation Model Development Cheatsheet: A Review of Tools &amp; Resources</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.16746v2</guid>
      <dc:creator>Shayne Longpre, Stella Biderman, Alon Albalak, Hailey Schoelkopf, Daniel McDuff, Sayash Kapoor, Kevin Klyman, Kyle Lo, Gabriel Ilharco, Nay San, Maribeth Rauh, Aviya Skowron, Bertie Vidgen, Laura Weidinger, Arvind Narayanan, Victor Sanh, David Adelani, Percy Liang, Rishi Bommasani, Peter Henderson, Sasha Luccioni, Yacine Jernite, Luca Soldaini</dc:creator>
      <pubDate>Wed, 26 Jun 2024 02:19:01 GMT</pubDate>
    </item>
    <item>
      <title>NIFTY Financial News Headlines Dataset</title>
      <link>http://arxiv.org/abs/2405.09747v1</link>
      <description>We introduce and make publicly available the NIFTY Financial News Headlines dataset, designed to facilitate and advance research in financial market forecasting using large language models (LLMs). This dataset comprises two distinct versions tailored for different modeling approaches: (i) NIFTY-LM, which targets supervised fine-tuning (SFT) of LLMs with an auto-regressive, causal language-modeling objective, and (ii) NIFTY-RL, formatted specifically for alignment methods (like reinforcement learning from human feedback (RLHF)) to align LLMs via rejection sampling and reward modeling. Each dataset version provides curated, high-quality data incorporating comprehensive metadata, market indices, and deduplicated financial news headlines systematically filtered and ranked to suit modern LLM frameworks. We also include experiments demonstrating some applications of the dataset in tasks like stock price movement and the role of LLM embeddings in information acquisition/richness. The NIFTY dataset along with utilities (like truncating prompt's context length systematically) are available on Hugging Face at https://huggingface.co/datasets/raeidsaqur/NIFTY.\n\n\nNIFTY Financial News Headlines Dataset</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.09747v1</guid>
      <dc:creator>Raeid Saqur, Ken Kato, Nicholas Vinden, Frank Rudzicz</dc:creator>
      <pubDate>Thu, 16 May 2024 01:09:33 GMT</pubDate>
    </item>
    <item>
      <title>TRAP: Targeted Random Adversarial Prompt Honeypot for Black-Box Identification</title>
      <link>http://arxiv.org/abs/2402.12991v2</link>
      <description>Large Language Model (LLM) services and models often come with legal rules on who can use them and how they must use them. Assessing the compliance of the released LLMs is crucial, as these rules protect the interests of the LLM contributor and prevent misuse. In this context, we describe the novel fingerprinting problem of Black-box Identity Verification (BBIV). The goal is to determine whether a third-party application uses a certain LLM through its chat function. We propose a method called Targeted Random Adversarial Prompt (TRAP) that identifies the specific LLM in use. We repurpose adversarial suffixes, originally proposed for jailbreaking, to get a pre-defined answer from the target LLM, while other models give random answers. TRAP detects the target LLMs with over 95% true positive rate at under 0.2% false positive rate even after a single interaction. TRAP remains effective even if the LLM has minor changes that do not significantly alter the original function.\n\n\nTRAP: Targeted Random Adversarial Prompt Honeypot for Black-Box Identification</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.12991v2</guid>
      <dc:creator>Martin Gubri, Dennis Ulmer, Hwaran Lee, Sangdoo Yun, Seong Joon Oh</dc:creator>
      <pubDate>Thu, 06 Jun 2024 17:46:48 GMT</pubDate>
    </item>
    <item>
      <title>Active Preference Learning for Large Language Models</title>
      <link>http://arxiv.org/abs/2402.08114v2</link>
      <description>As large language models (LLMs) become more capable, fine-tuning techniques for aligning with human intent are increasingly important. A key consideration for aligning these models is how to most effectively use human resources, or model resources in the case where LLMs themselves are used as oracles. Reinforcement learning from Human or AI preferences (RLHF/RLAIF) is the most prominent example of such a technique, but is complex and often unstable. Direct Preference Optimization (DPO) has recently been proposed as a simpler and more stable alternative. In this work, we develop an active learning strategy for DPO to make better use of preference labels. We propose a practical acquisition function for prompt/completion pairs based on the predictive entropy of the language model and a measure of certainty of the implicit preference model optimized by DPO. We demonstrate how our approach improves both the rate of learning and final performance of fine-tuning on pairwise preference data.\n\n\nActive Preference Learning for Large Language Models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.08114v2</guid>
      <dc:creator>William Muldrew, Peter Hayes, Mingtian Zhang, David Barber</dc:creator>
      <pubDate>Fri, 28 Jun 2024 08:22:01 GMT</pubDate>
    </item>
    <item>
      <title>Reinforcement Learning from Human Feedback without Reward Inference: Model-Free Algorithm and Instance-Dependent Analysis</title>
      <link>http://arxiv.org/abs/2406.07455v1</link>
      <description>In this paper, we study reinforcement learning from human feedback (RLHF) under an episodic Markov decision process with a general trajectory-wise reward model. We developed a model-free RLHF best policy identification algorithm, called $\mathsf{BSAD}$, without explicit reward model inference, which is a critical intermediate step in the contemporary RLHF paradigms for training large language models (LLM). The algorithm identifies the optimal policy directly from human preference information in a backward manner, employing a dueling bandit sub-routine that constantly duels actions to identify the superior one. $\mathsf{BSAD}$ adopts a reward-free exploration and best-arm-identification-like adaptive stopping criteria to equalize the visitation among all states in the same decision step while moving to the previous step as soon as the optimal action is identifiable, leading to a provable, instance-dependent sample complexity $\tilde{\mathcal{O}}(c_{\mathcal{M}}SA^3H^3M\log\frac{1}{\delta})$ which resembles the result in classic RL, where $c_{\mathcal{M}}$ is the instance-dependent constant and $M$ is the batch size. Moreover, $\mathsf{BSAD}$ can be transformed into an explore-then-commit algorithm with logarithmic regret and generalized to discounted MDPs using a frame-based approach. Our results show: (i) sample-complexity-wise, RLHF is not significantly harder than classic RL and (ii) end-to-end RLHF may deliver improved performance by avoiding pitfalls in reward inferring such as overfit and distribution shift.\n\n\nReinforcement Learning from Human Feedback without Reward Inference: Model-Free Algorithm and Instance-Dependent Analysis</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.07455v1</guid>
      <dc:creator>Qining Zhang, Honghao Wei, Lei Ying</dc:creator>
      <pubDate>Tue, 11 Jun 2024 17:01:41 GMT</pubDate>
    </item>
    <item>
      <title>Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing</title>
      <link>http://arxiv.org/abs/2406.08464v1</link>
      <description>High-quality instruction data is critical for aligning large language models (LLMs). Although some models, such as Llama-3-Instruct, have open weights, their alignment data remain private, which hinders the democratization of AI. High human labor costs and a limited, predefined scope for prompting prevent existing open-source data creation methods from scaling effectively, potentially limiting the diversity and quality of public alignment datasets. Is it possible to synthesize high-quality instruction data at scale by extracting it directly from an aligned LLM? We present a self-synthesis method for generating large-scale alignment data named Magpie. Our key observation is that aligned LLMs like Llama-3-Instruct can generate a user query when we input only the left-side templates up to the position reserved for user messages, thanks to their auto-regressive nature. We use this method to prompt Llama-3-Instruct and generate 4 million instructions along with their corresponding responses. We perform a comprehensive analysis of the extracted data and select 300K high-quality instances. To compare Magpie data with other public instruction datasets, we fine-tune Llama-3-8B-Base with each dataset and evaluate the performance of the fine-tuned models. Our results indicate that in some tasks, models fine-tuned with Magpie perform comparably to the official Llama-3-8B-Instruct, despite the latter being enhanced with 10 million data points through supervised fine-tuning (SFT) and subsequent feedback learning. We also show that using Magpie solely for SFT can surpass the performance of previous public datasets utilized for both SFT and preference optimization, such as direct preference optimization with UltraFeedback. This advantage is evident on alignment benchmarks such as AlpacaEval, ArenaHard, and WildBench.\n\n\nMagpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.08464v1</guid>
      <dc:creator>Zhangchen Xu, Fengqing Jiang, Luyao Niu, Yuntian Deng, Radha Poovendran, Yejin Choi, Bill Yuchen Lin</dc:creator>
      <pubDate>Wed, 12 Jun 2024 17:52:30 GMT</pubDate>
    </item>
    <item>
      <title>Group Robust Preference Optimization in Reward-free RLHF</title>
      <link>http://arxiv.org/abs/2405.20304v1</link>
      <description>Adapting large language models (LLMs) for specific tasks usually involves fine-tuning through reinforcement learning with human feedback (RLHF) on preference data. While these data often come from diverse labelers' groups (e.g., different demographics, ethnicities, company teams, etc.), traditional RLHF approaches adopt a &quot;one-size-fits-all&quot; approach, i.e., they indiscriminately assume and optimize a single preference model, thus not being robust to unique characteristics and needs of the various groups. To address this limitation, we propose a novel Group Robust Preference Optimization (GRPO) method to align LLMs to individual groups' preferences robustly. Our approach builds upon reward-free direct preference optimization methods, but unlike previous approaches, it seeks a robust policy which maximizes the worst-case group performance. To achieve this, GRPO adaptively and sequentially weights the importance of different groups, prioritizing groups with worse cumulative loss. We theoretically study the feasibility of GRPO and analyze its convergence for the log-linear policy class. By fine-tuning LLMs with GRPO using diverse group-based global opinion data, we significantly improved performance for the worst-performing groups, reduced loss imbalances across groups, and improved probability accuracies compared to non-robust baselines.\n\n\nGroup Robust Preference Optimization in Reward-free RLHF</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.20304v1</guid>
      <dc:creator>Shyam Sundhar Ramesh, Yifan Hu, Iason Chaimalas, Viraj Mehta, Pier Giuseppe Sessa, Haitham Bou Ammar, Ilija Bogunovic</dc:creator>
      <pubDate>Thu, 30 May 2024 17:50:04 GMT</pubDate>
    </item>
    <item>
      <title>Rethinking the Role of Proxy Rewards in Language Model Alignment</title>
      <link>http://arxiv.org/abs/2402.03469v2</link>
      <description>Learning from human feedback via proxy reward modeling has been studied to align Large Language Models (LLMs) with human values. However, achieving reliable training through that proxy reward model (RM) is not a trivial problem, and its behavior remained as a black-box. In this paper, we study the role of proxy rewards in the LLM alignment via `reverse reward engineering' by composing interpretable features as a white-box reward function. We aim to replicate the ground truth (gold) reward signal by achieving a monotonic relationship between the proxy and gold reward signals after training the model using the proxy reward in reinforcement learning (RL). Our findings indicate that successfully emulating the gold reward requires generating responses that are relevant with enough length to open-ended questions, while also ensuring response consistency in closed-ended questions. Furthermore, resulting models optimizing our devised white-box reward show competitive performances with strong open-source RMs in alignment benchmarks. We highlight its potential usage as a simple but strong reward baseline for the LLM alignment, not requiring explicit human feedback dataset and RM training. Our code is available at https://github.com/naver-ai/rethinking-proxy-reward.\n\n\nCost-Effective Proxy Reward Model Construction with On-Policy and Active Learning</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.03469v2</guid>
      <dc:creator>Sungdong Kim, Minjoon Seo</dc:creator>
      <pubDate>Mon, 29 Apr 2024 01:09:06 GMT</pubDate>
    </item>
    <item>
      <title>The Poison of Alignment</title>
      <link>http://arxiv.org/abs/2308.13449v1</link>
      <description>From the perspective of content safety issues, alignment has shown to limit large language models' (LLMs) harmful content generation. This intentional method of reinforcing models to not respond to certain user inputs seem to be present in many modern open-source instruction tuning datasets such as OpenAssistant or Guanaco. We introduce a novel insight to an instruction-tuned model's performance affected by the presence of alignment in supervised fine-tuning dataset. To be specific, we noticed that alignment acts as if it is poisoning the instruction dataset. Experimentally, we demonstrate that aligned answers significantly worsen the performance of the resulting fine-tuned model's on various reasoning benchmarks such as Big Bench (BBH), Massive Multitask Language Understanding (MMLU), Human Eval, and Discrete Reasoning Over Paragraphs (DROP), performing worse than the counterpart tuned without alignment by 4-33%.\n\n\nAligning Large Language Models via Fine-grained Supervision</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2308.13449v1</guid>
      <dc:creator>Aibek Bekbayev, Sungbae Chun, Yerzat Dulat, James Yamazaki</dc:creator>
      <pubDate>Fri, 25 Aug 2023 15:51:15 GMT</pubDate>
    </item>
    <item>
      <title>GoEX: Perspectives and Designs Towards a Runtime for Autonomous LLM Applications</title>
      <link>http://arxiv.org/abs/2404.06921v1</link>
      <description>Large Language Models (LLMs) are evolving beyond their classical role of providing information within dialogue systems to actively engaging with tools and performing actions on real-world applications and services. Today, humans verify the correctness and appropriateness of the LLM-generated outputs (e.g., code, functions, or actions) before putting them into real-world execution. This poses significant challenges as code comprehension is well known to be notoriously difficult. In this paper, we study how humans can efficiently collaborate with, delegate to, and supervise autonomous LLMs in the future. We argue that in many cases, &quot;post-facto validation&quot; - verifying the correctness of a proposed action after seeing the output - is much easier than the aforementioned &quot;pre-facto validation&quot; setting. The core concept behind enabling a post-facto validation system is the integration of an intuitive undo feature, and establishing a damage confinement for the LLM-generated actions as effective strategies to mitigate the associated risks. Using this, a human can now either revert the effect of an LLM-generated output or be confident that the potential risk is bounded. We believe this is critical to unlock the potential for LLM agents to interact with applications and services with limited (post-facto) human involvement. We describe the design and implementation of our open-source runtime for executing LLM actions, Gorilla Execution Engine (GoEX), and present open research questions towards realizing the goal of LLMs and applications interacting with each other with minimal human supervision. We release GoEX at https://github.com/ShishirPatil/gorilla/.\n\n\nGoEX: Perspectives and Designs Towards a Runtime for Autonomous LLM Applications</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2404.06921v1</guid>
      <dc:creator>Shishir G. Patil, Tianjun Zhang, Vivian Fang, Noppapon C., Roy Huang, Aaron Hao, Martin Casado, Joseph E. Gonzalez, Raluca Ada Popa, Ion Stoica</dc:creator>
      <pubDate>Wed, 10 Apr 2024 11:17:33 GMT</pubDate>
    </item>
    <item>
      <title>PAL: Pluralistic Alignment Framework for Learning from Heterogeneous Preferences</title>
      <link>http://arxiv.org/abs/2406.08469v1</link>
      <description>Large foundation models pretrained on raw web-scale data are not readily deployable without additional step of extensive alignment to human preferences. Such alignment is typically done by collecting large amounts of pairwise comparisons from humans (&quot;Do you prefer output A or B?&quot;) and learning a reward model or a policy with the Bradley-Terry-Luce (BTL) model as a proxy for a human's underlying implicit preferences. These methods generally suffer from assuming a universal preference shared by all humans, which lacks the flexibility of adapting to plurality of opinions and preferences. In this work, we propose PAL, a framework to model human preference complementary to existing pretraining strategies, which incorporates plurality from the ground up. We propose using the ideal point model as a lens to view alignment using preference comparisons. Together with our novel reformulation and using mixture modeling, our framework captures the plurality of population preferences while simultaneously learning a common preference latent space across different preferences, which can few-shot generalize to new, unseen users. Our approach enables us to use the penultimate-layer representation of large foundation models and simple MLP layers to learn reward functions that are on-par with the existing large state-of-the-art reward models, thereby enhancing efficiency of reward modeling significantly. We show that PAL achieves competitive reward model accuracy compared to strong baselines on 1) Language models with Summary dataset ; 2) Image Generative models with Pick-a-Pic dataset ; 3) A new semisynthetic heterogeneous dataset generated using Anthropic Personas. Finally, our experiments also highlight the shortcoming of current preference datasets that are created using rigid rubrics which wash away heterogeneity, and call for more nuanced data collection approaches.\n\n\nPAL: Pluralistic Alignment Framework for Learning from Heterogeneous Preferences</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.08469v1</guid>
      <dc:creator>Daiwei Chen, Yi Chen, Aniket Rege, Ramya Korlakai Vinayak</dc:creator>
      <pubDate>Wed, 12 Jun 2024 17:54:54 GMT</pubDate>
    </item>
    <item>
      <title>LLMs-as-Instructors: Learning from Errors Toward Automating Model Improvement</title>
      <link>http://arxiv.org/abs/2407.00497v1</link>
      <description>This paper introduces the innovative &quot;LLMs-as-Instructors&quot; framework, which leverages the advanced Large Language Models (LLMs) to autonomously enhance the training of smaller target models. Inspired by the theory of &quot;Learning from Errors&quot;, this framework employs an instructor LLM to meticulously analyze the specific errors within a target model, facilitating targeted and efficient training cycles. Within this framework, we implement two strategies: &quot;Learning from Error,&quot; which focuses solely on incorrect responses to tailor training data, and &quot;Learning from Error by Contrast&quot;, which uses contrastive learning to analyze both correct and incorrect responses for a deeper understanding of errors.   Our empirical studies, conducted with several open-source models, demonstrate significant improvements across multiple benchmarks, including mathematical reasoning, coding abilities, and factual knowledge. Notably, the refined Llama-3-8b-Instruction has outperformed ChatGPT, illustrating the effectiveness of our approach. By leveraging the strengths of both strategies, we have attained a more balanced performance improvement on both in-domain and out-of-domain benchmarks. Our code can be found at https://yingjiahao14.github.io/LLMs-as-Instructors-pages/.\n\n\nLLMs-as-Instructors: Learning from Errors Toward Automating Model Improvement</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2407.00497v1</guid>
      <dc:creator>Jiahao Ying, Mingbao Lin, Yixin Cao, Wei Tang, Bo Wang, Qianru Sun, Xuanjing Huang, Shuicheng Yan</dc:creator>
      <pubDate>Sat, 29 Jun 2024 17:16:04 GMT</pubDate>
    </item>
    <item>
      <title>Unchosen Experts Can Contribute Too: Unleashing MoE Models' Power by Self-Contrast</title>
      <link>http://arxiv.org/abs/2405.14507v1</link>
      <description>Mixture-of-Experts (MoE) has emerged as a prominent architecture for scaling model size while maintaining computational efficiency. In MoE, each token in the input sequence activates a different subset of experts determined by a routing mechanism. However, the unchosen experts in MoE models do not contribute to the output, potentially leading to underutilization of the model's capacity. In this work, we first conduct exploratory studies to demonstrate that increasing the number of activated experts does not necessarily improve and can even degrade the output quality. Then, we show that output distributions from an MoE model using different routing strategies substantially differ, indicating that different experts do not always act synergistically. Motivated by these findings, we propose Self-Contrast Mixture-of-Experts (SCMoE), a training-free strategy that utilizes unchosen experts in a self-contrast manner during inference. In SCMoE, the next-token probabilities are determined by contrasting the outputs from strong and weak activation using the same MoE model. Our method is conceptually simple and computationally lightweight, as it incurs minimal latency compared to greedy decoding. Experiments on several benchmarks (GSM8K, StrategyQA, MBPP and HumanEval) demonstrate that SCMoE can consistently enhance Mixtral 8x7B's reasoning capability across various domains. For example, it improves the accuracy on GSM8K from 61.79 to 66.94. Moreover, combining SCMoE with self-consistency yields additional gains, increasing major@20 accuracy from 75.59 to 78.31.\n\n\nUnchosen Experts Can Contribute Too: Unleashing MoE Models' Power by Self-Contrast</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.14507v1</guid>
      <dc:creator>Chufan Shi, Cheng Yang, Xinyu Zhu, Jiahao Wang, Taiqiang Wu, Siheng Li, Deng Cai, Yujiu Yang, Yu Meng</dc:creator>
      <pubDate>Thu, 23 May 2024 12:45:29 GMT</pubDate>
    </item>
    <item>
      <title>RichRAG: Crafting Rich Responses for Multi-faceted Queries in Retrieval-Augmented Generation</title>
      <link>http://arxiv.org/abs/2406.12566v2</link>
      <description>Retrieval-augmented generation (RAG) effectively addresses issues of static knowledge and hallucination in large language models. Existing studies mostly focus on question scenarios with clear user intents and concise answers. However, it is prevalent that users issue broad, open-ended queries with diverse sub-intents, for which they desire rich and long-form answers covering multiple relevant aspects. To tackle this important yet underexplored problem, we propose a novel RAG framework, namely RichRAG. It includes a sub-aspect explorer to identify potential sub-aspects of input questions, a multi-faceted retriever to build a candidate pool of diverse external documents related to these sub-aspects, and a generative list-wise ranker, which is a key module to provide the top-k most valuable documents for the final generator. These ranked documents sufficiently cover various query aspects and are aware of the generator's preferences, hence incentivizing it to produce rich and comprehensive responses for users. The training of our ranker involves a supervised fine-tuning stage to ensure the basic coverage of documents, and a reinforcement learning stage to align downstream LLM's preferences to the ranking of documents. Experimental results on two publicly available datasets prove that our framework effectively and efficiently provides comprehensive and satisfying responses to users.\n\n\nRichRAG: Crafting Rich Responses for Multi-faceted Queries in Retrieval-Augmented Generation</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.12566v2</guid>
      <dc:creator>Shuting Wang, Xin Yu, Mang Wang, Weipeng Chen, Yutao Zhu, Zhicheng Dou</dc:creator>
      <pubDate>Fri, 21 Jun 2024 18:12:15 GMT</pubDate>
    </item>
    <item>
      <title>Improving Arithmetic Reasoning Ability of Large Language Models through Relation Tuples, Verification and Dynamic Feedback</title>
      <link>http://arxiv.org/abs/2406.17873v1</link>
      <description>Current representations used in reasoning steps of large language models can mostly be categorized into two main types: (1) natural language, which is difficult to verify; and (2) non-natural language, usually programming code, which is difficult for people who are unfamiliar with coding to read. In this paper, we propose to use a semi-structured form to represent reasoning steps of large language models. Specifically, we use relation tuples, which are not only human-readable but also machine-friendly and easier to verify than natural language. We implement a framework that includes three main components: (1) introducing relation tuples into the reasoning steps of large language models; (2) implementing an automatic verification process of reasoning steps with a local code interpreter based on relation tuples; and (3) integrating a simple and effective dynamic feedback mechanism, which we found helpful for self-improvement of large language models. The experimental results on various arithmetic datasets demonstrate the effectiveness of our method in improving the arithmetic reasoning ability of large language models. The source code is available at https://github.com/gpgg/art.\n\n\nSelf-training Language Models for Arithmetic Reasoning</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.17873v1</guid>
      <dc:creator>Zhongtao Miao, Kaiyan Zhao, Yoshimasa Tsuruoka</dc:creator>
      <pubDate>Tue, 25 Jun 2024 18:21:00 GMT</pubDate>
    </item>
    <item>
      <title>Bootstrapping Language Models with DPO Implicit Rewards</title>
      <link>http://arxiv.org/abs/2406.09760v1</link>
      <description>Human alignment in large language models (LLMs) is an active area of research. A recent groundbreaking work, direct preference optimization (DPO), has greatly simplified the process from past work in reinforcement learning from human feedback (RLHF) by bypassing the reward learning stage in RLHF. DPO, after training, provides an implicit reward model. In this work, we make a novel observation that this implicit reward model can by itself be used in a bootstrapping fashion to further align the LLM. Our approach is to use the rewards from a current LLM model to construct a preference dataset, which is then used in subsequent DPO rounds. We incorporate refinements that debias the length of the responses and improve the quality of the preference dataset to further improve our approach. Our approach, named self-alignment with DPO ImpliCit rEwards (DICE), shows great improvements in alignment and achieves superior performance than Gemini Pro on AlpacaEval 2, reaching 27.55% length-controlled win rate against GPT-4 Turbo, but with only 8B parameters and no external feedback. Our code is available at https://github.com/sail-sg/dice.\n\n\nBootstrapping Language Models with DPO Implicit Rewards</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.09760v1</guid>
      <dc:creator>Changyu Chen, Zichen Liu, Chao Du, Tianyu Pang, Qian Liu, Arunesh Sinha, Pradeep Varakantham, Min Lin</dc:creator>
      <pubDate>Fri, 14 Jun 2024 06:57:18 GMT</pubDate>
    </item>
    <item>
      <title>Characterizing Similarities and Divergences in Conversational Tones in Humans and LLMs by Sampling with People</title>
      <link>http://arxiv.org/abs/2406.04278v1</link>
      <description>Conversational tones -- the manners and attitudes in which speakers communicate -- are essential to effective communication. Amidst the increasing popularization of Large Language Models (LLMs) over recent years, it becomes necessary to characterize the divergences in their conversational tones relative to humans. However, existing investigations of conversational modalities rely on pre-existing taxonomies or text corpora, which suffer from experimenter bias and may not be representative of real-world distributions for the studies' psycholinguistic domains. Inspired by methods from cognitive science, we propose an iterative method for simultaneously eliciting conversational tones and sentences, where participants alternate between two tasks: (1) one participant identifies the tone of a given sentence and (2) a different participant generates a sentence based on that tone. We run 100 iterations of this process with human participants and GPT-4, then obtain a dataset of sentences and frequent conversational tones. In an additional experiment, humans and GPT-4 annotated all sentences with all tones. With data from 1,339 human participants, 33,370 human judgments, and 29,900 GPT-4 queries, we show how our approach can be used to create an interpretable geometric representation of relations between conversational tones in humans and GPT-4. This work demonstrates how combining ideas from machine learning and cognitive science can address challenges in human-computer interactions.\n\n\nCharacterizing Similarities and Divergences in Conversational Tones in Humans and LLMs by Sampling with People</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.04278v1</guid>
      <dc:creator>Dun-Ming Huang, Pol Van Rijn, Ilia Sucholutsky, Raja Marjieh, Nori Jacoby</dc:creator>
      <pubDate>Thu, 06 Jun 2024 17:26:00 GMT</pubDate>
    </item>
    <item>
      <title>Multiplexable frequency retuning of MKID arrays using their non-linear kinetic inductance</title>
      <link>http://arxiv.org/abs/2204.05715v1</link>
      <description>Microwave Kinetic Inductance Detector (MKID) arrays are currently being developed and deployed for astronomical applications in the visible and near infrared and for sub-millimetre astronomy. One of the main drawbacks of MKIDs is that large arrays would exhibit a pixel yield, the percentage of individually distinguishable pixels to the total number of pixels, of 75 - 80 %. Imperfections arising during the fabrication can induce an uncontrolled shift in the resonance frequency of individual resonators which can end up resonating at the same frequency of a different resonator. This makes a number of resonators indistinguishable and therefore unusable for imaging. This paper proposes an approach to individually re-tune the colliding resonators in order to remove the degeneracy and increase the number of MKIDs with unique resonant frequencies. The frequency re-tuning is achieved through a DC bias of the resonator, the kinetic inductance of a superconducting thin film is current dependent and its dependence is non linear. Even though this approach has been already proposed, an innovative pixel design, described in this paper, may solve two issues previously described in literature such as increased electromagnetic losses to the DC-bias line, and the multiplexibility of multiple resonators on a single feedline.\n\n\nRe-Tuning: Overcoming the Compositionality Limits of Large Language Models with Recursive Tuning</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2204.05715v1</guid>
      <dc:creator>Mario De Lucia, Eoin Baldwin, Gerhard Ulbricht, Colm Bracken, Plamen Stamenov, Tom Ray</dc:creator>
      <pubDate>Tue, 12 Apr 2022 11:16:38 GMT</pubDate>
    </item>
    <item>
      <title>PAS: Data-Efficient Plug-and-Play Prompt Augmentation System</title>
      <link>http://arxiv.org/abs/2407.06027v3</link>
      <description>In recent years, the rise of Large Language Models (LLMs) has spurred a growing demand for plug-and-play AI systems. Among the various AI techniques, prompt engineering stands out as particularly significant. However, users often face challenges in writing prompts due to the steep learning curve and significant time investment, and existing automatic prompt engineering (APE) models can be difficult to use. To address this issue, we propose PAS, an LLM-based plug-and-play APE system. PAS utilizes LLMs trained on high-quality, automatically generated prompt complementary datasets, resulting in exceptional performance. In comprehensive benchmarks, PAS achieves state-of-the-art (SoTA) results compared to previous APE models, with an average improvement of 6.09 points. Moreover, PAS is highly efficient, achieving SoTA performance with only 9000 data points. Additionally, PAS can autonomously generate prompt augmentation data without requiring additional human labor. Its flexibility also allows it to be compatible with all existing LLMs and applicable to a wide range of tasks. PAS excels in human evaluations, underscoring its suitability as a plug-in for users. This combination of high performance, efficiency, and flexibility makes PAS a valuable system for enhancing the usability and effectiveness of LLMs through improved prompt engineering.\n\n\nPAS: Data-Efficient Plug-and-Play Prompt Augmentation System</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2407.06027v3</guid>
      <dc:creator>Miao Zheng, Hao Liang, Fan Yang, Haoze Sun, Tianpeng Li, Lingchu Xiong, Yan Zhang, Youzhen Wu, Kun Li, Yanjun Shen, Mingan Lin, Tao Zhang, Guosheng Dong, Yujing Qiao, Kun Fang, Weipeng Chen, Bin Cui, Wentao Zhang, Zenan Zhou</dc:creator>
      <pubDate>Fri, 12 Jul 2024 10:04:50 GMT</pubDate>
    </item>
    <item>
      <title>Relevant or Random: Can LLMs Truly Perform Analogical Reasoning?</title>
      <link>http://arxiv.org/abs/2404.12728v2</link>
      <description>Analogical reasoning is a unique ability of humans to address unfamiliar challenges by transferring strategies from relevant past experiences. One key finding in psychology is that compared with irrelevant past experiences, recalling relevant ones can help humans better handle new tasks. Coincidentally, the NLP community has also recently found that self-generating relevant examples in the context can help large language models (LLMs) better solve a given problem than hand-crafted prompts. However, it is yet not clear whether relevance is the key factor eliciting such capability, i.e., can LLMs benefit more from self-generated relevant examples than irrelevant ones? In this work, we systematically explore whether LLMs can truly perform analogical reasoning on a diverse set of reasoning tasks. With extensive experiments and analysis, we show that self-generated random examples can surprisingly achieve comparable or even better performance, e.g., 4% performance boost on GSM8K with random biological examples. We find that the accuracy of self-generated examples is the key factor and subsequently design two improved methods with significantly reduced inference costs. Overall, we aim to advance a deeper understanding of LLM analogical reasoning and hope this work stimulates further research in the design of self-generated contexts.\n\n\nRelevant or Random: Can LLMs Truly Perform Analogical Reasoning?</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2404.12728v2</guid>
      <dc:creator>Chengwei Qin, Wenhan Xia, Tan Wang, Fangkai Jiao, Yuchen Hu, Bosheng Ding, Ruirui Chen, Shafiq Joty</dc:creator>
      <pubDate>Sun, 23 Jun 2024 05:18:50 GMT</pubDate>
    </item>
    <item>
      <title>SafeAligner: Safety Alignment against Jailbreak Attacks via Response Disparity Guidance</title>
      <link>http://arxiv.org/abs/2406.18118v2</link>
      <description>As the development of large language models (LLMs) rapidly advances, securing these models effectively without compromising their utility has become a pivotal area of research. However, current defense strategies against jailbreak attacks (i.e., efforts to bypass security protocols) often suffer from limited adaptability, restricted general capability, and high cost. To address these challenges, we introduce SafeAligner, a methodology implemented at the decoding stage to fortify defenses against jailbreak attacks. We begin by developing two specialized models: the Sentinel Model, which is trained to foster safety, and the Intruder Model, designed to generate riskier responses. SafeAligner leverages the disparity in security levels between the responses from these models to differentiate between harmful and beneficial tokens, effectively guiding the safety alignment by altering the output token distribution of the target model. Extensive experiments show that SafeAligner can increase the likelihood of beneficial tokens, while reducing the occurrence of harmful ones, thereby ensuring secure alignment with minimal loss to generality.\n\n\nSafeAligner: Safety Alignment against Jailbreak Attacks via Response Disparity Guidance</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.18118v2</guid>
      <dc:creator>Caishuang Huang, Wanxu Zhao, Rui Zheng, Huijie Lv, Shihan Dou, Sixian Li, Xiao Wang, Enyu Zhou, Junjie Ye, Yuming Yang, Tao Gui, Qi Zhang, Xuanjing Huang</dc:creator>
      <pubDate>Fri, 28 Jun 2024 06:06:59 GMT</pubDate>
    </item>
    <item>
      <title>LIRE: listwise reward enhancement for preference alignment</title>
      <link>http://arxiv.org/abs/2405.13516v2</link>
      <description>Recently, tremendous strides have been made to align the generation of Large Language Models (LLMs) with human values to mitigate toxic or unhelpful content. Leveraging Reinforcement Learning from Human Feedback (RLHF) proves effective and is widely adopted by researchers. However, implementing RLHF is complex, and its sensitivity to hyperparameters renders achieving stable performance and scalability challenging. Furthermore, prevailing approaches to preference alignment primarily concentrate on pairwise comparisons, with limited exploration into multi-response scenarios, thereby overlooking the potential richness within the candidate pool. For the above reasons, we propose a new approach: Listwise Reward Enhancement for Preference Alignment (LIRE), a gradient-based reward optimization approach that incorporates the offline rewards of multiple responses into a streamlined listwise framework, thus eliminating the need for online sampling during training. LIRE is straightforward to implement, requiring minimal parameter tuning, and seamlessly aligns with the pairwise paradigm while naturally extending to multi-response scenarios. Moreover, we introduce a self-enhancement algorithm aimed at iteratively refining the reward during training. Our experiments demonstrate that LIRE consistently outperforms existing methods across several benchmarks on dialogue and summarization tasks, with good transferability to out-of-distribution data, assessed using proxy reward models and human annotators.\n\n\nLIRE: listwise reward enhancement for preference alignment</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.13516v2</guid>
      <dc:creator>Mingye Zhu, Yi Liu, Lei Zhang, Junbo Guo, Zhendong Mao</dc:creator>
      <pubDate>Tue, 04 Jun 2024 08:21:05 GMT</pubDate>
    </item>
    <item>
      <title>HAF-RM: A Hybrid Alignment Framework for Reward Model Training</title>
      <link>http://arxiv.org/abs/2407.04185v2</link>
      <description>The reward model has become increasingly important in alignment, assessment, and data construction for large language models (LLMs). Most existing researchers focus on enhancing reward models through data improvements, following the conventional training framework for reward models that directly optimizes the predicted rewards. In this paper, we propose a hybrid alignment framework HaF-RM for reward model training by introducing an additional constraint on token-level policy probabilities in addition to the reward score. It can simultaneously supervise the internal preference model at the token level and optimize the mapping layer of the reward model at the sequence level. Theoretical justifications and experiment results on five datasets show the validity and effectiveness of our proposed hybrid framework for training a high-quality reward model. By decoupling the reward modeling procedure and incorporating hybrid supervision, our HaF-RM framework offers a principled and effective approach to enhancing the performance and alignment of reward models, a critical component in the responsible development of powerful language models. We release our code at https://haf-rm.github.io.\n\n\nHAF-RM: A Hybrid Alignment Framework for Reward Model Training</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2407.04185v2</guid>
      <dc:creator>Shujun Liu, Xiaoyu Shen, Yuhang Lai, Siyuan Wang, Shengbin Yue, Zengfeng Huang, Xuanjing Huang, Zhongyu Wei</dc:creator>
      <pubDate>Thu, 11 Jul 2024 07:35:06 GMT</pubDate>
    </item>
    <item>
      <title>Bayesian Neural Networks</title>
      <link>http://arxiv.org/abs/1801.07710v2</link>
      <description>This paper describes and discusses Bayesian Neural Network (BNN). The paper showcases a few different applications of them for classification and regression problems. BNNs are comprised of a Probabilistic Model and a Neural Network. The intent of such a design is to combine the strengths of Neural Networks and Stochastic modeling. Neural Networks exhibit continuous function approximator capabilities. Stochastic models allow direct specification of a model with known interaction between parameters to generate data. During the prediction phase, stochastic models generate a complete posterior distribution and produce probabilistic guarantees on the predictions. Thus BNNs are a unique combination of neural network and stochastic models with the stochastic model forming the core of this integration. BNNs can then produce probabilistic guarantees on it's predictions and also generate the distribution of parameters that it has learnt from the observations. That means, in the parameter space, one can deduce the nature and shape of the neural network's learnt parameters. These two characteristics makes them highly attractive to theoreticians as well as practitioners. Recently there has been a lot of activity in this area, with the advent of numerous probabilistic programming libraries such as: PyMC3, Edward, Stan etc. Further this area is rapidly gaining ground as a standard machine learning approach for numerous problems\n\n\nBayesian WeakS-to-Strong from Text Classification to Generation</description>
      <guid isPermaLink="false">http://arxiv.org/abs/1801.07710v2</guid>
      <dc:creator>Vikram Mullachery, Aniruddh Khera, Amir Husain</dc:creator>
      <pubDate>Tue, 30 Jan 2018 15:30:26 GMT</pubDate>
    </item>
    <item>
      <title>STLLaVA-Med: Self-Training Large Language and Vision Assistant for Medical</title>
      <link>http://arxiv.org/abs/2406.19973v1</link>
      <description>Large Vision-Language Models (LVLMs) have shown significant potential in assisting medical diagnosis by leveraging extensive biomedical datasets. However, the advancement of medical image understanding and reasoning critically depends on building high-quality visual instruction data, which is costly and labor-intensive to obtain, particularly in the medical domain. To mitigate this data-starving issue, we introduce Self-Training Large Language and Vision Assistant for Medical (STLLaVA-Med). The proposed method is designed to train a policy model (an LVLM) capable of auto-generating medical visual instruction data to improve data efficiency, guided through Direct Preference Optimization (DPO). Specifically, a more powerful and larger LVLM (e.g., GPT-4o) is involved as a biomedical expert to oversee the DPO fine-tuning process on the auto-generated data, encouraging the policy model to align efficiently with human preferences. We validate the efficacy and data efficiency of STLLaVA-Med across three major medical Visual Question Answering (VQA) benchmarks, demonstrating competitive zero-shot performance with the utilization of only 9% of the medical data.\n\n\nSTLLaVA-Med: Self-Training Large Language and Vision Assistant for Medical</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.19973v1</guid>
      <dc:creator>Guohao Sun, Can Qin, Huazhu Fu, Linwei Wang, Zhiqiang Tao</dc:creator>
      <pubDate>Fri, 28 Jun 2024 15:01:23 GMT</pubDate>
    </item>
    <item>
      <title>LIONs: An Empirically Optimized Approach to Align Language Models</title>
      <link>http://arxiv.org/abs/2407.06542v1</link>
      <description>Alignment is a crucial step to enhance the instruction-following and conversational abilities of language models. Despite many recent work proposing new algorithms, datasets, and training pipelines, there is a lack of comprehensive studies measuring the impact of various design choices throughout the whole training process. We first conduct a rigorous analysis over a three-stage training pipeline consisting of supervised fine-tuning, offline preference learning, and online preference learning. We have found that using techniques like sequence packing, loss masking in SFT, increasing the preference dataset size in DPO, and online DPO training can significantly improve the performance of language models. We then train from Gemma-2b-base and LLama-3-8b-base, and find that our best models exceed the performance of the official instruct models tuned with closed-source data and algorithms. Our code and models can be found at https://github.com/Columbia-NLP-Lab/LionAlignment.\n\n\nLIONs: An Empirically Optimized Approach to Align Language Models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2407.06542v1</guid>
      <dc:creator>Xiao Yu, Qingyang Wu, Yu Li, Zhou Yu</dc:creator>
      <pubDate>Tue, 09 Jul 2024 04:34:39 GMT</pubDate>
    </item>
    <item>
      <title>Exploiting Semantic Reconstruction to Mitigate Hallucinations in Vision-Language Models</title>
      <link>http://arxiv.org/abs/2403.16167v3</link>
      <description>Hallucinations in vision-language models pose a significant challenge to their reliability, particularly in the generation of long captions. Current methods fall short of accurately identifying and mitigating these hallucinations. To address this issue, we introduce ESREAL, a novel unsupervised learning framework designed to suppress the generation of hallucinations through accurate localization and penalization of hallucinated tokens. Initially, ESREAL creates a reconstructed image based on the generated caption and aligns its corresponding regions with those of the original image. This semantic reconstruction aids in identifying both the presence and type of token-level hallucinations within the generated caption. Subsequently, ESREAL computes token-level hallucination scores by assessing the semantic similarity of aligned regions based on the type of hallucination. Finally, ESREAL employs a proximal policy optimization algorithm, where it selectively penalizes hallucinated tokens according to their token-level hallucination scores. Our framework notably reduces hallucinations in LLaVA, InstructBLIP, and mPLUG-Owl2 by 32.81%, 27.08%, and 7.46% on the CHAIR metric. This improvement is achieved solely through signals derived from the image itself, without the need for any image-text pairs.\n\n\nExploiting Semantic Reconstruction to Mitigate Hallucinations in Vision-Language Models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2403.16167v3</guid>
      <dc:creator>Minchan Kim, Minyeong Kim, Junik Bae, Suhwan Choi, Sungkyung Kim, Buru Chang</dc:creator>
      <pubDate>Sun, 05 May 2024 05:46:45 GMT</pubDate>
    </item>
    <item>
      <title>LLMs Beyond English: Scaling the Multilingual Capability of LLMs with Cross-Lingual Feedback</title>
      <link>http://arxiv.org/abs/2406.01771v1</link>
      <description>To democratize large language models (LLMs) to most natural languages, it is imperative to make these models capable of understanding and generating texts in many languages, in particular low-resource ones. While recent multilingual LLMs demonstrate remarkable performance in such capabilities, these LLMs still support a limited number of human languages due to the lack of training data for low-resource languages. Moreover, these LLMs are not yet aligned with human preference for downstream tasks, which is crucial for the success of LLMs in English. In this paper, we introduce xLLaMA-100 and xBLOOM-100 (collectively xLLMs-100), which scale the multilingual capabilities of LLaMA and BLOOM to 100 languages. To do so, we construct two datasets: a multilingual instruction dataset including 100 languages, which represents the largest language coverage to date, and a cross-lingual human feedback dataset encompassing 30 languages. We perform multilingual instruction tuning on the constructed instruction data and further align the LLMs with human feedback using the DPO algorithm on our cross-lingual human feedback dataset. We evaluate the multilingual understanding and generating capabilities of xLLMs-100 on five multilingual benchmarks. Experimental results show that xLLMs-100 consistently outperforms its peers across the benchmarks by considerable margins, defining a new state-of-the-art multilingual LLM that supports 100 languages.\n\n\nLLMs Beyond English: Scaling the Multilingual Capability of LLMs with Cross-Lingual Feedback</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.01771v1</guid>
      <dc:creator>Wen Lai, Mohsen Mesgar, Alexander Fraser</dc:creator>
      <pubDate>Mon, 03 Jun 2024 20:25:12 GMT</pubDate>
    </item>
    <item>
      <title>Continuous dynamical decoupling magnetometry</title>
      <link>http://arxiv.org/abs/1207.5729v1</link>
      <description>Solid-state qubits hold the promise to achieve unmatched combination of sensitivity and spatial resolution. To achieve their potential, the qubits need however to be shielded from the deleterious effects of the environment. While dynamical decoupling techniques can improve the coherence time, they impose a compromise between sensitivity and bandwidth, since to higher decoupling power correspond higher frequencies of the field to be measured. Moreover, the performance of pulse sequences is ultimately limited by control bounds and errors. Here we analyze a versatile alternative based on continuous driving. We find that continuous dynamical decoupling schemes can be used for AC magnetometry, providing similar frequency constraints on the AC field and improved sensitivity for some noise regimes. In addition, the exibility of phase and amplitude modulation could yield superior robustness to driving errors and a better adaptability to external experimental scenarios.\n\n\nDecoupled Alignment for Robust Plug-and-Play Adaptation</description>
      <guid isPermaLink="false">http://arxiv.org/abs/1207.5729v1</guid>
      <dc:creator>Masashi Hirose, Clarice D. Aiello, Paola Cappellaro</dc:creator>
      <pubDate>Tue, 24 Jul 2012 15:44:39 GMT</pubDate>
    </item>
    <item>
      <title>Watch Every Step! LLM Agent Learning via Iterative Step-Level Process Refinement</title>
      <link>http://arxiv.org/abs/2406.11176v1</link>
      <description>Large language model agents have exhibited exceptional performance across a range of complex interactive tasks. Recent approaches have utilized tuning with expert trajectories to enhance agent performance, yet they primarily concentrate on outcome rewards, which may lead to errors or suboptimal actions due to the absence of process supervision signals. In this paper, we introduce the Iterative step-level Process Refinement (IPR) framework, which provides detailed step-by-step guidance to enhance agent training. Specifically, we adopt the Monte Carlo method to estimate step-level rewards. During each iteration, the agent explores along the expert trajectory and generates new actions. These actions are then evaluated against the corresponding step of expert trajectory using step-level rewards. Such comparison helps identify discrepancies, yielding contrastive action pairs that serve as training data for the agent. Our experiments on three complex agent tasks demonstrate that our framework outperforms a variety of strong baselines. Moreover, our analytical findings highlight the effectiveness of IPR in augmenting action efficiency and its applicability to diverse models.\n\n\nWatch Every Step! LLM Agent Learning via Iterative Step-Level Process Refinement</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.11176v1</guid>
      <dc:creator>Weimin Xiong, Yifan Song, Xiutian Zhao, Wenhao Wu, Xun Wang, Ke Wang, Cheng Li, Wei Peng, Sujian Li</dc:creator>
      <pubDate>Mon, 17 Jun 2024 03:29:13 GMT</pubDate>
    </item>
    <item>
      <title>Enhancing Reinforcement Learning with Label-Sensitive Reward for Natural Language Understanding</title>
      <link>http://arxiv.org/abs/2405.19763v1</link>
      <description>Recent strides in large language models (LLMs) have yielded remarkable performance, leveraging reinforcement learning from human feedback (RLHF) to significantly enhance generation and alignment capabilities. However, RLHF encounters numerous challenges, including the objective mismatch issue, leading to suboptimal performance in Natural Language Understanding (NLU) tasks. To address this limitation, we propose a novel Reinforcement Learning framework enhanced with Label-sensitive Reward (RLLR) to amplify the performance of LLMs in NLU tasks. By incorporating label-sensitive pairs into reinforcement learning, our method aims to adeptly capture nuanced label-sensitive semantic features during RL, thereby enhancing natural language understanding. Experiments conducted on five diverse foundation models across eight tasks showcase promising results. In comparison to Supervised Fine-tuning models (SFT), RLLR demonstrates an average performance improvement of 1.54%. Compared with RLHF models, the improvement averages at 0.69%. These results reveal the effectiveness of our method for LLMs in NLU tasks. Code and data available at: https://github.com/MagiaSN/ACL2024_RLLR.\n\n\nEnhancing Reinforcement Learning with Label-Sensitive Reward for Natural Language Understanding</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.19763v1</guid>
      <dc:creator>Kuo Liao, Shuang Li, Meng Zhao, Liqun Liu, Mengge Xue, Zhenyu Hu, Honglin Han, Chengguo Yin</dc:creator>
      <pubDate>Thu, 30 May 2024 07:19:31 GMT</pubDate>
    </item>
    <item>
      <title>&quot;Vorbeşti Româneşte?&quot; A Recipe to Train Powerful Romanian LLMs with English Instructions</title>
      <link>http://arxiv.org/abs/2406.18266v2</link>
      <description>In recent years, Large Language Models (LLMs) have achieved almost human-like performance on various tasks. While some LLMs have been trained on multilingual data, most of the training data is in English; hence, their performance in English greatly exceeds other languages. To our knowledge, we are the first to collect and translate a large collection of texts, instructions, and benchmarks and train, evaluate, and release open-source LLMs tailored for Romanian. We evaluate our methods on four different categories, including academic benchmarks, MT-Bench (manually translated), and a professionally built historical, cultural, and social benchmark adapted to Romanian. We argue for the usefulness and high performance of RoLLMs by obtaining state-of-the-art results across the board. We publicly release all resources (i.e., data, training and evaluation code, models) to support and encourage research on Romanian LLMs while concurrently creating a generalizable recipe, adequate for other low or less-resourced languages.\n\n\n&quot; Vorbe\c {s} ti Rom\^ ane\c {s} te?&quot; A Recipe to Train Powerful Romanian LLMs with English Instructions</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.18266v2</guid>
      <dc:creator>Mihai Masala, Denis C. Ilie-Ablachim, Alexandru Dima, Dragos Corlatescu, Miruna Zavelca, Ovio Olaru, Simina Terian, Andrei Terian, Marius Leordeanu, Horia Velicu, Marius Popescu, Mihai Dascalu, Traian Rebedea</dc:creator>
      <pubDate>Thu, 27 Jun 2024 20:30:47 GMT</pubDate>
    </item>
    <item>
      <title>Mechanism Design for LLM Fine-tuning with Multiple Reward Models</title>
      <link>http://arxiv.org/abs/2405.16276v2</link>
      <description>Recent research on fine-tuning large language models (LLMs) through the aggregation of multiple preferences has attracted considerable attention. However, the existing literature predominantly focuses on the empirical performance of aggregation algorithms, while neglecting the underlying motivation for agents to misreport their preferences. In this paper, we formalize this as a multi-parameter mechanism design problem, where an LLM provider designs both training and payment rules to achieve specific objectives and promote the truthful reporting of preferences. Firstly, we claim the necessity of a payment scheme by demonstrating that without payments, truth-telling is a strictly dominated strategy under a wide range of training rules. Then, we introduce the affine maximizer payment scheme for the social welfare maximizing training rules that are widely used in practice, which ensures both dominant-strategy incentive compatibility (DSIC) and individual rationality (IR). Furthermore, we prove that under mild conditions, any other payment rule that also implements these training rules in DSIC can be converted to the affine maximizer payment by adding a factor irrelevant to the agents' own reports. We also show that this mechanism satisfies approximate DSIC when the input of the mechanism is a biased version of the reported preferences, showcasing its robustness in real-world applications.\n\n\nMechanism Design for LLM Fine-tuning with Multiple Reward Models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.16276v2</guid>
      <dc:creator>Haoran Sun, Yurong Chen, Siwei Wang, Wei Chen, Xiaotie Deng</dc:creator>
      <pubDate>Mon, 15 Jul 2024 11:48:07 GMT</pubDate>
    </item>
    <item>
      <title>Multimodal End-to-End Autonomous Driving</title>
      <link>http://arxiv.org/abs/1906.03199v2</link>
      <description>A crucial component of an autonomous vehicle (AV) is the artificial intelligence (AI) is able to drive towards a desired destination. Today, there are different paradigms addressing the development of AI drivers. On the one hand, we find modular pipelines, which divide the driving task into sub-tasks such as perception and maneuver planning and control. On the other hand, we find end-to-end driving approaches that try to learn a direct mapping from input raw sensor data to vehicle control signals. The later are relatively less studied, but are gaining popularity since they are less demanding in terms of sensor data annotation. This paper focuses on end-to-end autonomous driving. So far, most proposals relying on this paradigm assume RGB images as input sensor data. However, AVs will not be equipped only with cameras, but also with active sensors providing accurate depth information (e.g., LiDARs). Accordingly, this paper analyses whether combining RGB and depth modalities, i.e. using RGBD data, produces better end-to-end AI drivers than relying on a single modality. We consider multimodality based on early, mid and late fusion schemes, both in multisensory and single-sensor (monocular depth estimation) settings. Using the CARLA simulator and conditional imitation learning (CIL), we show how, indeed, early fusion multimodality outperforms single-modality.\n\n\nAutonomous Data Association and Intelligent Information Discovery Based on Multimodal Fusion Technology</description>
      <guid isPermaLink="false">http://arxiv.org/abs/1906.03199v2</guid>
      <dc:creator>Yi Xiao, Felipe Codevilla, Akhil Gurram, Onay Urfalioglu, Antonio M. López</dc:creator>
      <pubDate>Sun, 25 Oct 2020 10:42:37 GMT</pubDate>
    </item>
    <item>
      <title>BiVLC: Extending Vision-Language Compositionality Evaluation with Text-to-Image Retrieval</title>
      <link>http://arxiv.org/abs/2406.09952v1</link>
      <description>Existing Vision-Language Compositionality (VLC) benchmarks like SugarCrepe are formulated as image-to-text retrieval problems, where, given an image, the models need to select between the correct textual description and a synthetic hard negative text. In this work we present the Bidirectional Vision-Language Compositionality (BiVLC) dataset. The novelty of BiVLC is to add a synthetic hard negative image generated from the synthetic text, resulting in two image-to-text retrieval examples (one for each image) and, more importantly, two text-to-image retrieval examples (one for each text). Human annotators filter out ill-formed examples ensuring the validity of the benchmark. The experiments on BiVLC uncover a weakness of current multimodal models, as they perform poorly in the text-to-image direction. In fact, when considering both retrieval directions, the conclusions obtained in previous works change significantly. In addition to the benchmark, we show that a contrastive model trained using synthetic images and texts improves the state of the art in SugarCrepe and in BiVLC for both retrieval directions. The gap to human performance in BiVLC confirms that Vision-Language Compositionality is still a challenging problem. BiVLC and code are available at https://imirandam.github.io/BiVLC_project_page.\n\n\nBiVLC: Extending Vision-Language Compositionality Evaluation with Text-to-Image Retrieval</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.09952v1</guid>
      <dc:creator>Imanol Miranda, Ander Salaberria, Eneko Agirre, Gorka Azkune</dc:creator>
      <pubDate>Fri, 14 Jun 2024 11:58:49 GMT</pubDate>
    </item>
    <item>
      <title>Enhancing Confidence Expression in Large Language Models Through Learning from Past Experience</title>
      <link>http://arxiv.org/abs/2404.10315v1</link>
      <description>Large Language Models (LLMs) have exhibited remarkable performance across various downstream tasks, but they may generate inaccurate or false information with a confident tone. One of the possible solutions is to empower the LLM confidence expression capability, in which the confidence expressed can be well-aligned with the true probability of the generated answer being correct. However, leveraging the intrinsic ability of LLMs or the signals from the output logits of answers proves challenging in accurately capturing the response uncertainty in LLMs. Therefore, drawing inspiration from cognitive diagnostics, we propose a method of Learning from Past experience (LePe) to enhance the capability for confidence expression. Specifically, we first identify three key problems: (1) How to capture the inherent confidence of the LLM? (2) How to teach the LLM to express confidence? (3) How to evaluate the confidence expression of the LLM? Then we devise three stages in LePe to deal with these problems. Besides, to accurately capture the confidence of an LLM when constructing the training data, we design a complete pipeline including question preparation and answer sampling. We also conduct experiments using the Llama family of LLMs to verify the effectiveness of our proposed method on four datasets.\n\n\nEnhancing Confidence Expression in Large Language Models Through Learning from Past Experience</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2404.10315v1</guid>
      <dc:creator>Haixia Han, Tingyun Li, Shisong Chen, Jie Shi, Chengyu Du, Yanghua Xiao, Jiaqing Liang, Xin Lin</dc:creator>
      <pubDate>Tue, 16 Apr 2024 06:47:49 GMT</pubDate>
    </item>
    <item>
      <title>Regularized Conditional Diffusion Model for Multi-Task Preference Alignment</title>
      <link>http://arxiv.org/abs/2404.04920v1</link>
      <description>Sequential decision-making is desired to align with human intents and exhibit versatility across various tasks. Previous methods formulate it as a conditional generation process, utilizing return-conditioned diffusion models to directly model trajectory distributions. Nevertheless, the return-conditioned paradigm relies on pre-defined reward functions, facing challenges when applied in multi-task settings characterized by varying reward functions (versatility) and showing limited controllability concerning human preferences (alignment). In this work, we adopt multi-task preferences as a unified condition for both single- and multi-task decision-making, and propose preference representations aligned with preference labels. The learned representations are used to guide the conditional generation process of diffusion models, and we introduce an auxiliary objective to maximize the mutual information between representations and corresponding generated trajectories, improving alignment between trajectories and preferences. Extensive experiments in D4RL and Meta-World demonstrate that our method presents favorable performance in single- and multi-task scenarios, and exhibits superior alignment with preferences.\n\n\nRegularized Conditional Diffusion Model for Multi-Task Preference Alignment</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2404.04920v1</guid>
      <dc:creator>Xudong Yu, Chenjia Bai, Haoran He, Changhong Wang, Xuelong Li</dc:creator>
      <pubDate>Sun, 07 Apr 2024 11:20:32 GMT</pubDate>
    </item>
    <item>
      <title>Mixture of insighTful Experts (MoTE): The Synergy of Thought Chains and Expert Mixtures in Self-Alignment</title>
      <link>http://arxiv.org/abs/2405.00557v3</link>
      <description>As the capabilities of large language models (LLMs) have expanded dramatically, aligning these models with human values presents a significant challenge. Traditional alignment strategies rely heavily on human intervention, such as Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF), or on the self-alignment capacities of LLMs, which usually require a strong LLM's emergent ability to improve its original bad answer. To address these challenges, we propose a novel self-alignment method that utilizes a Chain of Thought (CoT) approach, termed AlignCoT. This method encompasses stages of Question Analysis, Answer Guidance, and Safe Answer production. It is designed to enable LLMs to generate high-quality, safe responses throughout various stages of their development. Furthermore, we introduce the Mixture of insighTful Experts (MoTE) architecture, which applies mixture of experts to enhance each component of the AlignCoT process, markedly increasing alignment efficiency. The MoTE approach not only outperforms existing methods in aligning LLMs with human values but also highlights the benefits of using self-generated data, revealing the dual benefits of improved alignment and training efficiency.\n\n\nMixture of insighTful Experts (MoTE): The Synergy of Thought Chains and Expert Mixtures in Self-Alignment</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.00557v3</guid>
      <dc:creator>Zhili Liu, Yunhao Gou, Kai Chen, Lanqing Hong, Jiahui Gao, Fei Mi, Yu Zhang, Zhenguo Li, Xin Jiang, Qun Liu, James T. Kwok</dc:creator>
      <pubDate>Mon, 08 Jul 2024 16:02:18 GMT</pubDate>
    </item>
    <item>
      <title>Speak Out of Turn: Safety Vulnerability of Large Language Models in Multi-turn Dialogue</title>
      <link>http://arxiv.org/abs/2402.17262v1</link>
      <description>Large Language Models (LLMs) have been demonstrated to generate illegal or unethical responses, particularly when subjected to &quot;jailbreak.&quot; Research on jailbreak has highlighted the safety issues of LLMs. However, prior studies have predominantly focused on single-turn dialogue, ignoring the potential complexities and risks presented by multi-turn dialogue, a crucial mode through which humans derive information from LLMs. In this paper, we argue that humans could exploit multi-turn dialogue to induce LLMs into generating harmful information. LLMs may not intend to reject cautionary or borderline unsafe queries, even if each turn is closely served for one malicious purpose in a multi-turn dialogue. Therefore, by decomposing an unsafe query into several sub-queries for multi-turn dialogue, we induced LLMs to answer harmful sub-questions incrementally, culminating in an overall harmful response. Our experiments, conducted across a wide range of LLMs, indicate current inadequacies in the safety mechanisms of LLMs in multi-turn dialogue. Our findings expose vulnerabilities of LLMs in complex scenarios involving multi-turn dialogue, presenting new challenges for the safety of LLMs.\n\n\nSpeak Out of Turn: Safety Vulnerability of Large Language Models in Multi-turn Dialogue</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.17262v1</guid>
      <dc:creator>Zhenhong Zhou, Jiuyang Xiang, Haopeng Chen, Quan Liu, Zherui Li, Sen Su</dc:creator>
      <pubDate>Tue, 27 Feb 2024 07:11:59 GMT</pubDate>
    </item>
    <item>
      <title>Merging Improves Self-Critique Against Jailbreak Attacks</title>
      <link>http://arxiv.org/abs/2406.07188v2</link>
      <description>The robustness of large language models (LLMs) against adversarial manipulations, such as jailbreak attacks, remains a significant challenge. In this work, we propose an approach that enhances the self-critique capability of the LLM and further fine-tunes it over sanitized synthetic data. This is done with the addition of an external critic model that can be merged with the original, thus bolstering self-critique capabilities and improving the robustness of the LLMs response to adversarial prompts. Our results demonstrate that the combination of merging and self-critique can reduce the attack success rate of adversaries significantly, thus offering a promising defense mechanism against jailbreak attacks. Code, data and models released at https://github.com/vicgalle/merging-self-critique-jailbreaks .\n\n\nMerging Improves Self-Critique Against Jailbreak Attacks</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.07188v2</guid>
      <dc:creator>Victor Gallego</dc:creator>
      <pubDate>Sun, 14 Jul 2024 18:27:14 GMT</pubDate>
    </item>
    <item>
      <title>It Takes Two: On the Seamlessness between Reward and Policy Model in RLHF</title>
      <link>http://arxiv.org/abs/2406.07971v2</link>
      <description>Reinforcement Learning from Human Feedback (RLHF) involves training policy models (PMs) and reward models (RMs) to align language models with human preferences. Instead of focusing solely on PMs and RMs independently, we propose to examine their interactions during fine-tuning, introducing the concept of seamlessness. Our study starts with observing the saturation phenomenon, where continual improvements in RM and PM do not translate into RLHF progress. Our analysis shows that RMs fail to assign proper scores to PM responses, resulting in a 35% mismatch rate with human preferences, highlighting a significant discrepancy between PM and RM. To measure seamlessness between PM and RM without human effort, we propose an automatic metric, SEAM. SEAM quantifies the discrepancies between PM and RM judgments induced by data samples. We validate the effectiveness of SEAM in data selection and model augmentation. Our experiments demonstrate that (1) using SEAM-filtered data for RL training improves RLHF performance by 4.5%, and (2) SEAM-guided model augmentation results in a 4% performance improvement over standard augmentation methods.\n\n\nIt Takes Two: On the Seamlessness between Reward and Policy Model in RLHF</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.07971v2</guid>
      <dc:creator>Taiming Lu, Lingfeng Shen, Xinyu Yang, Weiting Tan, Beidi Chen, Huaxiu Yao</dc:creator>
      <pubDate>Thu, 13 Jun 2024 05:13:50 GMT</pubDate>
    </item>
    <item>
      <title>Beyond Imitation: Learning Key Reasoning Steps from Dual Chain-of-Thoughts in Reasoning Distillation</title>
      <link>http://arxiv.org/abs/2405.19737v1</link>
      <description>As Large Language Models (LLMs) scale up and gain powerful Chain-of-Thoughts (CoTs) reasoning abilities, practical resource constraints drive efforts to distill these capabilities into more compact Smaller Language Models (SLMs). We find that CoTs consist mainly of simple reasoning forms, with a small proportion ($\approx 4.7\%$) of key reasoning steps that truly impact conclusions. However, previous distillation methods typically involve supervised fine-tuning student SLMs only on correct CoTs data produced by teacher LLMs, resulting in students struggling to learn the key reasoning steps, instead imitating the teacher's reasoning forms and making errors or omissions on these steps. To address these issues, drawing an analogy to human learning, where analyzing mistakes according to correct solutions often reveals the crucial steps leading to successes or failures, we propose mistak\textbf{E}-\textbf{D}riven key reason\textbf{I}ng step distilla\textbf{T}ion (\textbf{EDIT}), a novel method that further aids SLMs learning key reasoning steps rather than mere simple fine-tuning. Firstly, to expose these crucial steps in CoTs, we design specific prompts to generate dual CoTs data with similar reasoning paths but divergent conclusions. Then, we apply the minimum edit distance algorithm on the dual CoTs data to locate these key steps and optimize the likelihood of these steps. Extensive experiments validate the effectiveness of EDIT across both in-domain and out-of-domain benchmark reasoning datasets. Further analysis shows that EDIT can generate high-quality CoTs with more correct key reasoning steps. Notably, we also explore how different mistake patterns affect performance and find that EDIT benefits more from logical errors than from knowledge or mathematical calculation errors in dual CoTs\footnote{Code can be found at \url{https://github.com/C-W-D/EDIT}}.\n\n\nBeyond Imitation: Learning Key Reasoning Steps from Dual Chain-of-Thoughts in Reasoning Distillation</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.19737v1</guid>
      <dc:creator>Chengwei Dai, Kun Li, Wei Zhou, Songlin Hu</dc:creator>
      <pubDate>Thu, 30 May 2024 06:32:11 GMT</pubDate>
    </item>
    <item>
      <title>D2PO: Discriminator-Guided DPO with Response Evaluation Models</title>
      <link>http://arxiv.org/abs/2405.01511v1</link>
      <description>Varied approaches for aligning language models have been proposed, including supervised fine-tuning, RLHF, and direct optimization methods such as DPO. Although DPO has rapidly gained popularity due to its straightforward training process and competitive results, there is an open question of whether there remain practical advantages of using a discriminator, like a reward model, to evaluate responses. We propose D2PO, discriminator-guided DPO, an approach for the online setting where preferences are being collected throughout learning. As we collect gold preferences, we use these not only to train our policy, but to train a discriminative response evaluation model to silver-label even more synthetic data for policy training. We explore this approach across a set of diverse tasks, including a realistic chat setting, we find that our approach leads to higher-quality outputs compared to DPO with the same data budget, and greater efficiency in terms of preference data requirements. Furthermore, we show conditions under which silver labeling is most helpful: it is most effective when training the policy with DPO, outperforming traditional PPO, and benefits from maintaining a separate discriminator from the policy model.\n\n\nD2PO: Discriminator-Guided DPO with Response Evaluation Models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.01511v1</guid>
      <dc:creator>Prasann Singhal, Nathan Lambert, Scott Niekum, Tanya Goyal, Greg Durrett</dc:creator>
      <pubDate>Thu, 02 May 2024 17:44:41 GMT</pubDate>
    </item>
    <item>
      <title>Limitations of Agents Simulated by Predictive Models</title>
      <link>http://arxiv.org/abs/2402.05829v1</link>
      <description>There is increasing focus on adapting predictive models into agent-like systems, most notably AI assistants based on language models. We outline two structural reasons for why these models can fail when turned into agents. First, we discuss auto-suggestive delusions. Prior work has shown theoretically that models fail to imitate agents that generated the training data if the agents relied on hidden observations: the hidden observations act as confounding variables, and the models treat actions they generate as evidence for nonexistent observations. Second, we introduce and formally study a related, novel limitation: predictor-policy incoherence. When a model generates a sequence of actions, the model's implicit prediction of the policy that generated those actions can serve as a confounding variable. The result is that models choose actions as if they expect future actions to be suboptimal, causing them to be overly conservative. We show that both of those failures are fixed by including a feedback loop from the environment, that is, re-training the models on their own actions. We give simple demonstrations of both limitations using Decision Transformers and confirm that empirical results agree with our conceptual and formal analysis. Our treatment provides a unifying view of those failure modes, and informs the question of why fine-tuning offline learned policies with online learning makes them more effective.\n\n\nLimitations of Agents Simulated by Predictive Models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.05829v1</guid>
      <dc:creator>Raymond Douglas, Jacek Karwowski, Chan Bae, Andis Draguns, Victoria Krakovna</dc:creator>
      <pubDate>Thu, 08 Feb 2024 17:08:08 GMT</pubDate>
    </item>
    <item>
      <title>$\text{Memory}^3$: Language Modeling with Explicit Memory</title>
      <link>http://arxiv.org/abs/2407.01178v1</link>
      <description>The training and inference of large language models (LLMs) are together a costly process that transports knowledge from raw data to meaningful computation. Inspired by the memory hierarchy of the human brain, we reduce this cost by equipping LLMs with explicit memory, a memory format cheaper than model parameters and text retrieval-augmented generation (RAG). Conceptually, with most of its knowledge externalized to explicit memories, the LLM can enjoy a smaller parameter size, training cost, and inference cost, all proportional to the amount of remaining &quot;abstract knowledge&quot;. As a preliminary proof of concept, we train from scratch a 2.4B LLM, which achieves better performance than much larger LLMs as well as RAG models, and maintains higher decoding speed than RAG. The model is named $\text{Memory}^3$, since explicit memory is the third form of memory in LLMs after implicit memory (model parameters) and working memory (context key-values). We introduce a memory circuitry theory to support the externalization of knowledge, and present novel techniques including a memory sparsification mechanism that makes storage tractable and a two-stage pretraining scheme that facilitates memory formation.\n\n\n: Language Modeling with Explicit Memory</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2407.01178v1</guid>
      <dc:creator>Hongkang Yang, Zehao Lin, Wenjin Wang, Hao Wu, Zhiyu Li, Bo Tang, Wenqiang Wei, Jinbo Wang, Zeyun Tang, Shichao Song, Chenyang Xi, Yu Yu, Kai Chen, Feiyu Xiong, Linpeng Tang, Weinan E</dc:creator>
      <pubDate>Mon, 01 Jul 2024 11:07:23 GMT</pubDate>
    </item>
    <item>
      <title>OPTune: Efficient Online Preference Tuning</title>
      <link>http://arxiv.org/abs/2406.07657v1</link>
      <description>Reinforcement learning with human feedback~(RLHF) is critical for aligning Large Language Models (LLMs) with human preference. Compared to the widely studied offline version of RLHF, \emph{e.g.} direct preference optimization (DPO), recent works have shown that the online variants achieve even better alignment. However, online alignment requires on-the-fly generation of new training data, which is costly, hard to parallelize, and suffers from varying quality and utility. In this paper, we propose a more efficient data exploration strategy for online preference tuning (OPTune), which does not rely on human-curated or pre-collected teacher responses but dynamically samples informative responses for on-policy preference alignment. During data generation, OPTune only selects prompts whose (re)generated responses can potentially provide more informative and higher-quality training signals than the existing responses. In the training objective, OPTune reweights each generated response (pair) by its utility in improving the alignment so that learning can be focused on the most helpful samples. Throughout our evaluations, OPTune'd LLMs maintain the instruction-following benefits provided by standard preference tuning whilst enjoying 1.27-1.56x faster training speed due to the efficient data exploration strategy.\n\n\nOPTune: Efficient Online Preference Tuning</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.07657v1</guid>
      <dc:creator>Lichang Chen, Jiuhai Chen, Chenxi Liu, John Kirchenbauer, Davit Soselia, Chen Zhu, Tom Goldstein, Tianyi Zhou, Heng Huang</dc:creator>
      <pubDate>Tue, 11 Jun 2024 18:55:04 GMT</pubDate>
    </item>
    <item>
      <title>AMMUS : A Survey of Transformer-based Pretrained Models in Natural Language Processing</title>
      <link>http://arxiv.org/abs/2108.05542v2</link>
      <description>Transformer-based pretrained language models (T-PTLMs) have achieved great success in almost every NLP task. The evolution of these models started with GPT and BERT. These models are built on the top of transformers, self-supervised learning and transfer learning. Transformed-based PTLMs learn universal language representations from large volumes of text data using self-supervised learning and transfer this knowledge to downstream tasks. These models provide good background knowledge to downstream tasks which avoids training of downstream models from scratch. In this comprehensive survey paper, we initially give a brief overview of self-supervised learning. Next, we explain various core concepts like pretraining, pretraining methods, pretraining tasks, embeddings and downstream adaptation methods. Next, we present a new taxonomy of T-PTLMs and then give brief overview of various benchmarks including both intrinsic and extrinsic. We present a summary of various useful libraries to work with T-PTLMs. Finally, we highlight some of the future research directions which will further improve these models. We strongly believe that this comprehensive survey paper will serve as a good reference to learn the core concepts as well as to stay updated with the recent happenings in T-PTLMs.\n\n\nA Survey on Human-AI Teaming with Large Pre-Trained Models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2108.05542v2</guid>
      <dc:creator>Katikapalli Subramanyam Kalyan, Ajit Rajasekharan, Sivanesan Sangeetha</dc:creator>
      <pubDate>Sat, 28 Aug 2021 05:59:16 GMT</pubDate>
    </item>
    <item>
      <title>LLM-Personalize: Aligning LLM Planners with Human Preferences via Reinforced Self-Training for Housekeeping Robots</title>
      <link>http://arxiv.org/abs/2404.14285v1</link>
      <description>Large language models (LLMs) have shown significant potential for robotics applications, particularly task planning, by harnessing their language comprehension and text generation capabilities. However, in applications such as household robotics, a critical gap remains in the personalization of these models to individual user preferences. We introduce LLM-Personalize, a novel framework with an optimization pipeline designed to personalize LLM planners for household robotics. Our LLM-Personalize framework features an LLM planner that performs iterative planning in multi-room, partially-observable household scenarios, making use of a scene graph constructed with local observations. The generated plan consists of a sequence of high-level actions which are subsequently executed by a controller. Central to our approach is the optimization pipeline, which combines imitation learning and iterative self-training to personalize the LLM planner. In particular, the imitation learning phase performs initial LLM alignment from demonstrations, and bootstraps the model to facilitate effective iterative self-training, which further explores and aligns the model to user preferences. We evaluate LLM-Personalize on Housekeep, a challenging simulated real-world 3D benchmark for household rearrangements, and show that LLM-Personalize achieves more than a 30 percent increase in success rate over existing LLM planners, showcasing significantly improved alignment with human preferences. Project page: https://donggehan.github.io/projectllmpersonalize/.\n\n\nLLM-Personalize: Aligning LLM Planners with Human Preferences via Reinforced Self-Training for Housekeeping Robots</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2404.14285v1</guid>
      <dc:creator>Dongge Han, Trevor McInroe, Adam Jelley, Stefano V. Albrecht, Peter Bell, Amos Storkey</dc:creator>
      <pubDate>Mon, 22 Apr 2024 15:35:33 GMT</pubDate>
    </item>
    <item>
      <title>Beyond Static Evaluation: A Dynamic Approach to Assessing AI Assistants' API Invocation Capabilities</title>
      <link>http://arxiv.org/abs/2403.11128v2</link>
      <description>With the rise of Large Language Models (LLMs), AI assistants' ability to utilize tools, especially through API calls, has advanced notably. This progress has necessitated more accurate evaluation methods. Many existing studies adopt static evaluation, where they assess AI assistants' API call based on pre-defined dialogue histories. However, such evaluation method can be misleading, as an AI assistant might fail in generating API calls from preceding human interaction in real cases. Instead of the resource-intensive method of direct human-machine interactions, we propose Automated Dynamic Evaluation (AutoDE) to assess an assistant's API call capability without human involvement. In our framework, we endeavor to closely mirror genuine human conversation patterns in human-machine interactions, using a LLM-based user agent, equipped with a user script to ensure human alignment. Experimental results highlight that AutoDE uncovers errors overlooked by static evaluations, aligning more closely with human assessment. Testing four AI assistants using our crafted benchmark, our method further mirrored human evaluation compared to conventional static evaluations.\n\n\nBeyond Static Evaluation: A Dynamic Approach to Assessing AI Assistants' API Invocation Capabilities</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2403.11128v2</guid>
      <dc:creator>Honglin Mu, Yang Xu, Yunlong Feng, Xiaofeng Han, Yitong Li, Yutai Hou, Wanxiang Che</dc:creator>
      <pubDate>Wed, 27 Mar 2024 15:22:53 GMT</pubDate>
    </item>
    <item>
      <title>Towards Understanding Jailbreak Attacks in LLMs: A Representation Space Analysis</title>
      <link>http://arxiv.org/abs/2406.10794v2</link>
      <description>Large language models (LLMs) are susceptible to a type of attack known as jailbreaking, which misleads LLMs to output harmful contents. Although there are diverse jailbreak attack strategies, there is no unified understanding on why some methods succeed and others fail. This paper explores the behavior of harmful and harmless prompts in the LLM's representation space to investigate the intrinsic properties of successful jailbreak attacks. We hypothesize that successful attacks share some similar properties: They are effective in moving the representation of the harmful prompt towards the direction to the harmless prompts. We leverage hidden representations into the objective of existing jailbreak attacks to move the attacks along the acceptance direction, and conduct experiments to validate the above hypothesis using the proposed objective. We hope this study provides new insights into understanding how LLMs understand harmfulness information.\n\n\nTowards Understanding Jailbreak Attacks in LLMs: A Representation Space Analysis</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.10794v2</guid>
      <dc:creator>Yuping Lin, Pengfei He, Han Xu, Yue Xing, Makoto Yamada, Hui Liu, Jiliang Tang</dc:creator>
      <pubDate>Wed, 26 Jun 2024 13:50:32 GMT</pubDate>
    </item>
    <item>
      <title>Video Diffusion Alignment via Reward Gradients</title>
      <link>http://arxiv.org/abs/2407.08737v1</link>
      <description>We have made significant progress towards building foundational video diffusion models. As these models are trained using large-scale unsupervised data, it has become crucial to adapt these models to specific downstream tasks. Adapting these models via supervised fine-tuning requires collecting target datasets of videos, which is challenging and tedious. In this work, we utilize pre-trained reward models that are learned via preferences on top of powerful vision discriminative models to adapt video diffusion models. These models contain dense gradient information with respect to generated RGB pixels, which is critical to efficient learning in complex search spaces, such as videos. We show that backpropagating gradients from these reward models to a video diffusion model can allow for compute and sample efficient alignment of the video diffusion model. We show results across a variety of reward models and video diffusion models, demonstrating that our approach can learn much more efficiently in terms of reward queries and computation than prior gradient-free approaches. Our code, model weights,and more visualization are available at https://vader-vid.github.io.\n\n\nVideo Diffusion Alignment via Reward Gradients</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2407.08737v1</guid>
      <dc:creator>Mihir Prabhudesai, Russell Mendonca, Zheyang Qin, Katerina Fragkiadaki, Deepak Pathak</dc:creator>
      <pubDate>Thu, 11 Jul 2024 17:59:45 GMT</pubDate>
    </item>
    <item>
      <title>Reward Engineering for Generating Semi-structured Explanation</title>
      <link>http://arxiv.org/abs/2309.08347v2</link>
      <description>Semi-structured explanation depicts the implicit process of a reasoner with an explicit representation. This explanation highlights how available information in a specific query is utilised and supplemented with information a reasoner produces from its internal weights towards generating an answer. Despite the recent improvements in generative capabilities of language models, producing structured explanations to verify a model's true reasoning capabilities remains a challenge. This issue is particularly pronounced for not-so-large LMs (e.g., FLAN-T5-XXL). In this work, we first underscore the limitations of supervised fine-tuning (SFT) in tackling this challenge, and then introduce a carefully crafted reward engineering method in reinforcement learning (RL) to better address this problem. We investigate multiple reward aggregation methods and provide a detailed discussion which sheds light on the promising potential of RL for future research. Our proposed method on two semi-structured explanation generation benchmarks (ExplaGraph and COPA-SSE) achieves new state-of-the-art results.\n\n\nReward Engineering for Generating Semi-structured Explanation</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2309.08347v2</guid>
      <dc:creator>Jiuzhou Han, Wray Buntine, Ehsan Shareghi</dc:creator>
      <pubDate>Wed, 24 Jan 2024 04:53:13 GMT</pubDate>
    </item>
    <item>
      <title>Encourage or Inhibit Monosemanticity? Revisit Monosemanticity from a Feature Decorrelation Perspective</title>
      <link>http://arxiv.org/abs/2406.17969v1</link>
      <description>To better interpret the intrinsic mechanism of large language models (LLMs), recent studies focus on monosemanticity on its basic units. A monosemantic neuron is dedicated to a single and specific concept, which forms a one-to-one correlation between neurons and concepts. Despite extensive research in monosemanticity probing, it remains unclear whether monosemanticity is beneficial or harmful to model capacity. To explore this question, we revisit monosemanticity from the feature decorrelation perspective and advocate for its encouragement. We experimentally observe that the current conclusion by wang2024learning, which suggests that decreasing monosemanticity enhances model performance, does not hold when the model changes. Instead, we demonstrate that monosemanticity consistently exhibits a positive correlation with model capacity, in the preference alignment process. Consequently, we apply feature correlation as a proxy for monosemanticity and incorporate a feature decorrelation regularizer into the dynamic preference optimization process. The experiments show that our method not only enhances representation diversity and activation sparsity but also improves preference alignment performance.\n\n\nEncourage or Inhibit Monosemanticity? Revisit Monosemanticity from a Feature Decorrelation Perspective</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.17969v1</guid>
      <dc:creator>Hanqi Yan, Yanzheng Xiang, Guangyi Chen, Yifei Wang, Lin Gui, Yulan He</dc:creator>
      <pubDate>Tue, 25 Jun 2024 22:51:08 GMT</pubDate>
    </item>
    <item>
      <title>Latent Distance Guided Alignment Training for Large Language Models</title>
      <link>http://arxiv.org/abs/2404.06390v2</link>
      <description>Ensuring alignment with human preferences is a crucial characteristic of large language models (LLMs). Presently, the primary alignment methods, RLHF and DPO, require extensive human annotation, which is expensive despite their efficacy. The significant expenses associated with current alignment techniques motivate researchers to investigate the development of annotation-free alignment training methods. In pursuit of improved alignment without relying on external annotation, we introduce Latent Distance Guided Alignment Training (LD-Align). This approach seeks to align the model with a high-quality supervised fine-tune dataset using guidance from a latent space. The latent space is generated through sample reconstruction, akin to auto-encoding. Consequently, we utilize the distance between sample pairs in the latent space to guide DPO-based alignment training. Extensive experimentation and evaluation show the efficacy of our proposed method in achieving notable alignment.\n\n\nLatent Distance Guided Alignment Training for Large Language Models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2404.06390v2</guid>
      <dc:creator>Haotian Luo</dc:creator>
      <pubDate>Sat, 13 Apr 2024 05:20:45 GMT</pubDate>
    </item>
    <item>
      <title>Efficient Model-agnostic Alignment via Bayesian Persuasion</title>
      <link>http://arxiv.org/abs/2405.18718v1</link>
      <description>With recent advancements in large language models (LLMs), alignment has emerged as an effective technique for keeping LLMs consensus with human intent. Current methods primarily involve direct training through Supervised Fine-tuning (SFT) or Reinforcement Learning from Human Feedback (RLHF), both of which require substantial computational resources and extensive ground truth data. This paper explores an efficient method for aligning black-box large models using smaller models, introducing a model-agnostic and lightweight Bayesian Persuasion Alignment framework. We formalize this problem as an optimization of the signaling strategy from the small model's perspective. In the persuasion process, the small model (Advisor) observes the information item (i.e., state) and persuades large models (Receiver) to elicit improved responses. The Receiver then generates a response based on the input, the signal from the Advisor, and its updated belief about the information item. Through training using our framework, we demonstrate that the Advisor can significantly enhance the performance of various Receivers across a range of tasks. We theoretically analyze our persuasion framework and provide an upper bound on the Advisor's regret, confirming its effectiveness in learning the optimal signaling strategy. Our Empirical results demonstrates that GPT-2 can significantly improve the performance of various models, achieving an average enhancement of 16.1% in mathematical reasoning ability and 13.7% in code generation. We hope our work can provide an initial step toward rethinking the alignment framework from the Bayesian Persuasion perspective.\n\n\nEfficient Model-agnostic Alignment via Bayesian Persuasion</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.18718v1</guid>
      <dc:creator>Fengshuo Bai, Mingzhi Wang, Zhaowei Zhang, Boyuan Chen, Yinda Xu, Ying Wen, Yaodong Yang</dc:creator>
      <pubDate>Wed, 29 May 2024 02:57:07 GMT</pubDate>
    </item>
    <item>
      <title>JMedLoRA:Medical Domain Adaptation on Japanese Large Language Models using Instruction-tuning</title>
      <link>http://arxiv.org/abs/2310.10083v2</link>
      <description>In the ongoing wave of impact driven by large language models (LLMs) like ChatGPT, the adaptation of LLMs to medical domain has emerged as a crucial research frontier. Since mainstream LLMs tend to be designed for general-purpose applications, constructing a medical LLM through domain adaptation is a huge challenge. While instruction-tuning is used to fine-tune some LLMs, its precise roles in domain adaptation remain unknown. Here we show the contribution of LoRA-based instruction-tuning to performance in Japanese medical question-answering tasks. In doing so, we employ a multifaceted evaluation for multiple-choice questions, including scoring based on &quot;Exact match&quot; and &quot;Gestalt distance&quot; in addition to the conventional accuracy. Our findings suggest that LoRA-based instruction-tuning can partially incorporate domain-specific knowledge into LLMs, with larger models demonstrating more pronounced effects. Furthermore, our results underscore the potential of adapting English-centric models for Japanese applications in domain adaptation, while also highlighting the persisting limitations of Japanese-centric models. This initiative represents a pioneering effort in enabling medical institutions to fine-tune and operate models without relying on external services.\n\n\nAqulia-Med LLM: Pioneering Full-Process Open-Source Medical Language Models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2310.10083v2</guid>
      <dc:creator>Issey Sukeda, Masahiro Suzuki, Hiroki Sakaji, Satoshi Kodera</dc:creator>
      <pubDate>Fri, 01 Dec 2023 00:29:37 GMT</pubDate>
    </item>
    <item>
      <title>Crowdsourcing with Difficulty: A Bayesian Rating Model for Heterogeneous Items</title>
      <link>http://arxiv.org/abs/2405.19521v1</link>
      <description>In applied statistics and machine learning, the &quot;gold standards&quot; used for training are often biased and almost always noisy. Dawid and Skene's justifiably popular crowdsourcing model adjusts for rater (coder, annotator) sensitivity and specificity, but fails to capture distributional properties of rating data gathered for training, which in turn biases training. In this study, we introduce a general purpose measurement-error model with which we can infer consensus categories by adding item-level effects for difficulty, discriminativeness, and guessability. We further show how to constrain the bimodal posterior of these models to avoid (or if necessary, allow) adversarial raters. We validate our model's goodness of fit with posterior predictive checks, the Bayesian analogue of $\chi^2$ tests. Dawid and Skene's model is rejected by goodness of fit tests, whereas our new model, which adjusts for item heterogeneity, is not rejected. We illustrate our new model with two well-studied data sets, binary rating data for caries in dental X-rays and implication in natural language.\n\n\nCrowdsourcing with Difficulty: A Bayesian Rating Model for Heterogeneous Items</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.19521v1</guid>
      <dc:creator>Seong Woo Han, Ozan Adıgüzel, Bob Carpenter</dc:creator>
      <pubDate>Wed, 29 May 2024 20:59:28 GMT</pubDate>
    </item>
    <item>
      <title>Roleplay-doh: Enabling Domain-Experts to Create LLM-simulated Patients via Eliciting and Adhering to Principles</title>
      <link>http://arxiv.org/abs/2407.00870v2</link>
      <description>Recent works leverage LLMs to roleplay realistic social scenarios, aiding novices in practicing their social skills. However, simulating sensitive interactions, such as in mental health, is challenging. Privacy concerns restrict data access, and collecting expert feedback, although vital, is laborious. To address this, we develop Roleplay-doh, a novel human-LLM collaboration pipeline that elicits qualitative feedback from a domain-expert, which is transformed into a set of principles, or natural language rules, that govern an LLM-prompted roleplay. We apply this pipeline to enable senior mental health supporters to create customized AI patients for simulated practice partners for novice counselors. After uncovering issues in GPT-4 simulations not adhering to expert-defined principles, we also introduce a novel principle-adherence prompting pipeline which shows 30% improvements in response quality and principle following for the downstream task. Via a user study with 25 counseling experts, we demonstrate that the pipeline makes it easy and effective to create AI patients that more faithfully resemble real patients, as judged by creators and third-party counselors. See our project website at https://roleplay-doh.github.io/ for code and data.\n\n\nRoleplay-doh: Enabling Domain-Experts to Create LLM-simulated Patients via Eliciting and Adhering to Principles</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2407.00870v2</guid>
      <dc:creator>Ryan Louie, Ananjan Nandi, William Fang, Cheng Chang, Emma Brunskill, Diyi Yang</dc:creator>
      <pubDate>Sun, 14 Jul 2024 22:35:34 GMT</pubDate>
    </item>
    <item>
      <title>AgentGym: Evolving Large Language Model-based Agents across Diverse Environments</title>
      <link>http://arxiv.org/abs/2406.04151v1</link>
      <description>Building generalist agents that can handle diverse tasks and evolve themselves across different environments is a long-term goal in the AI community. Large language models (LLMs) are considered a promising foundation to build such agents due to their generalized capabilities. Current approaches either have LLM-based agents imitate expert-provided trajectories step-by-step, requiring human supervision, which is hard to scale and limits environmental exploration; or they let agents explore and learn in isolated environments, resulting in specialist agents with limited generalization. In this paper, we take the first step towards building generally-capable LLM-based agents with self-evolution ability. We identify a trinity of ingredients: 1) diverse environments for agent exploration and learning, 2) a trajectory set to equip agents with basic capabilities and prior knowledge, and 3) an effective and scalable evolution method. We propose AgentGym, a new framework featuring a variety of environments and tasks for broad, real-time, uni-format, and concurrent agent exploration. AgentGym also includes a database with expanded instructions, a benchmark suite, and high-quality trajectories across environments. Next, we propose a novel method, AgentEvol, to investigate the potential of agent self-evolution beyond previously seen data across tasks and environments. Experimental results show that the evolved agents can achieve results comparable to SOTA models. We release the AgentGym suite, including the platform, dataset, benchmark, checkpoints, and algorithm implementations. The AgentGym suite is available on https://github.com/WooooDyy/AgentGym.\n\n\nAgentGym: Evolving Large Language Model-based Agents across Diverse Environments</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.04151v1</guid>
      <dc:creator>Zhiheng Xi, Yiwen Ding, Wenxiang Chen, Boyang Hong, Honglin Guo, Junzhe Wang, Dingwen Yang, Chenyang Liao, Xin Guo, Wei He, Songyang Gao, Lu Chen, Rui Zheng, Yicheng Zou, Tao Gui, Qi Zhang, Xipeng Qiu, Xuanjing Huang, Zuxuan Wu, Yu-Gang Jiang</dc:creator>
      <pubDate>Thu, 06 Jun 2024 15:15:41 GMT</pubDate>
    </item>
    <item>
      <title>Synthetic Imitation Edit Feedback for Factual Alignment in Clinical Summarization</title>
      <link>http://arxiv.org/abs/2310.20033v2</link>
      <description>Large Language Models (LLMs) like the GPT and LLaMA families have demonstrated exceptional capabilities in capturing and condensing critical contextual information and achieving state-of-the-art performance in the summarization task. However, community concerns about these models' hallucination issues continue to rise. LLMs sometimes generate factually hallucinated summaries, which can be extremely harmful in the clinical domain NLP tasks (e.g., clinical note summarization), where factually incorrect statements can lead to critically erroneous diagnoses. Fine-tuning LLMs using human feedback has shown the promise of aligning LLMs to be factually consistent during generation, but such training procedure requires high-quality human-annotated data, which can be extremely expensive to get in the clinical domain. In this work, we propose a new pipeline using ChatGPT instead of human experts to generate high-quality feedback data for improving factual consistency in the clinical note summarization task. We focus specifically on edit feedback because recent work discusses the shortcomings of human alignment via preference feedback in complex situations (such as clinical NLP tasks that require extensive expert knowledge), as well as some advantages of collecting edit feedback from domain experts. In addition, although GPT has reached the expert level in many clinical NLP tasks (e.g., USMLE QA), there is not much previous work discussing whether GPT can generate expert-level edit feedback for LMs in the clinical note summarization task. We hope to fill this gap. Finally, our evaluations demonstrate the potential use of GPT edits in human alignment, especially from a factuality perspective.\n\n\nSYNFAC-EDIT: Synthetic Imitation Edit Feedback for Factual Alignment in Clinical Summarization</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2310.20033v2</guid>
      <dc:creator>Prakamya Mishra, Zonghai Yao, Shuwei Chen, Beining Wang, Rohan Mittal, Hong Yu</dc:creator>
      <pubDate>Fri, 03 Nov 2023 13:49:16 GMT</pubDate>
    </item>
    <item>
      <title>Can LLMs Learn by Teaching? A Preliminary Study</title>
      <link>http://arxiv.org/abs/2406.14629v1</link>
      <description>Teaching to improve student models (e.g., knowledge distillation) is an extensively studied methodology in LLMs. However, for humans, teaching not only improves students but also improves teachers. We ask: Can LLMs also learn by teaching (LbT)? If yes, we can potentially unlock the possibility of continuously advancing the models without solely relying on human-produced data or stronger models. In this paper, we provide a preliminary exploration of this ambitious agenda. We show that LbT ideas can be incorporated into existing LLM training/prompting pipelines and provide noticeable improvements. Specifically, we design three methods, each mimicking one of the three levels of LbT in humans: observing students' feedback, learning from the feedback, and learning iteratively, with the goals of improving answer accuracy without training and improving models' inherent capability with fine-tuning. The findings are encouraging. For example, similar to LbT in human, we see that: (1) LbT can induce weak-to-strong generalization: strong models can improve themselves by teaching other weak models; (2) Diversity in students might help: teaching multiple students could be better than teaching one student or the teacher itself. We hope that this early promise can inspire future research on LbT and more broadly adopting the advanced techniques in education to improve LLMs. The code is available at https://github.com/imagination-research/lbt.\n\n\nCan LLMs Learn by Teaching? A Preliminary Study</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.14629v1</guid>
      <dc:creator>Xuefei Ning, Zifu Wang, Shiyao Li, Zinan Lin, Peiran Yao, Tianyu Fu, Matthew B. Blaschko, Guohao Dai, Huazhong Yang, Yu Wang</dc:creator>
      <pubDate>Thu, 20 Jun 2024 18:00:17 GMT</pubDate>
    </item>
    <item>
      <title>DeTikZify: Synthesizing Graphics Programs for Scientific Figures and Sketches with TikZ</title>
      <link>http://arxiv.org/abs/2405.15306v2</link>
      <description>Creating high-quality scientific figures can be time-consuming and challenging, even though sketching ideas on paper is relatively easy. Furthermore, recreating existing figures that are not stored in formats preserving semantic information is equally complex. To tackle this problem, we introduce DeTikZify, a novel multimodal language model that automatically synthesizes scientific figures as semantics-preserving TikZ graphics programs based on sketches and existing figures. To achieve this, we create three new datasets: DaTikZv2, the largest TikZ dataset to date, containing over 360k human-created TikZ graphics; SketchFig, a dataset that pairs hand-drawn sketches with their corresponding scientific figures; and SciCap++, a collection of diverse scientific figures and associated metadata. We train DeTikZify on SciCap++ and DaTikZv2, along with synthetically generated sketches learned from SketchFig. We also introduce an MCTS-based inference algorithm that enables DeTikZify to iteratively refine its outputs without the need for additional training. Through both automatic and human evaluation, we demonstrate that DeTikZify outperforms commercial Claude 3 and GPT-4V in synthesizing TikZ programs, with the MCTS algorithm effectively boosting its performance. We make our code, models, and datasets publicly available.\n\n\nDeTikZify: Synthesizing Graphics Programs for Scientific Figures and Sketches with TikZ</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.15306v2</guid>
      <dc:creator>Jonas Belouadi, Simone Paolo Ponzetto, Steffen Eger</dc:creator>
      <pubDate>Tue, 28 May 2024 06:48:58 GMT</pubDate>
    </item>
    <item>
      <title>Tagengo: A Multilingual Chat Dataset</title>
      <link>http://arxiv.org/abs/2405.12612v1</link>
      <description>Open source large language models (LLMs) have shown great improvements in recent times. However, many of these models are focused solely on popular spoken languages. We present a high quality dataset of more than 70k prompt-response pairs in 74 languages which consist of human generated prompts and synthetic responses. We use this dataset to train a state-of-the-art open source English LLM to chat multilingually. We evaluate our model on MT-Bench chat benchmarks in 6 languages, finding that our multilingual model outperforms previous state-of-the-art open source LLMs across each language. We further find that training on more multilingual data is beneficial to the performance in a chosen target language (Japanese) compared to simply training on only data in that language. These results indicate the necessity of training on large amounts of high quality multilingual data to make a more accessible LLM.\n\n\nTagengo: A Multilingual Chat Dataset</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.12612v1</guid>
      <dc:creator>Peter Devine</dc:creator>
      <pubDate>Tue, 21 May 2024 09:06:36 GMT</pubDate>
    </item>
    <item>
      <title>Automated Multi-Language to English Machine Translation Using Generative Pre-Trained Transformers</title>
      <link>http://arxiv.org/abs/2404.14680v1</link>
      <description>The task of accurate and efficient language translation is an extremely important information processing task. Machine learning enabled and automated translation that is accurate and fast is often a large topic of interest in the machine learning and data science communities. In this study, we examine using local Generative Pretrained Transformer (GPT) models to perform automated zero shot black-box, sentence wise, multi-natural-language translation into English text. We benchmark 16 different open-source GPT models, with no custom fine-tuning, from the Huggingface LLM repository for translating 50 different non-English languages into English using translated TED Talk transcripts as the reference dataset. These GPT model inference calls are performed strictly locally, on single A100 Nvidia GPUs. Benchmark metrics that are reported are language translation accuracy, using BLEU, GLEU, METEOR, and chrF text overlap measures, and wall-clock time for each sentence translation. The best overall performing GPT model for translating into English text for the BLEU metric is ReMM-v2-L2-13B with a mean score across all tested languages of $0.152$, for the GLEU metric is ReMM-v2-L2-13B with a mean score across all tested languages of $0.256$, for the chrF metric is Llama2-chat-AYT-13B with a mean score across all tested languages of $0.448$, and for the METEOR metric is ReMM-v2-L2-13B with a mean score across all tested languages of $0.438$.\n\n\nAutomated Multi-Language to English Machine Translation Using Generative Pre-Trained Transformers</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2404.14680v1</guid>
      <dc:creator>Elijah Pelofske, Vincent Urias, Lorie M. Liebrock</dc:creator>
      <pubDate>Tue, 23 Apr 2024 02:19:35 GMT</pubDate>
    </item>
    <item>
      <title>Bayesian Constraint Inference from User Demonstrations Based on Margin-Respecting Preference Models</title>
      <link>http://arxiv.org/abs/2403.02431v1</link>
      <description>It is crucial for robots to be aware of the presence of constraints in order to acquire safe policies. However, explicitly specifying all constraints in an environment can be a challenging task. State-of-the-art constraint inference algorithms learn constraints from demonstrations, but tend to be computationally expensive and prone to instability issues. In this paper, we propose a novel Bayesian method that infers constraints based on preferences over demonstrations. The main advantages of our proposed approach are that it 1) infers constraints without calculating a new policy at each iteration, 2) uses a simple and more realistic ranking of groups of demonstrations, without requiring pairwise comparisons over all demonstrations, and 3) adapts to cases where there are varying levels of constraint violation. Our empirical results demonstrate that our proposed Bayesian approach infers constraints of varying severity, more accurately than state-of-the-art constraint inference methods.\n\n\nBayesian Constraint Inference from User Demonstrations Based on Margin-Respecting Preference Models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2403.02431v1</guid>
      <dc:creator>Dimitris Papadimitriou, Daniel S. Brown</dc:creator>
      <pubDate>Mon, 04 Mar 2024 19:23:50 GMT</pubDate>
    </item>
    <item>
      <title>Training LLMs to Better Self-Debug and Explain Code</title>
      <link>http://arxiv.org/abs/2405.18649v1</link>
      <description>In the domain of code generation, self-debugging is crucial. It allows LLMs to refine their generated code based on execution feedback. This is particularly important because generating correct solutions in one attempt proves challenging for complex tasks. Prior works on self-debugging mostly focus on prompting methods by providing LLMs with few-shot examples, which work poorly on small open-sourced LLMs. In this work, we propose a training framework that significantly improves self-debugging capability of LLMs. Intuitively, we observe that a chain of explanations on the wrong code followed by code refinement helps LLMs better analyze the wrong code and do refinement. We thus propose an automated pipeline to collect a high-quality dataset for code explanation and refinement by generating a number of explanations and refinement trajectories and filtering via execution verification. We perform supervised fine-tuning (SFT) and further reinforcement learning (RL) on both success and failure trajectories with a novel reward design considering code explanation and refinement quality. SFT improves the pass@1 by up to 15.92% and pass@10 by 9.30% over four benchmarks. RL training brings additional up to 3.54% improvement on pass@1 and 2.55% improvement on pass@10. The trained LLMs show iterative refinement ability, and can keep refining code continuously. Lastly, our human evaluation shows that the LLMs trained with our framework generate more useful code explanations and help developers better understand bugs in source code.\n\n\nTraining LLMs to Better Self-Debug and Explain Code</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.18649v1</guid>
      <dc:creator>Nan Jiang, Xiaopeng Li, Shiqi Wang, Qiang Zhou, Soneya Binta Hossain, Baishakhi Ray, Varun Kumar, Xiaofei Ma, Anoop Deoras</dc:creator>
      <pubDate>Tue, 28 May 2024 23:20:24 GMT</pubDate>
    </item>
    <item>
      <title>Knowledge Editing in Language Models via Adapted Direct Preference Optimization</title>
      <link>http://arxiv.org/abs/2406.09920v1</link>
      <description>Large Language Models (LLMs) can become outdated over time as they may lack updated world knowledge, leading to factual knowledge errors and gaps. Knowledge Editing (KE) aims to overcome this challenge using weight updates that do not require expensive retraining. We propose treating KE as an LLM alignment problem. Toward this goal, we introduce Knowledge Direct Preference Optimization (KDPO), a variation of the Direct Preference Optimization (DPO) that is more effective for knowledge modifications. Our method is based on an online approach that continually updates the knowledge stored in the model. We use the current knowledge as a negative sample and the new knowledge we want to introduce as a positive sample in a process called DPO. We also use teacher-forcing for negative sample generation and optimize using the positive sample, which helps maintain localized changes. We tested our KE method on various datasets and models, comparing it to several cutting-edge methods, with 100 and 500 sequential edits. Additionally, we conducted an ablation study comparing our method to the standard DPO approach. Our experimental results show that our modified DPO method allows for more refined KE, achieving similar or better performance compared to previous methods.\n\n\nKnowledge Editing in Language Models via Adapted Direct Preference Optimization</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.09920v1</guid>
      <dc:creator>Amit Rozner, Barak Battash, Lior Wolf, Ofir Lindenbaum</dc:creator>
      <pubDate>Fri, 14 Jun 2024 11:02:21 GMT</pubDate>
    </item>
    <item>
      <title>PERL: Parameter Efficient Reinforcement Learning from Human Feedback</title>
      <link>http://arxiv.org/abs/2403.10704v1</link>
      <description>Reinforcement Learning from Human Feedback (RLHF) has proven to be a strong method to align Pretrained Large Language Models (LLMs) with human preferences. But training models with RLHF is computationally expensive, and an overall complex process. In this work, we study RLHF where the underlying models are trained using the parameter efficient method of Low-Rank Adaptation (LoRA) introduced by Hu et al. [2021]. We investigate the setup of &quot;Parameter Efficient Reinforcement Learning&quot; (PERL), in which we perform reward model training and reinforcement learning using LoRA. We compare PERL to conventional fine-tuning (full-tuning) across various configurations for 7 benchmarks, including 2 novel datasets, of reward modeling and reinforcement learning. We find that PERL performs on par with the conventional RLHF setting, while training faster, and with less memory. This enables the high performance of RLHF, while reducing the computational burden that limits its adoption as an alignment technique for Large Language Models. We also release 2 novel thumbs up/down preference datasets: &quot;Taskmaster Coffee&quot;, and &quot;Taskmaster Ticketing&quot; to promote research around RLHF.\n\n\nPERL: Parameter Efficient Reinforcement Learning from Human Feedback</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2403.10704v1</guid>
      <dc:creator>Hakim Sidahmed, Samrat Phatale, Alex Hutcheson, Zhuonan Lin, Zhang Chen, Zac Yu, Jarvis Jin, Roman Komarytsia, Christiane Ahlheim, Yonghao Zhu, Simral Chaudhary, Bowen Li, Saravanan Ganesh, Bill Byrne, Jessica Hoffmann, Hassan Mansoor, Wei Li, Abhinav Rastogi, Lucas Dixon</dc:creator>
      <pubDate>Fri, 15 Mar 2024 21:43:46 GMT</pubDate>
    </item>
    <item>
      <title>On Overcoming Miscalibrated Conversational Priors in LLM-based Chatbots</title>
      <link>http://arxiv.org/abs/2406.01633v1</link>
      <description>We explore the use of Large Language Model (LLM-based) chatbots to power recommender systems. We observe that the chatbots respond poorly when they encounter under-specified requests (e.g., they make incorrect assumptions, hedge with a long response, or refuse to answer). We conjecture that such miscalibrated response tendencies (i.e., conversational priors) can be attributed to LLM fine-tuning using annotators -- single-turn annotations may not capture multi-turn conversation utility, and the annotators' preferences may not even be representative of users interacting with a recommender system.   We first analyze public LLM chat logs to conclude that query under-specification is common. Next, we study synthetic recommendation problems with configurable latent item utilities and frame them as Partially Observed Decision Processes (PODP). We find that pre-trained LLMs can be sub-optimal for PODPs and derive better policies that clarify under-specified queries when appropriate. Then, we re-calibrate LLMs by prompting them with learned control messages to approximate the improved policy. Finally, we show empirically that our lightweight learning approach effectively uses logged conversation data to re-calibrate the response strategies of LLM-based chatbots for recommendation tasks.\n\n\nOn Overcoming Miscalibrated Conversational Priors in LLM-based Chatbots</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.01633v1</guid>
      <dc:creator>Christine Herlihy, Jennifer Neville, Tobias Schnabel, Adith Swaminathan</dc:creator>
      <pubDate>Sat, 01 Jun 2024 15:54:45 GMT</pubDate>
    </item>
    <item>
      <title>Flooding Spread of Manipulated Knowledge in LLM-Based Multi-Agent Communities</title>
      <link>http://arxiv.org/abs/2407.07791v1</link>
      <description>The rapid adoption of large language models (LLMs) in multi-agent systems has highlighted their impressive capabilities in various applications, such as collaborative problem-solving and autonomous negotiation. However, the security implications of these LLM-based multi-agent systems have not been thoroughly investigated, particularly concerning the spread of manipulated knowledge. In this paper, we investigate this critical issue by constructing a detailed threat model and a comprehensive simulation environment that mirrors real-world multi-agent deployments in a trusted platform. Subsequently, we propose a novel two-stage attack method involving Persuasiveness Injection and Manipulated Knowledge Injection to systematically explore the potential for manipulated knowledge (i.e., counterfactual and toxic knowledge) spread without explicit prompt manipulation.   Our method leverages the inherent vulnerabilities of LLMs in handling world knowledge, which can be exploited by attackers to unconsciously spread fabricated information. Through extensive experiments, we demonstrate that our attack method can successfully induce LLM-based agents to spread both counterfactual and toxic knowledge without degrading their foundational capabilities during agent communication. Furthermore, we show that these manipulations can persist through popular retrieval-augmented generation frameworks, where several benign agents store and retrieve manipulated chat histories for future interactions. This persistence indicates that even after the interaction has ended, the benign agents may continue to be influenced by manipulated knowledge. Our findings reveal significant security risks in LLM-based multi-agent systems, emphasizing the imperative need for robust defenses against manipulated knowledge spread, such as introducing ``guardian'' agents and advanced fact-checking tools.\n\n\nFlooding Spread of Manipulated Knowledge in LLM-Based Multi-Agent Communities</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2407.07791v1</guid>
      <dc:creator>Tianjie Ju, Yiting Wang, Xinbei Ma, Pengzhou Cheng, Haodong Zhao, Yulong Wang, Lifeng Liu, Jian Xie, Zhuosheng Zhang, Gongshen Liu</dc:creator>
      <pubDate>Wed, 10 Jul 2024 16:08:46 GMT</pubDate>
    </item>
    <item>
      <title>Addressing Topic Granularity and Hallucination in Large Language Models for Topic Modelling</title>
      <link>http://arxiv.org/abs/2405.00611v1</link>
      <description>Large language models (LLMs) with their strong zero-shot topic extraction capabilities offer an alternative to probabilistic topic modelling and closed-set topic classification approaches. As zero-shot topic extractors, LLMs are expected to understand human instructions to generate relevant and non-hallucinated topics based on the given documents. However, LLM-based topic modelling approaches often face difficulties in generating topics with adherence to granularity as specified in human instructions, often resulting in many near-duplicate topics. Furthermore, methods for addressing hallucinated topics generated by LLMs have not yet been investigated. In this paper, we focus on addressing the issues of topic granularity and hallucinations for better LLM-based topic modelling. To this end, we introduce a novel approach that leverages Direct Preference Optimisation (DPO) to fine-tune open-source LLMs, such as Mistral-7B. Our approach does not rely on traditional human annotation to rank preferred answers but employs a reconstruction pipeline to modify raw topics generated by LLMs, thus enabling a fast and efficient training and inference framework. Comparative experiments show that our fine-tuning approach not only significantly improves the LLM's capability to produce more coherent, relevant, and precise topics, but also reduces the number of hallucinated topics.\n\n\nAddressing Topic Granularity and Hallucination in Large Language Models for Topic Modelling</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.00611v1</guid>
      <dc:creator>Yida Mu, Peizhen Bai, Kalina Bontcheva, Xingyi Song</dc:creator>
      <pubDate>Wed, 01 May 2024 16:32:07 GMT</pubDate>
    </item>
    <item>
      <title>Ensuring Safe and High-Quality Outputs: A Guideline Library Approach for Language Models</title>
      <link>http://arxiv.org/abs/2403.11838v2</link>
      <description>Large Language Models (LLMs) exhibit impressive capabilities but also present risks such as biased content generation and privacy issues. One of the current alignment techniques includes principle-driven integration, but it faces challenges arising from the imprecision of manually crafted rules and inadequate risk perception in models without safety training. To address these, we introduce Guide-Align, a two-stage approach. Initially, a safety-trained model identifies potential risks and formulates specific guidelines for various inputs, establishing a comprehensive library of guidelines and a model for input-guidelines retrieval. Subsequently, the retrieval model correlates new inputs with relevant guidelines, which guide LLMs in response generation to ensure safe and high-quality outputs, thereby aligning with human values. An additional optional stage involves fine-tuning a model with well-aligned datasets generated through the process implemented in the second stage. Our method customizes guidelines to accommodate diverse inputs, thereby enhancing the fine-grainedness and comprehensiveness of the guideline library. Furthermore, it incorporates safety expertise from a safety-trained LLM through a lightweight retrieval model. We evaluate our approach on three benchmarks, demonstrating significant improvements in LLM security and quality. Notably, our fine-tuned model, Labrador, even at 13 billion parameters, outperforms GPT-3.5-turbo and surpasses GPT-4 in alignment capabilities.\n\n\nEnsuring Safe and High-Quality Outputs: A Guideline Library Approach for Language Models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2403.11838v2</guid>
      <dc:creator>Yi Luo, Zhenghao Lin, Yuhao Zhang, Jiashuo Sun, Chen Lin, Chengjin Xu, Xiangdong Su, Yelong Shen, Jian Guo, Yeyun Gong</dc:creator>
      <pubDate>Sat, 23 Mar 2024 06:26:41 GMT</pubDate>
    </item>
    <item>
      <title>Efficient Two-Phase Offline Deep Reinforcement Learning from Preference Feedback</title>
      <link>http://arxiv.org/abs/2401.00330v1</link>
      <description>In this work, we consider the offline preference-based reinforcement learning problem. We focus on the two-phase learning approach that is prevalent in previous reinforcement learning from human preference works. We find a challenge in applying two-phase learning in the offline PBRL setting that the learned utility model can be too hard for the learning agent to optimize during the second learning phase. To overcome the challenge, we propose a two-phasing learning approach under behavior regularization through action clipping. The insight is that the state-actions which are poorly covered by the dataset can only provide limited information and increase the complexity of the problem in the second learning phase. Our method ignores such state-actions during the second learning phase to achieve higher learning efficiency. We empirically verify that our method has high learning efficiency on a variety of datasets in robotic control environments.\n\n\nEfficient Two-Phase Offline Deep Reinforcement Learning from Preference Feedback</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2401.00330v1</guid>
      <dc:creator>Yinglun Xu, Gagandeep Singh</dc:creator>
      <pubDate>Sat, 30 Dec 2023 21:37:18 GMT</pubDate>
    </item>
    <item>
      <title>Optimizing Language Models for Human Preferences is a Causal Inference Problem</title>
      <link>http://arxiv.org/abs/2402.14979v2</link>
      <description>As large language models (LLMs) see greater use in academic and commercial settings, there is increasing interest in methods that allow language models to generate texts aligned with human preferences. In this paper, we present an initial exploration of language model optimization for human preferences from direct outcome datasets, where each sample consists of a text and an associated numerical outcome measuring the reader's response. We first propose that language model optimization should be viewed as a causal problem to ensure that the model correctly learns the relationship between the text and the outcome. We formalize this causal language optimization problem, and we develop a method--causal preference optimization (CPO)--that solves an unbiased surrogate objective for the problem. We further extend CPO with doubly robust CPO (DR-CPO), which reduces the variance of the surrogate objective while retaining provably strong guarantees on bias. Finally, we empirically demonstrate the effectiveness of (DR-)CPO in optimizing state-of-the-art LLMs for human preferences on direct outcome data, and we validate the robustness of DR-CPO under difficult confounding conditions.\n\n\nOptimizing Language Models for Human Preferences is a Causal Inference Problem</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.14979v2</guid>
      <dc:creator>Victoria Lin, Eli Ben-Michael, Louis-Philippe Morency</dc:creator>
      <pubDate>Wed, 05 Jun 2024 23:19:19 GMT</pubDate>
    </item>
    <item>
      <title>EmpathyEar: An Open-source Avatar Multimodal Empathetic Chatbot</title>
      <link>http://arxiv.org/abs/2406.15177v1</link>
      <description>This paper introduces EmpathyEar, a pioneering open-source, avatar-based multimodal empathetic chatbot, to fill the gap in traditional text-only empathetic response generation (ERG) systems. Leveraging the advancements of a large language model, combined with multimodal encoders and generators, EmpathyEar supports user inputs in any combination of text, sound, and vision, and produces multimodal empathetic responses, offering users, not just textual responses but also digital avatars with talking faces and synchronized speeches. A series of emotion-aware instruction-tuning is performed for comprehensive emotional understanding and generation capabilities. In this way, EmpathyEar provides users with responses that achieve a deeper emotional resonance, closely emulating human-like empathy. The system paves the way for the next emotional intelligence, for which we open-source the code for public access.\n\n\nAn Empathetic User-Centric Chatbot for Emotional Support</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.15177v1</guid>
      <dc:creator>Hao Fei, Han Zhang, Bin Wang, Lizi Liao, Qian Liu, Erik Cambria</dc:creator>
      <pubDate>Fri, 21 Jun 2024 14:23:31 GMT</pubDate>
    </item>
    <item>
      <title>A Declarative System for Optimizing AI Workloads</title>
      <link>http://arxiv.org/abs/2405.14696v2</link>
      <description>A long-standing goal of data management systems has been to build systems which can compute quantitative insights over large corpora of unstructured data in a cost-effective manner. Until recently, it was difficult and expensive to extract facts from company documents, data from scientific papers, or metrics from image and video corpora. Today's models can accomplish these tasks with high accuracy. However, a programmer who wants to answer a substantive AI-powered query must orchestrate large numbers of models, prompts, and data operations. For even a single query, the programmer has to make a vast number of decisions such as the choice of model, the right inference method, the most cost-effective inference hardware, the ideal prompt design, and so on. The optimal set of decisions can change as the query changes and as the rapidly-evolving technical landscape shifts. In this paper we present Palimpzest, a system that enables anyone to process AI-powered analytical queries simply by defining them in a declarative language. The system uses its cost optimization framework to implement the query plan with the best trade-offs between runtime, financial cost, and output data quality. We describe the workload of AI-powered analytics tasks, the optimization methods that Palimpzest uses, and the prototype system itself. We evaluate Palimpzest on tasks in Legal Discovery, Real Estate Search, and Medical Schema Matching. We show that even our simple prototype offers a range of appealing plans, including one that is 3.3x faster and 2.9x cheaper than the baseline method, while also offering better data quality. With parallelism enabled, Palimpzest can produce plans with up to a 90.3x speedup at 9.1x lower cost relative to a single-threaded GPT-4 baseline, while obtaining an F1-score within 83.5% of the baseline. These require no additional work by the user.\n\n\nA Declarative System for Optimizing AI Workloads</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.14696v2</guid>
      <dc:creator>Chunwei Liu, Matthew Russo, Michael Cafarella, Lei Cao, Peter Baille Chen, Zui Chen, Michael Franklin, Tim Kraska, Samuel Madden, Gerardo Vitagliano</dc:creator>
      <pubDate>Wed, 29 May 2024 15:27:07 GMT</pubDate>
    </item>
    <item>
      <title>Optimal Reward Labeling: Bridging Offline Preference and Reward-Based Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2406.10445v2</link>
      <description>Offline reinforcement learning has become one of the most practical RL settings. A recent success story has been RLHF, offline preference-based RL (PBRL) with preference from humans. However, most existing works on offline RL focus on the standard setting with scalar reward feedback. It remains unknown how to universally transfer the existing rich understanding of offline RL from the reward-based to the preference-based setting. In this work, we propose a general framework to bridge this gap. Our key insight is transforming preference feedback to scalar rewards via optimal reward labeling (ORL), and then any reward-based offline RL algorithms can be applied to the dataset with the reward labels. We theoretically show the connection between several recent PBRL techniques and our framework combined with specific offline RL algorithms in terms of how they utilize the preference signals. By combining reward labeling with different algorithms, our framework can lead to new and potentially more efficient offline PBRL algorithms. We empirically test our framework on preference datasets based on the standard D4RL benchmark. When combined with a variety of efficient reward-based offline RL algorithms, the learning result achieved under our framework is comparable to training the same algorithm on the dataset with actual rewards in many cases and better than the recent PBRL baselines in most cases.\n\n\nOptimal Reward Labeling: Bridging Offline Preference and Reward-Based Reinforcement Learning</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.10445v2</guid>
      <dc:creator>Yinglun Xu, David Zhu, Rohan Gumaste, Gagandeep Singh</dc:creator>
      <pubDate>Sat, 06 Jul 2024 20:03:16 GMT</pubDate>
    </item>
    <item>
      <title>Weak-to-Strong Search: Align Large Language Models via Searching over Small Language Models</title>
      <link>http://arxiv.org/abs/2405.19262v1</link>
      <description>Large language models are usually fine-tuned to align with human preferences. However, fine-tuning a large language model can be challenging. In this work, we introduce $\textit{weak-to-strong search}$, framing the alignment of a large language model as a test-time greedy search to maximize the log-likelihood difference between small tuned and untuned models while sampling from the frozen large model. This method serves both as (i) a compute-efficient model up-scaling strategy that avoids directly tuning the large model and as (ii) an instance of weak-to-strong generalization that enhances a strong model with weak test-time guidance. Empirically, we demonstrate the flexibility of weak-to-strong search across different tasks. In controlled-sentiment generation and summarization, we use tuned and untuned $\texttt{gpt2}$s to effectively improve the alignment of large models without additional training. Crucially, in a more difficult instruction-following benchmark, AlpacaEval 2.0, we show that reusing off-the-shelf small model pairs (e.g., $\texttt{zephyr-7b-beta}$ and its untuned version) can significantly improve the length-controlled win rates of both white-box and black-box large models against $\texttt{gpt-4-turbo}$ (e.g., $34.4 \rightarrow 37.9$ for $\texttt{Llama-3-70B-Instruct}$ and $16.0 \rightarrow 20.1$ for $\texttt{gpt-3.5-turbo-instruct}$), despite the small models' low win rates $\approx 10.0$.\n\n\nWeak-to-Strong Search: Align Large Language Models via Searching over Small Language Models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.19262v1</guid>
      <dc:creator>Zhanhui Zhou, Zhixuan Liu, Jie Liu, Zhichen Dong, Chao Yang, Yu Qiao</dc:creator>
      <pubDate>Wed, 29 May 2024 16:55:32 GMT</pubDate>
    </item>
    <item>
      <title>Mitigating Hallucination in Visual Language Models with Visual Supervision</title>
      <link>http://arxiv.org/abs/2311.16479v1</link>
      <description>Large vision-language models (LVLMs) suffer from hallucination a lot, generating responses that apparently contradict to the image content occasionally. The key problem lies in its weak ability to comprehend detailed content in a multi-modal context, which can be mainly attributed to two factors in training data and loss function. The vision instruction dataset primarily focuses on global description, and the auto-regressive loss function favors text modeling rather than image understanding. In this paper, we bring more detailed vision annotations and more discriminative vision models to facilitate the training of LVLMs, so that they can generate more precise responses without encounter hallucination. On one hand, we generate image-text pairs with detailed relationship annotations in panoptic scene graph dataset (PSG). These conversations pay more attention on detailed facts in the image, encouraging the model to answer questions based on multi-modal contexts. On the other hand, we integrate SAM and mask prediction loss as auxiliary supervision, forcing the LVLMs to have the capacity to identify context-related objects, so that they can generate more accurate responses, mitigating hallucination. Moreover, to provide a deeper evaluation on the hallucination in LVLMs, we propose a new benchmark, RAH-Bench. It divides vision hallucination into three different types that contradicts the image with wrong categories, attributes or relations, and introduces False Positive Rate as detailed sub-metric for each type. In this benchmark, our approach demonstrates an +8.4% enhancement compared to original LLaVA and achieves widespread performance improvements across other models.\n\n\nVisual Evidence Prompting Mitigates Hallucinations in Multimodal Large Language Models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2311.16479v1</guid>
      <dc:creator>Zhiyang Chen, Yousong Zhu, Yufei Zhan, Zhaowen Li, Chaoyang Zhao, Jinqiao Wang, Ming Tang</dc:creator>
      <pubDate>Mon, 27 Nov 2023 09:30:02 GMT</pubDate>
    </item>
    <item>
      <title>LongForm: Effective Instruction Tuning with Reverse Instructions</title>
      <link>http://arxiv.org/abs/2304.08460v2</link>
      <description>Instruction tuning enables language models to more effectively generalize and better follow user intent. However, obtaining instruction data is costly and challenging. Prior work employs methods such as expensive human annotation, crowd-sourced datasets with alignment issues, and generating noisy examples via LLMs. We introduce the LongForm-C dataset, which is created by reverse instructions. We generate instructions via LLMs for human-written corpus examples using reverse instructions. First we select a diverse set of human-written documents from corpora such as C4 and Wikipedia; then we generate instructions for these documents via LLMs. This approach provides a cheaper and cleaner instruction-tuning dataset with natural output and one suitable for long text generation. Our models outperform 10x larger language models without instruction tuning on tasks such as story/recipe generation and long-form question answering. Moreover, LongForm models outperform prior instruction-tuned models such as FLAN-T5 and Alpaca by a large margin, and improve language understanding capabilities further. Finally, our models can effectively follow and answer multilingual instructions; we demonstrate this for news generation. We publicly release our data and models: https://github.com/akoksal/LongForm.\n\n\nControlling long-form large language model outputs</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2304.08460v2</guid>
      <dc:creator>Abdullatif Köksal, Timo Schick, Anna Korhonen, Hinrich Schütze</dc:creator>
      <pubDate>Wed, 14 Feb 2024 18:00:33 GMT</pubDate>
    </item>
    <item>
      <title>Off-Policy Evaluation from Logged Human Feedback</title>
      <link>http://arxiv.org/abs/2406.10030v1</link>
      <description>Learning from human feedback has been central to recent advances in artificial intelligence and machine learning. Since the collection of human feedback is costly, a natural question to ask is if the new feedback always needs to collected. Or could we evaluate a new model with the human feedback on responses of another model? This motivates us to study off-policy evaluation from logged human feedback. We formalize the problem, propose both model-based and model-free estimators for policy values, and show how to optimize them. We analyze unbiasedness of our estimators and evaluate them empirically. Our estimators can predict the absolute values of evaluated policies, rank them, and be optimized.\n\n\nOff-Policy Evaluation from Logged Human Feedback</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.10030v1</guid>
      <dc:creator>Aniruddha Bhargava, Lalit Jain, Branislav Kveton, Ge Liu, Subhojyoti Mukherjee</dc:creator>
      <pubDate>Fri, 14 Jun 2024 13:38:18 GMT</pubDate>
    </item>
    <item>
      <title>Safe Unlearning: A Surprisingly Effective and Generalizable Solution to Defend Against Jailbreak Attacks</title>
      <link>http://arxiv.org/abs/2407.02855v1</link>
      <description>LLMs are known to be vulnerable to jailbreak attacks, even after safety alignment. An important observation is that, while different types of jailbreak attacks can generate significantly different queries, they mostly result in similar responses that are rooted in the same harmful knowledge (e.g., detailed steps to make a bomb). Therefore, we conjecture that directly unlearn the harmful knowledge in the LLM can be a more effective way to defend against jailbreak attacks than the mainstream supervised fine-tuning (SFT) based approaches. Our extensive experiments confirmed our insight and suggested surprising generalizability of our unlearning-based approach: using only 20 raw harmful questions \emph{without} any jailbreak prompt during training, our solution reduced the Attack Success Rate (ASR) in Vicuna-7B on \emph{out-of-distribution} (OOD) harmful questions wrapped with various complex jailbreak prompts from 82.6\% to 7.7\%. This significantly outperforms Llama2-7B-Chat, which is fine-tuned on about 0.1M safety alignment samples but still has an ASR of 21.9\% even under the help of an additional safety system prompt. Further analysis reveals that the generalization ability of our solution stems from the intrinsic relatedness among harmful responses across harmful questions (e.g., response patterns, shared steps and actions, and similarity among their learned representations in the LLM). Our code is available at \url{https://github.com/thu-coai/SafeUnlearning}.\n\n\nSafe Unlearning: A Surprisingly Effective and Generalizable Solution to Defend Against Jailbreak Attacks</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2407.02855v1</guid>
      <dc:creator>Zhexin Zhang, Junxiao Yang, Pei Ke, Shiyao Cui, Chujie Zheng, Hongning Wang, Minlie Huang</dc:creator>
      <pubDate>Wed, 03 Jul 2024 07:14:05 GMT</pubDate>
    </item>
    <item>
      <title>i-SRT: Aligning Large Multimodal Models for Videos by Iterative Self-Retrospective Judgment</title>
      <link>http://arxiv.org/abs/2406.11280v1</link>
      <description>Aligning Video Large Multimodal Models (VLMMs) face challenges such as modality misalignment and verbose responses. Although iterative approaches such as self-rewarding or iterative direct preference optimization (DPO) recently showed a significant improvement in language model alignment, particularly on reasoning tasks, self-aligned models applied to large video-language models often result in lengthy and irrelevant responses. To address these challenges, we propose a novel method that employs self-retrospection to enhance both response generation and preference modeling, and call iterative self-retrospective judgment (i-SRT). By revisiting and evaluating already generated content and preference in loop, i-SRT improves the alignment between textual and visual modalities, reduce verbosity, and enhances content relevance. Our empirical evaluations across diverse video question answering benchmarks demonstrate that i-SRT significantly outperforms prior arts. We are committed to opensourcing our code, models, and datasets to encourage further investigation.\n\n\ni-SRT: Aligning Large Multimodal Models for Videos by Iterative Self-Retrospective Judgment</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.11280v1</guid>
      <dc:creator>Daechul Ahn, Yura Choi, San Kim, Youngjae Yu, Dongyeop Kang, Jonghyun Choi</dc:creator>
      <pubDate>Mon, 17 Jun 2024 07:33:30 GMT</pubDate>
    </item>
    <item>
      <title>Aligner: One Global Token is Worth Millions of Parameters When Aligning Large Language Models</title>
      <link>http://arxiv.org/abs/2312.05503v1</link>
      <description>We introduce Aligner, a novel Parameter-Efficient Fine-Tuning (PEFT) method for aligning multi-billion-parameter-sized Large Language Models (LLMs). Aligner employs a unique design that constructs a globally shared set of tunable tokens that modify the attention of every layer. Remarkably with this method, even when using one token accounting for a mere 5,000 parameters, Aligner can still perform comparably well to state-of-the-art LLM adaptation methods like LoRA that require millions of parameters. This capacity is substantiated in both instruction following and value alignment tasks. Besides the multiple order-of-magnitude improvement in parameter efficiency, the insight Aligner provides into the internal mechanisms of LLMs is also valuable. The architectural features and efficacy of our method, in addition to our experiments demonstrate that an LLM separates its internal handling of &quot;form&quot; and &quot;knowledge&quot; in a somewhat orthogonal manner. This finding promises to motivate new research into LLM mechanism understanding and value alignment.\n\n\nAligner: One Global Token is Worth Millions of Parameters When Aligning Large Language Models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2312.05503v1</guid>
      <dc:creator>Zhou Ziheng, Yingnian Wu, Song-Chun Zhu, Demetri Terzopoulos</dc:creator>
      <pubDate>Sat, 09 Dec 2023 08:25:55 GMT</pubDate>
    </item>
    <item>
      <title>Stick to Your Role! Context-dependence and Stability of Personal Value Expression in Large Language Models</title>
      <link>http://arxiv.org/abs/2402.14846v3</link>
      <description>The standard way to study Large Language Models (LLMs) with benchmarks or psychology questionnaires is to provide many different queries from similar minimal contexts (e.g. multiple choice questions). However, due to LLMs' highly context-dependent nature, conclusions from such minimal-context evaluations may be little informative about the model's behavior in deployment (where it will be exposed to many new contexts). We argue that context-dependence (specifically, value stability) should be studied a specific property of LLMs and used as another dimension of LLM comparison (alongside others such as cognitive abilities, knowledge, or model size). We present a case-study on the stability of value expression over different contexts (simulated conversations on different topics) as measured using a standard psychology questionnaire (PVQ) and on behavioral downstream tasks. Reusing methods from psychology, we study Rank-order stability on the population (interpersonal) level, and Ipsative stability on the individual (intrapersonal) level. We consider two settings (with and without instructing LLMs to simulate particular personas), two simulated populations, and three downstream tasks. We observe consistent trends in the stability of models and model families - Mixtral, Mistral, GPT-3.5 and Qwen families are more stable than LLaMa-2 and Phi. The consistency of these trends implies that some models exhibit higher value-stability than others, and that value stability can be estimated with the set of introduced methodological tools. When instructed to simulate particular personas, LLMs exhibit low Rank-Order stability, which further diminishes with conversation length. This highlights the need for future research on LLMs that coherently simulate different personas. This paper provides a foundational step in that direction, and, to our knowledge, it is the first study of value stability in LLMs.\n\n\nStick to your Role! Stability of Personal Values Expressed in Large Language Models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.14846v3</guid>
      <dc:creator>Grgur Kovač, Rémy Portelas, Masataka Sawayama, Peter Ford Dominey, Pierre-Yves Oudeyer</dc:creator>
      <pubDate>Tue, 30 Apr 2024 07:09:22 GMT</pubDate>
    </item>
    <item>
      <title>Cascade Reward Sampling for Efficient Decoding-Time Alignment</title>
      <link>http://arxiv.org/abs/2406.16306v1</link>
      <description>Aligning large language models (LLMs) with human preferences is critical for their deployment. Recently, decoding-time alignment has emerged as an effective plug-and-play technique that requires no fine-tuning of model parameters. However, generating text that achieves both high reward and high likelihood remains a significant challenge. Existing methods often fail to generate high-reward text or incur substantial computational costs. In this paper, we propose Cascade Reward Sampling (CARDS) to address both issues, guaranteeing the generation of high-reward and high-likelihood text with significantly low costs. Based on our analysis of reward models (RMs) on incomplete text and our observation that high-reward prefixes induce high-reward complete text, we use rejection sampling to iteratively generate small semantic segments to form such prefixes. The segment length is dynamically determined by the predictive uncertainty of LLMs. This strategy guarantees desirable prefixes for subsequent generations and significantly reduces wasteful token re-generations and the number of reward model scoring. Our experiments demonstrate substantial gains in both generation efficiency and alignment ratings compared to the baselines, achieving five times faster text generation and 99\% win-ties in GPT-4/Claude-3 helpfulness evaluation.\n\n\nCascade Reward Sampling for Efficient Decoding-Time Alignment</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.16306v1</guid>
      <dc:creator>Bolian Li, Yifan Wang, Ananth Grama, Ruqi Zhang</dc:creator>
      <pubDate>Mon, 24 Jun 2024 04:08:35 GMT</pubDate>
    </item>
    <item>
      <title>Object detection on aerial imagery using CenterNet</title>
      <link>http://arxiv.org/abs/1908.08244v1</link>
      <description>Detection and classification of objects in aerial imagery have several applications like urban planning, crop surveillance, and traffic surveillance. However, due to the lower resolution of the objects and the effect of noise in aerial images, extracting distinguishing features for the objects is a challenge. We evaluate CenterNet, a state of the art method for real-time 2D object detection, on the VisDrone2019 dataset. We evaluate the performance of the model with different backbone networks in conjunction with varying resolutions during training and testing.\n\n\nCMCA-YOLO: A Study on a Real-Time Object Detection Model for Parking Lot Surveillance Imagery</description>
      <guid isPermaLink="false">http://arxiv.org/abs/1908.08244v1</guid>
      <dc:creator>Dheeraj Reddy Pailla, Varghese Kollerathu, Sai Saketh Chennamsetty</dc:creator>
      <pubDate>Thu, 22 Aug 2019 08:10:23 GMT</pubDate>
    </item>
    <item>
      <title>LLM-jp: A Cross-organizational Project for the Research and Development of Fully Open Japanese LLMs</title>
      <link>http://arxiv.org/abs/2407.03963v1</link>
      <description>This paper introduces LLM-jp, a cross-organizational project for the research and development of Japanese large language models (LLMs). LLM-jp aims to develop open-source and strong Japanese LLMs, and as of this writing, more than 1,500 participants from academia and industry are working together for this purpose. This paper presents the background of the establishment of LLM-jp, summaries of its activities, and technical reports on the LLMs developed by LLM-jp. For the latest activities, visit https://llm-jp.nii.ac.jp/en/.\n\n\nLLM-jp: A Cross-organizational Project for the Research and Development of Fully Open Japanese LLMs</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2407.03963v1</guid>
      <dc:creator>LLM-jp, :, Akiko Aizawa, Eiji Aramaki, Bowen Chen, Fei Cheng, Hiroyuki Deguchi, Rintaro Enomoto, Kazuki Fujii, Kensuke Fukumoto, Takuya Fukushima, Namgi Han, Yuto Harada, Chikara Hashimoto, Tatsuya Hiraoka, Shohei Hisada, Sosuke Hosokawa, Lu Jie, Keisuke Kamata, Teruhito Kanazawa, Hiroki Kanezashi, Hiroshi Kataoka, Satoru Katsumata, Daisuke Kawahara, Seiya Kawano, Atsushi Keyaki, Keisuke Kiryu, Hirokazu Kiyomaru, Takashi Kodama, Takahiro Kubo, Yohei Kuga, Ryoma Kumon, Shuhei Kurita, Sadao Kurohashi, Conglong Li, Taiki Maekawa, Hiroshi Matsuda, Yusuke Miyao, Kentaro Mizuki, Sakae Mizuki, Yugo Murawaki, Ryo Nakamura, Taishi Nakamura, Kouta Nakayama, Tomoka Nakazato, Takuro Niitsuma, Jiro Nishitoba, Yusuke Oda, Hayato Ogawa, Takumi Okamoto, Naoaki Okazaki, Yohei Oseki, Shintaro Ozaki, Koki Ryu, Rafal Rzepka, Keisuke Sakaguchi, Shota Sasaki, Satoshi Sekine, Kohei Suda, Saku Sugawara, Issa Sugiura, Hiroaki Sugiyama, Hisami Suzuki, Jun Suzuki, Toyotaro Suzumura, Kensuke Tachibana, Yu Takagi, Kyosuke Takami, Koichi Takeda, Masashi Takeshita, Masahiro Tanaka, Kenjiro Taura, Arseny Tolmachev, Nobuhiro Ueda, Zhen Wan, Shuntaro Yada, Sakiko Yahata, Yuya Yamamoto, Yusuke Yamauchi, Hitomi Yanaka, Rio Yokota, Koichiro Yoshino</dc:creator>
      <pubDate>Thu, 04 Jul 2024 14:33:03 GMT</pubDate>
    </item>
    <item>
      <title>Do LLMs Implicitly Determine the Suitable Text Difficulty for Users?</title>
      <link>http://arxiv.org/abs/2402.14453v1</link>
      <description>Education that suits the individual learning level is necessary to improve students' understanding. The first step in achieving this purpose by using large language models (LLMs) is to adjust the textual difficulty of the response to students. This work analyzes how LLMs can implicitly adjust text difficulty between user input and its generated text. To conduct the experiments, we created a new dataset from Stack-Overflow to explore the performance of question-answering-based conversation. Experimental results on the Stack-Overflow dataset and the TSCC dataset, including multi-turn conversation show that LLMs can implicitly handle text difficulty between user input and its generated response. We also observed that some LLMs can surpass humans in handling text difficulty and the importance of instruction-tuning.\n\n\nDo LLMs Implicitly Determine the Suitable Text Difficulty for Users?</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.14453v1</guid>
      <dc:creator>Seiji Gobara, Hidetaka Kamigaito, Taro Watanabe</dc:creator>
      <pubDate>Thu, 22 Feb 2024 11:16:23 GMT</pubDate>
    </item>
    <item>
      <title>SPO: Multi-Dimensional Preference Sequential Alignment With Implicit Reward Modeling</title>
      <link>http://arxiv.org/abs/2405.12739v1</link>
      <description>Human preference alignment is critical in building powerful and reliable large language models (LLMs). However, current methods either ignore the multi-dimensionality of human preferences (e.g. helpfulness and harmlessness) or struggle with the complexity of managing multiple reward models. To address these issues, we propose Sequential Preference Optimization (SPO), a method that sequentially fine-tunes LLMs to align with multiple dimensions of human preferences. SPO avoids explicit reward modeling, directly optimizing the models to align with nuanced human preferences. We theoretically derive closed-form optimal SPO policy and loss function. Gradient analysis is conducted to show how SPO manages to fine-tune the LLMs while maintaining alignment on previously optimized dimensions. Empirical results on LLMs of different size and multiple evaluation datasets demonstrate that SPO successfully aligns LLMs across multiple dimensions of human preferences and significantly outperforms the baselines.\n\n\nSPO: Multi-Dimensional Preference Sequential Alignment With Implicit Reward Modeling</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.12739v1</guid>
      <dc:creator>Xingzhou Lou, Junge Zhang, Jian Xie, Lifeng Liu, Dong Yan, Kaiqi Huang</dc:creator>
      <pubDate>Tue, 21 May 2024 12:47:17 GMT</pubDate>
    </item>
    <item>
      <title>Distributional Preference Alignment of LLMs via Optimal Transport</title>
      <link>http://arxiv.org/abs/2406.05882v1</link>
      <description>Current LLM alignment techniques use pairwise human preferences at a sample level, and as such, they do not imply an alignment on the distributional level. We propose in this paper Alignment via Optimal Transport (AOT), a novel method for distributional preference alignment of LLMs. AOT aligns LLMs on unpaired preference data by making the reward distribution of the positive samples stochastically dominant in the first order on the distribution of negative samples. We introduce a convex relaxation of this first-order stochastic dominance and cast it as an optimal transport problem with a smooth and convex cost. Thanks to the one-dimensional nature of the resulting optimal transport problem and the convexity of the cost, it has a closed-form solution via sorting on empirical measures. We fine-tune LLMs with this AOT objective, which enables alignment by penalizing the violation of the stochastic dominance of the reward distribution of the positive samples on the reward distribution of the negative samples. We analyze the sample complexity of AOT by considering the dual of the OT problem and show that it converges at the parametric rate. Empirically, we show on a diverse set of alignment datasets and LLMs that AOT leads to state-of-the-art models in the 7B family of models when evaluated with Open LLM Benchmarks and AlpacaEval.\n\n\nDistributional Preference Alignment of LLMs via Optimal Transport</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.05882v1</guid>
      <dc:creator>Igor Melnyk, Youssef Mroueh, Brian Belgodere, Mattia Rigotti, Apoorva Nitsure, Mikhail Yurochkin, Kristjan Greenewald, Jiri Navratil, Jerret Ross</dc:creator>
      <pubDate>Sun, 09 Jun 2024 18:41:05 GMT</pubDate>
    </item>
    <item>
      <title>An adaptive variational model for multireference alignment with mixed noise</title>
      <link>http://arxiv.org/abs/2107.10425v1</link>
      <description>Multireference alignment (MRA) problem is to estimate an underlying signal from a large number of noisy circularly-shifted observations. The existing methods are always proposed under the hypothesis of a single Gaussian noise. However, the hypothesis of a single-type noise is inefficient for solving practical problems like single particle cryo-EM. In this paper, We focus on the MRA problem under the assumption of Gaussian mixture noise. We derive an adaptive variational model by combining maximum a posteriori (MAP) estimation and soft-max method. There are two adaptive weights which are for detecting cyclical shifts and types of noise. Furthermore, we provide a statistical interpretation of our model by using expectation-maximization(EM) algorithm. The existence of a minimizer is mathematically proved. The numerical results show that the proposed model has a more impressive performance than the existing methods when one Gaussian noise is large and the other is small.\n\n\nMulti-Reference Preference Optimization for Large Language Models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2107.10425v1</guid>
      <dc:creator>Cuicui Zhao, Jun Liu, Xinqi Gong</dc:creator>
      <pubDate>Thu, 22 Jul 2021 02:44:42 GMT</pubDate>
    </item>
    <item>
      <title>A Bayesian Solution To The Imitation Gap</title>
      <link>http://arxiv.org/abs/2407.00495v1</link>
      <description>In many real-world settings, an agent must learn to act in environments where no reward signal can be specified, but a set of expert demonstrations is available. Imitation learning (IL) is a popular framework for learning policies from such demonstrations. However, in some cases, differences in observability between the expert and the agent can give rise to an imitation gap such that the expert's policy is not optimal for the agent and a naive application of IL can fail catastrophically. In particular, if the expert observes the Markov state and the agent does not, then the expert will not demonstrate the information-gathering behavior needed by the agent but not the expert. In this paper, we propose a Bayesian solution to the Imitation Gap (BIG), first using the expert demonstrations, together with a prior specifying the cost of exploratory behavior that is not demonstrated, to infer a posterior over rewards with Bayesian inverse reinforcement learning (IRL). BIG then uses the reward posterior to learn a Bayes-optimal policy. Our experiments show that BIG, unlike IL, allows the agent to explore at test time when presented with an imitation gap, whilst still learning to behave optimally using expert demonstrations when no such gap exists.\n\n\nA Bayesian Solution To The Imitation Gap</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2407.00495v1</guid>
      <dc:creator>Risto Vuorio, Mattie Fellows, Cong Lu, Clémence Grislain, Shimon Whiteson</dc:creator>
      <pubDate>Sat, 29 Jun 2024 17:13:37 GMT</pubDate>
    </item>
    <item>
      <title>Understanding Jailbreak Success: A Study of Latent Space Dynamics in Large Language Models</title>
      <link>http://arxiv.org/abs/2406.09289v1</link>
      <description>Conversational Large Language Models are trained to refuse to answer harmful questions. However, emergent jailbreaking techniques can still elicit unsafe outputs, presenting an ongoing challenge for model alignment. To better understand how different jailbreak types circumvent safeguards, this paper analyses model activations on different jailbreak inputs. We find that it is possible to extract a jailbreak vector from a single class of jailbreaks that works to mitigate jailbreak effectiveness from other classes. This may indicate that different kinds of effective jailbreaks operate via similar internal mechanisms. We investigate a potential common mechanism of harmfulness feature suppression, and provide evidence for its existence by looking at the harmfulness vector component. These findings offer actionable insights for developing more robust jailbreak countermeasures and lay the groundwork for a deeper, mechanistic understanding of jailbreak dynamics in language models.\n\n\nUnderstanding Jailbreak Success: A Study of Latent Space Dynamics in Large Language Models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.09289v1</guid>
      <dc:creator>Sarah Ball, Frauke Kreuter, Nina Rimsky</dc:creator>
      <pubDate>Thu, 13 Jun 2024 16:26:47 GMT</pubDate>
    </item>
    <item>
      <title>3D-Properties: Identifying Challenges in DPO and Charting a Path Forward</title>
      <link>http://arxiv.org/abs/2406.07327v1</link>
      <description>Aligning large language models (LLMs) with human preference has recently gained tremendous attention, with the canonical yet costly RLHF-PPO and the simple and straightforward Direct Preference Optimization (DPO) as two examples. Despite the efficiency, DPO has rarely be used in the state-of-the-art production-level LLMs, implying its potential pathologies. In this work, we revisit DPO with a comprehensive examination of its empirical efficacy and a systematic comparison with RLHF-PPO. We identify the \textbf{3D}-properties of DPO's learning outcomes: the \textbf{D}rastic drop in the likelihood of rejected responses, the \textbf{D}egradation into LLM unlearning, and the \textbf{D}ispersion effect on unseen responses through experiments with both a carefully designed toy model and practical LLMs on tasks including mathematical problem-solving and instruction following. These findings inherently connect to some observations made by related works and we additionally contribute a plausible theoretical explanation for them. Accordingly, we propose easy regularization methods to mitigate the issues caused by \textbf{3D}-properties, improving the training stability and final performance of DPO. Our contributions also include an investigation into how the distribution of the paired preference data impacts the effectiveness of DPO. We hope this work could offer research directions to narrow the gap between reward-free preference learning methods and reward-based ones.\n\n\n3D-Properties: Identifying Challenges in DPO and Charting a Path Forward</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.07327v1</guid>
      <dc:creator>Yuzi Yan, Yibo Miao, Jialian Li, Yipin Zhang, Jian Xie, Zhijie Deng, Dong Yan</dc:creator>
      <pubDate>Tue, 11 Jun 2024 14:59:24 GMT</pubDate>
    </item>
    <item>
      <title>Prior Constraints-based Reward Model Training for Aligning Large Language Models</title>
      <link>http://arxiv.org/abs/2404.00978v1</link>
      <description>Reinforcement learning with human feedback for aligning large language models (LLMs) trains a reward model typically using ranking loss with comparison pairs.However, the training procedure suffers from an inherent problem: the uncontrolled scaling of reward scores during reinforcement learning due to the lack of constraints while training the reward model.This paper proposes a Prior Constraints-based Reward Model (namely PCRM) training method to mitigate this problem. PCRM incorporates prior constraints, specifically, length ratio and cosine similarity between outputs of each comparison pair, during reward model training to regulate optimization magnitude and control score margins. We comprehensively evaluate PCRM by examining its rank correlation with human preferences and its effectiveness in aligning LLMs via RL. Experimental results demonstrate that PCRM significantly improves alignment performance by effectively constraining reward score scaling. As another bonus, our method is easily integrated into arbitrary rank-based alignment methods, such as direct preference optimization, and can yield consistent improvement.\n\n\nPrior Constraints-based Reward Model Training for Aligning Large Language Models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2404.00978v1</guid>
      <dc:creator>Hang Zhou, Chenglong Wang, Yimin Hu, Tong Xiao, Chunliang Zhang, Jingbo Zhu</dc:creator>
      <pubDate>Mon, 01 Apr 2024 07:49:11 GMT</pubDate>
    </item>
    <item>
      <title>Nash CoT: Multi-Path Inference with Preference Equilibrium</title>
      <link>http://arxiv.org/abs/2407.07099v1</link>
      <description>Chain-of-thought (CoT) prompting has emerged as a powerful technique for enhancing the reasoning capabilities of Large Language Models (LLMs) on complex problems. Among CoT-related studies, self-consistency (Multi-path inference with answer filtering through voting) involves generating multiple reasoning paths using the CoT framework and then selecting the most frequently produced outputs standing out as a concise yet competitive approach. While self-consistency has indeed led to the improvements in LLM inference, the use of multi-path inference also escalates deployment costs. Therefore, maintaining the performance benefits of self-consistency inherited from multi-path inference while reducing the inference costs holds significant value. In this research, we conceptualize language decoding as a preference consensus game, constructing a bi-player gaming system within each local path, and introduce Nash Chain-of-Thought (Nash CoT). Specifically, for a given question, we leverage LLM to autonomously select the contextually relevant template and generate outputs guided by this template, aiming to reach Nash Equilibrium alongside normal generation in each path. This approach allows us to achieve comparable or improved performance compared to self-consistency while using fewer inference paths on various inference tasks, including Arabic reasoning, Commonsense Question answering, and Symbolic inference.\n\n\nNash CoT: Multi-Path Inference with Preference Equilibrium</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2407.07099v1</guid>
      <dc:creator>Ziqi Zhang, Cunxiang Wang, Xiong Xiao, Yue Zhang, Donglin Wang</dc:creator>
      <pubDate>Tue, 18 Jun 2024 07:46:13 GMT</pubDate>
    </item>
    <item>
      <title>Alleviating Hallucinations in Large Vision-Language Models through Hallucination-Induced Optimization</title>
      <link>http://arxiv.org/abs/2405.15356v1</link>
      <description>Although Large Visual Language Models (LVLMs) have demonstrated exceptional abilities in understanding multimodal data, they invariably suffer from hallucinations, leading to a disconnect between the generated text and the corresponding images. Almost all current visual contrastive decoding methods attempt to mitigate these hallucinations by introducing visual uncertainty information that appropriately widens the contrastive logits gap between hallucinatory and targeted ones. However, due to uncontrollable nature of the global visual uncertainty, they struggle to precisely induce the hallucinatory tokens, which severely limits their effectiveness in mitigating hallucinations and may even lead to the generation of undesired hallucinations. To tackle this issue, we conducted the theoretical analysis to promote the effectiveness of contrast decoding. Building on this insight, we introduce a novel optimization strategy named Hallucination-Induced Optimization (HIO). This strategy seeks to amplify the contrast between hallucinatory and targeted tokens relying on a fine-tuned theoretical preference model (i.e., Contrary Bradley-Terry Model), thereby facilitating efficient contrast decoding to alleviate hallucinations in LVLMs. Extensive experimental research demonstrates that our HIO strategy can effectively reduce hallucinations in LVLMs, outperforming state-of-the-art methods across various benchmarks.\n\n\nAlleviating Hallucinations in Large Vision-Language Models through Hallucination-Induced Optimization</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.15356v1</guid>
      <dc:creator>Beitao Chen, Xinyu Lyu, Lianli Gao, Jingkuan Song, Heng Tao Shen</dc:creator>
      <pubDate>Fri, 24 May 2024 08:46:31 GMT</pubDate>
    </item>
    <item>
      <title>Humor in AI: Massive Scale Crowd-Sourced Preferences and Benchmarks for Cartoon Captioning</title>
      <link>http://arxiv.org/abs/2406.10522v1</link>
      <description>We present a novel multimodal preference dataset for creative tasks, consisting of over 250 million human ratings on more than 2.2 million captions, collected through crowdsourcing rating data for The New Yorker's weekly cartoon caption contest over the past eight years. This unique dataset supports the development and evaluation of multimodal large language models and preference-based fine-tuning algorithms for humorous caption generation. We propose novel benchmarks for judging the quality of model-generated captions, utilizing both GPT4 and human judgments to establish ranking-based evaluation strategies. Our experimental results highlight the limitations of current fine-tuning methods, such as RLHF and DPO, when applied to creative tasks. Furthermore, we demonstrate that even state-of-the-art models like GPT4 and Claude currently underperform top human contestants in generating humorous captions. As we conclude this extensive data collection effort, we release the entire preference dataset to the research community, fostering further advancements in AI humor generation and evaluation.\n\n\nHumor in AI: Massive Scale Crowd-Sourced Preferences and Benchmarks for Cartoon Captioning</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.10522v1</guid>
      <dc:creator>Jifan Zhang, Lalit Jain, Yang Guo, Jiayi Chen, Kuan Lok Zhou, Siddharth Suresh, Andrew Wagenmaker, Scott Sievert, Timothy Rogers, Kevin Jamieson, Robert Mankoff, Robert Nowak</dc:creator>
      <pubDate>Sat, 15 Jun 2024 06:26:25 GMT</pubDate>
    </item>
    <item>
      <title>Toward Optimal LLM Alignments Using Two-Player Games</title>
      <link>http://arxiv.org/abs/2406.10977v1</link>
      <description>The standard Reinforcement Learning from Human Feedback (RLHF) framework primarily focuses on optimizing the performance of large language models using pre-collected prompts. However, collecting prompts that provide comprehensive coverage is both tedious and challenging, and often fails to include scenarios that LLMs need to improve on the most. In this paper, we investigate alignment through the lens of two-agent games, involving iterative interactions between an adversarial and a defensive agent. The adversarial agent's task at each step is to generate prompts that expose the weakness of the defensive agent. In return, the defensive agent seeks to improve its responses to these newly identified prompts it struggled with, based on feedback from the reward model. We theoretically demonstrate that this iterative reinforcement learning optimization converges to a Nash Equilibrium for the game induced by the agents. Experimental results in safety scenarios demonstrate that learning in such a competitive environment not only fully trains agents but also leads to policies with enhanced generalization capabilities for both adversarial and defensive agents.\n\n\nToward Optimal LLM Alignments Using Two-Player Games</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.10977v1</guid>
      <dc:creator>Rui Zheng, Hongyi Guo, Zhihan Liu, Xiaoying Zhang, Yuanshun Yao, Xiaojun Xu, Zhaoran Wang, Zhiheng Xi, Tao Gui, Qi Zhang, Xuanjing Huang, Hang Li, Yang Liu</dc:creator>
      <pubDate>Sun, 16 Jun 2024 15:24:50 GMT</pubDate>
    </item>
    <item>
      <title>Enhancing LLM-based Test Generation for Hard-to-Cover Branches via Program Analysis</title>
      <link>http://arxiv.org/abs/2404.04966v1</link>
      <description>Automatic test generation plays a critical role in software quality assurance. While the recent advances in Search-Based Software Testing (SBST) and Large Language Models (LLMs) have shown promise in generating useful tests, these techniques still struggle to cover certain branches. Reaching these hard-to-cover branches usually requires constructing complex objects and resolving intricate inter-procedural dependencies in branch conditions, which poses significant challenges for existing test generation techniques. In this work, we propose TELPA, a novel technique aimed at addressing these challenges. Its key insight lies in extracting real usage scenarios of the target method under test to learn how to construct complex objects and extracting methods entailing inter-procedural dependencies with hard-to-cover branches to learn the semantics of branch constraints. To enhance efficiency and effectiveness, TELPA identifies a set of ineffective tests as counter-examples for LLMs and employs a feedback-based process to iteratively refine these counter-examples. Then, TELPA integrates program analysis results and counter-examples into the prompt, guiding LLMs to gain deeper understandings of the semantics of the target method and generate diverse tests that can reach the hard-to-cover branches. Our experimental results on 27 open-source Python projects demonstrate that TELPA significantly outperforms the state-of-the-art SBST and LLM-based techniques, achieving an average improvement of 31.39% and 22.22% in terms of branch coverage.\n\n\nEnhancing LLM-based Test Generation for Hard-to-Cover Branches via Program Analysis</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2404.04966v1</guid>
      <dc:creator>Chen Yang, Junjie Chen, Bin Lin, Jianyi Zhou, Ziqi Wang</dc:creator>
      <pubDate>Sun, 07 Apr 2024 14:08:28 GMT</pubDate>
    </item>
    <item>
      <title>On the Principles behind Opinion Dynamics in Multi-Agent Systems of Large Language Models</title>
      <link>http://arxiv.org/abs/2406.15492v1</link>
      <description>We study the evolution of opinions inside a population of interacting large language models (LLMs). Every LLM needs to decide how much funding to allocate to an item with three initial possibilities: full, partial, or no funding. We identify biases that drive the exchange of opinions based on the LLM's tendency to (i) find consensus with the other LLM's opinion, (ii) display caution when specifying funding, and (iii) consider ethical concerns in its opinion. We find these biases are affected by the perceived absence of compelling reasons for opinion change, the perceived willingness to engage in discussion, and the distribution of allocation values. Moreover, tensions among biases can lead to the survival of funding for items with negative connotations. We also find that the final distribution of full, partial, and no funding opinions is more diverse when an LLM freely forms its opinion after an interaction than when its opinion is a multiple-choice selection among the three allocation options. In the latter case, consensus or polarization is generally attained. When agents are aware of past opinions, they seek to maintain consistency with them, and more diverse updating rules emerge. Our study is performed using a Llama 3 LLM.\n\n\nOn the Principles behind Opinion Dynamics in Multi-Agent Systems of Large Language Models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.15492v1</guid>
      <dc:creator>Pedro Cisneros-Velarde</dc:creator>
      <pubDate>Tue, 18 Jun 2024 18:37:23 GMT</pubDate>
    </item>
    <item>
      <title>Integrating Emotional and Linguistic Models for Ethical Compliance in Large Language Models</title>
      <link>http://arxiv.org/abs/2405.07076v2</link>
      <description>This research develops advanced methodologies for Large Language Models (LLMs) to better manage linguistic behaviors related to emotions and ethics. We introduce DIKE, an adversarial framework that enhances the LLMs' ability to internalize and reflect global human values, adapting to varied cultural contexts to promote transparency and trust among users. The methodology involves detailed modeling of emotions, classification of linguistic behaviors, and implementation of ethical guardrails. Our innovative approaches include mapping emotions and behaviors using self-supervised learning techniques, refining these guardrails through adversarial reviews, and systematically adjusting outputs to ensure ethical alignment. This framework establishes a robust foundation for AI systems to operate with ethical integrity and cultural sensitivity, paving the way for more responsible and context-aware AI interactions.\n\n\nIntegrating Emotional and Linguistic Models for Ethical Compliance in Large Language Models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.07076v2</guid>
      <dc:creator>Edward Y. Chang</dc:creator>
      <pubDate>Tue, 14 May 2024 03:08:12 GMT</pubDate>
    </item>
    <item>
      <title>The Power of Active Multi-Task Learning in Reinforcement Learning from Human Feedback</title>
      <link>http://arxiv.org/abs/2405.11226v1</link>
      <description>Reinforcement learning from human feedback (RLHF) has contributed to performance improvements in large language models. To tackle its reliance on substantial amounts of human-labeled data, a successful approach is multi-task representation learning, which involves learning a high-quality, low-dimensional representation from a wide range of source tasks. In this paper, we formulate RLHF as the contextual dueling bandit problem and assume a common linear representation. We demonstrate that the sample complexity of source tasks in multi-task RLHF can be reduced by considering task relevance and allocating different sample sizes to source tasks with varying task relevance. We further propose an algorithm to estimate task relevance by a small number of additional data and then learn a policy. We prove that to achieve $\varepsilon-$optimal, the sample complexity of the source tasks can be significantly reduced compared to uniform sampling. Additionally, the sample complexity of the target task is only linear in the dimension of the latent space, thanks to representation learning.\n\n\nThe Power of Active Multi-Task Learning in Reinforcement Learning from Human Feedback</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.11226v1</guid>
      <dc:creator>Ruitao Chen, Liwei Wang</dc:creator>
      <pubDate>Sat, 18 May 2024 08:29:15 GMT</pubDate>
    </item>
    <item>
      <title>Towards Answering Climate Questionnaires from Unstructured Climate Reports</title>
      <link>http://arxiv.org/abs/2301.04253v2</link>
      <description>The topic of Climate Change (CC) has received limited attention in NLP despite its urgency. Activists and policymakers need NLP tools to effectively process the vast and rapidly growing unstructured textual climate reports into structured form. To tackle this challenge we introduce two new large-scale climate questionnaire datasets and use their existing structure to train self-supervised models. We conduct experiments to show that these models can learn to generalize to climate disclosures of different organizations types than seen during training. We then use these models to help align texts from unstructured climate documents to the semi-structured questionnaires in a human pilot study. Finally, to support further NLP research in the climate domain we introduce a benchmark of existing climate text classification datasets to better evaluate and compare existing models.\n\n\nClimate Change from Large Language Models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2301.04253v2</guid>
      <dc:creator>Daniel Spokoyny, Tanmay Laud, Tom Corringham, Taylor Berg-Kirkpatrick</dc:creator>
      <pubDate>Thu, 27 Jul 2023 18:13:40 GMT</pubDate>
    </item>
    <item>
      <title>SoP: Unlock the Power of Social Facilitation for Automatic Jailbreak Attack</title>
      <link>http://arxiv.org/abs/2407.01902v1</link>
      <description>The widespread applications of large language models (LLMs) have brought about concerns regarding their potential misuse. Although aligned with human preference data before release, LLMs remain vulnerable to various malicious attacks. In this paper, we adopt a red-teaming strategy to enhance LLM safety and introduce SoP, a simple yet effective framework to design jailbreak prompts automatically. Inspired by the social facilitation concept, SoP generates and optimizes multiple jailbreak characters to bypass the guardrails of the target LLM. Different from previous work which relies on proprietary LLMs or seed jailbreak templates crafted by human expertise, SoP can generate and optimize the jailbreak prompt in a cold-start scenario using open-sourced LLMs without any seed jailbreak templates. Experimental results show that SoP achieves attack success rates of 88% and 60% in bypassing the safety alignment of GPT-3.5-1106 and GPT-4, respectively. Furthermore, we extensively evaluate the transferability of the generated templates across different LLMs and held-out malicious requests, while also exploring defense strategies against the jailbreak attack designed by SoP. Code is available at https://github.com/Yang-Yan-Yang-Yan/SoP.\n\n\nSoP: Unlock the Power of Social Facilitation for Automatic Jailbreak Attack</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2407.01902v1</guid>
      <dc:creator>Yan Yang, Zeguan Xiao, Xin Lu, Hongru Wang, Hailiang Huang, Guanhua Chen, Yun Chen</dc:creator>
      <pubDate>Tue, 02 Jul 2024 02:58:29 GMT</pubDate>
    </item>
    <item>
      <title>Jailbreaking as a Reward Misspecification Problem</title>
      <link>http://arxiv.org/abs/2406.14393v2</link>
      <description>The widespread adoption of large language models (LLMs) has raised concerns about their safety and reliability, particularly regarding their vulnerability to adversarial attacks. In this paper, we propose a novel perspective that attributes this vulnerability to reward misspecification during the alignment process. We introduce a metric ReGap to quantify the extent of reward misspecification and demonstrate its effectiveness and robustness in detecting harmful backdoor prompts. Building upon these insights, we present ReMiss, a system for automated red teaming that generates adversarial prompts against various target aligned LLMs. ReMiss achieves state-of-the-art attack success rates on the AdvBench benchmark while preserving the human readability of the generated prompts. Detailed analysis highlights the unique advantages brought by the proposed reward misspecification objective compared to previous methods.\n\n\nJailbreaking as a Reward Misspecification Problem</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.14393v2</guid>
      <dc:creator>Zhihui Xie, Jiahui Gao, Lei Li, Zhenguo Li, Qi Liu, Lingpeng Kong</dc:creator>
      <pubDate>Fri, 12 Jul 2024 08:15:45 GMT</pubDate>
    </item>
    <item>
      <title>PLaD: Preference-based Large Language Model Distillation with Pseudo-Preference Pairs</title>
      <link>http://arxiv.org/abs/2406.02886v2</link>
      <description>Large Language Models (LLMs) have exhibited impressive capabilities in various tasks, yet their vast parameter sizes restrict their applicability in resource-constrained settings. Knowledge distillation (KD) offers a viable solution by transferring expertise from large teacher models to compact student models. However, traditional KD techniques face specific challenges when applied to LLMs, including restricted access to LLM outputs, significant teacher-student capacity gaps, and the inherited mis-calibration issue. In this work, we present PLaD, a novel preference-based LLM distillation framework. PLaD exploits the teacher-student capacity discrepancy to generate pseudo-preference pairs where teacher outputs are preferred over student outputs. Then, PLaD leverages a ranking loss to re-calibrate student's estimation of sequence likelihood, which steers the student's focus towards understanding the relative quality of outputs instead of simply imitating the teacher. PLaD bypasses the need for access to teacher LLM's internal states, tackles the student's expressivity limitations, and mitigates the student mis-calibration issue. Through extensive experiments on two sequence generation tasks and with various LLMs, we demonstrate the effectiveness of our proposed PLaD framework.\n\n\nPLaD: Preference-based Large Language Model Distillation with Pseudo-Preference Pairs</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.02886v2</guid>
      <dc:creator>Rongzhi Zhang, Jiaming Shen, Tianqi Liu, Haorui Wang, Zhen Qin, Feng Han, Jialu Liu, Simon Baumgartner, Michael Bendersky, Chao Zhang</dc:creator>
      <pubDate>Thu, 06 Jun 2024 12:47:31 GMT</pubDate>
    </item>
    <item>
      <title>Aligning Diffusion Models with Noise-Conditioned Perception</title>
      <link>http://arxiv.org/abs/2406.17636v1</link>
      <description>Recent advancements in human preference optimization, initially developed for Language Models (LMs), have shown promise for text-to-image Diffusion Models, enhancing prompt alignment, visual appeal, and user preference. Unlike LMs, Diffusion Models typically optimize in pixel or VAE space, which does not align well with human perception, leading to slower and less efficient training during the preference alignment stage. We propose using a perceptual objective in the U-Net embedding space of the diffusion model to address these issues. Our approach involves fine-tuning Stable Diffusion 1.5 and XL using Direct Preference Optimization (DPO), Contrastive Preference Optimization (CPO), and supervised fine-tuning (SFT) within this embedding space. This method significantly outperforms standard latent-space implementations across various metrics, including quality and computational cost. For SDXL, our approach provides 60.8\% general preference, 62.2\% visual appeal, and 52.1\% prompt following against original open-sourced SDXL-DPO on the PartiPrompts dataset, while significantly reducing compute. Our approach not only improves the efficiency and quality of human preference alignment for diffusion models but is also easily integrable with other optimization techniques. The training code and LoRA weights will be available here: https://huggingface.co/alexgambashidze/SDXL\_NCP-DPO\_v0.1\n\n\nAligning Diffusion Models with Noise-Conditioned Perception</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.17636v1</guid>
      <dc:creator>Alexander Gambashidze, Anton Kulikov, Yuriy Sosnin, Ilya Makarov</dc:creator>
      <pubDate>Tue, 25 Jun 2024 15:21:50 GMT</pubDate>
    </item>
    <item>
      <title>Reward Steering with Evolutionary Heuristics for Decoding-time Alignment</title>
      <link>http://arxiv.org/abs/2406.15193v4</link>
      <description>The widespread applicability and increasing omnipresence of LLMs have instigated a need to align LLM responses to user and stakeholder preferences. Many preference optimization approaches have been proposed that fine-tune LLM parameters to achieve good alignment. However, such parameter tuning is known to interfere with model performance on many tasks. Moreover, keeping up with shifting user preferences is tricky in such a situation. Decoding-time alignment with reward model guidance solves these issues at the cost of increased inference time. However, most of such methods fail to strike the right balance between exploration and exploitation of reward -- often due to the conflated formulation of these two aspects - to give well-aligned responses. To remedy this we decouple these two aspects and implement them in an evolutionary fashion: exploration is enforced by decoding from mutated instructions and exploitation is represented as the periodic replacement of poorly-rewarded generations with well-rewarded ones. Empirical evidences indicate that this strategy outperforms many preference optimization and decode-time alignment approaches on two widely accepted alignment benchmarks AlpacaEval 2 and MT-Bench. Our implementation will be available at: https://darwin-alignment.github.io.\n\n\nReward Steering with Evolutionary Heuristics for Decoding-time Alignment</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.15193v4</guid>
      <dc:creator>Chia-Yu Hung, Navonil Majumder, Ambuj Mehrish, Soujanya Poria</dc:creator>
      <pubDate>Mon, 08 Jul 2024 13:34:01 GMT</pubDate>
    </item>
    <item>
      <title>Advancing Tool-Augmented Large Language Models: Integrating Insights from Errors in Inference Trees</title>
      <link>http://arxiv.org/abs/2406.07115v1</link>
      <description>Tool-augmented large language models (LLMs) leverage tools, often in the form of APIs, to enhance their reasoning capabilities on complex tasks, thus taking on the role of intelligent agents interacting with the real world. The recently introduced ToolLLaMA model by Qin et al. [2024] utilizes the depth-first search-based decision tree (DFSDT) method for reasoning with $16000+$ real-world APIs, which effectively improves the planning and inferencing performance of tool-augmented LLMs compared to traditional chain reasoning approaches. However, their approach only employs successful paths from decision trees (also called inference trees) for supervised fine-tuning (SFT) during training, which does not fully exploit the advantages of the tree of thought. In this study, we propose an inference trajectory optimization framework based on the preference data extracted from decision trees to address this limitation. We first introduce a novel method for constructing preference data from the tree of thought, capitalizing on the failed explorations previously overlooked in the trees. Specifically, we generate an effective step-wise preference dataset, named ToolPreference, for tool use based on the ToolBench dataset. In the subsequent training phase, we first fine-tune the LLM with tool-usage expert trajectories and then use these step-wise preference pairs for direct preference optimization (DPO) to update the policy of the LLM, resulting in our ToolPrefer-LLaMA (TP-LLaMA) model. Our experiments demonstrate that by obtaining insights from errors in inference trees, TP-LLaMA significantly outperforms the baselines across almost all test scenarios by a large margin and exhibits better generalization capabilities with unseen APIs. At the same time, TP-LLaMA has also demonstrated superior reasoning efficiency compared to the baselines, making it more suitable for complex tool-usage reasoning tasks.\n\n\nAdvancing Tool-Augmented Large Language Models: Integrating Insights from Errors in Inference Trees</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.07115v1</guid>
      <dc:creator>Sijia Chen, Yibo Wang, Yi-Feng Wu, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, Lijun Zhang</dc:creator>
      <pubDate>Tue, 11 Jun 2024 10:00:18 GMT</pubDate>
    </item>
    <item>
      <title>GraphArena: Benchmarking Large Language Models on Graph Computational Problems</title>
      <link>http://arxiv.org/abs/2407.00379v1</link>
      <description>The &quot;arms race&quot; of Large Language Models (LLMs) demands novel, challenging, and diverse benchmarks to faithfully examine their progresses. We introduce GraphArena, a benchmarking tool designed to evaluate LLMs on graph computational problems using million-scale real-world graphs from diverse scenarios such as knowledge graphs, social networks, and molecular structures. GraphArena offers a suite of 10 computational tasks, encompassing four polynomial-time (e.g., Shortest Distance) and six NP-complete challenges (e.g., Travelling Salesman Problem). It features a rigorous evaluation framework that classifies LLM outputs as correct, suboptimal (feasible but not optimal), or hallucinatory (properly formatted but infeasible). Evaluation of 10 leading LLMs, including GPT-4o and LLaMA3-70B-Instruct, reveals that even top-performing models struggle with larger, more complex graph problems and exhibit hallucination issues. Despite the application of strategies such as chain-of-thought prompting, these issues remain unresolved. GraphArena contributes a valuable supplement to the existing LLM benchmarks and is open-sourced at https://github.com/squareRoot3/GraphArena.\n\n\nGraphArena: Benchmarking Large Language Models on Graph Computational Problems</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2407.00379v1</guid>
      <dc:creator>Jianheng Tang, Qifan Zhang, Yuhan Li, Jia Li</dc:creator>
      <pubDate>Sat, 29 Jun 2024 09:19:23 GMT</pubDate>
    </item>
    <item>
      <title>Exposing Privacy Gaps: Membership Inference Attack on Preference Data for LLM Alignment</title>
      <link>http://arxiv.org/abs/2407.06443v1</link>
      <description>Large Language Models (LLMs) have seen widespread adoption due to their remarkable natural language capabilities. However, when deploying them in real-world settings, it is important to align LLMs to generate texts according to acceptable human standards. Methods such as Proximal Policy Optimization (PPO) and Direct Preference Optimization (DPO) have made significant progress in refining LLMs using human preference data. However, the privacy concerns inherent in utilizing such preference data have yet to be adequately studied. In this paper, we investigate the vulnerability of LLMs aligned using human preference datasets to membership inference attacks (MIAs), highlighting the shortcomings of previous MIA approaches with respect to preference data. Our study has two main contributions: first, we introduce a novel reference-based attack framework specifically for analyzing preference data called PREMIA (\uline{Pre}ference data \uline{MIA}); second, we provide empirical evidence that DPO models are more vulnerable to MIA compared to PPO models. Our findings highlight gaps in current privacy-preserving practices for LLM alignment.\n\n\nExposing Privacy Gaps: Membership Inference Attack on Preference Data for LLM Alignment</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2407.06443v1</guid>
      <dc:creator>Qizhang Feng, Siva Rajesh Kasa, Hyokun Yun, Choon Hui Teo, Sravan Babu Bodapati</dc:creator>
      <pubDate>Mon, 08 Jul 2024 22:53:23 GMT</pubDate>
    </item>
    <item>
      <title>On-Policy Fine-grained Knowledge Feedback for Hallucination Mitigation</title>
      <link>http://arxiv.org/abs/2406.12221v1</link>
      <description>Hallucination occurs when large language models (LLMs) exhibit behavior that deviates from the boundaries of their knowledge during the response generation process. Previous learning-based methods focus on detecting knowledge boundaries and finetuning models with instance-level feedback, but they suffer from inaccurate signals due to off-policy data sampling and coarse-grained feedback. In this paper, we introduce \textit{\b{R}einforcement \b{L}earning \b{f}or \b{H}allucination} (RLFH), a fine-grained feedback-based online reinforcement learning method for hallucination mitigation. Unlike previous learning-based methods, RLFH enables LLMs to explore the boundaries of their internal knowledge and provide on-policy, fine-grained feedback on these explorations. To construct fine-grained feedback for learning reliable generation behavior, RLFH decomposes the outcomes of large models into atomic facts, provides statement-level evaluation signals, and traces back the signals to the tokens of the original responses. Finally, RLFH adopts the online reinforcement algorithm with these token-level rewards to adjust model behavior for hallucination mitigation. For effective on-policy optimization, RLFH also introduces an LLM-based fact assessment framework to verify the truthfulness and helpfulness of atomic facts without human intervention. Experiments on HotpotQA, SQuADv2, and Biography benchmarks demonstrate that RLFH can balance their usage of internal knowledge during the generation process to eliminate the hallucination behavior of LLMs.\n\n\nOn-Policy Fine-grained Knowledge Feedback for Hallucination Mitigation</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.12221v1</guid>
      <dc:creator>Xueru Wen, Xinyu Lu, Xinyan Guan, Yaojie Lu, Hongyu Lin, Ben He, Xianpei Han, Le Sun</dc:creator>
      <pubDate>Tue, 18 Jun 2024 02:43:49 GMT</pubDate>
    </item>
    <item>
      <title>PAFT: A Parallel Training Paradigm for Effective LLM Fine-Tuning</title>
      <link>http://arxiv.org/abs/2406.17923v1</link>
      <description>Large language models (LLMs) have shown remarkable abilities in diverse natural language processing (NLP) tasks. The LLMs generally undergo supervised fine-tuning (SFT) followed by preference alignment to be usable in downstream applications. However, this sequential training pipeline leads to alignment tax that degrades the LLM performance.   This paper introduces PAFT, a new PArallel training paradigm for effective LLM Fine-Tuning, which independently performs SFT and preference alignment (e.g., DPO and ORPO, etc.) with the same pre-trained model on respective datasets. The model produced by SFT and the model from preference alignment are then merged into a final model by parameter fusing for use in downstream applications. This work reveals important findings that preference alignment like DPO naturally results in a sparse model while SFT leads to a natural dense model which needs to be sparsified for effective model merging. This paper introduces an effective interference resolution which reduces the redundancy by sparsifying the delta parameters. The LLM resulted from the new training paradigm achieved Rank #1 on the HuggingFace Open LLM Leaderboard. Comprehensive evaluation shows the effectiveness of the parallel training paradigm.\n\n\nPAFT: A Parallel Training Paradigm for Effective LLM Fine-Tuning</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.17923v1</guid>
      <dc:creator>Shiva Kumar Pentyala, Zhichao Wang, Bin Bi, Kiran Ramnath, Xiang-Bo Mao, Regunathan Radhakrishnan, Sitaram Asur, Na, Cheng</dc:creator>
      <pubDate>Tue, 25 Jun 2024 20:11:37 GMT</pubDate>
    </item>
    <item>
      <title>Compass: Large Multilingual Language Model for South-east Asia</title>
      <link>http://arxiv.org/abs/2404.09220v1</link>
      <description>Large language models have exhibited significant proficiency in languages endowed with extensive linguistic resources, such as English and Chinese. Nevertheless, their effectiveness notably diminishes when applied to languages characterized by limited linguistic resources, particularly within the Southeast Asian linguistic landscape, such as Indonesian. The scarcity of linguistic resources for these languages presents challenges associated with inadequate training, restricted vocabulary coverage, and challenging evaluation processes. In response to these exigencies, we have introduced CompassLLM, a large multilingual model specifically tailored for Southeast Asian languages, with the primary aim of supporting the developmental requirements of Shopee. Our methodology encompasses several key strategies. To progressively enhance multilingual proficiencies, we implemented a multi-stage pre-training strategy integrated with curriculum learning, gradually intensifying the focus on low-resource languages. Concurrently, to better accommodate low-resource human instructions, we curated and generated a repository of high-quality multilingual human instructions, culminating the CompassLLM-SFT model through supervised instruction fine-tuning. Finally, to reinforce the model's alignment with human preference behaviors, we have embraced the principle of Direct Preference Optimization (DPO) to obtain CompassLLM-DPO model. Preliminary evaluation of the CompassLLM model yields promising results, with our model surpassing benchmark models like Vicuna-7b-v1.5, Sealion, Falcon and SeaLLM, across diverse evaluation tasks, as verified through both automated and human-driven assessments. Notably, our model exhibits its superior performance in South-east Asia languages, such as Indonesian language.\n\n\nCompass: Large Multilingual Language Model for South-east Asia</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2404.09220v1</guid>
      <dc:creator>Sophia Maria</dc:creator>
      <pubDate>Sun, 14 Apr 2024 11:48:33 GMT</pubDate>
    </item>
    <item>
      <title>Hybrid Preference Optimization: Augmenting Direct Preference Optimization with Auxiliary Objectives</title>
      <link>http://arxiv.org/abs/2405.17956v2</link>
      <description>For aligning large language models (LLMs), prior work has leveraged reinforcement learning via human feedback (RLHF) or variations of direct preference optimization (DPO). While DPO offers a simpler framework based on maximum likelihood estimation, it compromises on the ability to tune language models to easily maximize non-differentiable and non-binary objectives according to the LLM designer's preferences (e.g., using simpler language or minimizing specific kinds of harmful content). These may neither align with user preferences nor even be able to be captured tractably by binary preference data. To leverage the simplicity and performance of DPO with the generalizability of RL, we propose a hybrid approach between DPO and RLHF. With a simple augmentation to the implicit reward decomposition of DPO, we allow for tuning LLMs to maximize a set of arbitrary auxiliary rewards using offline RL. The proposed method, Hybrid Preference Optimization (HPO), shows the ability to effectively generalize to both user preferences and auxiliary designer objectives, while preserving alignment performance across a range of challenging benchmarks and model sizes.\n\n\nHybrid Preference Optimization: Augmenting Direct Preference Optimization with Auxiliary Objectives</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.17956v2</guid>
      <dc:creator>Anirudhan Badrinath, Prabhat Agarwal, Jiajing Xu</dc:creator>
      <pubDate>Wed, 29 May 2024 20:48:47 GMT</pubDate>
    </item>
    <item>
      <title>A Survey of Large Language Models on Generative Graph Analytics: Query, Learning, and Applications</title>
      <link>http://arxiv.org/abs/2404.14809v1</link>
      <description>A graph is a fundamental data model to represent various entities and their complex relationships in society and nature, such as social networks, transportation networks, financial networks, and biomedical systems. Recently, large language models (LLMs) have showcased a strong generalization ability to handle various NLP and multi-mode tasks to answer users' arbitrary questions and specific-domain content generation. Compared with graph learning models, LLMs enjoy superior advantages in addressing the challenges of generalizing graph tasks by eliminating the need for training graph learning models and reducing the cost of manual annotation. In this survey, we conduct a comprehensive investigation of existing LLM studies on graph data, which summarizes the relevant graph analytics tasks solved by advanced LLM models and points out the existing remaining challenges and future directions. Specifically, we study the key problems of LLM-based generative graph analytics (LLM-GGA) with three categories: LLM-based graph query processing (LLM-GQP), LLM-based graph inference and learning (LLM-GIL), and graph-LLM-based applications. LLM-GQP focuses on an integration of graph analytics techniques and LLM prompts, including graph understanding and knowledge graph (KG) based augmented retrieval, while LLM-GIL focuses on learning and reasoning over graphs, including graph learning, graph-formed reasoning and graph representation. We summarize the useful prompts incorporated into LLM to handle different graph downstream tasks. Moreover, we give a summary of LLM model evaluation, benchmark datasets/tasks, and a deep pro and cons analysis of LLM models. We also explore open problems and future directions in this exciting interdisciplinary research area of LLMs and graph analytics.\n\n\nA Survey of Large Language Models on Generative Graph Analytics: Query, Learning, and Applications</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2404.14809v1</guid>
      <dc:creator>Wenbo Shang, Xin Huang</dc:creator>
      <pubDate>Tue, 23 Apr 2024 07:39:24 GMT</pubDate>
    </item>
    <item>
      <title>Is QCD asymptotically free?</title>
      <link>http://arxiv.org/abs/hep-ph/0306103v1</link>
      <description>The asymptotical behaviour of the QCD coupling strength is considered. Its upper bound is found from the present experimental data. The assumption about the finite asymptotical value of the coupling strength leads to the energy scale where the perturbative predictions would fail. Possible tests of this assumption are considered.\n\n\nIs Free Self-Alignment Possible?</description>
      <guid isPermaLink="false">http://arxiv.org/abs/hep-ph/0306103v1</guid>
      <dc:creator>I. M. Dremin</dc:creator>
      <pubDate>Thu, 12 Jun 2003 07:30:40 GMT</pubDate>
    </item>
    <item>
      <title>WangchanLion and WangchanX MRC Eval</title>
      <link>http://arxiv.org/abs/2403.16127v2</link>
      <description>This technical report describes the development of WangchanLion, an instruction fine-tuned model focusing on Machine Reading Comprehension (MRC) in the Thai language. Our model is based on SEA-LION and a collection of instruction following datasets. To promote open research and reproducibility, we publicly release all training data, code, and the final model weights under the Apache-2 license. To assess the contextual understanding capability, we conducted extensive experimental studies using two Thai MRC datasets, XQuAD and Iapp_wiki_qa_squad. Experimental results demonstrate the model's ability to comprehend the context and produce an answer faithful to the reference one in 0-shot and 1-shot settings. In addition, our evaluation goes beyond the traditional MRC. We propose a new evaluation scheme assessing the answer's correctness, helpfulness, conciseness, and contextuality. Our code is available publicly at https://github.com/vistec-AI/WangchanLion.\n\n\nWangchanLion and WangchanX MRC Eval</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2403.16127v2</guid>
      <dc:creator>Wannaphong Phatthiyaphaibun, Surapon Nonesung, Patomporn Payoungkhamdee, Peerat Limkonchotiwat, Can Udomcharoenchaikit, Jitkapat Sawatphol, Chompakorn Chaksangchaichot, Ekapol Chuangsuwanich, Sarana Nutanong</dc:creator>
      <pubDate>Tue, 23 Apr 2024 12:31:30 GMT</pubDate>
    </item>
    <item>
      <title>Exploring the Value of Personalized Word Embeddings</title>
      <link>http://arxiv.org/abs/2011.06057v1</link>
      <description>In this paper, we introduce personalized word embeddings, and examine their value for language modeling. We compare the performance of our proposed prediction model when using personalized versus generic word representations, and study how these representations can be leveraged for improved performance. We provide insight into what types of words can be more accurately predicted when building personalized models. Our results show that a subset of words belonging to specific psycholinguistic categories tend to vary more in their representations across users and that combining generic and personalized word embeddings yields the best performance, with a 4.7% relative reduction in perplexity. Additionally, we show that a language model using personalized word embeddings can be effectively used for authorship attribution.\n\n\nExploring Safety-Utility Trade-Offs in Personalized Language Models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2011.06057v1</guid>
      <dc:creator>Charles Welch, Jonathan K. Kummerfeld, Verónica Pérez-Rosas, Rada Mihalcea</dc:creator>
      <pubDate>Wed, 11 Nov 2020 20:23:09 GMT</pubDate>
    </item>
    <item>
      <title>Coder Reviewer Reranking for Code Generation</title>
      <link>http://arxiv.org/abs/2211.16490v1</link>
      <description>Sampling diverse programs from a code language model and reranking with model likelihood is a popular method for code generation but it is prone to preferring degenerate solutions. Inspired by collaborative programming, we propose Coder-Reviewer reranking. We augment Coder language models from past work, which generate programs given language instructions, with Reviewer models, which evaluate the likelihood of the instruction given the generated programs. We perform an extensive study across six datasets with eight models from three model families. Experimental results show that Coder-Reviewer reranking leads to consistent and significant improvement (up to 17% absolute accuracy gain) over reranking with the Coder model only. When combined with executability filtering, Coder-Reviewer reranking can often outperform the minimum Bayes risk method. Coder-Reviewer reranking is easy to implement by prompting, can generalize to different programming languages, and works well with off-the-shelf hyperparameters.\n\n\n-Coder: Value-Based Deep Reinforcement Learning for Program Synthesis</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2211.16490v1</guid>
      <dc:creator>Tianyi Zhang, Tao Yu, Tatsunori B. Hashimoto, Mike Lewis, Wen-tau Yih, Daniel Fried, Sida I. Wang</dc:creator>
      <pubDate>Tue, 29 Nov 2022 18:56:33 GMT</pubDate>
    </item>
    <item>
      <title>On the Intrinsic Self-Correction Capability of LLMs: Uncertainty and Latent Concept</title>
      <link>http://arxiv.org/abs/2406.02378v1</link>
      <description>Large Language Models (LLMs) can improve their responses when instructed to do so, a capability known as self-correction. When these instructions lack specific details about the issues in the response, this is referred to as leveraging the intrinsic self-correction capability. The empirical success of self-correction can be found in various applications, e.g., text detoxification and social bias mitigation. However, leveraging this self-correction capability may not always be effective, as it has the potential to revise an initially correct response into an incorrect one. In this paper, we endeavor to understand how and why leveraging the self-correction capability is effective. We identify that appropriate instructions can guide LLMs to a convergence state, wherein additional self-correction steps do not yield further performance improvements. We empirically demonstrate that model uncertainty and activated latent concepts jointly characterize the effectiveness of self-correction. Furthermore, we provide a mathematical formulation indicating that the activated latent concept drives the convergence of the model uncertainty and self-correction performance. Our analysis can also be generalized to the self-correction behaviors observed in Vision-Language Models (VLMs). Moreover, we highlight that task-agnostic debiasing can benefit from our principle in terms of selecting effective fine-tuning samples. Such initial success demonstrates the potential extensibility for better instruction tuning and safety alignment.\n\n\nOn the Intrinsic Self-Correction Capability of LLMs: Uncertainty and Latent Concept</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.02378v1</guid>
      <dc:creator>Guangliang Liu, Haitao Mao, Bochuan Cao, Zhiyu Xue, Kristen Johnson, Jiliang Tang, Rongrong Wang</dc:creator>
      <pubDate>Tue, 04 Jun 2024 14:55:43 GMT</pubDate>
    </item>
    <item>
      <title>Refined Direct Preference Optimization with Synthetic Data for Behavioral Alignment of LLMs</title>
      <link>http://arxiv.org/abs/2402.08005v1</link>
      <description>In this paper, we introduce \emph{refined Direct Preference Optimization} (rDPO), a method for improving the behavioral alignment of Large Language Models (LLMs) without the need for human-annotated data. The method involves creating synthetic data using self-critique prompting by a teacher LLM and then utilising a generalized DPO loss function to distil to a student LLM. The loss function incorporates an additional external reward model to improve the quality of synthetic data, making rDPO robust to potential noise in the synthetic dataset. rDPO is shown to be effective in a diverse set of behavioural alignment tasks, such as improved safety, robustness against role-playing, and reduced sycophancy. Code to be released at https://github.com/vicgalle/refined-dpo.\n\n\nRefined Direct Preference Optimization with Synthetic Data for Behavioral Alignment of LLMs</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.08005v1</guid>
      <dc:creator>Víctor Gallego</dc:creator>
      <pubDate>Mon, 12 Feb 2024 19:10:13 GMT</pubDate>
    </item>
    <item>
      <title>Exploration of Language Dependency for Japanese Self-Supervised Speech Representation Models</title>
      <link>http://arxiv.org/abs/2305.05201v1</link>
      <description>Self-supervised learning (SSL) has been dramatically successful not only in monolingual but also in cross-lingual settings. However, since the two settings have been studied individually in general, there has been little research focusing on how effective a cross-lingual model is in comparison with a monolingual model. In this paper, we investigate this fundamental question empirically with Japanese automatic speech recognition (ASR) tasks. First, we begin by comparing the ASR performance of cross-lingual and monolingual models for two different language tasks while keeping the acoustic domain as identical as possible. Then, we examine how much unlabeled data collected in Japanese is needed to achieve performance comparable to a cross-lingual model pre-trained with tens of thousands of hours of English and/or multilingual data. Finally, we extensively investigate the effectiveness of SSL in Japanese and demonstrate state-of-the-art performance on multiple ASR tasks. Since there is no comprehensive SSL study for Japanese, we hope this study will guide Japanese SSL research.\n\n\nExploring Open Large Language Models for the Japanese Language: A Practical Guide</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2305.05201v1</guid>
      <dc:creator>Takanori Ashihara, Takafumi Moriya, Kohei Matsuura, Tomohiro Tanaka</dc:creator>
      <pubDate>Tue, 09 May 2023 06:28:10 GMT</pubDate>
    </item>
    <item>
      <title>Statistical ranking with dynamic covariates</title>
      <link>http://arxiv.org/abs/2406.16507v2</link>
      <description>We consider a covariate-assisted ranking model grounded in the Plackett--Luce framework. Unlike existing works focusing on pure covariates or individual effects with fixed covariates, our approach integrates individual effects with dynamic covariates. This added flexibility enhances realistic ranking yet poses significant challenges for analyzing the associated estimation procedures. This paper makes an initial attempt to address these challenges. We begin by discussing the sufficient and necessary condition for the model's identifiability. We then introduce an efficient alternating maximization algorithm to compute the maximum likelihood estimator (MLE). Under suitable assumptions on the topology of comparison graphs and dynamic covariates, we establish a quantitative uniform consistency result for the MLE with convergence rates characterized by the asymptotic graph connectivity. The proposed graph topology assumption holds for several popular random graph models under optimal leading-order sparsity conditions. A comprehensive numerical study is conducted to corroborate our theoretical findings and demonstrate the application of the proposed model to real-world datasets, including horse racing and tennis competitions.\n\n\nStatistical ranking with dynamic covariates</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.16507v2</guid>
      <dc:creator>Pinjun Dong, Ruijian Han, Binyan Jiang, Yiming Xu</dc:creator>
      <pubDate>Mon, 08 Jul 2024 12:33:38 GMT</pubDate>
    </item>
    <item>
      <title>Programming Puzzles</title>
      <link>http://arxiv.org/abs/2106.05784v3</link>
      <description>We introduce a new type of programming challenge called programming puzzles, as an objective and comprehensive evaluation of program synthesis, and release an open-source dataset of Python Programming Puzzles (P3). Each puzzle is defined by a short Python program $f$, and the goal is to find an input which makes $f$ return True. The puzzles are objective in that each one is specified entirely by the source code of its verifier $f$, so evaluating $f$ is all that is needed to test a candidate solution. They do not require an answer key or input/output examples, nor do they depend on natural language understanding. The dataset is comprehensive in that it spans problems of a range of difficulties and domains, ranging from trivial string manipulation problems, to classic programming puzzles (e.g., Tower of Hanoi), to interview/competitive-programming problems (e.g., dynamic programming), to longstanding open problems in algorithms and mathematics (e.g., factoring). We develop baseline enumerative program synthesis, GPT-3 and Codex solvers that are capable of solving puzzles -- even without access to any reference solutions -- by learning from their own past solutions. Codex performs best, solving up to 18% of 397 test problems with a single try and 80% of the problems with 1,000 tries per problem. In a small user study, we find a positive correlation between puzzle-solving performance and coding experience, and between the puzzle difficulty for humans and AI solvers. Therefore, further improvements on P3 could have a significant impact on many program synthesis areas.\n\n\n-Puzzle: A Cost-Efficient Testbed for Benchmarking Reinforcement Learning Algorithms in Generative Language Model</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2106.05784v3</guid>
      <dc:creator>Tal Schuster, Ashwin Kalyan, Oleksandr Polozov, Adam Tauman Kalai</dc:creator>
      <pubDate>Sat, 06 Nov 2021 20:53:50 GMT</pubDate>
    </item>
    <item>
      <title>Prompt Exploration with Prompt Regression</title>
      <link>http://arxiv.org/abs/2405.11083v1</link>
      <description>In the advent of democratized usage of large language models (LLMs), there is a growing desire to systematize LLM prompt creation and selection processes beyond iterative trial-and-error. Prior works majorly focus on searching the space of prompts without accounting for relations between prompt variations.   Here we propose a framework, Prompt Exploration with Prompt Regression (PEPR), to predict the effect of prompt combinations given results for individual prompt elements as well as a simple method to select an effective prompt for a given use-case. We evaluate our approach with open-source LLMs of different sizes on several different tasks.\n\n\nPrompt Exploration with Prompt Regression</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.11083v1</guid>
      <dc:creator>Michael Feffer, Ronald Xu, Yuekai Sun, Mikhail Yurochkin</dc:creator>
      <pubDate>Fri, 17 May 2024 20:30:49 GMT</pubDate>
    </item>
    <item>
      <title>Energy Rank Alignment: Using Preference Optimization to Search Chemical Space at Scale</title>
      <link>http://arxiv.org/abs/2405.12961v1</link>
      <description>Searching through chemical space is an exceptionally challenging problem because the number of possible molecules grows combinatorially with the number of atoms. Large, autoregressive models trained on databases of chemical compounds have yielded powerful generators, but we still lack robust strategies for generating molecules with desired properties. This molecular search problem closely resembles the &quot;alignment&quot; problem for large language models, though for many chemical tasks we have a specific and easily evaluable reward function. Here, we introduce an algorithm called energy rank alignment (ERA) that leverages an explicit reward function to produce a gradient-based objective that we use to optimize autoregressive policies. We show theoretically that this algorithm is closely related to proximal policy optimization (PPO) and direct preference optimization (DPO), but has a minimizer that converges to an ideal Gibbs-Boltzmann distribution with the reward playing the role of an energy function. Furthermore, this algorithm is highly scalable, does not require reinforcement learning, and performs well relative to DPO when the number of preference observations per pairing is small. We deploy this approach to align molecular transformers to generate molecules with externally specified properties and find that it does so robustly, searching through diverse parts of chemical space. While our focus here is on chemical search, we also obtain excellent results on an AI supervised task for LLM alignment, showing that the method is scalable and general.\n\n\nEnergy Rank Alignment: Using Preference Optimization to Search Chemical Space at Scale</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.12961v1</guid>
      <dc:creator>Shriram Chennakesavalu, Frank Hu, Sebastian Ibarraran, Grant M. Rotskoff</dc:creator>
      <pubDate>Tue, 21 May 2024 17:35:20 GMT</pubDate>
    </item>
    <item>
      <title>Financial Knowledge Large Language Model</title>
      <link>http://arxiv.org/abs/2407.00365v1</link>
      <description>Artificial intelligence is making significant strides in the finance industry, revolutionizing how data is processed and interpreted. Among these technologies, large language models (LLMs) have demonstrated substantial potential to transform financial services by automating complex tasks, enhancing customer service, and providing detailed financial analysis. Firstly, we introduce IDEA-FinBench, an evaluation benchmark specifically tailored for assessing financial knowledge in large language models (LLMs). This benchmark utilizes questions from two globally respected and authoritative financial professional exams, aimimg to comprehensively evaluate the capability of LLMs to directly address exam questions pertinent to the finance sector. Secondly, we propose IDEA-FinKER, a Financial Knowledge Enhancement framework designed to facilitate the rapid adaptation of general LLMs to the financial domain, introducing a retrieval-based few-shot learning method for real-time context-level knowledge injection, and a set of high-quality financial knowledge instructions for fine-tuning any general LLM. Finally, we present IDEA-FinQA, a financial question-answering system powered by LLMs. This system is structured around a scheme of real-time knowledge injection and factual enhancement using external knowledge. IDEA-FinQA is comprised of three main modules: the data collector, the data querying module, and LLM-based agents tasked with specific functions.\n\n\nFinancial Knowledge Large Language Model</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2407.00365v1</guid>
      <dc:creator>Cehao Yang, Chengjin Xu, Yiyan Qi</dc:creator>
      <pubDate>Sat, 29 Jun 2024 08:26:49 GMT</pubDate>
    </item>
    <item>
      <title>Research Re: search &amp; Re-search</title>
      <link>http://arxiv.org/abs/2403.13705v1</link>
      <description>Search algorithms are often categorized by their node expansion strategy. One option is the depth-first strategy, a simple backtracking strategy that traverses the search space in the order in which successor nodes are generated. An alternative is the best-first strategy, which was designed to make it possible to use domain-specific heuristic information. By exploring promising parts of the search space first, best-first algorithms are usually more efficient than depth-first algorithms.   In programs that play minimax games such as chess and checkers, the efficiency of the search is of crucial importance. Given the success of best-first algorithms in other domains, one would expect them to be used for minimax games too. However, all high-performance game-playing programs are based on a depth-first algorithm.   This study takes a closer look at a depth-first algorithm, AB, and a best-first algorithm, SSS. The prevailing opinion on these algorithms is that SSS offers the potential for a more efficient search, but that its complicated formulation and exponential memory requirements render it impractical. The theoretical part of this work shows that there is a surprisingly straightforward link between the two algorithms -- for all practical purposes, SSS is a special case of AB. Subsequent empirical evidence proves the prevailing opinion on SSS to be wrong: it is not a complicated algorithm, it does not need too much memory, and it is also not more efficient than depth-first search.\n\n\nSearching for Best Practices in Retrieval-Augmented Generation</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2403.13705v1</guid>
      <dc:creator>Aske Plaat</dc:creator>
      <pubDate>Wed, 20 Mar 2024 16:08:57 GMT</pubDate>
    </item>
    <item>
      <title>Towards Boosting the Accuracy of Non-Latin Scene Text Recognition</title>
      <link>http://arxiv.org/abs/2201.03185v1</link>
      <description>Scene-text recognition is remarkably better in Latin languages than the non-Latin languages due to several factors like multiple fonts, simplistic vocabulary statistics, updated data generation tools, and writing systems. This paper examines the possible reasons for low accuracy by comparing English datasets with non-Latin languages. We compare various features like the size (width and height) of the word images and word length statistics. Over the last decade, generating synthetic datasets with powerful deep learning techniques has tremendously improved scene-text recognition. Several controlled experiments are performed on English, by varying the number of (i) fonts to create the synthetic data and (ii) created word images. We discover that these factors are critical for the scene-text recognition systems. The English synthetic datasets utilize over 1400 fonts while Arabic and other non-Latin datasets utilize less than 100 fonts for data generation. Since some of these languages are a part of different regions, we garner additional fonts through a region-based search to improve the scene-text recognition models in Arabic and Devanagari. We improve the Word Recognition Rates (WRRs) on Arabic MLT-17 and MLT-19 datasets by 24.54% and 2.32% compared to previous works or baselines. We achieve WRR gains of 7.88% and 3.72% for IIIT-ILST and MLT-19 Devanagari datasets.\n\n\nTowards Fine-Grained Pedagogical Control over English Grammar Complexity in Educational Text Generation</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2201.03185v1</guid>
      <dc:creator>Sanjana Gunna, Rohit Saluja, C. V. Jawahar</dc:creator>
      <pubDate>Mon, 10 Jan 2022 06:36:43 GMT</pubDate>
    </item>
    <item>
      <title>Ridge Regression for Paired Comparisons: A Tractable New Approach, with Application to Premier League Football</title>
      <link>http://arxiv.org/abs/2406.09597v1</link>
      <description>Paired comparison models, such as Bradley-Terry and Thurstone-Mosteller, are commonly used to estimate relative strengths of pairwise compared items in tournament-style datasets. With predictive performance as primary criterion, we discuss estimation of paired comparison models with a ridge penalty. A new approach is derived which combines empirical Bayes and composite likelihoods without any need to re-fit the model, as a convenient alternative to cross-validation of the ridge tuning parameter. Simulation studies, together with application to 28 seasons of English Premier League football, demonstrate much better predictive accuracy of the new approach relative to ordinary maximum likelihood. While the application of a standard bias-reducing penalty was found to improve appreciably the performance of maximum likelihood, the ridge penalty with tuning as developed here yields greater accuracy still.\n\n\nRidge Regression for Paired Comparisons: A Tractable New Approach, with Application to Premier League Football</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.09597v1</guid>
      <dc:creator>Cristiano Varin, David Firth</dc:creator>
      <pubDate>Thu, 13 Jun 2024 21:36:04 GMT</pubDate>
    </item>
    <item>
      <title>Improved lattice actions for supersymmetric quantum mechanics</title>
      <link>http://arxiv.org/abs/1210.5404v1</link>
      <description>We analyze the Euclidean version of supersymmetric quantum mechanics on the lattice by means of a numerical path integral. We consider two different lattice derivatives and improve the actions containing them with respect to supersymmetry by systematically adding interaction terms with non-zero extent. To quantize this improvement, we measure boson and fermion masses and Ward identities for the naive as well as the improved models. The masses are degenerate in all models, but the magnitude of the Ward identities decreases significantly for both derivative operators using the improved actions. This is a clear sign that the breaking of supersymmetry due to lattice artifacts is reduced.\n\n\nClass-Conditional self-reward mechanism for improved Text-to-Image models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/1210.5404v1</guid>
      <dc:creator>Sebastian Schierenberg, Falk Bruckmann</dc:creator>
      <pubDate>Fri, 19 Oct 2012 12:39:54 GMT</pubDate>
    </item>
    <item>
      <title>Towards Human Understanding of Paraphrase Types in ChatGPT</title>
      <link>http://arxiv.org/abs/2407.02302v1</link>
      <description>Paraphrases represent a human's intuitive ability to understand expressions presented in various different ways. Current paraphrase evaluations of language models primarily use binary approaches, offering limited interpretability of specific text changes. Atomic paraphrase types (APT) decompose paraphrases into different linguistic changes and offer a granular view of the flexibility in linguistic expression (e.g., a shift in syntax or vocabulary used). In this study, we assess the human preferences towards ChatGPT in generating English paraphrases with ten APTs and five prompting techniques. We introduce APTY (Atomic Paraphrase TYpes), a dataset of 500 sentence-level and word-level annotations by 15 annotators. The dataset also provides a human preference ranking of paraphrases with different types that can be used to fine-tune models with RLHF and DPO methods. Our results reveal that ChatGPT can generate simple APTs, such as additions and deletions, but struggle with complex structures (e.g., subordination changes). This study contributes to understanding which aspects of paraphrasing language models have already succeeded at understanding and what remains elusive. In addition, our curated datasets can be used to develop language models with specific linguistic capabilities.\n\n\nTowards Human Understanding of Paraphrase Types in ChatGPT</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2407.02302v1</guid>
      <dc:creator>Dominik Meier, Jan Philip Wahle, Terry Ruas, Bela Gipp</dc:creator>
      <pubDate>Tue, 02 Jul 2024 14:35:10 GMT</pubDate>
    </item>
    <item>
      <title>CURATRON: Complete Robust Preference Data for Robust Alignment of Large Language Models</title>
      <link>http://arxiv.org/abs/2403.02745v1</link>
      <description>This paper addresses the challenges of aligning large language models (LLMs) with human values via preference learning (PL), with a focus on the issues of incomplete and corrupted data in preference datasets. We propose a novel method for robustly and completely recalibrating values within these datasets to enhance LLMs resilience against the issues. In particular, we devise a guaranteed polynomial time ranking algorithm that robustifies several existing models, such as the classic Bradley--Terry--Luce (BTL) (Bradley and Terry, 1952) model and certain generalizations of it. To the best of our knowledge, our present work is the first to propose an algorithm that provably recovers an {\epsilon}-optimal ranking with high probability while allowing as large as O(n) perturbed pairwise comparison results per model response. Furthermore, we show robust recovery results in the partially observed setting. Our experiments confirm that our algorithms handle adversarial noise and unobserved comparisons well in both general and LLM preference dataset settings. This work contributes to the development and scaling of more reliable and ethically aligned AI models by equipping the dataset curation pipeline with the ability to handle missing and maliciously manipulated inputs.\n\n\nCURATRON: Complete Robust Preference Data for Robust Alignment of Large Language Models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2403.02745v1</guid>
      <dc:creator>Son The Nguyen, Niranjan Uma Naresh, Theja Tulabandhula</dc:creator>
      <pubDate>Tue, 05 Mar 2024 07:58:12 GMT</pubDate>
    </item>
    <item>
      <title>TaxoLLaMA: WordNet-based Model for Solving Multiple Lexical Semantic Tasks</title>
      <link>http://arxiv.org/abs/2403.09207v2</link>
      <description>In this paper, we explore the capabilities of LLMs in capturing lexical-semantic knowledge from WordNet on the example of the LLaMA-2-7b model and test it on multiple lexical semantic tasks. As the outcome of our experiments, we present TaxoLLaMA, the everything-in-one model, lightweight due to 4-bit quantization and LoRA. It achieves 11 SotA results, 4 top-2 results out of 16 tasks for the Taxonomy Enrichment, Hypernym Discovery, Taxonomy Construction, and Lexical Entailment tasks. Moreover, it demonstrates very strong zero-shot performance on Lexical Entailment and Taxonomy Construction with no fine-tuning. We also explore its hidden multilingual and domain adaptation capabilities with a little tuning or few-shot learning. All datasets, code, and model are available online at https://github.com/VityaVitalich/TaxoLLaMA\n\n\nTaxoLLaMA: WordNet-based Model for Solving Multiple Lexical Sematic Tasks</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2403.09207v2</guid>
      <dc:creator>Viktor Moskvoretskii, Ekaterina Neminova, Alina Lobanova, Alexander Panchenko, Irina Nikishina</dc:creator>
      <pubDate>Mon, 17 Jun 2024 16:43:10 GMT</pubDate>
    </item>
    <item>
      <title>Prompting Large Language Models for Zero-shot Essay Scoring via Multi-trait Specialization</title>
      <link>http://arxiv.org/abs/2404.04941v1</link>
      <description>Advances in automated essay scoring (AES) have traditionally relied on labeled essays, requiring tremendous cost and expertise for their acquisition. Recently, large language models (LLMs) have achieved great success in various tasks, but their potential is less explored in AES. In this paper, we propose Multi Trait Specialization (MTS), a zero-shot prompting framework to elicit essay scoring capabilities in LLMs. Specifically, we leverage ChatGPT to decompose writing proficiency into distinct traits and generate scoring criteria for each trait. Then, an LLM is prompted to extract trait scores from several conversational rounds, each round scoring one of the traits based on the scoring criteria. Finally, we derive the overall score via trait averaging and min-max scaling. Experimental results on two benchmark datasets demonstrate that MTS consistently outperforms straightforward prompting (Vanilla) in average QWK across all LLMs and datasets, with maximum gains of 0.437 on TOEFL11 and 0.355 on ASAP. Additionally, with the help of MTS, the small-sized Llama2-13b-chat substantially outperforms ChatGPT, facilitating an effective deployment in real applications.\n\n\nPrompting Large Language Models for Zero-shot Essay Scoring via Multi-trait Specialization</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2404.04941v1</guid>
      <dc:creator>Sanwoo Lee, Yida Cai, Desong Meng, Ziyang Wang, Yunfang Wu</dc:creator>
      <pubDate>Sun, 07 Apr 2024 12:25:35 GMT</pubDate>
    </item>
    <item>
      <title>Navigating the Landscape of Large Language Models: A Comprehensive Review and Analysis of Paradigms and Fine-Tuning Strategies</title>
      <link>http://arxiv.org/abs/2404.09022v1</link>
      <description>With the surge of ChatGPT,the use of large models has significantly increased,rapidly rising to prominence across the industry and sweeping across the internet. This article is a comprehensive review of fine-tuning methods for large models. This paper investigates the latest technological advancements and the application of advanced methods in aspects such as task-adaptive fine-tuning,domain-adaptive fine-tuning,few-shot learning,knowledge distillation,multi-task learning,parameter-efficient fine-tuning,and dynamic fine-tuning.\n\n\nNavigating the Landscape of Large Language Models: A Comprehensive Review and Analysis of Paradigms and Fine-Tuning Strategies</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2404.09022v1</guid>
      <dc:creator>Benjue Weng</dc:creator>
      <pubDate>Sat, 13 Apr 2024 15:03:03 GMT</pubDate>
    </item>
    <item>
      <title>DockGame: Cooperative Games for Multimeric Rigid Protein Docking</title>
      <link>http://arxiv.org/abs/2310.06177v1</link>
      <description>Protein interactions and assembly formation are fundamental to most biological processes. Predicting the assembly structure from constituent proteins -- referred to as the protein docking task -- is thus a crucial step in protein design applications. Most traditional and deep learning methods for docking have focused mainly on binary docking, following either a search-based, regression-based, or generative modeling paradigm. In this paper, we focus on the less-studied multimeric (i.e., two or more proteins) docking problem. We introduce DockGame, a novel game-theoretic framework for docking -- we view protein docking as a cooperative game between proteins, where the final assembly structure(s) constitute stable equilibria w.r.t. the underlying game potential. Since we do not have access to the true potential, we consider two approaches - i) learning a surrogate game potential guided by physics-based energy functions and computing equilibria by simultaneous gradient updates, and ii) sampling from the Gibbs distribution of the true potential by learning a diffusion generative model over the action spaces (rotations and translations) of all proteins. Empirically, on the Docking Benchmark 5.5 (DB5.5) dataset, DockGame has much faster runtimes than traditional docking methods, can generate multiple plausible assembly structures, and achieves comparable performance to existing binary docking baselines, despite solving the harder task of coordinating multiple protein chains.\n\n\nDockGame: Cooperative Games for Multimeric Rigid Protein Docking</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2310.06177v1</guid>
      <dc:creator>Vignesh Ram Somnath, Pier Giuseppe Sessa, Maria Rodriguez Martinez, Andreas Krause</dc:creator>
      <pubDate>Mon, 09 Oct 2023 22:02:05 GMT</pubDate>
    </item>
    <item>
      <title>BPO: Supercharging Online Preference Learning by Adhering to the Proximity of Behavior LLM</title>
      <link>http://arxiv.org/abs/2406.12168v2</link>
      <description>Direct alignment from preferences (DAP) has emerged as a promising paradigm for aligning large language models (LLMs) to human desiderata from pre-collected, offline preference datasets. While recent studies indicate that existing offline DAP methods can directly benefit from online training samples, we highlight the need to develop specific online DAP algorithms to fully harness the power of online training. Specifically, we identify that the learned LLM should adhere to the proximity of the behavior LLM, which collects the training samples. To this end, we propose online Preference Optimization in proximity to the Behavior LLM (BPO), emphasizing the importance of constructing a proper trust region for LLM alignment.   We conduct extensive experiments to validate the effectiveness and applicability of our approach by integrating it with various DAP methods, resulting in significant performance improvements across a wide range of tasks when training with the same amount of preference data. Even when only introducing one additional data collection phase, our online BPO improves its offline DAP baseline from 72.0% to 80.2% on TL;DR and from 82.2% to 89.1% on Anthropic Helpfulness in terms of win rate against human reference text.\n\n\nBPO: Supercharging Online Preference Learning by Adhering to the Proximity of Behavior LLM</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.12168v2</guid>
      <dc:creator>Wenda Xu, Jiachen Li, William Yang Wang, Lei Li</dc:creator>
      <pubDate>Wed, 19 Jun 2024 05:25:27 GMT</pubDate>
    </item>
    <item>
      <title>Uncovering Limitations of Large Language Models in Information Seeking from Tables</title>
      <link>http://arxiv.org/abs/2406.04113v1</link>
      <description>Tables are recognized for their high information density and widespread usage, serving as essential sources of information. Seeking information from tables (TIS) is a crucial capability for Large Language Models (LLMs), serving as the foundation of knowledge-based Q&amp;A systems. However, this field presently suffers from an absence of thorough and reliable evaluation. This paper introduces a more reliable benchmark for Table Information Seeking (TabIS). To avoid the unreliable evaluation caused by text similarity-based metrics, TabIS adopts a single-choice question format (with two options per question) instead of a text generation format. We establish an effective pipeline for generating options, ensuring their difficulty and quality. Experiments conducted on 12 LLMs reveal that while the performance of GPT-4-turbo is marginally satisfactory, both other proprietary and open-source models perform inadequately. Further analysis shows that LLMs exhibit a poor understanding of table structures, and struggle to balance between TIS performance and robustness against pseudo-relevant tables (common in retrieval-augmented systems). These findings uncover the limitations and potential challenges of LLMs in seeking information from tables. We release our data and code to facilitate further research in this field.\n\n\nUncovering Limitations of Large Language Models in Information Seeking from Tables</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.04113v1</guid>
      <dc:creator>Chaoxu Pang, Yixuan Cao, Chunhao Yang, Ping Luo</dc:creator>
      <pubDate>Thu, 06 Jun 2024 14:30:59 GMT</pubDate>
    </item>
    <item>
      <title>Local Large Language Models for Complex Structured Medical Tasks</title>
      <link>http://arxiv.org/abs/2308.01727v1</link>
      <description>This paper introduces an approach that combines the language reasoning capabilities of large language models (LLMs) with the benefits of local training to tackle complex, domain-specific tasks. Specifically, the authors demonstrate their approach by extracting structured condition codes from pathology reports. The proposed approach utilizes local LLMs, which can be fine-tuned to respond to specific generative instructions and provide structured outputs. The authors collected a dataset of over 150k uncurated surgical pathology reports, containing gross descriptions, final diagnoses, and condition codes. They trained different model architectures, including LLaMA, BERT and LongFormer and evaluated their performance. The results show that the LLaMA-based models significantly outperform BERT-style models across all evaluated metrics, even with extremely reduced precision. The LLaMA models performed especially well with large datasets, demonstrating their ability to handle complex, multi-label tasks. Overall, this work presents an effective approach for utilizing LLMs to perform domain-specific tasks using accessible hardware, with potential applications in the medical domain, where complex data extraction and classification are required.\n\n\nOpenLLM-Ro--Technical Report on Open-source Romanian LLMs trained starting from Llama 2</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2308.01727v1</guid>
      <dc:creator>V. K. Cody Bumgardner, Aaron Mullen, Sam Armstrong, Caylin Hickey, Jeff Talbert</dc:creator>
      <pubDate>Thu, 03 Aug 2023 12:36:13 GMT</pubDate>
    </item>
    <item>
      <title>Banishing LLM Hallucinations Requires Rethinking Generalization</title>
      <link>http://arxiv.org/abs/2406.17642v1</link>
      <description>Despite their powerful chat, coding, and reasoning abilities, Large Language Models (LLMs) frequently hallucinate. Conventional wisdom suggests that hallucinations are a consequence of a balance between creativity and factuality, which can be mitigated, but not eliminated, by grounding the LLM in external knowledge sources. Through extensive systematic experiments, we show that these traditional approaches fail to explain why LLMs hallucinate in practice. Specifically, we show that LLMs augmented with a massive Mixture of Memory Experts (MoME) can easily memorize large datasets of random numbers. We corroborate these experimental findings with a theoretical construction showing that simple neural networks trained to predict the next token hallucinate when the training loss is above a threshold as it usually does in practice when training on internet scale data. We interpret our findings by comparing against traditional retrieval methods for mitigating hallucinations. We use our findings to design a first generation model for removing hallucinations -- Lamini-1 -- that stores facts in a massive mixture of millions of memory experts that are retrieved dynamically.\n\n\nBanishing LLM Hallucinations Requires Rethinking Generalization</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.17642v1</guid>
      <dc:creator>Johnny Li, Saksham Consul, Eda Zhou, James Wong, Naila Farooqui, Yuxin Ye, Nithyashree Manohar, Zhuxiaona Wei, Tian Wu, Ben Echols, Sharon Zhou, Gregory Diamos</dc:creator>
      <pubDate>Tue, 25 Jun 2024 15:31:01 GMT</pubDate>
    </item>
    <item>
      <title>Towards Safer Large Language Models through Machine Unlearning</title>
      <link>http://arxiv.org/abs/2402.10058v2</link>
      <description>The rapid advancement of Large Language Models (LLMs) has demonstrated their vast potential across various domains, attributed to their extensive pretraining knowledge and exceptional generalizability. However, LLMs often encounter challenges in generating harmful content when faced with problematic prompts. To address this problem, existing work attempted to implement a gradient ascent based approach to prevent LLMs from producing harmful output. While these methods can be effective, they frequently impact the model utility in responding to normal prompts. To address this gap, we introduce Selective Knowledge negation Unlearning (SKU), a novel unlearning framework for LLMs, designed to eliminate harmful knowledge while preserving utility on normal prompts. Specifically, SKU is consisted of two stages: harmful knowledge acquisition stage and knowledge negation stage. The first stage aims to identify and acquire harmful knowledge within the model, whereas the second is dedicated to remove this knowledge. SKU selectively isolates and removes harmful knowledge in model parameters, ensuring the model's performance remains robust on normal prompts. Our experiments conducted across various LLM architectures demonstrate that SKU identifies a good balance point between removing harmful information and preserving utility.\n\n\nToward robust unlearning for LLMs</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.10058v2</guid>
      <dc:creator>Zheyuan Liu, Guangyao Dou, Zhaoxuan Tan, Yijun Tian, Meng Jiang</dc:creator>
      <pubDate>Wed, 05 Jun 2024 04:57:09 GMT</pubDate>
    </item>
    <item>
      <title>Exploring Mathematical Extrapolation of Large Language Models with Synthetic Data</title>
      <link>http://arxiv.org/abs/2406.02100v1</link>
      <description>Large Language Models (LLMs) have shown excellent performance in language understanding, text generation, code synthesis, and many other tasks, while they still struggle in complex multi-step reasoning problems, such as mathematical reasoning. In this paper, through a newly proposed arithmetical puzzle problem, we show that the model can perform well on multi-step reasoning tasks via fine-tuning on high-quality synthetic data. Experimental results with the open-llama-3B model on three different test datasets show that not only the model can reach a zero-shot pass@1 at 0.44 on the in-domain dataset, it also demonstrates certain generalization capabilities on the out-of-domain datasets. Specifically, this paper has designed two out-of-domain datasets in the form of extending the numerical range and the composing components of the arithmetical puzzle problem separately. The fine-tuned models have shown encouraging performance on these two far more difficult tasks with the zero-shot pass@1 at 0.33 and 0.35, respectively.\n\n\nExploring Mathematical Extrapolation of Large Language Models with Synthetic Data</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.02100v1</guid>
      <dc:creator>Haolong Li, Yu Ma, Yinqi Zhang, Chen Ye, Jie Chen</dc:creator>
      <pubDate>Tue, 04 Jun 2024 08:30:37 GMT</pubDate>
    </item>
    <item>
      <title>Q*: Improving Multi-step Reasoning for LLMs with Deliberative Planning</title>
      <link>http://arxiv.org/abs/2406.14283v3</link>
      <description>Large Language Models (LLMs) have demonstrated impressive capability in many natural language tasks. However, the auto-regressive generation process makes LLMs prone to produce errors, hallucinations and inconsistent statements when performing multi-step reasoning. In this paper, by casting multi-step reasoning of LLMs as a heuristic search problem, we aim to alleviate the pathology by introducing Q*, a general, versatile and agile framework for guiding LLMs decoding process with deliberative planning. By learning a plug-and-play Q-value model as heuristic function for estimating expected future rewards, our Q* can effectively guide LLMs to select the most promising next reasoning step without fine-tuning LLMs for the current task, which avoids the significant computational overhead and potential risk of performance degeneration on other tasks. Extensive experiments on GSM8K, MATH and MBPP demonstrate the superiority of our method, contributing to improving the reasoning performance of existing open-source LLMs.\n\n\nQ*: Improving Multi-step Reasoning for LLMs with Deliberative Planning</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.14283v3</guid>
      <dc:creator>Chaojie Wang, Yanchen Deng, Zhiyi Lv, Zeng Liang, Jujie He, Shuicheng Yan, An Bo</dc:creator>
      <pubDate>Thu, 27 Jun 2024 09:44:45 GMT</pubDate>
    </item>
    <item>
      <title>Direct Preference-Based Evolutionary Multi-Objective Optimization with Dueling Bandit</title>
      <link>http://arxiv.org/abs/2311.14003v1</link>
      <description>Optimization problems find widespread use in both single-objective and multi-objective scenarios. In practical applications, users aspire for solutions that converge to the region of interest (ROI) along the Pareto front (PF). While the conventional approach involves approximating a fitness function or an objective function to reflect user preferences, this paper explores an alternative avenue. Specifically, we aim to discover a method that sidesteps the need for calculating the fitness function, relying solely on human feedback. Our proposed approach entails conducting direct preference learning facilitated by an active dueling bandit algorithm. The experimental phase is structured into three sessions. Firstly, we assess the performance of our active dueling bandit algorithm. Secondly, we implement our proposed method within the context of Multi-objective Evolutionary Algorithms (MOEAs). Finally, we deploy our method in a practical problem, specifically in protein structure prediction (PSP). This research presents a novel interactive preference-based MOEA framework that not only addresses the limitations of traditional techniques but also unveils new possibilities for optimization problems.\n\n\nDirect Preference-Based Evolutionary Multi-Objective Optimization with Dueling Bandit</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2311.14003v1</guid>
      <dc:creator>Tian Huang, Ke Li</dc:creator>
      <pubDate>Thu, 23 Nov 2023 13:38:43 GMT</pubDate>
    </item>
    <item>
      <title>Bailong: Bilingual Transfer Learning based on QLoRA and Zip-tie Embedding</title>
      <link>http://arxiv.org/abs/2404.00862v1</link>
      <description>Large language models (LLMs) have demonstrated exceptional performance in various NLP applications. However, the majority of existing open-source LLMs are pre-trained primarily on English data and little part of other languages. This deficiency in multilingual training data results in suboptimal performance when applied to languages with fewer available resources. Furthermore, enhancing the performance of LLMs on low-resource languages by full-parameter fine-tuning with additional data requires substantial computational resources, posing computational barriers for research organizations and individual researchers. Consequently, several techniques such as parameter-efficient tuning and advanced embedding initialization have been proposed to address these challenges. In this work, we combine them to facilitate cross-lingual transfer on English-dominated open-source LLM. To effectively enhance the model's proficiency in Traditional Chinese, we conduct secondary pre-training on Llama 2 7B with Traditional Chinese data by leveraging QLoRA and our proposed zip-tie embedding initialization. The resulting model called Bailong, which stands for Bilingual trAnsfer learnIng based on qLOra and zip-tie embeddiNG. We present Bailong-instruct 7B, a fine-tuned version of Bailong 7B optimized for multi-turn dialogue scenarios. Recognizing the inadequacy of benchmark datasets in Traditional Chinese, we further introduce Bailong-bench to assess the alignment of models with human preferences and the capability to follow instructions in both Traditional Chinese and English tasks. In our evaluation, Bailong-instruct 7B exhibits competitive performance on Bailong-bench and other benchmark datasets when compared to other open-source models of similar or even larger parameter sizes. Bailong-instruct 7B and Bailong-bench are publicly available with the aim of empowering the community to build upon our efforts.\n\n\nBailong: Bilingual Transfer Learning based on QLoRA and Zip-tie Embedding</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2404.00862v1</guid>
      <dc:creator>Lung-Chuan Chen, Zong-Ru Li</dc:creator>
      <pubDate>Mon, 01 Apr 2024 02:04:44 GMT</pubDate>
    </item>
    <item>
      <title>DeTox: Toxic Subspace Projection for Model Editing</title>
      <link>http://arxiv.org/abs/2405.13967v3</link>
      <description>Recent alignment algorithms such as direct preference optimization (DPO) have been developed to improve the safety of large language models (LLMs) by training these models to match human behaviors exemplified by preference data. However, these methods are both computationally intensive and lacking in controllability and transparency, making them prone to jailbreaking and inhibiting their widespread use. Furthermore, these tuning-based methods require large-scale preference data for training and are susceptible to noisy preference data. In this paper, we introduce a tuning-free alignment alternative (DeTox) and demonstrate its effectiveness under the use case of toxicity reduction. Grounded on theory from factor analysis, DeTox is a sample-efficient model editing approach that identifies a toxic subspace in the model parameter space and reduces model toxicity by projecting away the detected subspace. The toxic sub-space is identified by extracting preference data embeddings from the language model, and removing non-toxic information from these embeddings. We show that DeTox is more sample-efficient than DPO, further showcasing greater robustness to noisy data. Finally, we establish both theoretical and empirical connections between DeTox and DPO, showing that DeTox can be interpreted as a denoised version of a single DPO step.\n\n\nDeTox: Toxic Subspace Projection for Model Editing</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.13967v3</guid>
      <dc:creator>Rheeya Uppaal, Apratim Dey, Yiting He, Yiqiao Zhong, Junjie Hu</dc:creator>
      <pubDate>Tue, 28 May 2024 19:55:16 GMT</pubDate>
    </item>
    <item>
      <title>$i$REPO: $i$mplicit Reward Pairwise Difference based Empirical Preference Optimization</title>
      <link>http://arxiv.org/abs/2405.15230v1</link>
      <description>While astonishingly capable, large Language Models (LLM) can sometimes produce outputs that deviate from human expectations. Such deviations necessitate an alignment phase to prevent disseminating untruthful, toxic, or biased information. Traditional alignment methods based on reinforcement learning often struggle with the identified instability, whereas preference optimization methods are limited by their overfitting to pre-collected hard-label datasets. In this paper, we propose a novel LLM alignment framework named $i$REPO, which utilizes implicit Reward pairwise difference regression for Empirical Preference Optimization. Particularly, $i$REPO employs self-generated datasets labelled by empirical human (or AI annotator) preference to iteratively refine the aligned policy through a novel regression-based loss function. Furthermore, we introduce an innovative algorithm backed by theoretical guarantees for achieving optimal results under ideal assumptions and providing a practical performance-gap result without such assumptions. Experimental results with Phi-2 and Mistral-7B demonstrate that $i$REPO effectively achieves self-alignment using soft-label, self-generated responses and the logit of empirical AI annotators. Furthermore, our approach surpasses preference optimization baselines in evaluations using the Language Model Evaluation Harness and Multi-turn benchmarks.\n\n\nREPO: mplicit Reward Pairwise Difference based Empirical Preference Optimization</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.15230v1</guid>
      <dc:creator>Long Tan Le, Han Shu, Tung-Anh Nguyen, Choong Seon Hong, Nguyen H. Tran</dc:creator>
      <pubDate>Fri, 24 May 2024 05:42:11 GMT</pubDate>
    </item>
    <item>
      <title>Enhancing Context Through Contrast</title>
      <link>http://arxiv.org/abs/2401.03314v1</link>
      <description>Neural machine translation benefits from semantically rich representations. Considerable progress in learning such representations has been achieved by language modelling and mutual information maximization objectives using contrastive learning. The language-dependent nature of language modelling introduces a trade-off between the universality of the learned representations and the model's performance on the language modelling tasks. Although contrastive learning improves performance, its success cannot be attributed to mutual information alone. We propose a novel Context Enhancement step to improve performance on neural machine translation by maximizing mutual information using the Barlow Twins loss. Unlike other approaches, we do not explicitly augment the data but view languages as implicit augmentations, eradicating the risk of disrupting semantic information. Further, our method does not learn embeddings from scratch and can be generalised to any set of pre-trained embeddings. Finally, we evaluate the language-agnosticism of our embeddings through language classification and use them for neural machine translation to compare with state-of-the-art approaches.\n\n\nContrastive Preference Learning for Neural Machine Translation</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2401.03314v1</guid>
      <dc:creator>Kshitij Ambilduke, Aneesh Shetye, Diksha Bagade, Rishika Bhagwatkar, Khurshed Fitter, Prasad Vagdargi, Shital Chiddarwar</dc:creator>
      <pubDate>Sat, 06 Jan 2024 22:13:51 GMT</pubDate>
    </item>
    <item>
      <title>FIPO: Free-form Instruction-oriented Prompt Optimization with Preference Dataset and Modular Fine-tuning Schema</title>
      <link>http://arxiv.org/abs/2402.11811v2</link>
      <description>When the quality of naive prompts is carefully optimized by human experts, the task performance of large language models (LLMs) can be significantly improved. However, expert-based prompt optimizations are expensive. Herein, some works have proposed Automatic Prompt Optimization (APO), to optimize naive prompts according to task outputs of given in-box testing models, with the help of advanced LLMs (e.g., GPT-4) in an ad-hoc way. Although effective, existing schemes suffer from poor generalization ability and privacy risk. To this end, we collect the first large-scale Prompt Optimization Preference dataset (POP), fine-tune offline local LLM-based optimizers, then fairly test with various downstream models. Our method allows accurate optimization of the core task instruction part within the naive prompt in a model-agnostic manner, and thus is named Free-from Instruction-oriented Prompt Optimization (FIPO). In specific, FIPO uses a modular APO template that dynamically integrate the naive task instruction, optional instruction responses, and optional ground truth to produce finely optimized prompts. The POP dataset is meticulously constructed using advanced LLMs, undergoing rigorous cross-validation by human experts and analytical models. Leveraging insights from the data with Tulu2 models and diverse fine-tuning strategies, we validate the efficacy of FIPO framework across five public benchmarks and three testing models. Check codes and data here: https://github.com/LuJunru/FIPO_Project.\n\n\nFIPO: Free-form Instruction-oriented Prompt Optimization with Preference Dataset and Modular Fine-tuning Schema</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.11811v2</guid>
      <dc:creator>Junru Lu, Siyu An, Min Zhang, Yulan He, Di Yin, Xing Sun</dc:creator>
      <pubDate>Sun, 16 Jun 2024 10:29:03 GMT</pubDate>
    </item>
    <item>
      <title>IITK at SemEval-2024 Task 2: Exploring the Capabilities of LLMs for Safe Biomedical Natural Language Inference for Clinical Trials</title>
      <link>http://arxiv.org/abs/2404.04510v1</link>
      <description>Large Language models (LLMs) have demonstrated state-of-the-art performance in various natural language processing (NLP) tasks across multiple domains, yet they are prone to shortcut learning and factual inconsistencies. This research investigates LLMs' robustness, consistency, and faithful reasoning when performing Natural Language Inference (NLI) on breast cancer Clinical Trial Reports (CTRs) in the context of SemEval 2024 Task 2: Safe Biomedical Natural Language Inference for Clinical Trials. We examine the reasoning capabilities of LLMs and their adeptness at logical problem-solving. A comparative analysis is conducted on pre-trained language models (PLMs), GPT-3.5, and Gemini Pro under zero-shot settings using Retrieval-Augmented Generation (RAG) framework, integrating various reasoning chains. The evaluation yields an F1 score of 0.69, consistency of 0.71, and a faithfulness score of 0.90 on the test dataset.\n\n\nNycu-nlp at semeval-2024 task 2: Aggregating large language models in biomedical natural language inference for clinical trials</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2404.04510v1</guid>
      <dc:creator>Shreyasi Mandal, Ashutosh Modi</dc:creator>
      <pubDate>Sat, 06 Apr 2024 05:44:53 GMT</pubDate>
    </item>
    <item>
      <title>Retrieval-Augmented Generation-based Relation Extraction</title>
      <link>http://arxiv.org/abs/2404.13397v1</link>
      <description>Information Extraction (IE) is a transformative process that converts unstructured text data into a structured format by employing entity and relation extraction (RE) methodologies. The identification of the relation between a pair of entities plays a crucial role within this framework. Despite the existence of various techniques for relation extraction, their efficacy heavily relies on access to labeled data and substantial computational resources. In addressing these challenges, Large Language Models (LLMs) emerge as promising solutions; however, they might return hallucinating responses due to their own training data. To overcome these limitations, Retrieved-Augmented Generation-based Relation Extraction (RAG4RE) in this work is proposed, offering a pathway to enhance the performance of relation extraction tasks.   This work evaluated the effectiveness of our RAG4RE approach utilizing different LLMs. Through the utilization of established benchmarks, such as TACRED, TACREV, Re-TACRED, and SemEval RE datasets, our aim is to comprehensively evaluate the efficacy of our RAG4RE approach. In particularly, we leverage prominent LLMs including Flan T5, Llama2, and Mistral in our investigation. The results of our study demonstrate that our RAG4RE approach surpasses performance of traditional RE approaches based solely on LLMs, particularly evident in the TACRED dataset and its variations. Furthermore, our approach exhibits remarkable performance compared to previous RE methodologies across both TACRED and TACREV datasets, underscoring its efficacy and potential for advancing RE tasks in natural language processing.\n\n\nRelation Extraction with Fine-Tuned Large Language Models in Retrieval Augmented Generation Frameworks</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2404.13397v1</guid>
      <dc:creator>Sefika Efeoglu, Adrian Paschke</dc:creator>
      <pubDate>Sat, 20 Apr 2024 14:42:43 GMT</pubDate>
    </item>
    <item>
      <title>Improving Automated Distractor Generation for Math Multiple-choice Questions with Overgenerate-and-rank</title>
      <link>http://arxiv.org/abs/2405.05144v2</link>
      <description>Multiple-choice questions (MCQs) are commonly used across all levels of math education since they can be deployed and graded at a large scale. A critical component of MCQs is the distractors, i.e., incorrect answers crafted to reflect student errors or misconceptions. Automatically generating them in math MCQs, e.g., with large language models, has been challenging. In this work, we propose a novel method to enhance the quality of generated distractors through overgenerate-and-rank, training a ranking model to predict how likely distractors are to be selected by real students. Experimental results on a real-world dataset and human evaluation with math teachers show that our ranking model increases alignment with human-authored distractors, although human-authored ones are still preferred over generated ones.\n\n\nImproving Automated Distractor Generation for Math Multiple-choice Questions with Overgenerate-and-rank</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.05144v2</guid>
      <dc:creator>Alexander Scarlatos, Wanyong Feng, Digory Smith, Simon Woodhead, Andrew Lan</dc:creator>
      <pubDate>Mon, 13 May 2024 18:10:19 GMT</pubDate>
    </item>
    <item>
      <title>Adversarial DPO: Harnessing Harmful Data for Reducing Toxicity with Minimal Impact on Coherence and Evasiveness in Dialogue Agents</title>
      <link>http://arxiv.org/abs/2405.12900v1</link>
      <description>Recent advancements in open-domain dialogue systems have been propelled by the emergence of high-quality large language models (LLMs) and various effective training methodologies. Nevertheless, the presence of toxicity within these models presents a significant challenge that can potentially diminish the user experience. In this study, we introduce an innovative training algorithm, an improvement upon direct preference optimization (DPO), called adversarial DPO (ADPO). The ADPO algorithm is designed to train models to assign higher probability distributions to preferred responses and lower distributions to unsafe responses, which are self-generated using the toxic control token. We demonstrate that ADPO enhances the model's resilience against harmful conversations while minimizing performance degradation. Furthermore, we illustrate that ADPO offers a more stable training procedure compared to the traditional DPO. To the best of our knowledge, this is the first adaptation of the DPO algorithm that directly incorporates harmful data into the generative model, thereby reducing the need to artificially create safe dialogue data.\n\n\nAdversarial DPO: Harnessing Harmful Data for Reducing Toxicity with Minimal Impact on Coherence and Evasiveness in Dialogue Agents</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.12900v1</guid>
      <dc:creator>San Kim, Gary Geunbae Lee</dc:creator>
      <pubDate>Tue, 21 May 2024 16:14:55 GMT</pubDate>
    </item>
    <item>
      <title>Understand What LLM Needs: Dual Preference Alignment for Retrieval-Augmented Generation</title>
      <link>http://arxiv.org/abs/2406.18676v1</link>
      <description>Retrieval-augmented generation (RAG) has demonstrated effectiveness in mitigating the hallucination problem of large language models (LLMs). However, the difficulty of aligning the retriever with the diverse LLMs' knowledge preferences inevitably poses an inevitable challenge in developing a reliable RAG system. To address this issue, we propose DPA-RAG, a universal framework designed to align diverse knowledge preferences within RAG systems. Specifically, we initially introduce a preference knowledge construction pipline and incorporate five novel query augmentation strategies to alleviate preference data scarcity. Based on preference data, DPA-RAG accomplishes both external and internal preference alignment: 1) It jointly integrate pair-wise, point-wise, and contrastive preference alignment abilities into the reranker, achieving external preference alignment among RAG components. 2) It further introduces a pre-aligned stage before vanilla Supervised Fine-tuning (SFT), enabling LLMs to implicitly capture knowledge aligned with their reasoning preferences, achieving LLMs' internal alignment. Experimental results across four knowledge-intensive QA datasets demonstrate that DPA-RAG outperforms all baselines and seamlessly integrates both black-box and open-sourced LLM readers. Further qualitative analysis and discussions also provide empirical guidance for achieving reliable RAG systems. Our code is publicly available at https://github.com/dongguanting/DPA-RAG.\n\n\nUnderstand What LLM Needs: Dual Preference Alignment for Retrieval-Augmented Generation</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.18676v1</guid>
      <dc:creator>Guanting Dong, Yutao Zhu, Chenghao Zhang, Zechen Wang, Zhicheng Dou, Ji-Rong Wen</dc:creator>
      <pubDate>Wed, 26 Jun 2024 18:26:53 GMT</pubDate>
    </item>
    <item>
      <title>Improving Reward-Conditioned Policies for Multi-Armed Bandits using Normalized Weight Functions</title>
      <link>http://arxiv.org/abs/2406.10795v1</link>
      <description>Recently proposed reward-conditioned policies (RCPs) offer an appealing alternative in reinforcement learning. Compared with policy gradient methods, policy learning in RCPs is simpler since it is based on supervised learning, and unlike value-based methods, it does not require optimization in the action space to take actions. However, for multi-armed bandit (MAB) problems, we find that RCPs are slower to converge and have inferior expected rewards at convergence, compared with classic methods such as the upper confidence bound and Thompson sampling. In this work, we show that the performance of RCPs can be enhanced by constructing policies through the marginalization of rewards using normalized weight functions, whose sum or integral equal $1$, although the function values may be negative. We refer to this technique as generalized marginalization, whose advantage is that negative weights for policies conditioned on low rewards can make the resulting policies more distinct from them. Strategies to perform generalized marginalization in MAB with discrete action spaces are studied. Through simulations, we demonstrate that the proposed technique improves RCPs and makes them competitive with classic methods, showing superior performance on challenging MABs with large action spaces and sparse reward signals.\n\n\nImproving Reward-Conditioned Policies for Multi-Armed Bandits using Normalized Weight Functions</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.10795v1</guid>
      <dc:creator>Kai Xu, Farid Tajaddodianfar, Ben Allison</dc:creator>
      <pubDate>Sun, 16 Jun 2024 03:43:55 GMT</pubDate>
    </item>
    <item>
      <title>Advanced Natural-based interaction for the ITAlian language: LLaMAntino-3-ANITA</title>
      <link>http://arxiv.org/abs/2405.07101v1</link>
      <description>In the pursuit of advancing natural language processing for the Italian language, we introduce a state-of-the-art Large Language Model (LLM) based on the novel Meta LLaMA-3 model: LLaMAntino-3-ANITA-8B-Inst-DPO-ITA. We fine-tuned the original 8B parameters instruction tuned model using the Supervised Fine-tuning (SFT) technique on the English and Italian language datasets in order to improve the original performance. Consequently, a Dynamic Preference Optimization (DPO) process has been used to align preferences, avoid dangerous and inappropriate answers, and limit biases and prejudices. Our model leverages the efficiency of QLoRA to fine-tune the model on a smaller portion of the original model weights and then adapt the model specifically for the Italian linguistic structure, achieving significant improvements in both performance and computational efficiency. Concurrently, DPO is employed to refine the model's output, ensuring that generated content aligns with quality answers. The synergy between SFT, QLoRA's parameter efficiency and DPO's user-centric optimization results in a robust LLM that excels in a variety of tasks, including but not limited to text completion, zero-shot classification, and contextual understanding. The model has been extensively evaluated over standard benchmarks for the Italian and English languages, showing outstanding results. The model is freely available over the HuggingFace hub and, examples of use can be found in our GitHub repository. https://huggingface.co/swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA\n\n\nAdvanced Natural-based interaction for the ITAlian language: LLaMAntino-3-ANITA</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.07101v1</guid>
      <dc:creator>Marco Polignano, Pierpaolo Basile, Giovanni Semeraro</dc:creator>
      <pubDate>Sat, 11 May 2024 22:02:55 GMT</pubDate>
    </item>
    <item>
      <title>Evaluating AI for Law: Bridging the Gap with Open-Source Solutions</title>
      <link>http://arxiv.org/abs/2404.12349v1</link>
      <description>This study evaluates the performance of general-purpose AI, like ChatGPT, in legal question-answering tasks, highlighting significant risks to legal professionals and clients. It suggests leveraging foundational models enhanced by domain-specific knowledge to overcome these issues. The paper advocates for creating open-source legal AI systems to improve accuracy, transparency, and narrative diversity, addressing general AI's shortcomings in legal contexts.\n\n\nEvaluating AI for Law: Bridging the Gap with Open-Source Solutions</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2404.12349v1</guid>
      <dc:creator>Rohan Bhambhoria, Samuel Dahan, Jonathan Li, Xiaodan Zhu</dc:creator>
      <pubDate>Thu, 18 Apr 2024 17:26:01 GMT</pubDate>
    </item>
    <item>
      <title>Evolving to be Your Soulmate: Personalized Dialogue Agents with Dynamically Adapted Personas</title>
      <link>http://arxiv.org/abs/2406.13960v1</link>
      <description>Previous research on persona-based dialogue agents typically preset the agent's persona before deployment, which remains static thereafter. In this paper, we take a step further and explore a new paradigm called Self-evolving Personalized Dialogue Agents (SPDA), where the agent continuously evolves during the conversation to better align with the user's anticipation by dynamically adapting its persona. This paradigm could enable better personalization for each user, but also introduce unique challenges, which mainly lie in the process of persona adaptation. Two key issues include how to achieve persona alignment with the user and how to ensure smooth transition in the adaptation process. To address them, we propose a novel framework that refines the persona at hierarchical levels to progressively align better with the user in a controllable way. Experiments show that integrating the personas adapted by our framework consistently enhances personalization and overall dialogue performance across various base systems.\n\n\nEvolving to be Your Soulmate: Personalized Dialogue Agents with Dynamically Adapted Personas</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.13960v1</guid>
      <dc:creator>Yi Cheng, Wenge Liu, Kaishuai Xu, Wenjun Hou, Yi Ouyang, Chak Tou Leong, Xian Wu, Yefeng Zheng</dc:creator>
      <pubDate>Thu, 20 Jun 2024 03:02:38 GMT</pubDate>
    </item>
    <item>
      <title>DualNet: Continual Learning, Fast and Slow</title>
      <link>http://arxiv.org/abs/2110.00175v1</link>
      <description>According to Complementary Learning Systems (CLS) theory~\citep{mcclelland1995there} in neuroscience, humans do effective \emph{continual learning} through two complementary systems: a fast learning system centered on the hippocampus for rapid learning of the specifics and individual experiences, and a slow learning system located in the neocortex for the gradual acquisition of structured knowledge about the environment. Motivated by this theory, we propose a novel continual learning framework named &quot;DualNet&quot;, which comprises a fast learning system for supervised learning of pattern-separated representation from specific tasks and a slow learning system for unsupervised representation learning of task-agnostic general representation via a Self-Supervised Learning (SSL) technique. The two fast and slow learning systems are complementary and work seamlessly in a holistic continual learning framework. Our extensive experiments on two challenging continual learning benchmarks of CORE50 and miniImageNet show that DualNet outperforms state-of-the-art continual learning methods by a large margin. We further conduct ablation studies of different SSL objectives to validate DualNet's efficacy, robustness, and scalability. Code will be made available upon acceptance.\n\n\nGELEX: Generative AI-Hybrid System for Example-Based Learning</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2110.00175v1</guid>
      <dc:creator>Quang Pham, Chenghao Liu, Steven Hoi</dc:creator>
      <pubDate>Fri, 01 Oct 2021 02:31:59 GMT</pubDate>
    </item>
    <item>
      <title>Fake Artificial Intelligence Generated Contents (FAIGC): A Survey of Theories, Detection Methods, and Opportunities</title>
      <link>http://arxiv.org/abs/2405.00711v2</link>
      <description>In recent years, generative artificial intelligence models, represented by Large Language Models (LLMs) and Diffusion Models (DMs), have revolutionized content production methods. These artificial intelligence-generated content (AIGC) have become deeply embedded in various aspects of daily life and work. However, these technologies have also led to the emergence of Fake Artificial Intelligence Generated Content (FAIGC), posing new challenges in distinguishing genuine information. It is crucial to recognize that AIGC technology is akin to a double-edged sword; its potent generative capabilities, while beneficial, also pose risks for the creation and dissemination of FAIGC. In this survey, We propose a new taxonomy that provides a more comprehensive breakdown of the space of FAIGC methods today. Next, we explore the modalities and generative technologies of FAIGC. We introduce FAIGC detection methods and summarize the related benchmark from various perspectives. Finally, we discuss outstanding challenges and promising areas for future research.\n\n\nFake Artificial Intelligence Generated Contents (FAIGC): A Survey of Theories, Detection Methods, and Opportunities</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.00711v2</guid>
      <dc:creator>Xiaomin Yu, Yezhaohui Wang, Yanfang Chen, Zhen Tao, Dinghao Xi, Shichao Song, Simin Niu, Zhiyu Li</dc:creator>
      <pubDate>Fri, 03 May 2024 04:47:01 GMT</pubDate>
    </item>
    <item>
      <title>Generative Adversarial Reward Learning for Generalized Behavior Tendency Inference</title>
      <link>http://arxiv.org/abs/2105.00822v2</link>
      <description>Recent advances in reinforcement learning have inspired increasing interest in learning user modeling adaptively through dynamic interactions, e.g., in reinforcement learning based recommender systems. Reward function is crucial for most of reinforcement learning applications as it can provide the guideline about the optimization. However, current reinforcement-learning-based methods rely on manually-defined reward functions, which cannot adapt to dynamic and noisy environments. Besides, they generally use task-specific reward functions that sacrifice generalization ability. We propose a generative inverse reinforcement learning for user behavioral preference modelling, to address the above issues. Instead of using predefined reward functions, our model can automatically learn the rewards from user's actions based on discriminative actor-critic network and Wasserstein GAN. Our model provides a general way of characterizing and explaining underlying behavioral tendencies, and our experiments show our method outperforms state-of-the-art methods in a variety of scenarios, namely traffic signal control, online recommender systems, and scanpath prediction.\n\n\nGeneralizing Reward Modeling for Out-of-Distribution Preference Learning</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2105.00822v2</guid>
      <dc:creator>Xiaocong Chen, Lina Yao, Xianzhi Wang, Aixin Sun, Wenjie Zhang, Quan Z. Sheng</dc:creator>
      <pubDate>Wed, 05 May 2021 13:01:28 GMT</pubDate>
    </item>
    <item>
      <title>What Teaches Robots to Walk, Teaches Them to Trade too -- Regime Adaptive Execution using Informed Data and LLMs</title>
      <link>http://arxiv.org/abs/2406.15508v1</link>
      <description>Machine learning techniques applied to the problem of financial market forecasting struggle with dynamic regime switching, or underlying correlation and covariance shifts in true (hidden) market variables. Drawing inspiration from the success of reinforcement learning in robotics, particularly in agile locomotion adaptation of quadruped robots to unseen terrains, we introduce an innovative approach that leverages world knowledge of pretrained LLMs (aka. 'privileged information' in robotics) and dynamically adapts them using intrinsic, natural market rewards using LLM alignment technique we dub as &quot;Reinforcement Learning from Market Feedback&quot; (**RLMF**). Strong empirical results demonstrate the efficacy of our method in adapting to regime shifts in financial markets, a challenge that has long plagued predictive models in this domain. The proposed algorithmic framework outperforms best-performing SOTA LLM models on the existing (FLARE) benchmark stock-movement (SM) tasks by more than 15\% improved accuracy. On the recently proposed NIFTY SM task, our adaptive policy outperforms the SOTA best performing trillion parameter models like GPT-4. The paper details the dual-phase, teacher-student architecture and implementation of our model, the empirical results obtained, and an analysis of the role of language embeddings in terms of Information Gain.\n\n\nWhat Teaches Robots to Walk, Teaches Them to Trade too--Regime Adaptive Execution using Informed Data and LLMs</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.15508v1</guid>
      <dc:creator>Raeid Saqur</dc:creator>
      <pubDate>Thu, 20 Jun 2024 00:17:28 GMT</pubDate>
    </item>
    <item>
      <title>Asymptotic Optimality for Decentralised Bandits</title>
      <link>http://arxiv.org/abs/2109.09427v1</link>
      <description>We consider a large number of agents collaborating on a multi-armed bandit problem with a large number of arms. The goal is to minimise the regret of each agent in a communication-constrained setting. We present a decentralised algorithm which builds upon and improves the Gossip-Insert-Eliminate method of Chawla et al. arxiv:2001.05452. We provide a theoretical analysis of the regret incurred which shows that our algorithm is asymptotically optimal. In fact, our regret guarantee matches the asymptotically optimal rate achievable in the full communication setting. Finally, we present empirical results which support our conclusions\n\n\nAsymptotically Optimal Regret for Black-Box Predict-then-Optimize</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2109.09427v1</guid>
      <dc:creator>Conor Newton, Ayalvadi Ganesh, Henry W. J. Reeve</dc:creator>
      <pubDate>Mon, 20 Sep 2021 11:10:10 GMT</pubDate>
    </item>
    <item>
      <title>Harmonizing Human Insights and AI Precision: Hand in Hand for Advancing Knowledge Graph Task</title>
      <link>http://arxiv.org/abs/2405.09477v1</link>
      <description>Knowledge graph embedding (KGE) has caught significant interest for its effectiveness in knowledge graph completion (KGC), specifically link prediction (LP), with recent KGE models cracking the LP benchmarks. Despite the rapidly growing literature, insufficient attention has been paid to the cooperation between humans and AI on KG. However, humans' capability to analyze graphs conceptually may further improve the efficacy of KGE models with semantic information. To this effect, we carefully designed a human-AI team (HAIT) system dubbed KG-HAIT, which harnesses the human insights on KG by leveraging fully human-designed ad-hoc dynamic programming (DP) on KG to produce human insightful feature (HIF) vectors that capture the subgraph structural feature and semantic similarities. By integrating HIF vectors into the training of KGE models, notable improvements are observed across various benchmarks and metrics, accompanied by accelerated model convergence. Our results underscore the effectiveness of human-designed DP in the task of LP, emphasizing the pivotal role of collaboration between humans and AI on KG. We open avenues for further exploration and innovation through KG-HAIT, paving the way towards more effective and insightful KG analysis techniques.\n\n\nHarmonizing Human Insights and AI Precision: Hand in Hand for Advancing Knowledge Graph Task</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.09477v1</guid>
      <dc:creator>Shurong Wang, Yufei Zhang, Xuliang Huang, Hongwei Wang</dc:creator>
      <pubDate>Wed, 15 May 2024 16:16:37 GMT</pubDate>
    </item>
    <item>
      <title>There and Back Again: The AI Alignment Paradox</title>
      <link>http://arxiv.org/abs/2405.20806v1</link>
      <description>The field of AI alignment aims to steer AI systems toward human goals, preferences, and ethical principles. Its contributions have been instrumental for improving the output quality, safety, and trustworthiness of today's AI models. This perspective article draws attention to a fundamental challenge inherent in all AI alignment endeavors, which we term the &quot;AI alignment paradox&quot;: The better we align AI models with our values, the easier we make it for adversaries to misalign the models. We illustrate the paradox by sketching three concrete example incarnations for the case of language models, each corresponding to a distinct way in which adversaries can exploit the paradox. With AI's increasing real-world impact, it is imperative that a broad community of researchers be aware of the AI alignment paradox and work to find ways to break out of it, in order to ensure the beneficial use of AI for the good of humanity.\n\n\nThere and Back Again: The AI Alignment Paradox</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.20806v1</guid>
      <dc:creator>Robert West, Roland Aydin</dc:creator>
      <pubDate>Fri, 31 May 2024 14:06:24 GMT</pubDate>
    </item>
    <item>
      <title>HumanRankEval: Automatic Evaluation of LMs as Conversational Assistants</title>
      <link>http://arxiv.org/abs/2405.09186v1</link>
      <description>Language models (LMs) as conversational assistants recently became popular tools that help people accomplish a variety of tasks. These typically result from adapting LMs pretrained on general domain text sequences through further instruction-tuning and possibly preference optimisation methods. The evaluation of such LMs would ideally be performed using human judgement, however, this is not scalable. On the other hand, automatic evaluation featuring auxiliary LMs as judges and/or knowledge-based tasks is scalable but struggles with assessing conversational ability and adherence to instructions. To help accelerate the development of LMs as conversational assistants, we propose a novel automatic evaluation task: HumanRankEval (HRE). It consists of a large-scale, diverse and high-quality set of questions, each with several answers authored and scored by humans. To perform evaluation, HRE ranks these answers based on their log-likelihood under the LM's distribution, and subsequently calculates their correlation with the corresponding human rankings. We support HRE's efficacy by investigating how efficiently it separates pretrained and instruction-tuned LMs of various sizes. We show that HRE correlates well with human judgements and is particularly responsive to model changes following instruction-tuning.\n\n\nHumanRankEval: Automatic Evaluation of LMs as Conversational Assistants</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.09186v1</guid>
      <dc:creator>Milan Gritta, Gerasimos Lampouras, Ignacio Iacobacci</dc:creator>
      <pubDate>Wed, 15 May 2024 08:47:26 GMT</pubDate>
    </item>
    <item>
      <title>Integrating Physician Diagnostic Logic into Large Language Models: Preference Learning from Process Feedback</title>
      <link>http://arxiv.org/abs/2401.05695v1</link>
      <description>The use of large language models in medical dialogue generation has garnered significant attention, with a focus on improving response quality and fluency. While previous studies have made progress in optimizing model performance for single-round medical Q&amp;A tasks, there is a need to enhance the model's capability for multi-round conversations to avoid logical inconsistencies. To address this, we propose an approach called preference learning from process feedback~(PLPF), which integrates the doctor's diagnostic logic into LLMs. PLPF involves rule modeling, preference data generation, and preference alignment to train the model to adhere to the diagnostic process. Experimental results using Standardized Patient Testing show that PLPF enhances the diagnostic accuracy of the baseline model in medical conversations by 17.6%, outperforming traditional reinforcement learning from human feedback. Additionally, PLPF demonstrates effectiveness in both multi-round and single-round dialogue tasks, showcasing its potential for improving medical dialogue generation.\n\n\nIntegrating Physician Diagnostic Logic into Large Language Models: Preference Learning from Process Feedback</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2401.05695v1</guid>
      <dc:creator>Chengfeng Dou, Zhi Jin, Wenpin Jiao, Haiyan Zhao, Yongqiang Zhao, Zhenwei Tao</dc:creator>
      <pubDate>Thu, 11 Jan 2024 06:42:45 GMT</pubDate>
    </item>
    <item>
      <title>ALMol: Aligned Language-Molecule Translation LLMs through Offline Preference Contrastive Optimisation</title>
      <link>http://arxiv.org/abs/2405.08619v3</link>
      <description>The field of chemistry and Artificial Intelligence (AI) intersection is an area of active research that aims to accelerate scientific discovery. The integration of large language models (LLMs) with scientific modalities has shown significant promise in this endeavour. However, challenges persist in effectively addressing training efficacy and the out-of-distribution problem, particularly as existing approaches rely on larger models and datasets. In this context, we focus on machine language-molecule translation and deploy a novel training approach called contrastive preference optimisation, which avoids generating translations that are merely adequate but not perfect. To ensure generalisability and mitigate memorisation effects, we conduct experiments using only 10% of the data. Our results demonstrate that our models achieve up to a 32% improvement compared to counterpart models. Finally, we introduce a fine-grained, domain-agnostic evaluation method to assess hallucination in LLMs and promote responsible use.\n\n\nALMol: Aligned Language-Molecule Translation LLMs through Offline Preference Contrastive Optimisation</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.08619v3</guid>
      <dc:creator>Dimitris Gkoumas</dc:creator>
      <pubDate>Mon, 15 Jul 2024 08:53:55 GMT</pubDate>
    </item>
    <item>
      <title>Hippocrates: An Open-Source Framework for Advancing Large Language Models in Healthcare</title>
      <link>http://arxiv.org/abs/2404.16621v1</link>
      <description>The integration of Large Language Models (LLMs) into healthcare promises to transform medical diagnostics, research, and patient care. Yet, the progression of medical LLMs faces obstacles such as complex training requirements, rigorous evaluation demands, and the dominance of proprietary models that restrict academic exploration. Transparent, comprehensive access to LLM resources is essential for advancing the field, fostering reproducibility, and encouraging innovation in healthcare AI. We present Hippocrates, an open-source LLM framework specifically developed for the medical domain. In stark contrast to previous efforts, it offers unrestricted access to its training datasets, codebase, checkpoints, and evaluation protocols. This open approach is designed to stimulate collaborative research, allowing the community to build upon, refine, and rigorously evaluate medical LLMs within a transparent ecosystem. Also, we introduce Hippo, a family of 7B models tailored for the medical domain, fine-tuned from Mistral and LLaMA2 through continual pre-training, instruction tuning, and reinforcement learning from human and AI feedback. Our models outperform existing open medical LLMs models by a large-margin, even surpassing models with 70B parameters. Through Hippocrates, we aspire to unlock the full potential of LLMs not just to advance medical knowledge and patient care but also to democratize the benefits of AI research in healthcare, making them available across the globe.\n\n\nHippocrates: An Open-Source Framework for Advancing Large Language Models in Healthcare</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2404.16621v1</guid>
      <dc:creator>Emre Can Acikgoz, Osman Batur İnce, Rayene Bench, Arda Anıl Boz, İlker Kesen, Aykut Erdem, Erkut Erdem</dc:creator>
      <pubDate>Thu, 25 Apr 2024 14:06:37 GMT</pubDate>
    </item>
    <item>
      <title>Blockchain Large Language Models</title>
      <link>http://arxiv.org/abs/2304.12749v2</link>
      <description>This paper presents a dynamic, real-time approach to detecting anomalous blockchain transactions. The proposed tool, BlockGPT, generates tracing representations of blockchain activity and trains from scratch a large language model to act as a real-time Intrusion Detection System. Unlike traditional methods, BlockGPT is designed to offer an unrestricted search space and does not rely on predefined rules or patterns, enabling it to detect a broader range of anomalies. We demonstrate the effectiveness of BlockGPT through its use as an anomaly detection tool for Ethereum transactions. In our experiments, it effectively identifies abnormal transactions among a dataset of 68M transactions and has a batched throughput of 2284 transactions per second on average. Our results show that, BlockGPT identifies abnormal transactions by ranking 49 out of 124 attacks among the top-3 most abnormal transactions interacting with their victim contracts. This work makes contributions to the field of blockchain transaction analysis by introducing a custom data encoding compatible with the transformer architecture, a domain-specific tokenization technique, and a tree encoding method specifically crafted for the Ethereum Virtual Machine (EVM) trace representation.\n\n\nLarge Language Model-Informed X-ray Photoelectron Spectroscopy Data Analysis</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2304.12749v2</guid>
      <dc:creator>Yu Gai, Liyi Zhou, Kaihua Qin, Dawn Song, Arthur Gervais</dc:creator>
      <pubDate>Sat, 29 Apr 2023 16:26:40 GMT</pubDate>
    </item>
    <item>
      <title>AdaCQR: Enhancing Query Reformulation for Conversational Search via Sparse and Dense Retrieval Alignment</title>
      <link>http://arxiv.org/abs/2407.01965v1</link>
      <description>Conversational Query Reformulation (CQR) has significantly advanced in addressing the challenges of conversational search, particularly those stemming from the latent user intent and the need for historical context. Recent works aimed to boost the performance of CRQ through alignment. However, they are designed for one specific retrieval system, which potentially results in poor generalization. To overcome this limitation, we present a novel framework AdaCQR. By aligning reformulation models with both term-based and semantic-based retrieval systems, AdaCQR enhances the generalizability of information-seeking queries across diverse retrieval environments through a dual-phase training strategy. We also developed two effective approaches for acquiring superior labels and diverse input candidates, boosting the efficiency and robustness of the framework. Experimental evaluations on the TopiOCQA and QReCC datasets demonstrate that AdaCQR significantly outperforms existing methods, offering both quantitative and qualitative improvements in conversational query reformulation.\n\n\nAdaCQR: Enhancing Query Reformulation for Conversational Search via Sparse and Dense Retrieval Alignment</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2407.01965v1</guid>
      <dc:creator>Yilong Lai, Jialong Wu, Congzhi Zhang, Haowen Sun, Deyu Zhou</dc:creator>
      <pubDate>Tue, 02 Jul 2024 05:50:16 GMT</pubDate>
    </item>
    <item>
      <title>Inverse Constitutional AI: Compressing Preferences into Principles</title>
      <link>http://arxiv.org/abs/2406.06560v1</link>
      <description>Feedback data plays an important role in fine-tuning and evaluating state-of-the-art AI models. Often pairwise text preferences are used: given two texts, human (or AI) annotators select the &quot;better&quot; one. Such feedback data is widely used to align models to human preferences (e.g., reinforcement learning from human feedback), or to rank models according to human preferences (e.g., Chatbot Arena). Despite its wide-spread use, prior work has demonstrated that human-annotated pairwise text preference data often exhibits unintended biases. For example, human annotators have been shown to prefer assertive over truthful texts in certain contexts. Models trained or evaluated on this data may implicitly encode these biases in a manner hard to identify. In this paper, we formulate the interpretation of existing pairwise text preference data as a compression task: the Inverse Constitutional AI (ICAI) problem. In constitutional AI, a set of principles (or constitution) is used to provide feedback and fine-tune AI models. The ICAI problem inverts this process: given a dataset of feedback, we aim to extract a constitution that best enables a large language model (LLM) to reconstruct the original annotations. We propose a corresponding initial ICAI algorithm and validate its generated constitutions quantitatively based on reconstructed annotations. Generated constitutions have many potential use-cases -- they may help identify undesirable biases, scale feedback to unseen data or assist with adapting LLMs to individual user preferences. We demonstrate our approach on a variety of datasets: (a) synthetic feedback datasets with known underlying principles; (b) the AlpacaEval dataset of cross-annotated human feedback; and (c) the crowdsourced Chatbot Arena data set. We release the code for our algorithm and experiments at https://github.com/rdnfn/icai .\n\n\nInverse Constitutional AI: Compressing Preferences into Principles</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.06560v1</guid>
      <dc:creator>Arduin Findeis, Timo Kaufmann, Eyke Hüllermeier, Samuel Albanie, Robert Mullins</dc:creator>
      <pubDate>Sun, 02 Jun 2024 11:54:50 GMT</pubDate>
    </item>
    <item>
      <title>Quantifying and Optimizing Global Faithfulness in Persona-driven Role-playing</title>
      <link>http://arxiv.org/abs/2405.07726v1</link>
      <description>Persona-driven role-playing (PRP) aims to build AI characters that can respond to user queries by faithfully sticking with all persona statements. Unfortunately, existing faithfulness criteria for PRP are limited to coarse-grained LLM-based scoring without a clear definition or formulation. This paper presents a pioneering exploration to quantify PRP faithfulness as a fine-grained and explainable criterion, which also serves as a reliable reference for optimization. Our criterion first discriminates persona statements into active and passive constraints by identifying the query-statement relevance. Then, we incorporate all constraints following the principle that the AI character's response should be (a) entailed by active (relevant) constraints and (b) not contradicted by passive (irrelevant) constraints. We translate this principle mathematically into a novel Active-Passive-Constraint (APC) score, a constraint-wise sum of natural language inference (NLI) scores weighted by relevance scores. In practice, we build the APC scoring system by symbolically distilling small discriminators from GPT-4 for efficiency. We validate the quality of the APC score against human evaluation based on example personas with tens of statements, and the results show a high correlation. We further leverage it as a reward system in direct preference optimization (DPO) for better AI characters. Our experiments offer a fine-grained and explainable comparison between existing PRP techniques, revealing their advantages and limitations. We further find APC-based DPO to be one of the most competitive techniques for sticking with all constraints and can be well incorporated with other techniques. We then extend the scale of the experiments to real persons with hundreds of statements and reach a consistent conclusion.\n\n\nQuantifying and Optimizing Global Faithfulness in Persona-driven Role-playing</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.07726v1</guid>
      <dc:creator>Letian Peng, Jingbo Shang</dc:creator>
      <pubDate>Mon, 13 May 2024 13:21:35 GMT</pubDate>
    </item>
    <item>
      <title>LLM experiments with simulation: Large Language Model Multi-Agent System for Process Simulation Parametrization in Digital Twins</title>
      <link>http://arxiv.org/abs/2405.18092v1</link>
      <description>This paper presents a novel design of a multi-agent system framework that applies a large language model (LLM) to automate the parametrization of process simulations in digital twins. We propose a multi-agent framework that includes four types of agents: observation, reasoning, decision and summarization. By enabling dynamic interaction between LLM agents and simulation model, the developed system can automatically explore the parametrization of the simulation and use heuristic reasoning to determine a set of parameters to control the simulation to achieve an objective. The proposed approach enhances the simulation model by infusing it with heuristics from LLM and enables autonomous search for feasible parametrization to solve a user task. Furthermore, the system has the potential to increase user-friendliness and reduce the cognitive load on human users by assisting in complex decision-making processes. The effectiveness and functionality of the system are demonstrated through a case study, and the visualized demos are available at a GitHub Repository: https://github.com/YuchenXia/LLMDrivenSimulation\n\n\nLLM experiments with simulation: Large Language Model Multi-Agent System for Process Simulation Parametrization in Digital Twins</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.18092v1</guid>
      <dc:creator>Yuchen Xia, Daniel Dittler, Nasser Jazdi, Haonan Chen, Michael Weyrich</dc:creator>
      <pubDate>Tue, 28 May 2024 11:59:40 GMT</pubDate>
    </item>
    <item>
      <title>Aligning Large Language Models with Counterfactual DPO</title>
      <link>http://arxiv.org/abs/2401.09566v2</link>
      <description>Advancements in large language models (LLMs) have demonstrated remarkable capabilities across a diverse range of applications. These models excel in generating text completions that are contextually coherent and cover an extensive array of subjects. However, the vast datasets required for their training make aligning response styles during the pretraining and instruction tuning phases challenging. Consequently, an additional alignment phase is typically employed, wherein the model is further trained with human preference data to better align its outputs with human expectations. While this process doesn't introduce new capabilities per se, it does accentuate generation styles innate to the model. This paper explores the utilization of counterfactual prompting within the framework of Direct Preference Optimization (DPO) to align the model's style without relying on human intervention. We demonstrate that this method effectively instils desirable behaviour, mitigates undesirable ones, and encourages the model to disregard inappropriate instructions. Our findings suggest that counterfactual prompting with DPO presents a low-resource way to fine-tune LLMs to meet the demands for responsible and ethically aligned AI systems.\n\n\nAligning Large Language Models with Counterfactual DPO</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2401.09566v2</guid>
      <dc:creator>Bradley Butcher</dc:creator>
      <pubDate>Fri, 19 Jan 2024 08:57:19 GMT</pubDate>
    </item>
    <item>
      <title>How Many Parameters Does it Take to Change a Light Bulb? Evaluating Performance in Self-Play of Conversational Games as a Function of Model Characteristics</title>
      <link>http://arxiv.org/abs/2406.14051v1</link>
      <description>What makes a good Large Language Model (LLM)? That it performs well on the relevant benchmarks -- which hopefully measure, with some validity, the presence of capabilities that are also challenged in real application. But what makes the model perform well? What gives a model its abilities? We take a recently introduced type of benchmark that is meant to challenge capabilities in a goal-directed, agentive context through self-play of conversational games, and analyse how performance develops as a function of model characteristics like number of parameters, or type of training. We find that while there is a clear relationship between number of parameters and performance, there is still a wide spread of performance points within a given size bracket, which is to be accounted for by training parameters such as fine-tuning data quality and method. From a more practical angle, we also find a certain degree of unpredictability about performance across access methods, possible due to unexposed sampling parameters, and a, very welcome, performance stability against at least moderate weight quantisation during inference.\n\n\nHow Many Parameters Does it Take to Change a Light Bulb? Evaluating Performance in Self-Play of Conversational Games as a Function of Model Characteristics</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.14051v1</guid>
      <dc:creator>Nidhir Bhavsar, Jonathan Jordan, Sherzod Hakimov, David Schlangen</dc:creator>
      <pubDate>Thu, 20 Jun 2024 07:17:09 GMT</pubDate>
    </item>
    <item>
      <title>RAGSys: Item-Cold-Start Recommender as RAG System</title>
      <link>http://arxiv.org/abs/2405.17587v1</link>
      <description>Large Language Models (LLM) hold immense promise for real-world applications, but their generic knowledge often falls short of domain-specific needs. Fine-tuning, a common approach, can suffer from catastrophic forgetting and hinder generalizability. In-Context Learning (ICL) offers an alternative, which can leverage Retrieval-Augmented Generation (RAG) to provide LLMs with relevant demonstrations for few-shot learning tasks. This paper explores the desired qualities of a demonstration retrieval system for ICL. We argue that ICL retrieval in this context resembles item-cold-start recommender systems, prioritizing discovery and maximizing information gain over strict relevance. We propose a novel evaluation method that measures the LLM's subsequent performance on NLP tasks, eliminating the need for subjective diversity scores. Our findings demonstrate the critical role of diversity and quality bias in retrieved demonstrations for effective ICL, and highlight the potential of recommender system techniques in this domain.\n\n\nRAGSys: Item-Cold-Start Recommender as RAG System</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.17587v1</guid>
      <dc:creator>Emile Contal, Garrin McGoldrick</dc:creator>
      <pubDate>Mon, 27 May 2024 18:40:49 GMT</pubDate>
    </item>
    <item>
      <title>WShEx: A language to describe and validate Wikibase entities</title>
      <link>http://arxiv.org/abs/2208.02697v1</link>
      <description>Wikidata is one of the most successful Semantic Web projects. Its underlying Wikibase data model departs from RDF with the inclusion of several features like qualifiers and references, built-in datatypes, etc. Those features are serialized to RDF for content negotiation, RDF dumps and in the SPARQL endpoint. Wikidata adopted the entity schemas namespace using the ShEx language to describe and validate the RDF serialization of Wikidata entities. In this paper we propose WShEx, a language inspired by ShEx that directly supports the Wikibase data model and can be used to describe and validate Wikibase entities. The paper presents a the abstract syntax and semantic of the WShEx language.\n\n\nWiki-based Prompts for Enhancing Relation Extraction using Language Models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2208.02697v1</guid>
      <dc:creator>Jose Emilio Labra Gayo</dc:creator>
      <pubDate>Thu, 04 Aug 2022 14:51:35 GMT</pubDate>
    </item>
    <item>
      <title>TUTORING: Instruction-Grounded Conversational Agent for Language Learners</title>
      <link>http://arxiv.org/abs/2302.12623v1</link>
      <description>In this paper, we propose Tutoring bot, a generative chatbot trained on a large scale of tutor-student conversations for English-language learning. To mimic a human tutor's behavior in language education, the tutor bot leverages diverse educational instructions and grounds to each instruction as additional input context for the tutor response generation. As a single instruction generally involves multiple dialogue turns to give the student sufficient speaking practice, the tutor bot is required to monitor and capture when the current instruction should be kept or switched to the next instruction. For that, the tutor bot is learned to not only generate responses but also infer its teaching action and progress on the current conversation simultaneously by a multi-task learning scheme. Our Tutoring bot is deployed under a non-commercial use license at https://tutoringai.com.\n\n\nDeveloping Conversational Intelligent Tutoring for Speaking Skills in Second Language Learning</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2302.12623v1</guid>
      <dc:creator>Hyungjoo Chae, Minjin Kim, Chaehyeong Kim, Wonseok Jeong, Hyejoong Kim, Junmyung Lee, Jinyoung Yeo</dc:creator>
      <pubDate>Fri, 24 Feb 2023 13:36:11 GMT</pubDate>
    </item>
    <item>
      <title>SmurfCat at PAN 2024 TextDetox: Alignment of Multilingual Transformers for Text Detoxification</title>
      <link>http://arxiv.org/abs/2407.05449v2</link>
      <description>This paper presents a solution for the Multilingual Text Detoxification task in the PAN-2024 competition of the SmurfCat team. Using data augmentation through machine translation and a special filtering procedure, we collected an additional multilingual parallel dataset for text detoxification. Using the obtained data, we fine-tuned several multilingual sequence-to-sequence models, such as mT0 and Aya, on a text detoxification task. We applied the ORPO alignment technique to the final model. Our final model has only 3.7 billion parameters and achieves state-of-the-art results for the Ukrainian language and near state-of-the-art results for other languages. In the competition, our team achieved first place in the automated evaluation with a score of 0.52 and second place in the final human evaluation with a score of 0.74.\n\n\nSmurfCat at PAN 2024 TextDetox: Alignment of Multilingual Transformers for Text Detoxification</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2407.05449v2</guid>
      <dc:creator>Elisei Rykov, Konstantin Zaytsev, Ivan Anisimov, Alexandr Voronin</dc:creator>
      <pubDate>Wed, 10 Jul 2024 14:44:18 GMT</pubDate>
    </item>
    <item>
      <title>Institutional Platform for Secure Self-Service Large Language Model Exploration</title>
      <link>http://arxiv.org/abs/2402.00913v1</link>
      <description>This paper introduces a user-friendly platform developed by the University of Kentucky Center for Applied AI, designed to make large, customized language models (LLMs) more accessible. By capitalizing on recent advancements in multi-LoRA inference, the system efficiently accommodates custom adapters for a diverse range of users and projects. The paper outlines the system's architecture and key features, encompassing dataset curation, model training, secure inference, and text-based feature extraction.   We illustrate the establishment of a tenant-aware computational network using agent-based methods, securely utilizing islands of isolated resources as a unified system. The platform strives to deliver secure LLM services, emphasizing process and data isolation, end-to-end encryption, and role-based resource authentication. This contribution aligns with the overarching goal of enabling simplified access to cutting-edge AI models and technology in support of scientific discovery.\n\n\nInstitutional Platform for Secure Self-Service Large Language Model Exploration</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.00913v1</guid>
      <dc:creator>V. K. Cody Bumgardner, Mitchell A. Klusty, W. Vaiden Logan, Samuel E. Armstrong, Caylin Hickey, Jeff Talbert</dc:creator>
      <pubDate>Thu, 01 Feb 2024 10:58:10 GMT</pubDate>
    </item>
    <item>
      <title>Clustering and Ranking: Diversity-preserved Instruction Selection through Expert-aligned Quality Estimation</title>
      <link>http://arxiv.org/abs/2402.18191v1</link>
      <description>With contributions from the open-source community, a vast amount of instruction tuning (IT) data has emerged. Given the significant resource allocation required by training and evaluating models, it is advantageous to have an efficient method for selecting high-quality IT data. However, existing methods for instruction data selection have limitations such as relying on fragile external APIs, being affected by biases in GPT models, or reducing the diversity of the selected instruction dataset. In this paper, we propose an industrial-friendly, expert-aligned and diversity-preserved instruction data selection method: Clustering and Ranking (CaR). CaR consists of two steps. The first step involves ranking instruction pairs using a scoring model that is well aligned with expert preferences (achieving an accuracy of 84.25%). The second step involves preserving dataset diversity through a clustering process.In our experiment, CaR selected a subset containing only 1.96% of Alpaca's IT data, yet the underlying AlpaCaR model trained on this subset outperforms Alpaca by an average of 32.1% in GPT-4 evaluations. Furthermore, our method utilizes small models (355M parameters) and requires only 11.2% of the monetary cost compared to existing methods, making it easily deployable in industrial scenarios.\n\n\nClustering and Ranking: Diversity-preserved Instruction Selection through Expert-aligned Quality Estimation</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.18191v1</guid>
      <dc:creator>Yuan Ge, Yilun Liu, Chi Hu, Weibin Meng, Shimin Tao, Xiaofeng Zhao, Hongxia Ma, Li Zhang, Hao Yang, Tong Xiao</dc:creator>
      <pubDate>Wed, 28 Feb 2024 09:27:29 GMT</pubDate>
    </item>
    <item>
      <title>Future of work: ethics</title>
      <link>http://arxiv.org/abs/2104.02580v1</link>
      <description>Work must be reshaped in the upcoming new era characterized by new challenges and the presence of new technologies and computational tools. Over-automation seems to be the driver of the digitalization process. Substitution is the paradigm leading Artificial Intelligence and robotics development against human cognition. Digital technology should be designed to enhance human skills and make more productive use of human cognition and capacities. Digital technology is characterized also by scalability because of its easy and inexpensive deployment. Thus, automation can lead to the absence of jobs and scalable negative impact in human development and the performance of business. A look at digitalization from the lens of Sustainable Development Goals can tell us how digitalization impact in different sectors and areas considering society as a complex interconnected system. Here, reflections on how AI and Data impact future of work and sustainable development are provided grounded on an ethical core that comprises human-level principles and also systemic principles.\n\n\nFuture of Evidence Synthesis: Automated, Living, and Interactive Systematic Reviews and Meta-Analyses</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2104.02580v1</guid>
      <dc:creator>David Pastor-Escuredo</dc:creator>
      <pubDate>Tue, 06 Apr 2021 15:20:30 GMT</pubDate>
    </item>
    <item>
      <title>Plan-Grounded Large Language Models for Dual Goal Conversational Settings</title>
      <link>http://arxiv.org/abs/2402.01053v1</link>
      <description>Training Large Language Models (LLMs) to follow user instructions has been shown to supply the LLM with ample capacity to converse fluently while being aligned with humans. Yet, it is not completely clear how an LLM can lead a plan-grounded conversation in mixed-initiative settings where instructions flow in both directions of the conversation, i.e. both the LLM and the user provide instructions to one another. In this paper, we tackle a dual goal mixed-initiative conversational setting where the LLM not only grounds the conversation on an arbitrary plan but also seeks to satisfy both a procedural plan and user instructions. The LLM is then responsible for guiding the user through the plan and, at the same time, adapting to new circumstances, answering questions, and activating safety guardrails when needed. We propose a novel LLM that grounds the dialogue on a procedural plan, can take the dialogue initiative, and enforces guardrails on the system's behavior, while also improving the LLM's responses to unexpected user behavior. Experiments in controlled settings and with real users show that the best-performing model, which we call PlanLLM, achieves a 2.1x improvement over a strong baseline. Moreover, experiments also show good generalization to unseen domains.\n\n\nPlan-Grounded Large Language Models for Dual Goal Conversational Settings</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.01053v1</guid>
      <dc:creator>Diogo Glória-Silva, Rafael Ferreira, Diogo Tavares, David Semedo, João Magalhães</dc:creator>
      <pubDate>Thu, 01 Feb 2024 22:56:39 GMT</pubDate>
    </item>
    <item>
      <title>Improving Neural Topic Models using Knowledge Distillation</title>
      <link>http://arxiv.org/abs/2010.02377v1</link>
      <description>Topic models are often used to identify human-interpretable topics to help make sense of large document collections. We use knowledge distillation to combine the best attributes of probabilistic topic models and pretrained transformers. Our modular method can be straightforwardly applied with any neural topic model to improve topic quality, which we demonstrate using two models having disparate architectures, obtaining state-of-the-art topic coherence. We show that our adaptable framework not only improves performance in the aggregate over all estimated topics, as is commonly reported, but also in head-to-head comparisons of aligned topics.\n\n\nInternet-scale topic modeling using large language models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2010.02377v1</guid>
      <dc:creator>Alexander Hoyle, Pranav Goel, Philip Resnik</dc:creator>
      <pubDate>Mon, 05 Oct 2020 22:49:16 GMT</pubDate>
    </item>
    <item>
      <title>MindLLM: Pre-training Lightweight Large Language Model from Scratch, Evaluations and Domain Applications</title>
      <link>http://arxiv.org/abs/2310.15777v2</link>
      <description>Large Language Models (LLMs) have demonstrated remarkable performance across various natural language tasks, marking significant strides towards general artificial intelligence. While general artificial intelligence is leveraged by developing increasingly large-scale models, there could be another branch to develop lightweight custom models that better serve certain domains, taking into account the high cost of training and deploying LLMs and the scarcity of resources. In this paper, we present MindLLM, a novel series of bilingual lightweight large language models, trained from scratch, alleviating such burdens by offering models with 1.3 billion and 3 billion parameters. A thorough account of experiences accrued during large model development is given, covering every step of the process, including data construction, model architecture, evaluation, and applications. Such insights are hopefully valuable for fellow academics and developers. MindLLM consistently matches or surpasses the performance of other open-source larger models on some public benchmarks. We also introduce an innovative instruction tuning framework tailored for smaller models to enhance their capabilities efficiently. Moreover, we explore the application of MindLLM in specific vertical domains such as law and finance, underscoring the agility and adaptability of our lightweight models.\n\n\nGEB-1.3 B: Open Lightweight Large Language Model</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2310.15777v2</guid>
      <dc:creator>Yizhe Yang, Huashan Sun, Jiawei Li, Runheng Liu, Yinghao Li, Yuhang Liu, Heyan Huang, Yang Gao</dc:creator>
      <pubDate>Sun, 29 Oct 2023 01:17:53 GMT</pubDate>
    </item>
    <item>
      <title>Introducing cosmosGPT: Monolingual Training for Turkish Language Models</title>
      <link>http://arxiv.org/abs/2404.17336v1</link>
      <description>The number of open source language models that can produce Turkish is increasing day by day, as in other languages. In order to create the basic versions of such models, the training of multilingual models is usually continued with Turkish corpora. The alternative is to train the model with only Turkish corpora. In this study, we first introduce the cosmosGPT models that we created with this alternative method. Then, we introduce new finetune datasets for basic language models to fulfill user requests and new evaluation datasets for measuring the capabilities of Turkish language models. Finally, a comprehensive comparison of the adapted Turkish language models on different capabilities is presented. The results show that the language models we built with the monolingual corpus have promising performance despite being about 10 times smaller than the others.\n\n\nIntroducing cosmosGPT: Monolingual Training for Turkish Language Models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2404.17336v1</guid>
      <dc:creator>H. Toprak Kesgin, M. Kaan Yuce, Eren Dogan, M. Egemen Uzun, Atahan Uz, H. Emre Seyrek, Ahmed Zeer, M. Fatih Amasyali</dc:creator>
      <pubDate>Fri, 26 Apr 2024 11:34:11 GMT</pubDate>
    </item>
    <item>
      <title>Cumulative Reports of the SoNDe Project July 2017</title>
      <link>http://arxiv.org/abs/1707.08679v1</link>
      <description>This are the cumulated reports of the SoNDe detector Project as of July 2017. The contained reports are: - Report on the 1x1 module technical demonstrator - Report on used materials - Report on radiation hardness of components - Report on potential additional applications - Report on the 2x2 module technical demonstrator - Report on test results of the 2x2 technical demonstrator\n\n\nTele-FLM Technical Report</description>
      <guid isPermaLink="false">http://arxiv.org/abs/1707.08679v1</guid>
      <dc:creator>Sebastian Jaksch, Ralf Engels, Günter Kemmerling, Codin Gheorghe, Philip Pahlsson, Sylvain Désert, Frederic Ott</dc:creator>
      <pubDate>Thu, 27 Jul 2017 01:34:45 GMT</pubDate>
    </item>
    <item>
      <title>From Instructions to Constraints: Language Model Alignment with Automatic Constraint Verification</title>
      <link>http://arxiv.org/abs/2403.06326v1</link>
      <description>User alignment is crucial for adapting general-purpose language models (LMs) to downstream tasks, but human annotations are often not available for all types of instructions, especially those with customized constraints. We observe that user instructions typically contain constraints. While assessing response quality in terms of the whole instruction is often costly, efficiently evaluating the satisfaction rate of constraints is feasible. We investigate common constraints in NLP tasks, categorize them into three classes based on the types of their arguments, and propose a unified framework, ACT (Aligning to ConsTraints), to automatically produce supervision signals for user alignment with constraints. Specifically, ACT uses constraint verifiers, which are typically easy to implement in practice, to compute constraint satisfaction rate (CSR) of each response. It samples multiple responses for each prompt and collect preference labels based on their CSR automatically. Subsequently, ACT adapts the LM to the target task through a ranking-based learning process. Experiments on fine-grained entity typing, abstractive summarization, and temporal question answering show that ACT is able to enhance LMs' capability to adhere to different classes of constraints, thereby improving task performance. Further experiments show that the constraint-following capabilities are transferable.\n\n\nFrom Instructions to Constraints: Language Model Alignment with Automatic Constraint Verification</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2403.06326v1</guid>
      <dc:creator>Fei Wang, Chao Shang, Sarthak Jain, Shuai Wang, Qiang Ning, Bonan Min, Vittorio Castelli, Yassine Benajiba, Dan Roth</dc:creator>
      <pubDate>Sun, 10 Mar 2024 22:14:54 GMT</pubDate>
    </item>
    <item>
      <title>MULTIVERSE: Exposing Large Language Model Alignment Problems in Diverse Worlds</title>
      <link>http://arxiv.org/abs/2402.01706v1</link>
      <description>Large Language Model (LLM) alignment aims to ensure that LLM outputs match with human values. Researchers have demonstrated the severity of alignment problems with a large spectrum of jailbreak techniques that can induce LLMs to produce malicious content during conversations. Finding the corresponding jailbreaking prompts usually requires substantial human intelligence or computation resources. In this paper, we report that LLMs have different levels of alignment in various contexts. As such, by systematically constructing many contexts, called worlds, leveraging a Domain Specific Language describing possible worlds (e.g., time, location, characters, actions and languages) and the corresponding compiler, we can cost-effectively expose latent alignment issues. Given the low cost of our method, we are able to conduct a large scale study regarding LLM alignment issues in different worlds. Our results show that our method outperforms the-state-of-the-art jailbreaking techniques on both effectiveness and efficiency. In addition, our results indicate that existing LLMs are extremely vulnerable to nesting worlds and programming language worlds. They imply that existing alignment training focuses on the real-world and is lacking in various (virtual) worlds where LLMs can be exploited.\n\n\nMULTIVERSE: Exposing Large Language Model Alignment Problems in Diverse Worlds</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.01706v1</guid>
      <dc:creator>Xiaolong Jin, Zhuo Zhang, Xiangyu Zhang</dc:creator>
      <pubDate>Thu, 25 Jan 2024 02:57:40 GMT</pubDate>
    </item>
    <item>
      <title>THaLLE: Text Hyperlocally Augmented Large Language Extension -- Technical Report</title>
      <link>http://arxiv.org/abs/2406.07505v1</link>
      <description>Recent advancements in Large Language Models (LLMs) have revealed new capabilities and opportunities across the technological landscape. However, the practicality of very large LLMs is challenged by their high compute cost, which does not justify the benefits given their limited capability compared to humans. While smaller, more practical LLMs have shown potential in financial analysis, though they are not yet fully proficient, as evidenced by their near-passing performance on the Chartered Financial Analyst (CFA) exam. In this work, we present Financial Analyst Extension to our Text Hyperlocally Augmented Large Language Extension (THaLLE), a series of 8B LLMs consistently achieving highest performance on mock CFA exams against models of comparable size. We thoroughly document the fine-tuning techniques used to facilitate future research. Additionally, we introduce the use of Flare CFA, a publicly available dataset for evaluating LLMs as a financial advisor.\n\n\nTHaLLE: Text Hyperlocally Augmented Large Language Extension--Technical Report</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.07505v1</guid>
      <dc:creator>KBTG Labs, Danupat Khamnuansin, Atthakorn Petchsod, Anuruth Lertpiya, Pornchanan Balee, Thanawat Lodkaew, Tawunrat Chalothorn, Thadpong Pongthawornkamol, Monchai Lertsutthiwong</dc:creator>
      <pubDate>Tue, 11 Jun 2024 17:40:00 GMT</pubDate>
    </item>
    <item>
      <title>Self-Supervised Noisy Label Learning for Source-Free Unsupervised Domain Adaptation</title>
      <link>http://arxiv.org/abs/2102.11614v1</link>
      <description>It is a strong prerequisite to access source data freely in many existing unsupervised domain adaptation approaches. However, source data is agnostic in many practical scenarios due to the constraints of expensive data transmission and data privacy protection. Usually, the given source domain pre-trained model is expected to optimize with only unlabeled target data, which is termed as source-free unsupervised domain adaptation. In this paper, we solve this problem from the perspective of noisy label learning, since the given pre-trained model can pre-generate noisy label for unlabeled target data via directly network inference. Under this problem modeling, incorporating self-supervised learning, we propose a novel Self-Supervised Noisy Label Learning method, which can effectively fine-tune the pre-trained model with pre-generated label as well as selfgenerated label on the fly. Extensive experiments had been conducted to validate its effectiveness. Our method can easily achieve state-of-the-art results and surpass other methods by a very large margin. Code will be released.\n\n\nCode-Optimise: Self-Generated Preference Data for Correctness and Efficiency</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2102.11614v1</guid>
      <dc:creator>Weijie Chen, Luojun Lin, Shicai Yang, Di Xie, Shiliang Pu, Yueting Zhuang, Wenqi Ren</dc:creator>
      <pubDate>Tue, 23 Feb 2021 10:51:45 GMT</pubDate>
    </item>
    <item>
      <title>InstructProtein: Aligning Human and Protein Language via Knowledge Instruction</title>
      <link>http://arxiv.org/abs/2310.03269v1</link>
      <description>Large Language Models (LLMs) have revolutionized the field of natural language processing, but they fall short in comprehending biological sequences such as proteins. To address this challenge, we propose InstructProtein, an innovative LLM that possesses bidirectional generation capabilities in both human and protein languages: (i) taking a protein sequence as input to predict its textual function description and (ii) using natural language to prompt protein sequence generation. To achieve this, we first pre-train an LLM on both protein and natural language corpora, enabling it to comprehend individual languages. Then supervised instruction tuning is employed to facilitate the alignment of these two distinct languages. Herein, we introduce a knowledge graph-based instruction generation framework to construct a high-quality instruction dataset, addressing annotation imbalance and instruction deficits in existing protein-text corpus. In particular, the instructions inherit the structural relations between proteins and function annotations in knowledge graphs, which empowers our model to engage in the causal modeling of protein functions, akin to the chain-of-thought processes in natural languages. Extensive experiments on bidirectional protein-text generation tasks show that InstructProtein outperforms state-of-the-art LLMs by large margins. Moreover, InstructProtein serves as a pioneering step towards text-based protein function prediction and sequence design, effectively bridging the gap between protein and human language understanding.\n\n\nInstructPLM: Aligning Protein Language Models to Follow Protein Structure Instructions</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2310.03269v1</guid>
      <dc:creator>Zeyuan Wang, Qiang Zhang, Keyan Ding, Ming Qin, Xiang Zhuang, Xiaotong Li, Huajun Chen</dc:creator>
      <pubDate>Thu, 05 Oct 2023 02:45:39 GMT</pubDate>
    </item>
    <item>
      <title>Dynamic Normativity: Necessary and Sufficient Conditions for Value Alignment</title>
      <link>http://arxiv.org/abs/2406.11039v2</link>
      <description>The critical inquiry pervading the realm of Philosophy, and perhaps extending its influence across all Humanities disciplines, revolves around the intricacies of morality and normativity. Surprisingly, in recent years, this thematic thread has woven its way into an unexpected domain, one not conventionally associated with pondering &quot;what ought to be&quot;: the field of artificial intelligence (AI) research. Central to morality and AI, we find &quot;alignment&quot;, a problem related to the challenges of expressing human goals and values in a manner that artificial systems can follow without leading to unwanted adversarial effects. More explicitly and with our current paradigm of AI development in mind, we can think of alignment as teaching human values to non-anthropomorphic entities trained through opaque, gradient-based learning techniques. This work addresses alignment as a technical-philosophical problem that requires solid philosophical foundations and practical implementations that bring normative theory to AI system development. To accomplish this, we propose two sets of necessary and sufficient conditions that, we argue, should be considered in any alignment process. While necessary conditions serve as metaphysical and metaethical roots that pertain to the permissibility of alignment, sufficient conditions establish a blueprint for aligning AI systems under a learning-based paradigm. After laying such foundations, we present implementations of this approach by using state-of-the-art techniques and methods for aligning general-purpose language systems. We call this framework Dynamic Normativity. Its central thesis is that any alignment process under a learning paradigm that cannot fulfill its necessary and sufficient conditions will fail in producing aligned systems.\n\n\nDynamic Normativity: Necessary and Sufficient Conditions for Value Alignment</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.11039v2</guid>
      <dc:creator>Nicholas Kluge Corrêa</dc:creator>
      <pubDate>Tue, 18 Jun 2024 12:15:06 GMT</pubDate>
    </item>
    <item>
      <title>Purple-teaming LLMs with Adversarial Defender Training</title>
      <link>http://arxiv.org/abs/2407.01850v1</link>
      <description>Existing efforts in safeguarding LLMs are limited in actively exposing the vulnerabilities of the target LLM and readily adapting to newly emerging safety risks. To address this, we present Purple-teaming LLMs with Adversarial Defender training (PAD), a pipeline designed to safeguard LLMs by novelly incorporating the red-teaming (attack) and blue-teaming (safety training) techniques. In PAD, we automatically collect conversational data that cover the vulnerabilities of an LLM around specific safety risks in a self-play manner, where the attacker aims to elicit unsafe responses and the defender generates safe responses to these attacks. We then update both modules in a generative adversarial network style by training the attacker to elicit more unsafe responses and updating the defender to identify them and explain the unsafe reason. Experimental results demonstrate that PAD significantly outperforms existing baselines in both finding effective attacks and establishing a robust safe guardrail. Furthermore, our findings indicate that PAD excels in striking a balance between safety and overall model quality. We also reveal key challenges in safeguarding LLMs, including defending multi-turn attacks and the need for more delicate strategies to identify specific risks.\n\n\nTraining Human-AI Teams</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2407.01850v1</guid>
      <dc:creator>Jingyan Zhou, Kun Li, Junan Li, Jiawen Kang, Minda Hu, Xixin Wu, Helen Meng</dc:creator>
      <pubDate>Mon, 01 Jul 2024 23:25:30 GMT</pubDate>
    </item>
    <item>
      <title>Deep Online Fused Video Stabilization</title>
      <link>http://arxiv.org/abs/2102.01279v2</link>
      <description>We present a deep neural network (DNN) that uses both sensor data (gyroscope) and image content (optical flow) to stabilize videos through unsupervised learning. The network fuses optical flow with real/virtual camera pose histories into a joint motion representation. Next, the LSTM block infers the new virtual camera pose, and this virtual pose is used to generate a warping grid that stabilizes the frame. Novel relative motion representation as well as a multi-stage training process are presented to optimize our model without any supervision. To the best of our knowledge, this is the first DNN solution that adopts both sensor data and image for stabilization. We validate the proposed framework through ablation studies and demonstrated the proposed method outperforms the state-of-art alternative solutions via quantitative evaluations and a user study.\n\n\nOnline Joint Fine-tuning of Multi-Agent Flows</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2102.01279v2</guid>
      <dc:creator>Zhenmei Shi, Fuhao Shi, Wei-Sheng Lai, Chia-Kai Liang, Yingyu Liang</dc:creator>
      <pubDate>Sun, 04 Apr 2021 01:59:44 GMT</pubDate>
    </item>
    <item>
      <title>Proxy-RLHF: Decoupling Generation and Alignment in Large Language Model with Proxy</title>
      <link>http://arxiv.org/abs/2403.04283v1</link>
      <description>Reinforcement Learning from Human Feedback (RLHF) is the prevailing approach to ensure Large Language Models (LLMs) align with human values. However, existing RLHF methods require a high computational cost, one main reason being that RLHF assigns both the generation and alignment tasks to the LLM simultaneously. In this paper, we introduce Proxy-RLHF, which decouples the generation and alignment processes of LLMs, achieving alignment with human values at a much lower computational cost. We start with a novel Markov Decision Process (MDP) designed for the alignment process and employ Reinforcement Learning (RL) to train a streamlined proxy model that oversees the token generation of the LLM, without altering the LLM itself. Experiments show that our method achieves a comparable level of alignment with only 1\% of the training parameters of other methods.\n\n\nProxy-RLHF: Decoupling Generation and Alignment in Large Language Model with Proxy</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2403.04283v1</guid>
      <dc:creator>Yu Zhu, Chuxiong Sun, Wenfei Yang, Wenqiang Wei, Bo Tang, Tianzhu Zhang, Zhiyu Li, Shifeng Zhang, Feiyu Xiong, Jie Hu, Mingchuan yang</dc:creator>
      <pubDate>Thu, 07 Mar 2024 07:31:00 GMT</pubDate>
    </item>
    <item>
      <title>Configurable Safety Tuning of Language Models with Synthetic Preference Data</title>
      <link>http://arxiv.org/abs/2404.00495v1</link>
      <description>State-of-the-art language model fine-tuning techniques, such as Direct Preference Optimization (DPO), restrict user control by hard-coding predefined behaviors into the model. To address this, we propose a novel method, Configurable Safety Tuning (CST), that augments DPO using synthetic preference data to facilitate flexible safety configuration of LLMs at inference time. CST overcomes the constraints of vanilla DPO by introducing a system prompt specifying safety configurations, enabling LLM deployers to disable/enable safety preferences based on their need, just changing the system prompt. Our experimental evaluations indicate that CST successfully manages different safety configurations and retains the original functionality of LLMs, showing it is a robust method for configurable deployment. Data and models available at https://github.com/vicgalle/configurable-safety-tuning\n\n\nConfigurable Safety Tuning of Language Models with Synthetic Preference Data</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2404.00495v1</guid>
      <dc:creator>Victor Gallego</dc:creator>
      <pubDate>Sat, 30 Mar 2024 23:28:05 GMT</pubDate>
    </item>
    <item>
      <title>I Learn Better If You Speak My Language: Understanding the Superior Performance of Fine-Tuning Large Language Models with LLM-Generated Responses</title>
      <link>http://arxiv.org/abs/2402.11192v2</link>
      <description>This paper explores an intriguing observation: fine-tuning a large language model (LLM) with responses generated by a LLM often yields better results than using responses generated by humans. We conduct an in-depth investigation to understand why this occurs. Contrary to the common belief that these instances is simply due to the more detailed nature of LLM-generated content, our study identifies another contributing factor: an LLM is inherently more &quot;familiar&quot; with LLM generated responses. This familiarity is evidenced by lower perplexity before fine-tuning. We design a series of experiments to understand the impact of the &quot;familiarity&quot; and our conclusion reveals that this &quot;familiarity&quot; significantly impacts learning performance. Training with LLM-generated responses not only enhances performance but also helps maintain the model's capabilities in other tasks after fine-tuning on a specific task.\n\n\nI Learn Better If You Speak My Language: Enhancing Large Language Model Fine-Tuning with Style-Aligned Response Adjustments</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.11192v2</guid>
      <dc:creator>Xuan Ren, Biao Wu, Lingqiao Liu</dc:creator>
      <pubDate>Sat, 01 Jun 2024 03:36:23 GMT</pubDate>
    </item>
    <item>
      <title>Environmental Averaging</title>
      <link>http://arxiv.org/abs/2211.00117v4</link>
      <description>Many classical examples of models of self-organized dynamics, including the Cucker-Smale, Motsch-Tadmor, multi-species, and several others, include an alignment force that is based upon density-weighted averaging protocol. Those protocols can be viewed as special cases of `environmental averaging'. In this paper we formalize this concept and introduce a unified framework for systematic analysis of alignment models.   A series of studies are presented including the mean-field limit in deterministic and stochastic settings, hydrodynamic limits in the monokinetic and Maxwellian regimes, hypocoercivity and global relaxation for dissipative kinetic models, several general alignment results based on chain connectivity and spectral gap analysis. These studies cover many of the known results and reveal new ones, which include asymptotic alignment criteria based on connectivity conditions, new estimates on the spectral gap of the alignment force that do not rely on the upper bound of the macroscopic density, uniform gain of positivity for solutions of the Fokker-Planck-Alignment model based on smooth environmental averaging. As a consequence, we establish unconditional relaxation result for global solutions to the Fokker-Planck-Alignment model, which presents a substantial improvement over previously known perturbative results.\n\n\nAveraging log-likelihoods in direct alignment</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2211.00117v4</guid>
      <dc:creator>Roman Shvydkoy</dc:creator>
      <pubDate>Tue, 12 Mar 2024 18:28:52 GMT</pubDate>
    </item>
    <item>
      <title>Language Alignment via Nash-learning and Adaptive feedback</title>
      <link>http://arxiv.org/abs/2406.15890v1</link>
      <description>Recent research has shown the potential of Nash Learning via Human Feedback for large language model alignment by incorporating the notion of a preference model in a minimax game setup. We take this idea further by casting the alignment as a mirror descent algorithm against the adaptive feedback of an improved opponent, thereby removing the need for learning a preference model or the existence of an annotated dataset altogether. The resulting algorithm, which we refer to as Language Alignment via Nash-learning and Adaptive feedback (LANA), is capable of self-alignment without the need for a human-annotated preference dataset. We support this statement with various experiments and mathematical discussion.\n\n\nLanguage Alignment via Nash-learning and Adaptive feedback</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.15890v1</guid>
      <dc:creator>Ari Azarafrooz, Farshid Faal</dc:creator>
      <pubDate>Sat, 22 Jun 2024 16:55:21 GMT</pubDate>
    </item>
    <item>
      <title>OpenBezoar: Small, Cost-Effective and Open Models Trained on Mixes of Instruction Data</title>
      <link>http://arxiv.org/abs/2404.12195v1</link>
      <description>Instruction fine-tuning pretrained LLMs for diverse downstream tasks has demonstrated remarkable success and has captured the interest of both academics and practitioners. To ensure such fine-tuned LLMs align with human preferences, techniques such as RLHF and DPO have emerged. At the same time, there is increasing interest in smaller parameter counts for models. In this work, using OpenLLaMA 3Bv2 as a base model, we describe the recipe used to fine-tune the OpenBezoar family of models. In this recipe: We first generate synthetic instruction fine-tuning data using an open and commercially non-restrictive instruction fine-tuned variant of the Falcon-40B model under three schemes based on: LaMini-LM, WizardLM/Evol-Instruct (with databricks-dolly-15k as a seed dataset) and Orca (with the Flan Collection as a seed dataset), then filter these generations using GPT-4 as a human proxy. We then perform cost-effective QLoRA-based supervised fine-tuning sequentially with each scheme. The resulting checkpoint is further fine-tuned with a subset of the HH-RLHF dataset to minimize distribution shift prior to using the DPO loss to obtain the final checkpoint. Evaluation is done with the LM Eval Harness tasks/metrics as well as on MT-Bench using the &quot;LLM-as-a-judge&quot; framework with Claude 2.1, with the finding that the final checkpoint, &quot;OpenBezoar-HH-RLHF-DPO&quot;, demonstrates superior performance over many models at the 3B parameter scale, even outperforming the top model in one of the categories on the Huggingface Open LLM Leaderboard. We release &quot;OpenBezoar-SFT&quot;, &quot;OpenBezoar-HH-RLHF-SFT&quot;, &quot;OpenBezoar-HH-RLHF-DPO&quot; checkpoints, alongside our generated datasets on HuggingFace at https://huggingface.co/collections/SurgeGlobal/open-bezoar-6620a24923e12127e9e2b9cc and our codebase at https://bitbucket.org/paladinanalytics/workspace/projects/OP.\n\n\nOpenBezoar: Small, Cost-Effective and Open Models Trained on Mixes of Instruction Data</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2404.12195v1</guid>
      <dc:creator>Chandeepa Dissanayake, Lahiru Lowe, Sachith Gunasekara, Yasiru Ratnayake</dc:creator>
      <pubDate>Thu, 18 Apr 2024 13:57:18 GMT</pubDate>
    </item>
    <item>
      <title>Language-conditioned Learning for Robotic Manipulation: A Survey</title>
      <link>http://arxiv.org/abs/2312.10807v2</link>
      <description>Language-conditioned robotic manipulation represents a cutting-edge area of research, enabling seamless communication and cooperation between humans and robotic agents. This field focuses on teaching robotic systems to comprehend and execute instructions conveyed in natural language. To achieve this, the development of robust language understanding models capable of extracting actionable insights from textual input is essential. In this comprehensive survey, we systematically explore recent advancements in language-conditioned approaches within the context of robotic manipulation. We analyze these approaches based on their learning paradigms, which encompass reinforcement learning, imitation learning, and the integration of foundational models, such as large language models and vision-language models. Furthermore, we conduct an in-depth comparative analysis, considering aspects like semantic information extraction, environment &amp; evaluation, auxiliary tasks, and task representation. Finally, we outline potential future research directions in the realm of language-conditioned learning for robotic manipulation, with the topic of generalization capabilities and safety issues. The GitHub repository of this paper can be found at https://github.com/hk-zh/language-conditioned-robot-manipulation-models\n\n\nA Survey on the Integration and Optimization of Large Language Models in Edge Computing Environments</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2312.10807v2</guid>
      <dc:creator>Hongkuan Zhou, Xiangtong Yao, Yuan Meng, Siming Sun, Zhenshan Bing, Kai Huang, Alois Knoll</dc:creator>
      <pubDate>Sat, 03 Feb 2024 16:06:58 GMT</pubDate>
    </item>
    <item>
      <title>NLLG Quarterly arXiv Report 09/23: What are the most influential current AI Papers?</title>
      <link>http://arxiv.org/abs/2312.05688v1</link>
      <description>Artificial Intelligence (AI) has witnessed rapid growth, especially in the subfields Natural Language Processing (NLP), Machine Learning (ML) and Computer Vision (CV). Keeping pace with this rapid progress poses a considerable challenge for researchers and professionals in the field. In this arXiv report, the second of its kind, which covers the period from January to September 2023, we aim to provide insights and analysis that help navigate these dynamic areas of AI. We accomplish this by 1) identifying the top-40 most cited papers from arXiv in the given period, comparing the current top-40 papers to the previous report, which covered the period January to June; 2) analyzing dataset characteristics and keyword popularity; 3) examining the global sectoral distribution of institutions to reveal differences in engagement across geographical areas. Our findings highlight the continued dominance of NLP: while only 16% of all submitted papers have NLP as primary category (more than 25% have CV and ML as primary category), 50% of the most cited papers have NLP as primary category, 90% of which target LLMs. Additionally, we show that i) the US dominates among both top-40 and top-9k papers, followed by China; ii) Europe clearly lags behind and is hardly represented in the top-40 most cited papers; iii) US industry is largely overrepresented in the top-40 most influential papers.\n\n\nNLLG Quarterly arXiv Report 09/23: What are the most influential current AI Papers?</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2312.05688v1</guid>
      <dc:creator>Ran Zhang, Aida Kostikova, Christoph Leiter, Jonas Belouadi, Daniil Larionov, Yanran Chen, Vivian Fresen, Steffen Eger</dc:creator>
      <pubDate>Sat, 09 Dec 2023 21:42:20 GMT</pubDate>
    </item>
    <item>
      <title>A Brief Introduction to Causal Inference in Machine Learning</title>
      <link>http://arxiv.org/abs/2405.08793v1</link>
      <description>This is a lecture note produced for DS-GA 3001.003 &quot;Special Topics in DS - Causal Inference in Machine Learning&quot; at the Center for Data Science, New York University in Spring, 2024. This course was created to target master's and PhD level students with basic background in machine learning but who were not exposed to causal inference or causal reasoning in general previously. In particular, this course focuses on introducing such students to expand their view and knowledge of machine learning to incorporate causal reasoning, as this aspect is at the core of so-called out-of-distribution generalization (or lack thereof.)\n\n\nA Brief Introduction to Causal Inference in Machine Learning</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.08793v1</guid>
      <dc:creator>Kyunghyun Cho</dc:creator>
      <pubDate>Tue, 14 May 2024 17:41:55 GMT</pubDate>
    </item>
    <item>
      <title>Mimicking User Data: On Mitigating Fine-Tuning Risks in Closed Large Language Models</title>
      <link>http://arxiv.org/abs/2406.10288v2</link>
      <description>Fine-tuning large language models on small, high-quality datasets can enhance their performance on specific downstream tasks. Recent research shows that fine-tuning on benign, instruction-following data can inadvertently undo the safety alignment process and increase a model's propensity to comply with harmful queries. Although critical, understanding and mitigating safety risks in well-defined tasks remains distinct from the instruction-following context due to structural differences in the data. Our work addresses the gap in our understanding of these risks across diverse types of data in closed models - where providers control how user data is utilized in the fine-tuning process. We demonstrate how malicious actors can subtly manipulate the structure of almost any task-specific dataset to foster significantly more dangerous model behaviors, while maintaining an appearance of innocuity and reasonable downstream task performance. To address this issue, we propose a novel mitigation strategy that mixes in safety data which mimics the task format and prompting style of the user data, showing this is more effective than existing baselines at re-establishing safety alignment while maintaining similar task performance.\n\n\nMimicking User Data: On Mitigating Fine-Tuning Risks in Closed Large Language Models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.10288v2</guid>
      <dc:creator>Francisco Eiras, Aleksandar Petrov, Phillip H. S. Torr, M. Pawan Kumar, Adel Bibi</dc:creator>
      <pubDate>Mon, 01 Jul 2024 10:17:58 GMT</pubDate>
    </item>
    <item>
      <title>Alignment For Performance Improvement in Conversation Bots</title>
      <link>http://arxiv.org/abs/2406.18954v1</link>
      <description>This paper shows that alignment methods can achieve superior adherence to guardrails compared to instruction fine-tuning alone in conversational agents, also known as bots, within predefined guidelines or 'guardrails'. It examines traditional training approaches such as instruction fine-tuning and the recent advancements in direct alignment methods like Identity Preference Optimization (IPO), and Kahneman-Tversky Optimization (KTO). The effectiveness of alignment techniques both pre and post-instruction tuning is highlighted, illustrating their potential to optimize conversational bots in domains that require strict adherence to specified rules, such as customer care.\n\n\nAlignment For Performance Improvement in Conversation Bots</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.18954v1</guid>
      <dc:creator>Raghav Garg, Kapil Sharma, Shrey Singla</dc:creator>
      <pubDate>Thu, 27 Jun 2024 07:36:25 GMT</pubDate>
    </item>
    <item>
      <title>A Novel Paradigm Boosting Translation Capabilities of Large Language Models</title>
      <link>http://arxiv.org/abs/2403.11430v2</link>
      <description>This paper presents a study on strategies to enhance the translation capabilities of large language models (LLMs) in the context of machine translation (MT) tasks. The paper proposes a novel paradigm consisting of three stages: Secondary Pre-training using Extensive Monolingual Data, Continual Pre-training with Interlinear Text Format Documents, and Leveraging Source-Language Consistent Instruction for Supervised Fine-Tuning. Previous research on LLMs focused on various strategies for supervised fine-tuning (SFT), but their effectiveness has been limited. While traditional machine translation approaches rely on vast amounts of parallel bilingual data, our paradigm highlights the importance of using smaller sets of high-quality bilingual data. We argue that the focus should be on augmenting LLMs' cross-lingual alignment abilities during pre-training rather than solely relying on extensive bilingual data during SFT. Experimental results conducted using the Llama2 model, particularly on Chinese-Llama2 after monolingual augmentation, demonstrate the improved translation capabilities of LLMs. A significant contribution of our approach lies in Stage2: Continual Pre-training with Interlinear Text Format Documents, which requires less than 1B training data, making our method highly efficient. Additionally, in Stage3, we observed that setting instructions consistent with the source language benefits the supervised fine-tuning process. Experimental results demonstrate that our approach surpasses previous work and achieves superior performance compared to models such as NLLB-54B and GPT3.5-text-davinci-003, despite having a significantly smaller parameter count of only 7B or 13B. This achievement establishes our method as a pioneering strategy in the field of machine translation.\n\n\nA preference-driven paradigm for enhanced translation with large language models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2403.11430v2</guid>
      <dc:creator>Jiaxin Guo, Hao Yang, Zongyao Li, Daimeng Wei, Hengchao Shang, Xiaoyu Chen</dc:creator>
      <pubDate>Mon, 15 Apr 2024 06:34:04 GMT</pubDate>
    </item>
    <item>
      <title>Large Language Model as an Assignment Evaluator: Insights, Feedback, and Challenges in a 1000+ Student Course</title>
      <link>http://arxiv.org/abs/2407.05216v1</link>
      <description>Using large language models (LLMs) for automatic evaluation has become an important evaluation method in NLP research. However, it is unclear whether these LLM-based evaluators can be applied in real-world classrooms to assess student assignments. This empirical report shares how we use GPT-4 as an automatic assignment evaluator in a university course with 1,028 students. Based on student responses, we find that LLM-based assignment evaluators are generally acceptable to students when students have free access to these LLM-based evaluators. However, students also noted that the LLM sometimes fails to adhere to the evaluation instructions. Additionally, we observe that students can easily manipulate the LLM-based evaluator to output specific strings, allowing them to achieve high scores without meeting the assignment rubric. Based on student feedback and our experience, we provide several recommendations for integrating LLM-based evaluators into future classrooms.\n\n\nLarge Language Model as an Assignment Evaluator: Insights, Feedback, and Challenges in a 1000+ Student Course</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2407.05216v1</guid>
      <dc:creator>Cheng-Han Chiang, Wei-Chih Chen, Chun-Yi Kuan, Chienchou Yang, Hung-yi Lee</dc:creator>
      <pubDate>Sun, 07 Jul 2024 00:17:24 GMT</pubDate>
    </item>
    <item>
      <title>Preference Alignment with Flow Matching</title>
      <link>http://arxiv.org/abs/2405.19806v1</link>
      <description>We present Preference Flow Matching (PFM), a new framework for preference-based reinforcement learning (PbRL) that streamlines the integration of preferences into an arbitrary class of pre-trained models. Existing PbRL methods require fine-tuning pre-trained models, which presents challenges such as scalability, inefficiency, and the need for model modifications, especially with black-box APIs like GPT-4. In contrast, PFM utilizes flow matching techniques to directly learn from preference data, thereby reducing the dependency on extensive fine-tuning of pre-trained models. By leveraging flow-based models, PFM transforms less preferred data into preferred outcomes, and effectively aligns model outputs with human preferences without relying on explicit or implicit reward function estimation, thus avoiding common issues like overfitting in reward models. We provide theoretical insights that support our method's alignment with standard PbRL objectives. Experimental results indicate the practical effectiveness of our method, offering a new direction in aligning a pre-trained model to preference.\n\n\nPreference Alignment with Flow Matching</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2405.19806v1</guid>
      <dc:creator>Minu Kim, Yongsik Lee, Sehyeok Kang, Jihwan Oh, Song Chong, Seyoung Yun</dc:creator>
      <pubDate>Thu, 30 May 2024 08:16:22 GMT</pubDate>
    </item>
    <item>
      <title>ProGen2: Exploring the Boundaries of Protein Language Models</title>
      <link>http://arxiv.org/abs/2206.13517v1</link>
      <description>Attention-based models trained on protein sequences have demonstrated incredible success at classification and generation tasks relevant for artificial intelligence-driven protein design. However, we lack a sufficient understanding of how very large-scale models and data play a role in effective protein model development. We introduce a suite of protein language models, named ProGen2, that are scaled up to 6.4B parameters and trained on different sequence datasets drawn from over a billion proteins from genomic, metagenomic, and immune repertoire databases. ProGen2 models show state-of-the-art performance in capturing the distribution of observed evolutionary sequences, generating novel viable sequences, and predicting protein fitness without additional finetuning. As large model sizes and raw numbers of protein sequences continue to become more widely accessible, our results suggest that a growing emphasis needs to be placed on the data distribution provided to a protein sequence model. We release the ProGen2 models and code at https://github.com/salesforce/progen.\n\n\nLikelihood-based fine-tuning of protein language models for few-shot fitness prediction and design</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2206.13517v1</guid>
      <dc:creator>Erik Nijkamp, Jeffrey Ruffolo, Eli N. Weinstein, Nikhil Naik, Ali Madani</dc:creator>
      <pubDate>Mon, 27 Jun 2022 17:55:02 GMT</pubDate>
    </item>
    <item>
      <title>EmPO: Theory-Driven Dataset Construction for Empathetic Response Generation through Preference Optimization</title>
      <link>http://arxiv.org/abs/2406.19071v1</link>
      <description>Empathetic response generation is a desirable aspect of conversational agents, crucial for facilitating engaging and emotionally intelligent multi-turn conversations between humans and machines. Leveraging large language models for this task has shown promising results, yet challenges persist in ensuring both the empathetic quality of the responses and retention of the generalization performance of the models. In this paper, we propose a novel approach where we construct theory-driven preference datasets and use them to align LLMs with preference optimization algorithms to address these challenges. To measure empathetic response generation, we employ the EmpatheticDialogues dataset, assessing empathy with the diff-EPITOME and BERTscore metrics, and evaluate the generalization performance on the MMLU benchmark. We make all datasets, source code, and models publicly available.\n\n\nEmPO: Theory-Driven Dataset Construction for Empathetic Response Generation through Preference Optimization</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.19071v1</guid>
      <dc:creator>Ondrej Sotolar</dc:creator>
      <pubDate>Thu, 27 Jun 2024 10:41:22 GMT</pubDate>
    </item>
    <item>
      <title>Evaluation of GPT and BERT-based models on identifying protein-protein interactions in biomedical text</title>
      <link>http://arxiv.org/abs/2303.17728v2</link>
      <description>Detecting protein-protein interactions (PPIs) is crucial for understanding genetic mechanisms, disease pathogenesis, and drug design. However, with the fast-paced growth of biomedical literature, there is a growing need for automated and accurate extraction of PPIs to facilitate scientific knowledge discovery. Pre-trained language models, such as generative pre-trained transformers (GPT) and bidirectional encoder representations from transformers (BERT), have shown promising results in natural language processing (NLP) tasks. We evaluated the performance of PPI identification of multiple GPT and BERT models using three manually curated gold-standard corpora: Learning Language in Logic (LLL) with 164 PPIs in 77 sentences, Human Protein Reference Database with 163 PPIs in 145 sentences, and Interaction Extraction Performance Assessment with 335 PPIs in 486 sentences. BERT-based models achieved the best overall performance, with BioBERT achieving the highest recall (91.95%) and F1-score (86.84%) and PubMedBERT achieving the highest precision (85.25%). Interestingly, despite not being explicitly trained for biomedical texts, GPT-4 achieved commendable performance, comparable to the top-performing BERT models. It achieved a precision of 88.37%, a recall of 85.14%, and an F1-score of 86.49% on the LLL dataset. These results suggest that GPT models can effectively detect PPIs from text data, offering promising avenues for application in biomedical literature mining. Further research could explore how these models might be fine-tuned for even more specialized tasks within the biomedical domain.\n\n\nProtein Generation via Genome-scale Language Models with Bio-physical Scoring</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2303.17728v2</guid>
      <dc:creator>Hasin Rehana, Nur Bengisu Çam, Mert Basmaci, Jie Zheng, Christianah Jemiyo, Yongqun He, Arzucan Özgür, Junguk Hur</dc:creator>
      <pubDate>Wed, 13 Dec 2023 00:18:46 GMT</pubDate>
    </item>
    <item>
      <title>Entity-Centric Query Refinement</title>
      <link>http://arxiv.org/abs/2204.00743v2</link>
      <description>We introduce the task of entity-centric query refinement. Given an input query whose answer is a (potentially large) collection of entities, the task output is a small set of query refinements meant to assist the user in efficient domain exploration and entity discovery. We propose a method to create a training dataset for this task. For a given input query, we use an existing knowledge base taxonomy as a source of candidate query refinements, and choose a final set of refinements from among these candidates using a search procedure designed to partition the set of entities answering the input query. We demonstrate that our approach identifies refinement sets which human annotators judge to be interesting, comprehensive, and non-redundant. In addition, we find that a text generation model trained on our newly-constructed dataset is able to offer refinements for novel queries not covered by an existing taxonomy. Our code and data are available at https://github.com/google-research/language/tree/master/language/qresp.\n\n\nRefining Large Language Models for Tabular Data Analysis in Business Domain by Laymen Text</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2204.00743v2</guid>
      <dc:creator>David Wadden, Nikita Gupta, Kenton Lee, Kristina Toutanova</dc:creator>
      <pubDate>Thu, 15 Sep 2022 22:09:48 GMT</pubDate>
    </item>
    <item>
      <title>Revisiting Iterative Relevance Feedback for Document and Passage Retrieval</title>
      <link>http://arxiv.org/abs/1812.05731v3</link>
      <description>As more and more search traffic comes from mobile phones, intelligent assistants, and smart-home devices, new challenges (e.g., limited presentation space) and opportunities come up in information retrieval. Previously, an effective technique, relevance feedback (RF), has rarely been used in real search scenarios due to the overhead of collecting users' relevance judgments. However, since users tend to interact more with the search results shown on the new interfaces, it becomes feasible to obtain users' assessments on a few results during each interaction. This makes iterative relevance feedback (IRF) techniques look promising today. IRF has not been studied systematically in the new search scenarios and its effectiveness is mostly unknown. In this paper, we re-visit IRF and extend it with RF models proposed in recent years. We conduct extensive experiments to analyze and compare IRF with the standard top-k RF framework on document and passage retrieval. Experimental results show that IRF is at least as effective as the standard top-k RF framework for documents and much more effective for passages. This indicates that IRF for passage retrieval has huge potential.\n\n\nRevisiting Document Expansion and Filtering for Effective First-Stage Retrieval</description>
      <guid isPermaLink="false">http://arxiv.org/abs/1812.05731v3</guid>
      <dc:creator>Keping Bi, Qingyao Ai, W. Bruce Croft</dc:creator>
      <pubDate>Sun, 09 Jun 2019 22:18:36 GMT</pubDate>
    </item>
    <item>
      <title>KTO: Model Alignment as Prospect Theoretic Optimization</title>
      <link>http://arxiv.org/abs/2402.01306v2</link>
      <description>Kahneman &amp; Tversky's $\textit{prospect theory}$ tells us that humans perceive random variables in a biased but well-defined manner (1992); for example, humans are famously loss-averse. We show that objectives for aligning LLMs with human feedback implicitly incorporate many of these biases -- the success of these objectives (e.g., DPO) over cross-entropy minimization can partly be ascribed to them belonging to a family of loss functions that we call $\textit{human-aware losses}$ (HALOs). However, the utility functions these methods attribute to humans still differ from those in the prospect theory literature. Using a Kahneman-Tversky model of human utility, we propose a HALO that directly maximizes the utility of generations instead of maximizing the log-likelihood of preferences, as current methods do. We call this approach KTO, and it matches or exceeds the performance of preference-based methods at scales from 1B to 30B, despite only learning from a binary signal of whether an output is desirable. More broadly, our work suggests that there is no one HALO that is universally superior; the best loss depends on the inductive biases most appropriate for a given setting, an oft-overlooked consideration.\n\n\nModel Alignment as Prospect Theoretic Optimization</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.01306v2</guid>
      <dc:creator>Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, Douwe Kiela</dc:creator>
      <pubDate>Mon, 03 Jun 2024 02:36:09 GMT</pubDate>
    </item>
    <item>
      <title>Efficient and Accurate Memorable Conversation Model using DPO based on sLLM</title>
      <link>http://arxiv.org/abs/2407.06537v1</link>
      <description>In multi-session dialog system, it is essential to continuously update the memory as the session progresses. Simply accumulating memory can make it difficult to focus on the content of the conversation for inference due to the limited input sentence size. Therefore, efficient and accurate conversation model that is capable of managing memory to reflect the conversation history continuously is necessary. This paper presents a conversation model that efficiently manages memory as sessions progress and incorporates this into the model to reflect the conversation history accurately with 3 methodologies: SFT, DPO and DPO with SFT model. Our model using DPO algorithm shows an improvement about 0.0591 of BERTScore in memory accuracy, and the rate of responses reflecting the memory increased as well. Also, response generation performance enhanced about 4.292 in fluency, 3.935 in coherence, and 2.896 in consistency. This paper describes a training method that yields better performance than models with more than twice the parameter size, even when the model size is smaller. Thus, our model demonstrates efficiency not only in terms of accuracy but also in resource utilization.\n\n\nEfficient and Accurate Memorable Conversation Model using DPO based on sLLM</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2407.06537v1</guid>
      <dc:creator>Youngkyung Seo, Yoonseok Heo, Jun-Seok Koh, Du-Seoung Chang</dc:creator>
      <pubDate>Tue, 09 Jul 2024 04:17:39 GMT</pubDate>
    </item>
    <item>
      <title>Defining and Evaluating Fair Natural Language Generation</title>
      <link>http://arxiv.org/abs/2008.01548v1</link>
      <description>Our work focuses on the biases that emerge in the natural language generation (NLG) task of sentence completion. In this paper, we introduce a framework of fairness for NLG followed by an evaluation of gender biases in two state-of-the-art language models. Our analysis provides a theoretical formulation for biases in NLG and empirical evidence that existing language generation models embed gender bias.\n\n\nFairness in transfer learning for natural language processing</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2008.01548v1</guid>
      <dc:creator>Catherine Yeo, Alyssa Chen</dc:creator>
      <pubDate>Tue, 28 Jul 2020 04:11:10 GMT</pubDate>
    </item>
    <item>
      <title>Dubious Deductions from AGN Survey Data</title>
      <link>http://arxiv.org/abs/astro-ph/0110343v1</link>
      <description>The participants in this meeting are almost all carrying out the hard work of making many different types of AGN surveys. Since it's so much easier to criticize other people's work than to do actual work myself, I'll just present some demurs regarding recent papers drawing conclusions from various AGN survey data. In particular I'll mention some questionable interpretations of surveys of Seyfert 2 near-UV polarization; interpreting the results of searches for polarized broad H-alpha lines in Seyfert 2s; testing the beam model for radio galaxies and quasars; testing the unification of Seyfert spectral types with a torus; and finally testing the energy sources for Ulirgs, especially those with Liner optical spectra. Only the polarized broad H-alpha results are examined in detail here.\n\n\nDubious Debiasing: Inherent Challenges in Achieving Fairness in Large Language Models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/astro-ph/0110343v1</guid>
      <dc:creator>Robert Antonucci</dc:creator>
      <pubDate>Mon, 15 Oct 2001 23:41:44 GMT</pubDate>
    </item>
    <item>
      <title>Teaching computer code at school</title>
      <link>http://arxiv.org/abs/1705.08507v1</link>
      <description>In today's education systems, there is a deep concern about the importance of teaching code and computer programming in schools. Moving digital learning from a simple use of tools to understanding the processes of the internal functioning of these tools is an old / new debate originated with the digital laboratories of the 1960. Today, it is emerging again under impulse of the large - scale public sphere digitalization and the new constructivist education theories. Teachers and educators discuss not only the viability of code teaching in the classroom, but also the intellectual and cognitive advantages for students. The debate thus takes several orientations and is resourced in the entanglement of arguments and interpretations of any order, technical, educational, cultural, cognitive and psychological. However, that phenomenon which undoubtedly augurs for a profound transformation in the future models of learning and teaching , is predicting a new and almost congenital digital humanism\n\n\nTeaching Large Language Models to Use Tools at Scale</description>
      <guid isPermaLink="false">http://arxiv.org/abs/1705.08507v1</guid>
      <dc:creator>Mokhtar Ben Henda</dc:creator>
      <pubDate>Wed, 03 May 2017 09:08:17 GMT</pubDate>
    </item>
    <item>
      <title>Report on the 8th International Workshop on Bibliometric-enhanced Information Retrieval (BIR 2019)</title>
      <link>http://arxiv.org/abs/1909.04954v1</link>
      <description>The Bibliometric-enhanced Information Retrieval workshop series (BIR) at ECIR tackled issues related to academic search, at the crossroads between Information Retrieval and Bibliometrics. BIR is a hot topic investigated by both academia (e.g., ArnetMiner, CiteSeerx, DocEar) and the industry (e.g., Google Scholar, Microsoft Academic Search, Semantic Scholar). This report presents the 8th iteration of the one-day BIR workshop held at ECIR 2019 in Cologne, Germany.\n\n\nReport on The Search Futures Workshop at ECIR 2024</description>
      <guid isPermaLink="false">http://arxiv.org/abs/1909.04954v1</guid>
      <dc:creator>Guillaume Cabanac, Ingo Frommholz, Philipp Mayr</dc:creator>
      <pubDate>Wed, 11 Sep 2019 10:07:59 GMT</pubDate>
    </item>
    <item>
      <title>Measuring Adversarial Datasets</title>
      <link>http://arxiv.org/abs/2311.03566v1</link>
      <description>In the era of widespread public use of AI systems across various domains, ensuring adversarial robustness has become increasingly vital to maintain safety and prevent undesirable errors. Researchers have curated various adversarial datasets (through perturbations) for capturing model deficiencies that cannot be revealed in standard benchmark datasets. However, little is known about how these adversarial examples differ from the original data points, and there is still no methodology to measure the intended and unintended consequences of those adversarial transformations. In this research, we conducted a systematic survey of existing quantifiable metrics that describe text instances in NLP tasks, among dimensions of difficulty, diversity, and disagreement. We selected several current adversarial effect datasets and compared the distributions between the original and their adversarial counterparts. The results provide valuable insights into what makes these datasets more challenging from a metrics perspective and whether they align with underlying assumptions.\n\n\nAdversarial Robustness for Estimation and Alignment</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2311.03566v1</guid>
      <dc:creator>Yuanchen Bai, Raoyi Huang, Vijay Viswanathan, Tzu-Sheng Kuo, Tongshuang Wu</dc:creator>
      <pubDate>Mon, 06 Nov 2023 22:08:16 GMT</pubDate>
    </item>
    <item>
      <title>Learning a Low-dimensional Representation of a Safe Region for Safe Reinforcement Learning on Dynamical Systems</title>
      <link>http://arxiv.org/abs/2010.09555v2</link>
      <description>For safely applying reinforcement learning algorithms on high-dimensional nonlinear dynamical systems, a simplified system model is used to formulate a safe reinforcement learning framework. Based on the simplified system model, a low-dimensional representation of the safe region is identified and is used to provide safety estimates for learning algorithms. However, finding a satisfying simplified system model for complex dynamical systems usually requires a considerable amount of effort. To overcome this limitation, we propose in this work a general data-driven approach that is able to efficiently learn a low-dimensional representation of the safe region. Through an online adaptation method, the low-dimensional representation is updated by using the feedback data such that more accurate safety estimates are obtained. The performance of the proposed approach for identifying the low-dimensional representation of the safe region is demonstrated with a quadcopter example. The results show that, compared to previous work, a more reliable and representative low-dimensional representation of the safe region is derived, which then extends the applicability of the safe reinforcement learning framework.\n\n\nSafe Reinforcement Learning using Finite-Horizon Gradient-based Estimation</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2010.09555v2</guid>
      <dc:creator>Zhehua Zhou, Ozgur S. Oguz, Marion Leibold, Martin Buss</dc:creator>
      <pubDate>Wed, 08 Sep 2021 06:02:21 GMT</pubDate>
    </item>
    <item>
      <title>Citation-Enhanced Generation for LLM-based Chatbots</title>
      <link>http://arxiv.org/abs/2402.16063v3</link>
      <description>Large language models (LLMs) exhibit powerful general intelligence across diverse scenarios, including their integration into chatbots. However, a vital challenge of LLM-based chatbots is that they may produce hallucinated content in responses, which significantly limits their applicability. Various efforts have been made to alleviate hallucination, such as retrieval augmented generation and reinforcement learning with human feedback, but most of them require additional training and data annotation. In this paper, we propose a novel post-hoc Citation-Enhanced Generation (CEG) approach combined with retrieval argumentation. Unlike previous studies that focus on preventing hallucinations during generation, our method addresses this issue in a post-hoc way. It incorporates a retrieval module to search for supporting documents relevant to the generated content, and employs a natural language inference-based citation generation module. Once the statements in the generated content lack of reference, our model can regenerate responses until all statements are supported by citations. Note that our method is a training-free plug-and-play plugin that is capable of various LLMs. Experiments on various hallucination-related datasets show our framework outperforms state-of-the-art methods in both hallucination detection and response regeneration on three benchmarks. Our codes and dataset will be publicly available.\n\n\nEnhancing Language Model with Both Human and Artificial Intelligence Feedback Data</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.16063v3</guid>
      <dc:creator>Weitao Li, Junkai Li, Weizhi Ma, Yang Liu</dc:creator>
      <pubDate>Mon, 04 Mar 2024 01:53:35 GMT</pubDate>
    </item>
    <item>
      <title>Optimal Linear Image Combination</title>
      <link>http://arxiv.org/abs/1105.2852v2</link>
      <description>A simple, yet general, formalism for the optimized linear combination of astrophysical images is constructed and demonstrated. The formalism allows the user to combine multiple undersampled images to provide oversampled output at high precision. The proposed method is general and may be used for any configuration of input pixels and point spread function; it also provides the noise covariance in the output image along with a powerful metric for describing undesired distortion to the image convolution kernel. The method explicitly provides knowledge and control of the inevitable compromise between noise and fidelity in the output image. We present a first prototype implementation of the method, outlining steps taken to generate an efficient algorithm. This implementation is then put to practical use in reconstructing fully-sampled output images using simulated, undersampled input exposures that are designed to mimic the proposed \emph{Wide Field InfraRed Survey Telescope} (\emph{WFIRST}). We examine results using randomly rotated and dithered input images, while also assessing better-known &quot;ideal&quot; dither patterns: comparing results we illustrate the use of the method as a survey design tool. Finally, we use the method to test the robustness of linear image combination when subject to practical realities such as missing input pixels and focal plane plate scale variations.\n\n\nCombining multiple metrics for evaluating retrieval-augmented conversations</description>
      <guid isPermaLink="false">http://arxiv.org/abs/1105.2852v2</guid>
      <dc:creator>Barnaby Rowe, Christopher Hirata, Jason Rhodes</dc:creator>
      <pubDate>Tue, 06 Sep 2011 16:00:17 GMT</pubDate>
    </item>
    <item>
      <title>HLA class I binding prediction via convolutional neural networks</title>
      <link>http://arxiv.org/abs/1701.00593v2</link>
      <description>Many biological processes are governed by protein-ligand interactions. One such example is the recognition of self and nonself cells by the immune system. This immune response process is regulated by the major histocompatibility complex (MHC) protein which is encoded by the human leukocyte antigen (HLA) complex. Understanding the binding potential between MHC and peptides can lead to the design of more potent, peptide-based vaccines and immunotherapies for infectious autoimmune diseases.   We apply machine learning techniques from the natural language processing (NLP) domain to address the task of MHC-peptide binding prediction. More specifically, we introduce a new distributed representation of amino acids, name HLA-Vec, that can be used for a variety of downstream proteomic machine learning tasks. We then propose a deep convolutional neural network architecture, name HLA-CNN, for the task of HLA class I-peptide binding prediction. Experimental results show combining the new distributed representation with our HLA-CNN architecture achieves state-of-the-art results in the majority of the latest two Immune Epitope Database (IEDB) weekly automated benchmark datasets. We further apply our model to predict binding on the human genome and identify 15 genes with potential for self binding.\n\n\nIntegrating MHC Class I visibility targets into the ProteinMPNN protein design process</description>
      <guid isPermaLink="false">http://arxiv.org/abs/1701.00593v2</guid>
      <dc:creator>Yeeleng Scott Vang, Xiaohui Xie</dc:creator>
      <pubDate>Wed, 12 Apr 2017 23:47:27 GMT</pubDate>
    </item>
    <item>
      <title>Understanding Misclassifications by Attributes</title>
      <link>http://arxiv.org/abs/1910.07416v1</link>
      <description>In this paper, we aim to understand and explain the decisions of deep neural networks by studying the behavior of predicted attributes when adversarial examples are introduced. We study the changes in attributes for clean as well as adversarial images in both standard and adversarially robust networks. We propose a metric to quantify the robustness of an adversarially robust network against adversarial attacks. In a standard network, attributes predicted for adversarial images are consistent with the wrong class, while attributes predicted for the clean images are consistent with the true class. In an adversarially robust network, the attributes predicted for adversarial images classified correctly are consistent with the true class. Finally, we show that the ability to robustify a network varies for different datasets. For the fine grained dataset, it is higher as compared to the coarse-grained dataset. Additionally, the ability to robustify a network increases with the increase in adversarial noise.\n\n\nUnderstanding and Guarding against Natural Language Adversarial Examples</description>
      <guid isPermaLink="false">http://arxiv.org/abs/1910.07416v1</guid>
      <dc:creator>Sadaf Gulshad, Zeynep Akata, Jan Hendrik Metzen, Arnold Smeulders</dc:creator>
      <pubDate>Tue, 15 Oct 2019 09:36:23 GMT</pubDate>
    </item>
    <item>
      <title>Two Gaussian Approaches to Black-Box Optomization</title>
      <link>http://arxiv.org/abs/1411.7806v1</link>
      <description>Outline of several strategies for using Gaussian processes as surrogate models for the covariance matrix adaptation evolution strategy (CMA-ES).\n\n\nSign-Averaging Covariance Matrix Adaptation Evolution Strategy</description>
      <guid isPermaLink="false">http://arxiv.org/abs/1411.7806v1</guid>
      <dc:creator>Lukáš Bajer, Martin Holeňa</dc:creator>
      <pubDate>Fri, 28 Nov 2014 10:39:24 GMT</pubDate>
    </item>
    <item>
      <title>A Threshold-based Scheme for Reinforcement Learning in Neural Networks</title>
      <link>http://arxiv.org/abs/1609.03348v4</link>
      <description>A generic and scalable Reinforcement Learning scheme for Artificial Neural Networks is presented, providing a general purpose learning machine. By reference to a node threshold three features are described 1) A mechanism for Primary Reinforcement, capable of solving linearly inseparable problems 2) The learning scheme is extended to include a mechanism for Conditioned Reinforcement, capable of forming long term strategy 3) The learning scheme is modified to use a threshold-based deep learning algorithm, providing a robust and biologically inspired alternative to backpropagation. The model may be used for supervised as well as unsupervised training regimes.\n\n\nUncertainty-Aware Unsupervised and Robust Reinforcement Learning</description>
      <guid isPermaLink="false">http://arxiv.org/abs/1609.03348v4</guid>
      <dc:creator>Thomas H. Ward</dc:creator>
      <pubDate>Sat, 14 Jan 2017 05:54:29 GMT</pubDate>
    </item>
    <item>
      <title>Towards Improving Faithfulness in Abstractive Summarization</title>
      <link>http://arxiv.org/abs/2210.01877v1</link>
      <description>Despite the success achieved in neural abstractive summarization based on pre-trained language models, one unresolved issue is that the generated summaries are not always faithful to the input document. There are two possible causes of the unfaithfulness problem: (1) the summarization model fails to understand or capture the gist of the input text, and (2) the model over-relies on the language model to generate fluent but inadequate words. In this work, we propose a Faithfulness Enhanced Summarization model (FES), which is designed for addressing these two problems and improving faithfulness in abstractive summarization. For the first problem, we propose to use question-answering (QA) to examine whether the encoder fully grasps the input document and can answer the questions on the key information in the input. The QA attention on the proper input words can also be used to stipulate how the decoder should attend to the source. For the second problem, we introduce a max-margin loss defined on the difference between the language and the summarization model, aiming to prevent the overconfidence of the language model. Extensive experiments on two benchmark summarization datasets, CNN/DM and XSum, demonstrate that our model significantly outperforms strong baselines. The evaluation of factual consistency also shows that our model generates more faithful summaries than baselines.\n\n\nImproving Abstractive Summarization and Information Consistency Assessment</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2210.01877v1</guid>
      <dc:creator>Xiuying Chen, Mingzhe Li, Xin Gao, Xiangliang Zhang</dc:creator>
      <pubDate>Tue, 04 Oct 2022 19:52:09 GMT</pubDate>
    </item>
    <item>
      <title>Large Language Models Sometimes Generate Purely Negatively-Reinforced Text</title>
      <link>http://arxiv.org/abs/2306.07567v2</link>
      <description>When using adversarial training, it is common practice to train against the most egregious failures. However, this might imply using examples with sensitive information (such as leaked passwords or security vulnerabilities) as training data. One might assume that language models trained with gradient descent never generate text snippets which were only present in examples associated with the lowest possible reward. In this paper, we show that this assumption is wrong: in some situations, large language models do learn from such negatively-reinforced examples. We present a specific training setup that enables Pythia-160M to guess passwords 13% more often than it would by guessing randomly, despite only showing it these passwords on examples where the model is incentivized to not output these passwords. Our code is available at www.github.com/FabienRoger/Learning-From-Negative-Examples\n\n\nLarge Language Models Sometimes Generate Purely Negatively-Reinforced Text</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2306.07567v2</guid>
      <dc:creator>Fabien Roger</dc:creator>
      <pubDate>Fri, 16 Jun 2023 16:23:21 GMT</pubDate>
    </item>
    <item>
      <title>Maximizing BCI Human Feedback using Active Learning</title>
      <link>http://arxiv.org/abs/2008.04873v1</link>
      <description>Recent advancements in \textit{Learning from Human Feedback} present an effective way to train robot agents via inputs from non-expert humans, without a need for a specially designed reward function. However, this approach needs a human to be present and attentive during robot learning to provide evaluative feedback. In addition, the amount of feedback needed grows with the level of task difficulty and the quality of human feedback might decrease over time because of fatigue. To overcome these limitations and enable learning more robot tasks with higher complexities, there is a need to maximize the quality of expensive feedback received and reduce the amount of human cognitive involvement required. In this work, we present an approach that uses active learning to smartly choose queries for the human supervisor based on the uncertainty of the robot and effectively reduces the amount of feedback needed to learn a given task. We also use a novel multiple buffer system to improve robustness to feedback noise and guard against catastrophic forgetting as the robot learning evolves. This makes it possible to learn tasks with more complexity using lesser amounts of human feedback compared to previous methods. We demonstrate the utility of our proposed method on a robot arm reaching task where the robot learns to reach a location in 3D without colliding with obstacles. Our approach is able to learn this task faster, with less human feedback and cognitive involvement, compared to previous methods that do not use active learning.\n\n\nLEARNING WITH AND WITHOUT HUMAN FEEDBACK</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2008.04873v1</guid>
      <dc:creator>Zizhao Wang, Junyao Shi, Iretiayo Akinola, Peter Allen</dc:creator>
      <pubDate>Tue, 11 Aug 2020 17:26:18 GMT</pubDate>
    </item>
    <item>
      <title>Improving abstractive summarization with energy-based re-ranking</title>
      <link>http://arxiv.org/abs/2210.15553v2</link>
      <description>Current abstractive summarization systems present important weaknesses which prevent their deployment in real-world applications, such as the omission of relevant information and the generation of factual inconsistencies (also known as hallucinations). At the same time, automatic evaluation metrics such as CTC scores have been recently proposed that exhibit a higher correlation with human judgments than traditional lexical-overlap metrics such as ROUGE. In this work, we intend to close the loop by leveraging the recent advances in summarization metrics to create quality-aware abstractive summarizers. Namely, we propose an energy-based model that learns to re-rank summaries according to one or a combination of these metrics. We experiment using several metrics to train our energy-based re-ranker and show that it consistently improves the scores achieved by the predicted summaries. Nonetheless, human evaluation results show that the re-ranking approach should be used with care for highly abstractive summaries, as the available metrics are not yet sufficiently reliable for this purpose.\n\n\nImproving the reliability of language models for summarization</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2210.15553v2</guid>
      <dc:creator>Diogo Pernes, Afonso Mendes, André F. T. Martins</dc:creator>
      <pubDate>Mon, 07 Nov 2022 16:05:29 GMT</pubDate>
    </item>
    <item>
      <title>RLHF and IIA: Perverse Incentives</title>
      <link>http://arxiv.org/abs/2312.01057v3</link>
      <description>Existing algorithms for reinforcement learning from human feedback (RLHF) can incentivize responses at odds with preferences because they are based on models that assume independence of irrelevant alternatives (IIA). The perverse incentives induced by IIA hinder innovations on query formats and learning algorithms.\n\n\nRLHF and IIA: Perverse Incentives</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2312.01057v3</guid>
      <dc:creator>Wanqiao Xu, Shi Dong, Xiuyuan Lu, Grace Lam, Zheng Wen, Benjamin Van Roy</dc:creator>
      <pubDate>Thu, 01 Feb 2024 18:57:20 GMT</pubDate>
    </item>
    <item>
      <title>Essays</title>
      <link>http://arxiv.org/abs/1807.04126v2</link>
      <description>A collection of short expository essays by the author on various topics in quantum mechanics, quantum cosmology, and physics in general.\n\n\nEssays in Experimental Economics: Intertemporal and Social Choice</description>
      <guid isPermaLink="false">http://arxiv.org/abs/1807.04126v2</guid>
      <dc:creator>James B. Hartle</dc:creator>
      <pubDate>Tue, 15 Jan 2019 18:07:54 GMT</pubDate>
    </item>
    <item>
      <title>Embodied Visual Active Learning for Semantic Segmentation</title>
      <link>http://arxiv.org/abs/2012.09503v1</link>
      <description>We study the task of embodied visual active learning, where an agent is set to explore a 3d environment with the goal to acquire visual scene understanding by actively selecting views for which to request annotation. While accurate on some benchmarks, today's deep visual recognition pipelines tend to not generalize well in certain real-world scenarios, or for unusual viewpoints. Robotic perception, in turn, requires the capability to refine the recognition capabilities for the conditions where the mobile system operates, including cluttered indoor environments or poor illumination. This motivates the proposed task, where an agent is placed in a novel environment with the objective of improving its visual recognition capability. To study embodied visual active learning, we develop a battery of agents - both learnt and pre-specified - and with different levels of knowledge of the environment. The agents are equipped with a semantic segmentation network and seek to acquire informative views, move and explore in order to propagate annotations in the neighbourhood of those views, then refine the underlying segmentation network by online retraining. The trainable method uses deep reinforcement learning with a reward function that balances two competing objectives: task performance, represented as visual recognition accuracy, which requires exploring the environment, and the necessary amount of annotated data requested during active exploration. We extensively evaluate the proposed models using the photorealistic Matterport3D simulator and show that a fully learnt method outperforms comparable pre-specified counterparts, even when requesting fewer annotations.\n\n\nActive Vision for Embodied Agents Using Reinforcement Learning</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2012.09503v1</guid>
      <dc:creator>David Nilsson, Aleksis Pirinen, Erik Gärtner, Cristian Sminchisescu</dc:creator>
      <pubDate>Thu, 17 Dec 2020 11:02:34 GMT</pubDate>
    </item>
    <item>
      <title>Intelligent wayfinding vehicle design based on visual recognition</title>
      <link>http://arxiv.org/abs/2209.10229v1</link>
      <description>Intelligent drug delivery trolley is an advanced intelligent drug delivery equipment. Compared with traditional manual drug delivery, it has higher drug delivery efficiency and lower error rate. In this project, an intelligent drug delivery car is designed and manufactured, which can recognize the road route and the room number of the target ward through visual recognition technology. The trolley selects the corresponding route according to the identified room number, accurately transports the drugs to the target ward, and can return to the pharmacy after the drugs are delivered. The intelligent drug delivery car uses DC power supply, and the motor drive module controls two DC motors, which overcomes the problem of excessive deviation of turning angle. The trolley line inspection function uses closed-loop control to improve the accuracy of line inspection and the controllability of trolley speed. The identification of ward number is completed by the camera module with microcontroller, and has the functions of adaptive adjustment of ambient brightness, distortion correction, automatic calibration and so on. The communication between two cooperative drug delivery vehicles is realized by Bluetooth module, which achieves efficient and accurate communication and interaction. Experiments show that the intelligent drug delivery car can accurately identify the room number and plan the route to deliver drugs to the far, middle and near wards, and has the characteristics of fast speed and accurate judgment. In addition, two drug delivery trolleys can cooperate to deliver drugs to the same ward, with high efficiency and high cooperation.\n\n\nIntelligent Photoresponsive Drug Delivery with Causal Language Models and Chemist Instruction Training</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2209.10229v1</guid>
      <dc:creator>Zhanyu Guo, Shenyuan Guo, Jialong Wang, Yifan Feng</dc:creator>
      <pubDate>Wed, 21 Sep 2022 09:49:16 GMT</pubDate>
    </item>
    <item>
      <title>Hybrid Contrastive Quantization for Efficient Cross-View Video Retrieval</title>
      <link>http://arxiv.org/abs/2202.03384v2</link>
      <description>With the recent boom of video-based social platforms (e.g., YouTube and TikTok), video retrieval using sentence queries has become an important demand and attracts increasing research attention. Despite the decent performance, existing text-video retrieval models in vision and language communities are impractical for large-scale Web search because they adopt brute-force search based on high-dimensional embeddings. To improve efficiency, Web search engines widely apply vector compression libraries (e.g., FAISS) to post-process the learned embeddings. Unfortunately, separate compression from feature encoding degrades the robustness of representations and incurs performance decay. To pursue a better balance between performance and efficiency, we propose the first quantized representation learning method for cross-view video retrieval, namely Hybrid Contrastive Quantization (HCQ). Specifically, HCQ learns both coarse-grained and fine-grained quantizations with transformers, which provide complementary understandings for texts and videos and preserve comprehensive semantic information. By performing Asymmetric-Quantized Contrastive Learning (AQ-CL) across views, HCQ aligns texts and videos at coarse-grained and multiple fine-grained levels. This hybrid-grained learning strategy serves as strong supervision on the cross-view video quantization model, where contrastive learning at different levels can be mutually promoted. Extensive experiments on three Web video benchmark datasets demonstrate that HCQ achieves competitive performance with state-of-the-art non-compressed retrieval methods while showing high efficiency in storage and computation. Code and configurations are available at https://github.com/gimpong/WWW22-HCQ.\n\n\nEfficient and robust web scale language model based retrieval, generation, and understanding</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2202.03384v2</guid>
      <dc:creator>Jinpeng Wang, Bin Chen, Dongliang Liao, Ziyun Zeng, Gongfu Li, Shu-Tao Xia, Jin Xu</dc:creator>
      <pubDate>Thu, 10 Feb 2022 01:30:08 GMT</pubDate>
    </item>
    <item>
      <title>Selective Question Answering under Domain Shift</title>
      <link>http://arxiv.org/abs/2006.09462v1</link>
      <description>To avoid giving wrong answers, question answering (QA) models need to know when to abstain from answering. Moreover, users often ask questions that diverge from the model's training data, making errors more likely and thus abstention more critical. In this work, we propose the setting of selective question answering under domain shift, in which a QA model is tested on a mixture of in-domain and out-of-domain data, and must answer (i.e., not abstain on) as many questions as possible while maintaining high accuracy. Abstention policies based solely on the model's softmax probabilities fare poorly, since models are overconfident on out-of-domain inputs. Instead, we train a calibrator to identify inputs on which the QA model errs, and abstain when it predicts an error is likely. Crucially, the calibrator benefits from observing the model's behavior on out-of-domain data, even if from a different domain than the test data. We combine this method with a SQuAD-trained QA model and evaluate on mixtures of SQuAD and five other QA datasets. Our method answers 56% of questions while maintaining 80% accuracy; in contrast, directly using the model's probabilities only answers 48% at 80% accuracy.\n\n\nSelect High-quality Synthetic QA Pairs to Augment Training Data in MRC under the Reward Guidance of Generative Language Models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2006.09462v1</guid>
      <dc:creator>Amita Kamath, Robin Jia, Percy Liang</dc:creator>
      <pubDate>Tue, 16 Jun 2020 19:13:21 GMT</pubDate>
    </item>
    <item>
      <title>Auto-survey Challenge</title>
      <link>http://arxiv.org/abs/2310.04480v2</link>
      <description>We present a novel platform for evaluating the capability of Large Language Models (LLMs) to autonomously compose and critique survey papers spanning a vast array of disciplines including sciences, humanities, education, and law. Within this framework, AI systems undertake a simulated peer-review mechanism akin to traditional scholarly journals, with human organizers serving in an editorial oversight capacity. Within this framework, we organized a competition for the AutoML conference 2023. Entrants are tasked with presenting stand-alone models adept at authoring articles from designated prompts and subsequently appraising them. Assessment criteria include clarity, reference appropriateness, accountability, and the substantive value of the content. This paper presents the design of the competition, including the implementation baseline submissions and methods of evaluation.\n\n\nChallenges and Methods for Alignment of Large Language Models with Human Preferences</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2310.04480v2</guid>
      <dc:creator>Thanh Gia Hieu Khuong, Benedictus Kent Rachmat</dc:creator>
      <pubDate>Tue, 10 Oct 2023 09:31:04 GMT</pubDate>
    </item>
    <item>
      <title>Towards a theory of model distillation</title>
      <link>http://arxiv.org/abs/2403.09053v2</link>
      <description>Distillation is the task of replacing a complicated machine learning model with a simpler model that approximates the original [BCNM06,HVD15]. Despite many practical applications, basic questions about the extent to which models can be distilled, and the runtime and amount of data needed to distill, remain largely open.   To study these questions, we initiate a general theory of distillation, defining PAC-distillation in an analogous way to PAC-learning [Val84]. As applications of this theory: (1) we propose new algorithms to extract the knowledge stored in the trained weights of neural networks -- we show how to efficiently distill neural networks into succinct, explicit decision tree representations when possible by using the ``linear representation hypothesis''; and (2) we prove that distillation can be much cheaper than learning from scratch, and make progress on characterizing its complexity.\n\n\nToward building more accessible large language models: A preliminary empirical study on data scarcity in knowledge distillation and algorithm complexity in …</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2403.09053v2</guid>
      <dc:creator>Enric Boix-Adsera</dc:creator>
      <pubDate>Sat, 04 May 2024 19:52:03 GMT</pubDate>
    </item>
    <item>
      <title>Consistency Regularization for Domain Generalization with Logit Attribution Matching</title>
      <link>http://arxiv.org/abs/2305.07888v2</link>
      <description>Domain generalization (DG) is about training models that generalize well under domain shift. Previous research on DG has been conducted mostly in single-source or multi-source settings. In this paper, we consider a third, lesser-known setting where a training domain is endowed with a collection of pairs of examples that share the same semantic information. Such semantic sharing (SS) pairs can be created via data augmentation and then utilized for consistency regularization (CR). We present a theory showing CR is conducive to DG and propose a novel CR method called Logit Attribution Matching (LAM). We conduct experiments on five DG benchmarks and four pretrained models with SS pairs created by both generic and targeted data augmentation methods. LAM outperforms representative single/multi-source DG methods and various CR methods that leverage SS pairs. The code and data of this project are available at https://github.com/Gaohan123/LAM\n\n\nConsistency Regularization for Domain Generalization with Logit Attribution Matching</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2305.07888v2</guid>
      <dc:creator>Han Gao, Kaican Li, Weiyan Xie, Zhi Lin, Yongxiang Huang, Luning Wang, Caleb Chen Cao, Nevin L. Zhang</dc:creator>
      <pubDate>Wed, 12 Jun 2024 13:14:07 GMT</pubDate>
    </item>
    <item>
      <title>Online Disinformation and the Role of Wikipedia</title>
      <link>http://arxiv.org/abs/1910.12596v1</link>
      <description>The aim of this study is to find key areas of research that can be useful to fight against disinformation on Wikipedia. To address this problem we perform a literature review trying to answer three main questions: (i) What is disinformation? (ii) What are the most popular mechanisms to spread online disinformation? and (iii) Which are the mechanisms that are currently being used to fight against disinformation?. In all these three questions we take first a general approach, considering studies from different areas such as journalism and communications, sociology, philosophy, information and political sciences. And comparing those studies with the current situation on the Wikipedia ecosystem. We conclude that in order to keep Wikipedia as free as possible from disinformation, it is necessary to help patrollers to early detect disinformation and assess the credibility of external sources. More research is needed to develop tools that use state-of-the-art machine learning techniques to detect potentially dangerous content, empowering patrollers to deal with attacks that are becoming more complex and sophisticated.\n\n\nControllable Text Generation to Fight Disinformation</description>
      <guid isPermaLink="false">http://arxiv.org/abs/1910.12596v1</guid>
      <dc:creator>Diego Saez-Trumper</dc:creator>
      <pubDate>Mon, 14 Oct 2019 19:58:15 GMT</pubDate>
    </item>
    <item>
      <title>Latent Diffusion for Language Generation</title>
      <link>http://arxiv.org/abs/2212.09462v2</link>
      <description>Diffusion models have achieved great success in modeling continuous data modalities such as images, audio, and video, but have seen limited use in discrete domains such as language. Recent attempts to adapt diffusion to language have presented diffusion as an alternative to existing pretrained language models. We view diffusion and existing language models as complementary. We demonstrate that encoder-decoder language models can be utilized to efficiently learn high-quality language autoencoders. We then demonstrate that continuous diffusion models can be learned in the latent space of the language autoencoder, enabling us to sample continuous latent representations that can be decoded into natural language with the pretrained decoder. We validate the effectiveness of our approach for unconditional, class-conditional, and sequence-to-sequence language generation. We demonstrate across multiple diverse data sets that our latent language diffusion models are significantly more effective than previous diffusion language models.\n\n\nDiffusion Domain Expansion: Learning to Coordinate Pre-Trained Diffusion Models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2212.09462v2</guid>
      <dc:creator>Justin Lovelace, Varsha Kishore, Chao Wan, Eliot Shekhtman, Kilian Q. Weinberger</dc:creator>
      <pubDate>Tue, 07 Nov 2023 15:35:45 GMT</pubDate>
    </item>
    <item>
      <title>Does Multi-Encoder Help? A Case Study on Context-Aware Neural Machine Translation</title>
      <link>http://arxiv.org/abs/2005.03393v2</link>
      <description>In encoder-decoder neural models, multiple encoders are in general used to represent the contextual information in addition to the individual sentence. In this paper, we investigate multi-encoder approaches in documentlevel neural machine translation (NMT). Surprisingly, we find that the context encoder does not only encode the surrounding sentences but also behaves as a noise generator. This makes us rethink the real benefits of multi-encoder in context-aware translation - some of the improvements come from robust training. We compare several methods that introduce noise and/or well-tuned dropout setup into the training of these encoders. Experimental results show that noisy training plays an important role in multi-encoder-based NMT, especially when the training data is small. Also, we establish a new state-of-the-art on IWSLT Fr-En task by careful use of noise generation and dropout methods.\n\n\nOn Generative Models and Joint Architectures for Document-level Relation Extraction</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2005.03393v2</guid>
      <dc:creator>Bei Li, Hui Liu, Ziyang Wang, Yufan Jiang, Tong Xiao, Jingbo Zhu, Tongran Liu, Changliang Li</dc:creator>
      <pubDate>Sun, 17 May 2020 03:09:33 GMT</pubDate>
    </item>
    <item>
      <title>ChrEnTranslate: Cherokee-English Machine Translation Demo with Quality Estimation and Corrective Feedback</title>
      <link>http://arxiv.org/abs/2107.14800v2</link>
      <description>We introduce ChrEnTranslate, an online machine translation demonstration system for translation between English and an endangered language Cherokee. It supports both statistical and neural translation models as well as provides quality estimation to inform users of reliability, two user feedback interfaces for experts and common users respectively, example inputs to collect human translations for monolingual data, word alignment visualization, and relevant terms from the Cherokee-English dictionary. The quantitative evaluation demonstrates that our backbone translation models achieve state-of-the-art translation performance and our quality estimation well correlates with both BLEU and human judgment. By analyzing 216 pieces of expert feedback, we find that NMT is preferable because it copies less than SMT, and, in general, current models can translate fragments of the source sentence but make major mistakes. When we add these 216 expert-corrected parallel texts back into the training set and retrain models, equal or slightly better performance is observed, which indicates the potential of human-in-the-loop learning. Our online demo is at https://chren.cs.unc.edu/ , our code is open-sourced at https://github.com/ZhangShiyue/ChrEnTranslate , and our data is available at https://github.com/ZhangShiyue/ChrEn\n\n\nMetis-A Python-Based User Interface to Collect Expert Feedback for Generative Chemistry Models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2107.14800v2</guid>
      <dc:creator>Shiyue Zhang, Benjamin Frey, Mohit Bansal</dc:creator>
      <pubDate>Mon, 02 Aug 2021 16:27:02 GMT</pubDate>
    </item>
    <item>
      <title>Bootstrapping incremental dialogue systems from minimal data: the generalisation power of dialogue grammars</title>
      <link>http://arxiv.org/abs/1709.07858v1</link>
      <description>We investigate an end-to-end method for automatically inducing task-based dialogue systems from small amounts of unannotated dialogue data. It combines an incremental semantic grammar - Dynamic Syntax and Type Theory with Records (DS-TTR) - with Reinforcement Learning (RL), where language generation and dialogue management are a joint decision problem. The systems thus produced are incremental: dialogues are processed word-by-word, shown previously to be essential in supporting natural, spontaneous dialogue. We hypothesised that the rich linguistic knowledge within the grammar should enable a combinatorially large number of dialogue variations to be processed, even when trained on very few dialogues. Our experiments show that our model can process 74% of the Facebook AI bAbI dataset even when trained on only 0.13% of the data (5 dialogues). It can in addition process 65% of bAbI+, a corpus we created by systematically adding incremental dialogue phenomena such as restarts and self-corrections to bAbI. We compare our model with a state-of-the-art retrieval model, MemN2N. We find that, in terms of semantic accuracy, MemN2N shows very poor robustness to the bAbI+ transformations even when trained on the full bAbI dataset.\n\n\nTHE ANALYSIS OF THE EFFICIENCY OF GENERATIVE AI ALGORITHMS FOR CREATING A NATURAL DIALOGUE</description>
      <guid isPermaLink="false">http://arxiv.org/abs/1709.07858v1</guid>
      <dc:creator>Arash Eshghi, Igor Shalyminov, Oliver Lemon</dc:creator>
      <pubDate>Fri, 22 Sep 2017 17:24:33 GMT</pubDate>
    </item>
    <item>
      <title>AraGPT2: Pre-Trained Transformer for Arabic Language Generation</title>
      <link>http://arxiv.org/abs/2012.15520v2</link>
      <description>Recently, pre-trained transformer-based architectures have proven to be very efficient at language modeling and understanding, given that they are trained on a large enough corpus. Applications in language generation for Arabic are still lagging in comparison to other NLP advances primarily due to the lack of advanced Arabic language generation models. In this paper, we develop the first advanced Arabic language generation model, AraGPT2, trained from scratch on a large Arabic corpus of internet text and news articles. Our largest model, AraGPT2-mega, has 1.46 billion parameters, which makes it the largest Arabic language model available. The Mega model was evaluated and showed success on different tasks including synthetic news generation, and zero-shot question answering. For text generation, our best model achieves a perplexity of 29.8 on held-out Wikipedia articles. A study conducted with human evaluators showed the significant success of AraGPT2-mega in generating news articles that are difficult to distinguish from articles written by humans. We thus develop and release an automatic discriminator model with a 98% percent accuracy in detecting model-generated text. The models are also publicly available, hoping to encourage new research directions and applications for Arabic NLP.\n\n\nAI-Generated News Articles Based on Large Language Models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2012.15520v2</guid>
      <dc:creator>Wissam Antoun, Fady Baly, Hazem Hajj</dc:creator>
      <pubDate>Sun, 07 Mar 2021 13:11:53 GMT</pubDate>
    </item>
    <item>
      <title>Interactive Learning with Pricing for Optimal and Stable Allocations in Markets</title>
      <link>http://arxiv.org/abs/2212.06891v1</link>
      <description>Large-scale online recommendation systems must facilitate the allocation of a limited number of items among competing users while learning their preferences from user feedback. As a principled way of incorporating market constraints and user incentives in the design, we consider our objectives to be two-fold: maximal social welfare with minimal instability. To maximize social welfare, our proposed framework enhances the quality of recommendations by exploring allocations that optimistically maximize the rewards. To minimize instability, a measure of users' incentives to deviate from recommended allocations, the algorithm prices the items based on a scheme derived from the Walrasian equilibria. Though it is known that these equilibria yield stable prices for markets with known user preferences, our approach accounts for the inherent uncertainty in the preferences and further ensures that the users accept their recommendations under offered prices. To the best of our knowledge, our approach is the first to integrate techniques from combinatorial bandits, optimal resource allocation, and collaborative filtering to obtain an algorithm that achieves sub-linear social welfare regret as well as sub-linear instability. Empirical studies on synthetic and real-world data also demonstrate the efficacy of our strategy compared to approaches that do not fully incorporate all these aspects.\n\n\nLearning to maximize the social welfare from preference feedback</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2212.06891v1</guid>
      <dc:creator>Yigit Efe Erginbas, Soham Phade, Kannan Ramchandran</dc:creator>
      <pubDate>Tue, 13 Dec 2022 20:33:54 GMT</pubDate>
    </item>
    <item>
      <title>New Desiderata for Direct Preference Optimization</title>
      <link>http://arxiv.org/abs/2407.09072v1</link>
      <description>Large language models in the past have typically relied on some form of reinforcement learning with human feedback (RLHF) to better align model responses with human preferences. However, because of oft-observed instabilities when implementing these RLHF pipelines, various reparameterization techniques have recently been introduced to sidestep the need for separately learning an RL reward model. Instead, directly fine-tuning for human preferences is achieved via the minimization of a single closed-form training objective, a process originally referred to as direct preference optimization (DPO) and followed by several notable descendants. Although effective in certain real-world settings, we introduce new evaluation criteria that serve to highlight unresolved shortcomings in the ability of existing DPO methods to interpolate between a pre-trained reference model and empirical measures of human preferences, as well as unavoidable trade-offs in how low- and high-quality responses are regularized and constraints are handled. Our insights then motivate an alternative DPO-like loss that provably mitigates these limitations. Empirical results serve to corroborate notable aspects of our analyses.\n\n\nNew Desiderata for Direct Preference Optimization</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2407.09072v1</guid>
      <dc:creator>Xiangkun Hu, Tong He, David Wipf</dc:creator>
      <pubDate>Fri, 12 Jul 2024 07:52:32 GMT</pubDate>
    </item>
    <item>
      <title>Self-directed Machine Learning</title>
      <link>http://arxiv.org/abs/2201.01289v2</link>
      <description>Conventional machine learning (ML) relies heavily on manual design from machine learning experts to decide learning tasks, data, models, optimization algorithms, and evaluation metrics, which is labor-intensive, time-consuming, and cannot learn autonomously like humans. In education science, self-directed learning, where human learners select learning tasks and materials on their own without requiring hands-on guidance, has been shown to be more effective than passive teacher-guided learning. Inspired by the concept of self-directed human learning, we introduce the principal concept of Self-directed Machine Learning (SDML) and propose a framework for SDML. Specifically, we design SDML as a self-directed learning process guided by self-awareness, including internal awareness and external awareness. Our proposed SDML process benefits from self task selection, self data selection, self model selection, self optimization strategy selection and self evaluation metric selection through self-awareness without human guidance. Meanwhile, the learning performance of the SDML process serves as feedback to further improve self-awareness. We propose a mathematical formulation for SDML based on multi-level optimization. Furthermore, we present case studies together with potential applications of SDML, followed by discussing future research directions. We expect that SDML could enable machines to conduct human-like self-directed learning and provide a new perspective towards artificial general intelligence.\n\n\nMACHINE LEARNING UNDER HUMAN GUIDANCE</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2201.01289v2</guid>
      <dc:creator>Wenwu Zhu, Xin Wang, Pengtao Xie</dc:creator>
      <pubDate>Sat, 08 Jan 2022 04:55:40 GMT</pubDate>
    </item>
    <item>
      <title>Deepfake Network Architecture Attribution</title>
      <link>http://arxiv.org/abs/2202.13843v2</link>
      <description>With the rapid progress of generation technology, it has become necessary to attribute the origin of fake images. Existing works on fake image attribution perform multi-class classification on several Generative Adversarial Network (GAN) models and obtain high accuracies. While encouraging, these works are restricted to model-level attribution, only capable of handling images generated by seen models with a specific seed, loss and dataset, which is limited in real-world scenarios when fake images may be generated by privately trained models. This motivates us to ask whether it is possible to attribute fake images to the source models' architectures even if they are finetuned or retrained under different configurations. In this work, we present the first study on Deepfake Network Architecture Attribution to attribute fake images on architecture-level. Based on an observation that GAN architecture is likely to leave globally consistent fingerprints while traces left by model weights vary in different regions, we provide a simple yet effective solution named DNA-Det for this problem. Extensive experiments on multiple cross-test setups and a large-scale dataset demonstrate the effectiveness of DNA-Det.\n\n\nAttributing Mode Collapse in the Fine-Tuning of Large Language Models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2202.13843v2</guid>
      <dc:creator>Tianyun Yang, Ziyao Huang, Juan Cao, Lei Li, Xirong Li</dc:creator>
      <pubDate>Mon, 14 Mar 2022 11:24:41 GMT</pubDate>
    </item>
    <item>
      <title>Comparing Fair Ranking Metrics</title>
      <link>http://arxiv.org/abs/2009.01311v2</link>
      <description>Ranked lists are frequently used by information retrieval (IR) systems to present results believed to be relevant to the users information need. Fairness is a relatively new but important aspect of these rankings to measure, joining a rich set of metrics that go beyond traditional accuracy or utility constructs to provide a more holistic understanding of IR system behavior. In the last few years, several metrics have been proposed to quantify the (un)fairness of rankings, particularly with respect to particular group(s) of content providers, but comparative analyses of these metrics -- particularly for IR -- is lacking. There is limited guidance, therefore, to decide what fairness metrics are applicable to a specific scenario, or assessment of the extent to which metrics agree or disagree applied to real data. In this paper, we describe several fair ranking metrics from existing literature in a common notation, enabling direct comparison of their assumptions, goals, and design choices; we then empirically compare them on multiple data sets covering both search and recommendation tasks.\n\n\nComparing Few to Rank Many: Optimal Design for Learning Preferences</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2009.01311v2</guid>
      <dc:creator>Amifa Raj, Michael D. Ekstrand</dc:creator>
      <pubDate>Sat, 08 Jan 2022 21:38:37 GMT</pubDate>
    </item>
    <item>
      <title>Aligning language models with human preferences</title>
      <link>http://arxiv.org/abs/2404.12150v1</link>
      <description>Language models (LMs) trained on vast quantities of text data can acquire sophisticated skills such as generating summaries, answering questions or generating code. However, they also manifest behaviors that violate human preferences, e.g., they can generate offensive content, falsehoods or perpetuate social biases. In this thesis, I explore several approaches to aligning LMs with human preferences. First, I argue that aligning LMs can be seen as Bayesian inference: conditioning a prior (base, pretrained LM) on evidence about human preferences (Chapter 2). Conditioning on human preferences can be implemented in numerous ways. In Chapter 3, I investigate the relation between two approaches to finetuning pretrained LMs using feedback given by a scoring function: reinforcement learning from human feedback (RLHF) and distribution matching. I show that RLHF can be seen as a special case of distribution matching but distributional matching is strictly more general. In chapter 4, I show how to extend the distribution matching to conditional language models. Finally, in chapter 5 I explore a different root: conditioning an LM on human preferences already during pretraining. I show that involving human feedback from the very start tends to be more effective than using it only during supervised finetuning. Overall, these results highlight the room for alignment techniques different from and complementary to RLHF.\n\n\nUncertainty-aware Preference Alignment in Reinforcement Learning from Human Feedback</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2404.12150v1</guid>
      <dc:creator>Tomasz Korbak</dc:creator>
      <pubDate>Thu, 18 Apr 2024 12:55:18 GMT</pubDate>
    </item>
    <item>
      <title>My Body My Choice: Human-Centric Full-Body Anonymization</title>
      <link>http://arxiv.org/abs/2406.09553v1</link>
      <description>In an era of increasing privacy concerns for our online presence, we propose that the decision to appear in a piece of content should only belong to the owner of the body. Although some automatic approaches for full-body anonymization have been proposed, human-guided anonymization can adapt to various contexts, such as cultural norms, personal relations, esthetic concerns, and security issues. ''My Body My Choice'' (MBMC) enables physical and adversarial anonymization by removal and swapping approaches aimed for four tasks, designed by single or multi, ControlNet or GAN modules, combining several diffusion models. We evaluate anonymization on seven datasets; compare with SOTA inpainting and anonymization methods; evaluate by image, adversarial, and generative metrics; and conduct reidentification experiments.\n\n\nMy Climate Advisor: An Application of NLP in Climate Adaptation for Agriculture</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.09553v1</guid>
      <dc:creator>Umur Aybars Ciftci, Ali Kemal Tanriverdi, Ilke Demir</dc:creator>
      <pubDate>Thu, 13 Jun 2024 19:40:30 GMT</pubDate>
    </item>
    <item>
      <title>Proximal-Proximal-Gradient Method</title>
      <link>http://arxiv.org/abs/1708.06908v2</link>
      <description>In this paper, we present the proximal-proximal-gradient method (PPG), a novel optimization method that is simple to implement and simple to parallelize. PPG generalizes the proximal-gradient method and ADMM and is applicable to minimization problems written as a sum of many differentiable and many non-differentiable convex functions. The non-differentiable functions can be coupled. We furthermore present a related stochastic variation, which we call stochastic PPG (S-PPG). S-PPG can be interpreted as a generalization of Finito and MISO over to the sum of many coupled non-differentiable convex functions. We present many applications that can benefit from PPG and S-PPG and prove convergence for both methods. A key strength of PPG and S-PPG is, compared to existing methods, its ability to directly handle a large sum of non-differentiable non-separable functions with a constant stepsize independent of the number of functions. Such non-diminishing stepsizes allows them to be fast.\n\n\nProximal Preference Optimization for Diffusion Models</description>
      <guid isPermaLink="false">http://arxiv.org/abs/1708.06908v2</guid>
      <dc:creator>Ernest K. Ryu, Wotao Yin</dc:creator>
      <pubDate>Wed, 18 Oct 2017 06:21:17 GMT</pubDate>
    </item>
    <item>
      <title>Inverse Reinforcement Learning for Text Summarization</title>
      <link>http://arxiv.org/abs/2212.09917v2</link>
      <description>We introduce inverse reinforcement learning (IRL) as an effective paradigm for training abstractive summarization models, imitating human summarization behaviors. Our IRL model estimates the reward function using a suite of important sub-rewards for summarization and concurrently optimizes the policy network. Experimental results across datasets in different domains (CNN/DailyMail and WikiHow) and various model sizes (BART-base and BART-large) demonstrate the superiority of our proposed IRL model for summarization over MLE and RL baselines. The resulting summaries exhibit greater similarity to human-crafted gold references, outperforming MLE and RL baselines on metrics such as ROUGE, coverage, novelty, compression ratio, factuality, and human evaluations.\n\n\nTask-Aligned Reward Modeling for Reinforcement Learning in Text Summarization: A Survey</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2212.09917v2</guid>
      <dc:creator>Yu Fu, Deyi Xiong, Yue Dong</dc:creator>
      <pubDate>Tue, 05 Dec 2023 01:06:17 GMT</pubDate>
    </item>
    <item>
      <title>Interactive Multi-fidelity Learning for Cost-effective Adaptation of Language Model with Sparse Human Supervision</title>
      <link>http://arxiv.org/abs/2310.20153v1</link>
      <description>Large language models (LLMs) have demonstrated remarkable capabilities in various tasks. However, their suitability for domain-specific tasks, is limited due to their immense scale at deployment, susceptibility to misinformation, and more importantly, high data annotation costs. We propose a novel Interactive Multi-Fidelity Learning (IMFL) framework for the cost-effective development of small domain-specific LMs under limited annotation budgets. Our approach formulates the domain-specific fine-tuning process as a multi-fidelity learning problem, focusing on identifying the optimal acquisition strategy that balances between low-fidelity automatic LLM annotations and high-fidelity human annotations to maximize model performance. We further propose an exploration-exploitation query strategy that enhances annotation diversity and informativeness, incorporating two innovative designs: 1) prompt retrieval that selects in-context examples from human-annotated samples to improve LLM annotation, and 2) variable batch size that controls the order for choosing each fidelity to facilitate knowledge distillation, ultimately enhancing annotation quality. Extensive experiments on financial and medical tasks demonstrate that IMFL achieves superior performance compared with single fidelity annotations. Given a limited budget of human annotation, IMFL significantly outperforms the human annotation baselines in all four tasks and achieves very close performance as human annotations on two of the tasks. These promising results suggest that the high human annotation costs in domain-specific tasks can be significantly reduced by employing IMFL, which utilizes fewer human annotations, supplemented with cheaper and faster LLM (e.g., GPT-3.5) annotations to achieve comparable performance.\n\n\nModelling Variability in Human Annotator Simulation</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2310.20153v1</guid>
      <dc:creator>Jiaxin Zhang, Zhuohang Li, Kamalika Das, Sricharan Kumar</dc:creator>
      <pubDate>Tue, 31 Oct 2023 03:39:23 GMT</pubDate>
    </item>
    <item>
      <title>OAK: Ontology-Based Knowledge Map Model for Digital Agriculture</title>
      <link>http://arxiv.org/abs/2011.11442v1</link>
      <description>Nowadays, a huge amount of knowledge has been amassed in digital agriculture. This knowledge and know-how information are collected from various sources, hence the question is how to organise this knowledge so that it can be efficiently exploited. Although this knowledge about agriculture practices can be represented using ontology, rule-based expert systems, or knowledge model built from data mining processes, the scalability still remains an open issue. In this study, we propose a knowledge representation model, called an ontology-based knowledge map, which can collect knowledge from different sources, store it, and exploit either directly by stakeholders or as an input to the knowledge discovery process (Data Mining). The proposed model consists of two stages, 1) build an ontology as a knowledge base for a specific domain and data mining concepts, and 2) build the ontology-based knowledge map model for representing and storing the knowledge mined on the crop datasets. A framework of the proposed model has been implemented in agriculture domain. It is an efficient and scalable model, and it can be used as knowledge repository a digital agriculture.\n\n\nOAK: Enriching Document Representations using Auxiliary Knowledge for Extreme Classification</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2011.11442v1</guid>
      <dc:creator>Quoc Hung Ngo, Tahar Kechadi, Nhien-An Le-Khac</dc:creator>
      <pubDate>Fri, 20 Nov 2020 14:16:12 GMT</pubDate>
    </item>
    <item>
      <title>When Do Curricula Work?</title>
      <link>http://arxiv.org/abs/2012.03107v3</link>
      <description>Inspired by human learning, researchers have proposed ordering examples during training based on their difficulty. Both curriculum learning, exposing a network to easier examples early in training, and anti-curriculum learning, showing the most difficult examples first, have been suggested as improvements to the standard i.i.d. training. In this work, we set out to investigate the relative benefits of ordered learning. We first investigate the \emph{implicit curricula} resulting from architectural and optimization bias and find that samples are learned in a highly consistent order. Next, to quantify the benefit of \emph{explicit curricula}, we conduct extensive experiments over thousands of orderings spanning three kinds of learning: curriculum, anti-curriculum, and random-curriculum -- in which the size of the training dataset is dynamically increased over time, but the examples are randomly ordered. We find that for standard benchmark datasets, curricula have only marginal benefits, and that randomly ordered samples perform as well or better than curricula and anti-curricula, suggesting that any benefit is entirely due to the dynamic training set size. Inspired by common use cases of curriculum learning in practice, we investigate the role of limited training time budget and noisy data in the success of curriculum learning. Our experiments demonstrate that curriculum, but not anti-curriculum can indeed improve the performance either with limited training time budget or in existence of noisy data.\n\n\nWhen is RL better than DPO in RLHF? A Representation and Optimization Perspective</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2012.03107v3</guid>
      <dc:creator>Xiaoxia Wu, Ethan Dyer, Behnam Neyshabur</dc:creator>
      <pubDate>Tue, 09 Feb 2021 17:38:58 GMT</pubDate>
    </item>
    <item>
      <title>SoK: Access Control Policy Generation from High-level Natural Language Requirements</title>
      <link>http://arxiv.org/abs/2310.03292v1</link>
      <description>Administrator-centered access control failures can cause data breaches, putting organizations at risk of financial loss and reputation damage. Existing graphical policy configuration tools and automated policy generation frameworks attempt to help administrators configure and generate access control policies by avoiding such failures. However, graphical policy configuration tools are prone to human errors, making them unusable. On the other hand, automated policy generation frameworks are prone to erroneous predictions, making them unreliable. Therefore, to find ways to improve their usability and reliability, we conducted a Systematic Literature Review analyzing 49 publications, to identify those tools, frameworks, and their limitations. Identifying those limitations will help develop effective access control policy generation solutions while avoiding access control failures.\n\n\nVision:“AccessFormer”: Feedback-Driven Access Control Policy Generation Framework</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2310.03292v1</guid>
      <dc:creator>Sakuna Harinda Jayasundara, Nalin Asanka Gamagedara Arachchilage, Giovanni Russello</dc:creator>
      <pubDate>Thu, 05 Oct 2023 03:45:20 GMT</pubDate>
    </item>
    <item>
      <title>New Mechanism for Multiagent Extensible Negotiations</title>
      <link>http://arxiv.org/abs/1402.3986v1</link>
      <description>Multiagent negotiation mechanisms advise original solutions to several problems for which usual problem solving methods are inappropriate. Mainly negotiation models are based on agents' interactions through messages. Agents interact in order to reach an agreement for solving a specific problem. In this work, we study a new variant of negotiations, which has not yet been addressed in existing works. This negotiation form is denoted extensible negotiation. In contrast with current negotiation models, this form of negotiation allows the agents to dynamically extend the set of items under negotiation. This facility gives more acceptable solutions for the agents in their negotiation. The advantage of enlarging the negotiation space is to certainly offer more facilities for the agents for reaching new agreements which would not have been obtained using usual negotiation methods. This paper presents the protocol and the strategies used by the agents to deal with such negotiations.\n\n\nMulti-Agent Meeting Scheduling: A Negotiation Perspective</description>
      <guid isPermaLink="false">http://arxiv.org/abs/1402.3986v1</guid>
      <dc:creator>Samir Aknine</dc:creator>
      <pubDate>Mon, 17 Feb 2014 12:54:45 GMT</pubDate>
    </item>
    <item>
      <title>Multi-Perspective Consistency Enhances Confidence Estimation in Large Language Models</title>
      <link>http://arxiv.org/abs/2402.11279v1</link>
      <description>In the deployment of large language models (LLMs), accurate confidence estimation is critical for assessing the credibility of model predictions. However, existing methods often fail to overcome the issue of overconfidence on incorrect answers. In this work, we focus on improving the confidence estimation of large language models. Considering the fragility of self-awareness in language models, we introduce a Multi-Perspective Consistency (MPC) method. We leverage complementary insights from different perspectives within models (MPC-Internal) and across different models (MPC-Across) to mitigate the issue of overconfidence arising from a singular viewpoint. The experimental results on eight publicly available datasets show that our MPC achieves state-of-the-art performance. Further analyses indicate that MPC can mitigate the problem of overconfidence and is effectively scalable to other models.\n\n\nLarge Language Models: Assessment for Singularity</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2402.11279v1</guid>
      <dc:creator>Pei Wang, Yejie Wang, Muxi Diao, Keqing He, Guanting Dong, Weiran Xu</dc:creator>
      <pubDate>Sat, 17 Feb 2024 13:37:39 GMT</pubDate>
    </item>
    <item>
      <title>Sample-Efficient Learning of Stackelberg Equilibria in General-Sum Games</title>
      <link>http://arxiv.org/abs/2102.11494v3</link>
      <description>Real world applications such as economics and policy making often involve solving multi-agent games with two unique features: (1) The agents are inherently asymmetric and partitioned into leaders and followers; (2) The agents have different reward functions, thus the game is general-sum. The majority of existing results in this field focuses on either symmetric solution concepts (e.g. Nash equilibrium) or zero-sum games. It remains open how to learn the Stackelberg equilibrium -- an asymmetric analog of the Nash equilibrium -- in general-sum games efficiently from noisy samples.   This paper initiates the theoretical study of sample-efficient learning of the Stackelberg equilibrium, in the bandit feedback setting where we only observe noisy samples of the reward. We consider three representative two-player general-sum games: bandit games, bandit-reinforcement learning (bandit-RL) games, and linear bandit games. In all these games, we identify a fundamental gap between the exact value of the Stackelberg equilibrium and its estimated version using finitely many noisy samples, which can not be closed information-theoretically regardless of the algorithm. We then establish sharp positive results on sample-efficient learning of Stackelberg equilibrium with value optimal up to the gap identified above, with matching lower bounds in the dependency on the gap, error tolerance, and the size of the action spaces. Overall, our results unveil unique challenges in learning Stackelberg equilibria under noisy bandit feedback, which we hope could shed light on future research on this topic.\n\n\nSTA-RLHF: Stackelberg Aligned Reinforcement Learning with Human Feedback</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2102.11494v3</guid>
      <dc:creator>Yu Bai, Chi Jin, Huan Wang, Caiming Xiong</dc:creator>
      <pubDate>Wed, 03 Nov 2021 17:44:58 GMT</pubDate>
    </item>
    <item>
      <title>How Does Batch Normalization Help Optimization?</title>
      <link>http://arxiv.org/abs/1805.11604v5</link>
      <description>Batch Normalization (BatchNorm) is a widely adopted technique that enables faster and more stable training of deep neural networks (DNNs). Despite its pervasiveness, the exact reasons for BatchNorm's effectiveness are still poorly understood. The popular belief is that this effectiveness stems from controlling the change of the layers' input distributions during training to reduce the so-called &quot;internal covariate shift&quot;. In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of BatchNorm. Instead, we uncover a more fundamental impact of BatchNorm on the training process: it makes the optimization landscape significantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training.\n\n\nHow Does RLHF Shift Behavior Distributions? Distinguishability and Steerability</description>
      <guid isPermaLink="false">http://arxiv.org/abs/1805.11604v5</guid>
      <dc:creator>Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, Aleksander Madry</dc:creator>
      <pubDate>Mon, 15 Apr 2019 02:34:55 GMT</pubDate>
    </item>
    <item>
      <title>Stacky Buildings</title>
      <link>http://arxiv.org/abs/1710.06968v2</link>
      <description>We introduce structures which model quotients of buildings by type-preserving group actions. These structures, which we call W-groupoids for W a Coxeter group, generalize Bruhat decompositions, chambers systems of type M, Tits amalgams, and buildings themselves. We define the fundamental group of a W-groupoid, and characterize buildings as connected simply connected W-groupoids. We give a brief outline of covering theory of W-groupoids, which produces buildings as universal covers equipped with an action of the fundamental group. The local-to-global theorem of Tits concerning spherical 3-resides allows for the construction of W-groupoids by amalgamating quotients of generalized polygons along groupoids. In this way, W-groupoids provide a powerful way to construct (lattices in) exotic, hyperbolic, and wild buildings.\n\n\nBuilding a Specialized ChatGpt-Like System</description>
      <guid isPermaLink="false">http://arxiv.org/abs/1710.06968v2</guid>
      <dc:creator>William Norledge</dc:creator>
      <pubDate>Tue, 13 Oct 2020 10:29:13 GMT</pubDate>
    </item>
    <item>
      <title>Relative rationality: Is machine rationality subjective?</title>
      <link>http://arxiv.org/abs/1902.04832v1</link>
      <description>Rational decision making in its linguistic description means making logical decisions. In essence, a rational agent optimally processes all relevant information to achieve its goal. Rationality has two elements and these are the use of relevant information and the efficient processing of such information. In reality, relevant information is incomplete, imperfect and the processing engine, which is a brain for humans, is suboptimal. Humans are risk averse rather than utility maximizers. In the real world, problems are predominantly non-convex and this makes the idea of rational decision-making fundamentally unachievable and Herbert Simon called this bounded rationality. There is a trade-off between the amount of information used for decision-making and the complexity of the decision model used. This explores whether machine rationality is subjective and concludes that indeed it is.\n\n\nRelatively Rational: Learning Utilities and Rationalities Jointly from Pairwise Preferences</description>
      <guid isPermaLink="false">http://arxiv.org/abs/1902.04832v1</guid>
      <dc:creator>Tshilidzi Marwala</dc:creator>
      <pubDate>Wed, 13 Feb 2019 10:08:12 GMT</pubDate>
    </item>
    <item>
      <title>Deep Extrapolation for Attribute-Enhanced Generation</title>
      <link>http://arxiv.org/abs/2107.02968v2</link>
      <description>Attribute extrapolation in sample generation is challenging for deep neural networks operating beyond the training distribution. We formulate a new task for extrapolation in sequence generation, focusing on natural language and proteins, and propose GENhance, a generative framework that enhances attributes through a learned latent space. Trained on movie reviews and a computed protein stability dataset, GENhance can generate strongly-positive text reviews and highly stable protein sequences without being exposed to similar data during training. We release our benchmark tasks and models to contribute to the study of generative modeling extrapolation and data-driven design in biology and chemistry.\n\n\nExtrapolative Protein Design through Triplet-based Preference Learning</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2107.02968v2</guid>
      <dc:creator>Alvin Chan, Ali Madani, Ben Krause, Nikhil Naik</dc:creator>
      <pubDate>Mon, 25 Oct 2021 23:03:20 GMT</pubDate>
    </item>
    <item>
      <title>Mission: Impossible Language Models</title>
      <link>http://arxiv.org/abs/2401.06416v1</link>
      <description>Chomsky and others have very directly claimed that large language models (LLMs) are equally capable of learning languages that are possible and impossible for humans to learn. However, there is very little published experimental evidence to support such a claim. Here, we develop a set of synthetic impossible languages of differing complexity, each designed by systematically altering English data with unnatural word orders and grammar rules. These languages lie on an impossibility continuum: at one end are languages that are inherently impossible, such as random and irreversible shuffles of English words, and on the other, languages that may not be intuitively impossible but are often considered so in linguistics, particularly those with rules based on counting word positions. We report on a wide range of evaluations to assess the capacity of GPT-2 small models to learn these uncontroversially impossible languages, and crucially, we perform these assessments at various stages throughout training to compare the learning process for each language. Our core finding is that GPT-2 struggles to learn impossible languages when compared to English as a control, challenging the core claim. More importantly, we hope our approach opens up a productive line of inquiry in which different LLM architectures are tested on a variety of impossible languages in an effort to learn more about how LLMs can be used as tools for these cognitive and typological investigations.\n\n\nMission Impossible: A Statistical Perspective on Jailbreaking LLMs</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2401.06416v1</guid>
      <dc:creator>Julie Kallini, Isabel Papadimitriou, Richard Futrell, Kyle Mahowald, Christopher Potts</dc:creator>
      <pubDate>Fri, 12 Jan 2024 07:24:26 GMT</pubDate>
    </item>
    <item>
      <title>Variational Intrinsic Control Revisited</title>
      <link>http://arxiv.org/abs/2010.03281v2</link>
      <description>In this paper, we revisit variational intrinsic control (VIC), an unsupervised reinforcement learning method for finding the largest set of intrinsic options available to an agent. In the original work by Gregor et al. (2016), two VIC algorithms were proposed: one that represents the options explicitly, and the other that does it implicitly. We show that the intrinsic reward used in the latter is subject to bias in stochastic environments, causing convergence to suboptimal solutions. To correct this behavior and achieve the maximal empowerment, we propose two methods respectively based on the transitional probability model and Gaussian mixture model. We substantiate our claims through rigorous mathematical derivations and experimental analyses.\n\n\nRevisiting Successor Features for Inverse Reinforcement Learning</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2010.03281v2</guid>
      <dc:creator>Taehwan Kwon</dc:creator>
      <pubDate>Wed, 17 Mar 2021 14:49:17 GMT</pubDate>
    </item>
    <item>
      <title>Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI</title>
      <link>http://arxiv.org/abs/2406.11563v1</link>
      <description>This perspective piece calls for the study of the new field of Intersymbolic AI, by which we mean the combination of symbolic AI, whose building blocks have inherent significance/meaning, with subsymbolic AI, whose entirety creates significance/effect despite the fact that individual building blocks escape meaning. Canonical kinds of symbolic AI are logic, games and planning. Canonical kinds of subsymbolic AI are (un)supervised machine and reinforcement learning. Intersymbolic AI interlinks the worlds of symbolic AI with its compositional symbolic significance and meaning and of subsymbolic AI with its summative significance or effect to enable culminations of insights from both worlds by going between and across symbolic AI insights with subsymbolic AI techniques that are being helped by symbolic AI principles. For example, Intersymbolic AI may start with symbolic AI to understand a dynamic system, continue with subsymbolic AI to learn its control, and end with symbolic AI to safely use the outcome of the learned subsymbolic AI controller in the dynamic system. Intersymbolic AI combines both symbolic and subsymbolic AI to increase the effectiveness of AI compared to either kind of AI alone, in much the same way that the combination of both conscious and subconscious thought increases the effectiveness of human thought compared to either kind of thought alone. Some successful contributions to the Intersymbolic AI paradigm are surveyed here but many more are considered possible by advancing Intersymbolic AI.\n\n\nRSI-LLM: Humans create a world for AI</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2406.11563v1</guid>
      <dc:creator>André Platzer</dc:creator>
      <pubDate>Mon, 17 Jun 2024 14:01:59 GMT</pubDate>
    </item>
    <item>
      <title>Enhancing Task-Oriented Dialogues with Chitchat: a Comparative Study Based on Lexical Diversity and Divergence</title>
      <link>http://arxiv.org/abs/2311.14067v2</link>
      <description>As a recent development, task-oriented dialogues (TODs) have been enriched with chitchat in an effort to make dialogues more diverse and engaging. This enhancement is particularly valuable as TODs are often confined to narrow domains, making the mitigation of repetitive and predictable responses a significant challenge. This paper presents a comparative analysis of three chitchat enhancements, aiming to identify the most effective approach in terms of diversity. Additionally, we quantify the divergence between the added chitchat, the original task-oriented language, and chitchat typically found in chitchat datasets, highlighting the top 20 divergent keywords for each comparison. Our findings drive a discussion on future enhancements for augmenting TODs, emphasizing the importance of grounding dialogues beyond the task to achieve more diverse and natural exchanges.\n\n\nA Chit-Chat between Llama2 and Chatgpt for the Automated Creation of Exploits</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2311.14067v2</guid>
      <dc:creator>Armand Stricker, Patrick Paroubek</dc:creator>
      <pubDate>Wed, 24 Jan 2024 20:56:59 GMT</pubDate>
    </item>
    <item>
      <title>Toward Solving 2-TBSG Efficiently</title>
      <link>http://arxiv.org/abs/1906.03553v1</link>
      <description>2-TBSG is a two-player game model which aims to find Nash equilibriums and is widely utilized in reinforced learning and AI. Inspired by the fact that the simplex method for solving the deterministic discounted Markov decision processes (MDPs) is strongly polynomial independent of the discounted factor, we are trying to answer an open problem whether there is a similar algorithm for 2-TBSG. We develop a simplex strategy iteration where one player updates its strategy with a simplex step while the other player finds an optimal counterstrategy in turn, and a modified simplex strategy iteration. Both of them belong to a class of geometrically converging algorithms. We establish the strongly polynomial property of these algorithms by considering a strategy combined from the current strategy and the equilibrium strategy. Moreover, we present a method to transform general 2-TBSGs into special 2-TBSGs where each state has exactly two actions.\n\n\nEfficient Offline Preference-Based Reinforcement Learning with Transition-Dependent Discounting</description>
      <guid isPermaLink="false">http://arxiv.org/abs/1906.03553v1</guid>
      <dc:creator>Zeyu Jia, Zaiwen Wen, Yinyu Ye</dc:creator>
      <pubDate>Sun, 09 Jun 2019 03:01:43 GMT</pubDate>
    </item>
    <item>
      <title>ROS Navigation Tuning Guide</title>
      <link>http://arxiv.org/abs/1706.09068v2</link>
      <description>The ROS navigation stack is powerful for mobile robots to move from place to place reliably. The job of navigation stack is to produce a safe path for the robot to execute, by processing data from odometry, sensors and environment map. Maximizing the performance of this navigation stack requires some fine tuning of parameters, and this is not as simple as it looks. One who is sophomoric about the concepts and reasoning may try things randomly, and wastes a lot of time.   This article intends to guide the reader through the process of fine tuning navigation parameters. It is the reference when someone need to know the &quot;how&quot; and &quot;why&quot; when setting the value of key parameters. This guide assumes that the reader has already set up the navigation stack and ready to optimize it. This is also a summary of my work with the ROS navigation stack.\n\n\nNavigating the Challenges of Fine Tuning and Catastrophic Forgetting Published on Mar 25, 2024</description>
      <guid isPermaLink="false">http://arxiv.org/abs/1706.09068v2</guid>
      <dc:creator>Kaiyu Zheng</dc:creator>
      <pubDate>Mon, 08 Apr 2019 23:53:12 GMT</pubDate>
    </item>
    <item>
      <title>Electronic structure, phase stability and chemical bonding in Th$_2$Al and Th$_2$AlH$_4$</title>
      <link>http://arxiv.org/abs/cond-mat/0108278v1</link>
      <description>We present the results of theoretical investigation on the electronic structure, bonding nature and ground state properties of Th$_2$Al and Th$_2$AlH$_4$ using generalized-gradient-corrected first-principles full-potential density-functional calculations. Th$_2$AlH$_4$ has been reported to violate the &quot;2 \AA rule&quot; of H-H separation in hydrides. From our total energy as well as force-minimization calculations, we found a shortest H-H separation of 1.95 {\AA} in accordance with recent high resolution powder neutron diffraction experiments. When the Th$_2$Al matrix is hydrogenated, the volume expansion is highly anisotropic, which is quite opposite to other hydrides having the same crystal structure. The bonding nature of these materials are analyzed from the density of states, crystal-orbital Hamiltonian population and valence-charge-density analyses. Our calculation predicts different nature of bonding for the H atoms along $a$ and $c$. The strongest bonding in Th$_2$AlH$_4$ is between Th and H along $c$ which form dumb-bell shaped H-Th-H subunits. Due to this strong covalent interaction there is very small amount of electrons present between H atoms along $c$ which makes repulsive interaction between the H atoms smaller and this is the precise reason why the 2 {\AA} rule is violated. The large difference in the interatomic distances between the interstitial region where one can accommodate H in the $ac$ and $ab$ planes along with the strong covalent interaction between Th and H are the main reasons for highly anisotropic volume expansion on hydrogenation of Th$_2$Al.\n\n\n0th Berlin Symposium on Artificial Teacher Avatars:: Workshop report</description>
      <guid isPermaLink="false">http://arxiv.org/abs/cond-mat/0108278v1</guid>
      <dc:creator>P. Vajeeston, R. Vidya, P. Ravindran, H. Fjellvåg, A. Kjekshus, A. Skjeltorp</dc:creator>
      <pubDate>Fri, 17 Aug 2001 11:14:29 GMT</pubDate>
    </item>
    <item>
      <title>Is RLHF More Difficult than Standard RL?</title>
      <link>http://arxiv.org/abs/2306.14111v2</link>
      <description>Reinforcement learning from Human Feedback (RLHF) learns from preference signals, while standard Reinforcement Learning (RL) directly learns from reward signals. Preferences arguably contain less information than rewards, which makes preference-based RL seemingly more difficult. This paper theoretically proves that, for a wide range of preference models, we can solve preference-based RL directly using existing algorithms and techniques for reward-based RL, with small or no extra costs. Specifically, (1) for preferences that are drawn from reward-based probabilistic models, we reduce the problem to robust reward-based RL that can tolerate small errors in rewards; (2) for general arbitrary preferences where the objective is to find the von Neumann winner, we reduce the problem to multiagent reward-based RL which finds Nash equilibria for factored Markov games with a restricted set of policies. The latter case can be further reduced to adversarial MDP when preferences only depend on the final state. We instantiate all reward-based RL subroutines by concrete provable algorithms, and apply our theory to a large class of models including tabular MDPs and MDPs with generic function approximation. We further provide guarantees when K-wise comparisons are available.\n\n\nRLHF for Grammar Error Correction</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2306.14111v2</guid>
      <dc:creator>Yuanhao Wang, Qinghua Liu, Chi Jin</dc:creator>
      <pubDate>Fri, 03 Nov 2023 18:36:03 GMT</pubDate>
    </item>
    <item>
      <title>Interdisciplinary Research Methodologies in Engineering Education Research</title>
      <link>http://arxiv.org/abs/2104.04062v2</link>
      <description>As Engineering Education Research (EER) develops as a discipline it is necessary for EER scholars to contribute to the development of learning theory rather than simply being informed by it. It has been suggested that to do this effectively will require partnerships between Engineering scholars and psychologists, education researchers, including other social scientists. The formation of such partnerships is particularly important when considering the introduction of business-related skills into engineering curriculum designed to prepare 21st Century Engineering Students for workplace challenges. In order to encourage scholars beyond Engineering to engage with EER, it is necessary to provide an introduction to the complexities of EER. With this aim in mind, this paper provides an outline review of what is considered rigorous research from an EER perspective as well as highlighting some of the core methodological traditions of EER. The paper aims to facilitate further discussion between EER scholars and researchers from other disciplines, ultimately leading to future collaboration on innovative and rigorous EER.\n\n\nResearch Preparation Criterion</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2104.04062v2</guid>
      <dc:creator>David Reynolds, Nicholas Dacre</dc:creator>
      <pubDate>Sun, 18 Apr 2021 14:52:49 GMT</pubDate>
    </item>
    <item>
      <title>Predicting Video with VQVAE</title>
      <link>http://arxiv.org/abs/2103.01950v1</link>
      <description>In recent years, the task of video prediction-forecasting future video given past video frames-has attracted attention in the research community. In this paper we propose a novel approach to this problem with Vector Quantized Variational AutoEncoders (VQ-VAE). With VQ-VAE we compress high-resolution videos into a hierarchical set of multi-scale discrete latent variables. Compared to pixels, this compressed latent space has dramatically reduced dimensionality, allowing us to apply scalable autoregressive generative models to predict video. In contrast to previous work that has largely emphasized highly constrained datasets, we focus on very diverse, large-scale datasets such as Kinetics-600. We predict video at a higher resolution on unconstrained videos, 256x256, than any other previous method to our knowledge. We further validate our approach against prior work via a crowdsourced human evaluation.\n\n\nVQ-VAE に基づく解釈可能なアクセント潜在変数を用いた多方言音声合成</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2103.01950v1</guid>
      <dc:creator>Jacob Walker, Ali Razavi, Aäron van den Oord</dc:creator>
      <pubDate>Tue, 02 Mar 2021 18:59:10 GMT</pubDate>
    </item>
    <item>
      <title>Green AI</title>
      <link>http://arxiv.org/abs/1907.10597v3</link>
      <description>The computations required for deep learning research have been doubling every few months, resulting in an estimated 300,000x increase from 2012 to 2018 [2]. These computations have a surprisingly large carbon footprint [38]. Ironically, deep learning was inspired by the human brain, which is remarkably energy efficient. Moreover, the financial cost of the computations can make it difficult for academics, students, and researchers, in particular those from emerging economies, to engage in deep learning research.   This position paper advocates a practical solution by making efficiency an evaluation criterion for research alongside accuracy and related measures. In addition, we propose reporting the financial cost or &quot;price tag&quot; of developing, training, and running models to provide baselines for the investigation of increasingly efficient methods. Our goal is to make AI both greener and more inclusive---enabling any inspired undergraduate with a laptop to write high-quality research papers. Green AI is an emerging focus at the Allen Institute for AI.\n\n\nAI アラインメント: 包括的サーベイ</description>
      <guid isPermaLink="false">http://arxiv.org/abs/1907.10597v3</guid>
      <dc:creator>Roy Schwartz, Jesse Dodge, Noah A. Smith, Oren Etzioni</dc:creator>
      <pubDate>Tue, 13 Aug 2019 20:09:57 GMT</pubDate>
    </item>
    <item>
      <title>Codebook LLMs: Adapting Political Science Codebooks for LLM Use and Adapting LLMs to Follow Codebooks</title>
      <link>http://arxiv.org/abs/2407.10747v1</link>
      <description>Codebooks -- documents that operationalize constructs and outline annotation procedures -- are used almost universally by social scientists when coding unstructured political texts. Recently, to reduce manual annotation costs, political scientists have looked to generative large language models (LLMs) to label and analyze text data. However, previous work using LLMs for classification has implicitly relied on the universal label assumption -- correct classification of documents is possible using only a class label or minimal definition and the information that the LLM inductively learns during its pre-training. In contrast, we argue that political scientists who care about valid measurement should instead make a codebook-construct label assumption -- an LLM should follow the definition and exclusion criteria of a construct/label provided in a codebook. In this work, we collect and curate three political science datasets and their original codebooks and conduct a set of experiments to understand whether LLMs comply with codebook instructions, whether rewriting codebooks improves performance, and whether instruction-tuning LLMs on codebook-document-label tuples improves performance over zero-shot classification. Using Mistral 7B Instruct as our LLM, we find re-structuring the original codebooks gives modest gains in zero-shot performance but the model still struggles to comply with the constraints of the codebooks. Optimistically, instruction-tuning Mistral on one of our datasets gives significant gains over zero-shot inference (0.76 versus 0.53 micro F1). We hope our conceptualization of the codebook-specific task, assumptions, and instruction-tuning pipeline as well our semi-structured LLM codebook format will help political scientists readily adapt to the LLM era.\n\n\n効果の高い広告文生成のためのLLM のInstruction Tuning と関連する広告属性の分析</description>
      <guid isPermaLink="false">http://arxiv.org/abs/2407.10747v1</guid>
      <dc:creator>Andrew Halterman, Katherine A. Keith</dc:creator>
      <pubDate>Mon, 15 Jul 2024 14:20:09 GMT</pubDate>
    </item>
  </channel>
</rss>
